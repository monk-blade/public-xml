<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><atom:link href="http://192.168.1.132/?platform=reddit&amp;subreddit=programming&amp;averagePostsPerDay=5&amp;content&amp;view=rss" rel="self" type="application/rss+xml"/><title>/r/programming</title><description>Hot posts in /r/programming (roughly 5 posts per day)</description><link>https://www.reddit.com/r/programming/</link><language>en-us</language><lastBuildDate>Mon, 15 Sep 2025 23:52:07 +0000</lastBuildDate><generator>Upvote RSS</generator><image><url>https://styles.redditmedia.com/t5_2fwo/styles/communityIcon_1bqa1ibfp8q11.png</url><title>/r/programming</title><link>https://www.reddit.com/r/programming/</link></image><item><link>https://bogdanthegeek.github.io/blog/projects/vapeserver/</link><title>Hosting a website on a disposable vape (bogdanthegeek.github.io)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhs5ti/hosting_a_website_on_a_disposable_vape/</guid><comments>https://www.reddit.com/r/programming/comments/1nhs5ti/hosting_a_website_on_a_disposable_vape/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 8 min | <a href='https://www.reddit.com/r/programming/comments/1nhs5ti/hosting_a_website_on_a_disposable_vape/'>Post permalink</a></p></section><section class='preview-image'><img src='https://bogdanthegeek.github.io/blog/images/vapeserver.jpg' /></section><section class='parsed-content'><div><h2>Preface<a href="https://bogdanthegeek.github.io#preface">#</a></h2><p>This article is <em>NOT</em> served from a web server running on a disposable vape. If you want to see the real deal, click <a href="http://ewaste.fka.wtf">here</a>. The content is otherwise identical.</p><h2>Background<a href="https://bogdanthegeek.github.io#background">#</a></h2><p>For a couple of years now, I have been collecting disposable vapes from friends and family. Initially, I only salvaged the batteries for &ldquo;future&rdquo; projects (It&rsquo;s not hoarding, I promise), but recently, disposable vapes have gotten more advanced. I wouldn&rsquo;t want to be the lawyer who one day will have to argue how a device with USB C and a rechargeable battery can be classified as &ldquo;disposable&rdquo;. Thankfully, I don&rsquo;t plan on pursuing law anytime soon.</p><p>Last year, I was tearing apart some of these fancier pacifiers for adults when I noticed something that caught my eye, instead of the expected black blob of goo hiding some ASIC (Application Specific Integrated Circuit) I see a little integrated circuit inscribed &ldquo;PUYA&rdquo;. I don&rsquo;t blame you if this name doesn&rsquo;t excite you as much it does me, most people have never heard of them. They are most well known for their flash chips, but I first came across them after reading Jay Carlson&rsquo;s blog post about <a href="https://jaycarlson.net/2023/02/04/the-cheapest-flash-microcontroller-you-can-buy-is-actually-an-arm-cortex-m0/">the cheapest flash microcontroller you can buy</a>. They are quite capable little ARM Cortex-M0+ micros.</p><p>Over the past year I have collected quite a few of these PY32 based vapes, all of them from different models of vape from the same manufacturer. It&rsquo;s not my place to do free advertising for big tobacco, so I won&rsquo;t mention the brand I got it from, but if anyone who worked on designing them reads this, thanks for labeling the debug pins!</p><h2>What are we working with<a href="https://bogdanthegeek.github.io#what-are-we-working-with">#</a></h2><p>The chip is marked <code>PUYA C642F15</code>, which wasn&rsquo;t very helpful. I was pretty sure it was a <code>PY32F002A</code>, but after poking around with <a href="http://pyocd.io/">pyOCD</a>, I noticed that the flash was 24k and we have 3k of RAM. The extra flash meant that it was more likely a <code>PY32F002B</code>, which is actually a very different chip.<sup><a href="https://bogdanthegeek.github.io#fn:1">1</a></sup></p><p>So here are the specs of a microcontroller so <em>bad</em>, it&rsquo;s basically disposable:</p><ul><li>24MHz Coretex M0+</li><li>24KiB of Flash Storage</li><li>3KiB of Static RAM</li><li>a few peripherals, none of which we will use.</li></ul><p>You may look at those specs and think that it&rsquo;s not much to work with. I don&rsquo;t blame you, a 10y old phone can barely load google, and this is about 100x slower. I on the other hand see a <em>blazingly</em> fast web server.</p><h2>Getting online<a href="https://bogdanthegeek.github.io#getting-online">#</a></h2><p>The idea of hosting a web server on a vape didn&rsquo;t come to me instantly. In fact, I have been playing around with them for a while, but after writing my post on <a href="https://bogdanthegeek.github.io/blog/insights/jlink-rtt-for-the-masses/">semihosting</a>, the penny dropped.</p><p>If you don&rsquo;t feel like reading that article, semihosting is basically syscalls for embedded ARM microcontrollers. You throw some values/pointers into some registers and call a breakpoint instruction. An attached debugger interprets the values in the registers and performs certain actions. Most people just use this to get some logs printed from the microcontroller, but they are actually bi-directional.</p><p>If you are older than me, you might remember a time before Wi-Fi and Ethernet, the dark ages, when you had to use dial-up modems to get online. You might also know that the ghosts of those modems still linger all around us. Almost all USB serial devices actually emulate those modems: a 56k modem is just 57600 baud serial device. Data between some of these modems was transmitted using a protocol called SLIP (Serial Line Internet Protocol).<sup><a href="https://bogdanthegeek.github.io#fn:2">2</a></sup></p><p>This may not come as a surprise, but Linux (and with some tweaking even macOS) supports SLIP. The <code>slattach</code> utility can make any <code>/dev/tty*</code> send and receive IP packets. All we have to do is put the data down the wire in the right format and provide a virtual tty. This is actually easier than you might imagine, pyOCD can forward all semihosting though a telnet port. Then, we use <code>socat</code> to link that port to a virtual tty:</p><div><pre><code><span><span>pyocd gdb -S -O semihost_console_type<span>=</span>telnet -T <span>$(</span>PORT<span>)</span> <span>$(</span>PYOCDFLAGS<span>)</span> &amp; </span></span><span><span>socat PTY,link<span>=</span><span>$(</span>TTY<span>)</span>,raw,echo<span>=</span> TCP:localhost:<span>$(</span>PORT<span>)</span>,nodelay &amp; </span></span><span><span>sudo slattach -L -p slip -s <span>115200</span> <span>$(</span>TTY<span>)</span> &amp; </span></span><span><span>sudo ip addr add 192.168.190.1 peer 192.168.190.2/24 dev sl0 </span></span><span><span>sudo ip link set mtu <span>1500</span> up dev sl0 </span></span></code></pre></div><p>Ok, so we have a &ldquo;modem&rdquo;, but that&rsquo;s hardly a web server. To actually talk TCP/IP, we need an IP stack. There are many choices, but I went with <a href="https://github.com/adamdunkels/uip/tree/uip-0-9">uIP</a> because it&rsquo;s pretty small, doesn&rsquo;t require an RTOS, and it&rsquo;s easy to port to other platforms. It also, helpfully, comes with a very minimal HTTP server example.</p><p>After porting the SLIP code to use semihosting, I had a working web server&hellip;half of the time. As with most highly optimised libraries, uIP was designed for 8 and 16-bit machines, which rarely have memory alignment requirements. On ARM however, if you dereference a <code>u16 *</code>, you better hope that address is even, or you&rsquo;ll get an exception. The <code>uip_chksum</code> assumed <code>u16</code> alignment, but the script that creates the filesystem didn&rsquo;t. I actually decided to modify a bit the structure of the filesystem to make it a bit more portable. This was my first time working with <code>perl</code> and I have to say, it&rsquo;s quite well suited to this kind of task.</p><h2>Blazingly fast<a href="https://bogdanthegeek.github.io#blazingly-fast">#</a></h2><p>So how fast is a web server running on a disposable microcontroller. Well, initially, not very fast. Pings took ~1.5s with 50% packet loss and a simple page took over 20s to load. That&rsquo;s so bad, it&rsquo;s actually funny, and I kind of wanted to leave it there.</p><p>However, the problem was actually between the seat and the steering wheel the whole time. The first implementation read and wrote a single character at a time, which had a massive overhead associated with it. I previously benchmarked semihosting on this device, and I was getting ~20KiB/s, but uIP&rsquo;s SLIP implementation was designed for very low memory devices, so it was serialising the data byte by byte. We have a whopping 3kiB of RAM to play with, so I added a ring buffer to cache reads from the host and feed them into the SLIP poll function. I also split writes in batches to allow for escaping.</p><p>Now this is what I call blazingly fast! Pings now take 20ms, no packet loss and a full page loads in about 160ms. This was using using almost all of the RAM, but I could also dial down the sizes of the buffer to have more than enough headroom to run other tasks. The project repo has everything set to a nice balance latency and RAM usage:</p><pre><code>Memory region Used Size Region Size %age Used FLASH: 5116 B 24 KB 20.82% RAM: 1380 B 3 KB 44.92% </code></pre><p>For this blog however, I paid for none of the RAM, so I&rsquo;ll use all of the RAM.</p><p>As you may have noticed, we have just under 20kiB (80%) of storage space. That may not be enough to ship all of React, but as you can see, it&rsquo;s more than enough to host this entire blog post. And this is not just a static page server, you can run any server-side code you want, if you know C that is.</p><p>Just for fun, I added a json api endpoint to get the number of requests to the main page (since the last crash) and the unique ID of the microcontroller.</p><h2>Resources<a href="https://bogdanthegeek.github.io#resources">#</a></h2><ul><li><a href="https://github.com/BogdanTheGeek/semihost-ip">Code for this project</a></li></ul></div></section>]]></description><pubDate>Mon, 15 Sep 2025 22:55:15 +0530</pubDate></item><item><link>https://in.relation.to/2025/01/24/jdbc-fetch-size/</link><title>Why you should care about the JDBC fetch size (in.relation.to)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhjby7/why_you_should_care_about_the_jdbc_fetch_size/</guid><comments>https://www.reddit.com/r/programming/comments/1nhjby7/why_you_should_care_about_the_jdbc_fetch_size/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 7 min | <a href='https://www.reddit.com/r/programming/comments/1nhjby7/why_you_should_care_about_the_jdbc_fetch_size/'>Post permalink</a></p></section><section class='preview-image'><img src='https://www.gravatar.com/avatar/77987db6d7dd75abd4a14651641b3d46?s=240' /></section><section class='parsed-content'><div><p>Last week, Jeroen Borgers asked on Twitter for a standard way to set the JDBC fetch size in JPA, that is, for Hibernate&rsquo;s <code>Query.setFetchSize()</code> to be added to the standard APIs. This took me slightly by surprise, because nobody has ever asked for that before, but I asked him to go ahead and <a href="https://github.com/jakartaee/persistence/issues/696">open an issue</a>. After some discussion, I think I&rsquo;m satisfied that his actual needs can be met in a different way, but the discussion did help to draw my attention to something important: <strong>the default JDBC fetch size for the Oracle driver is 10.</strong></p><p>Now, I would never pretend to be an expert in Oracle performance tuning, and I don&rsquo;t use Oracle every day. Even so, I felt like this is something that I definitely <em>should</em> have known off the top of my head, after so many years working with JDBC.</p><p>Out of curiosity, I ran a <a href="https://x.com/1ovthafew/status/1880004837515169969">poll</a> on Twitter, which was shared by Franck Pachot among others:</p><p>Now, look, <em>N</em>=121 is an okay sample size, but of course this was not a representative sample. So how would we expect this sample to skew compared to a typical random sample of developers? Well, I would like to think that my followers know quite a lot more about databases than most, and I&rsquo;m even more confident in saying this about Franck&rsquo;s followers.</p><p>But there&rsquo;s another problem: I&rsquo;m really interested in the responses of Oracle users, and I don&rsquo;t know how many of the people who clicked "What&rsquo;s a JDBC fetch size?" actually clicked it because I didn&rsquo;t have room to add a "Just show me the results" option. In a lame attempt to compensate for this, I&rsquo;m going to throw away all the people who claim to not know what a JDBC fetch size is, and focus on the remaining respondents.</p><p><strong>Of those respondents, more than 70% claim to be using Oracle and are either unaware of the default fetch size, or know it, but don&rsquo;t change it.</strong></p><p>I got in contact with Lo&iuml;c Lef&egrave;vre from Oracle to make sure that I fully understood the implications of this. He and his colleague Connor McDonald pointed out to me that actually the Oracle JDBC driver has an adaptive fetch size in the 23ai version, and that in the best case the driver will actually increase the fetch size to 250 on the fourth fetch, and that this behavior depends on the size of each row in the result set. Nice to know.</p><p>Alright, so, I&rsquo;m going to make the following assertions upfront:</p><div><ol> <li><p>Most Java data access code is doing online transaction processing, and not batch processing.</p></li> <li><p>For such programs, most queries return between 10<sup>0</sup>and 10<sup>2</sup>rows, with 10<sup>3</sup>rows being possible but already extremely rare. By contrast, 10<sup>4</sup>rows and above characterizes the offline batch processing case.</p></li> <li><p>The size of each row of such a query result set is not usually huge.</p></li> <li><p>Common practice&mdash;&#8203;especially for programs using Hibernate or JPA&mdash;&#8203;is to limit results using <code>LIMIT</code>, read the whole JDBC result set immediately, putting the results into a <code>List</code> or whatever, and then carry on working with that list.</p></li> <li><p>It&rsquo;s common for the Java client to be on a different physical machine to the database server.</p></li> <li><p>It&rsquo;s common for the Java client to have access to plentiful memory.</p></li> <li><p>The database server is typically the least scalable element of the system.</p></li> <li><p>For online transaction processing we care a lot about latency.</p></li> </ol> </div><p><em>Of course</em> one can easily concoct scenarios in which one or more of these assumptions is violated. Yes yes yes, I&rsquo;m perfectly aware that some people do batch processing in Java. The comments I&rsquo;m about to make do not apply to batch processing. But I insist that what I&rsquo;ve described above is a fairly good description of the <em>most common case</em>.</p><p>Now consider what happens for a query returning 12 rows:</p><div><ol> <li><p>On a first visit to the database server, the server executes the query, builds up the result set in memory, and then returns 10 rows to the client.</p></li> <li><p>The Java client iterates over those ten rows, hydrating a graph of Java objects, and putting them into a list or whatever, and then blocks waiting for the next 10 rows.</p></li> <li><p>The JDBC driver requests the remaining 2 rows from the server which has been keeping the result set waiting.</p></li> <li><p>The Java client can now process the remaining 2 rows, and finally carry on with what it was doing.</p></li> </ol> </div><p>This is bad.</p><p>Not only did we make two trips to the server when one trip would have been better, we also forced the server to maintain client-associated state across an interaction. I repeat: the database server is typically the <em>least scalable tier</em>. We almost never want the database server to hold state while waiting around for the client to do stuff.</p><p>For a query which returns 50 rows, the story is even worse. Even in the best case, the default behavior of the driver requires four trips to the database to retrieve those 50 rows. Folks, a typical JVM is just not going to blow up with an OOME if you send it 50 rows at once!</p><p>So, my recommendations are as follows:</p><div><ol> <li><p>The default JDBC fetch size should be set to a large number, somewhere between 10<sup>3</sup>and 2<sup>31</sup>-1. This can be controlled via <code>hibernate.jdbc.fetch_size</code>, or, even better, on Oracle, via the <code>defaultRowPrefetch</code> JDBC connection property. Note that most JDBC drivers have an unlimited fetch size by default, and I believe that this is the best default.</p></li> <li><p>Use pagination via a SQL <code>LIMIT</code>, that is, the standard JPA <code>setMaxResults()</code> API, to control the size of the result set if necessary. Remember: if you&rsquo;re calling JPA&rsquo;s <code>getResultList()</code>, setting a smaller fetch size is not going to help control the amount of data retrieved <em>at all</em>, since the JPA provider is just going to eagerly read it all into a list anyway!</p></li> <li><p>For special cases like batch processing of huge datasets, use <a href="https://docs.jboss.org/hibernate/orm/7.0/introduction/html_single/Hibernate_Introduction.html#stateless-sessions"><code>StatelessSession</code></a> or <a href="https://docs.jboss.org/hibernate/orm/7.0/introduction/html_single/Hibernate_Introduction.html#session-cache-management"><code>Session.clear()</code></a> to control the use of memory on the Java side, and <a href="https://docs.jboss.org/hibernate/orm/7.0/javadocs/org/hibernate/ScrollableResults.html"><code>ScrollableResults</code></a> together with <a href="https://docs.jboss.org/hibernate/orm/7.0/javadocs/org/hibernate/ScrollableResults.html#setFetchSize(int)"><code>setFetchSize()</code></a> to control fetching. Or even better, just make life easy for yourself and write a damn stored procedure.</p></li> </ol> </div><p>So if you belong to that 70% of Oracle users, you should be able to make your program more responsive and more scalable with almost no work, using this One Simple Trick.</p><p>UPDATE: Note that if you try searching for "JDBC fetch size" on Google, you&rsquo;ll encounter a bunch of misinformation. The <a href="https://franckpachot.medium.com/oracle-postgres-jdbc-fetch-size-3012d494712">one source which gets this right</a> is (coincidence?) from Franck Pachot.</p></div></section>]]></description><pubDate>Mon, 15 Sep 2025 17:03:17 +0530</pubDate></item><item><link>https://blog.phakorn.com/posts/2025/building-a-simple-vm/</link><title>Building a Simple Stack-Based Virtual Machine in Go (blog.phakorn.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhfigu/building_a_simple_stackbased_virtual_machine_in_go/</guid><comments>https://www.reddit.com/r/programming/comments/1nhfigu/building_a_simple_stackbased_virtual_machine_in_go/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 18 min | <a href='https://www.reddit.com/r/programming/comments/1nhfigu/building_a_simple_stackbased_virtual_machine_in_go/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Iâ€™ve been experimenting with building a minimal stack-based virtual machine in Go, inspired by WebAssembly and the EVM.</p><p>It handles compiled bytecode, basic arithmetic, and simple execution flow. Wrote up the process here</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://blog.phakorn.com/posts/2025/building-a-simple-vm/index.png' /></section><section class='parsed-content'><article> <h2>Introduction</h2><p>Inspired by virtual machines (VMs) like WebAssembly and the Ethereum Virtual Machine, I set out to challenge myself by crafting a simplified version. In this post, I share the process of implementing a VM in Go that operates on compiled bytecode and handles basic arithmetic operations.</p><h2>Architectural Design</h2> <h2>Stack Machine or Register Machine?</h2><p>When exploring the core architecture of VMs, two primary models stand out: <strong>stack machine</strong> and <strong>register machine</strong>.</p><p>A stack machine, such as WebAssembly, operates on a <strong>last-in, first-out (LIFO)</strong> basis. Instructions in this model primarily manipulate data at the top of the stack. This model is often used in implementation of VM due to its simplicity and ease of implementation.</p><p>A register machine akin to modern CPU designs, which use a set of registers (backed by hardware) that instructions can directly manipulate. This model can potentially enhance execution speed by reducing the overhead of repetitive data movement and often allows for more sophisticated optimizations during code compilation.</p><p>I&rsquo;ve chosen to go with a <strong>stack machine</strong> architecture. This decision was driven by the model&rsquo;s inherent simplicity and the straightforward nature of its execution flow, which significantly eases both the development and debugging processes.</p><h2>Word Size</h2><p>A <strong>word</strong> represents a fixed-sized data unit that the processor&rsquo;s instruction set or hardware handles collectively.</p><p>Choosing a larger word size, such as 64-bit, allows the VM to process more data per instruction. This capability enables more efficient management of complex data types and operations with fewer instructions. Although these benefits are clear, it&rsquo;s crucial to note that the actual performance improvements can vary, depending on the underlying system&rsquo;s architecture and the VM&rsquo;s ability to utilize it effectively.</p><p>I&rsquo;ve chosen a 64-bit architecture for the VM, which means that each instruction can process data up to 8 bytes in size.</p><h2>Memory</h2><p>A byte-addressable memory model has been chosen for the VM. This model allows each byte of memory to be individually addressed, enhancing the system&rsquo;s flexibility when managing different data size.</p><h2>Implementation</h2> <h2>Opcode</h2><p>Opcodes (operation codes) are fundamental elements that define the set of operations the VM can perform. Each opcode corresponds to a specific operation, ranging from simple data manipulation to complex arithmetic functions.</p><p>Importantly, opcodes operate on raw bytes, without any awareness of higher-level data types such as strings or signed integers. This approach highlights the VM&rsquo;s role as a flexible, low-level execution environment, handling data purely at the byte level.</p><p>Below are the opcodes supported by the VM.</p><pre><code><span><span>const</span><span> (</span></span> <span><span>// POP is used to discard the top value from the stack and decrement the stack pointer.</span></span> <span><span>POP</span><span> Opcode</span><span> =</span><span> iota</span></span> <span><span>// PUSH1 is used to place a 1 byte data onto the stack.</span></span> <span><span>PUSH1</span></span> <span><span>// PUSH8 is used to place a 8 byte (a word) data onto the stack.</span></span> <span><span>PUSH8</span></span> <span><span>// LOAD is used to load 8 byte (a word) data from memory into the stack at a specified offset using the top value of the stack</span></span> <span><span>LOAD8</span></span> <span><span>// ADD is used to add two operands.</span></span> <span><span>ADD</span></span> <span><span>// SUB subtracts the top value of the stack from the second top value of the stack.</span></span> <span><span>SUB</span></span> <span><span>// MUL multiplies the top two values on the stack.</span></span> <span><span>MUL</span></span> <span><span>// DIV divides the top value of the stack by the second top value of the stack, errors on division by zero.</span></span> <span><span>DIV</span></span> <span><span>// STORE1 stores a 1-byte value into memory at a specified offset, using the top value of the stack as the offset and the second top value as the data to store</span></span> <span><span>STORE1</span></span> <span><span>// STORE8 stores an 8-byte value (a word) into memory at a specified offset, using the top value of the stack as the offset and the second top value as the data to store</span></span> <span><span>STORE8</span></span> <span><span>// RETURN is used to exit the execution and return a block of memory.</span></span> <span><span>// It pops the size and offset from the stack and returns the specified block from memory.</span></span> <span><span>RETURN</span></span> <span><span>)</span></span></code></pre><p>This definition limits each opcode to one byte, allowing the VM to support up to 256 distinct opcodes.</p><h3>Opcode Functions</h3><p>Each opcode has a corresponding function of type <code>opFunc</code>, which operates on the VM&rsquo;s state to perform its designated task. Below are examples for <code>PUSH8</code> and <code>ADD</code>:</p><pre><code><span><span>// standardize type for opcode function in vm</span></span> <span><span>type</span><span> (</span></span> <span><span> opFunc</span><span> func</span><span>(</span><span>v</span><span> *</span><span>vm</span><span>) ([]</span><span>byte</span><span>, </span><span>error</span><span>)</span></span> <span><span>)</span></span> <span><span>func</span><span> opPush8</span><span>(</span><span>v</span><span> *</span><span>vm</span><span>) ([]</span><span>byte</span><span>, </span><span>error</span><span>) {</span></span> <span><span> value</span><span> :=</span><span> binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint64</span><span>(</span><span>v</span><span>.</span><span>bytecode</span><span>[</span><span>v</span><span>.</span><span>pc</span><span> : </span><span>v</span><span>.</span><span>pc</span><span>+</span><span>8</span><span>])</span></span> <span><span> v</span><span>.</span><span>stack</span><span>[</span><span>v</span><span>.</span><span>sp</span><span>] </span><span>=</span><span> value</span></span> <span><span> v</span><span>.</span><span>pc</span><span> +=</span><span> 8</span></span> <span><span> v</span><span>.</span><span>sp</span><span>++</span></span> <span><span> if</span><span> err</span><span> :=</span><span> v</span><span>.</span><span>checkStackOverflow</span><span>(); </span><span>err</span><span> !=</span><span> nil</span><span> {</span></span> <span><span> return</span><span> nil</span><span>, </span><span>err</span></span> <span><span> }</span></span> <span><span> return</span><span> nil</span><span>, </span><span>nil</span></span> <span><span>}</span></span> <span><span>func</span><span> opAdd</span><span>(</span><span>v</span><span> *</span><span>vm</span><span>) ([]</span><span>byte</span><span>, </span><span>error</span><span>) {</span></span> <span><span> if</span><span> err</span><span> :=</span><span> v</span><span>.</span><span>checkStackUnderflow</span><span>(</span><span>uint64</span><span>(</span><span>1</span><span>)); </span><span>err</span><span> !=</span><span> nil</span><span> {</span></span> <span><span> return</span><span> nil</span><span>, </span><span>err</span></span> <span><span> }</span></span> <span><span> a</span><span> :=</span><span> v</span><span>.</span><span>stack</span><span>[</span><span>v</span><span>.</span><span>sp</span><span>-</span><span>1</span><span>]</span></span> <span><span> b</span><span> :=</span><span> v</span><span>.</span><span>stack</span><span>[</span><span>v</span><span>.</span><span>sp</span><span>-</span><span>2</span><span>]</span></span> <span><span> v</span><span>.</span><span>sp</span><span> -=</span><span> 2</span></span> <span><span> v</span><span>.</span><span>stack</span><span>[</span><span>v</span><span>.</span><span>sp</span><span>] </span><span>=</span><span> a</span><span> +</span><span> b</span></span> <span><span> v</span><span>.</span><span>sp</span><span>++</span></span> <span><span> if</span><span> err</span><span> :=</span><span> v</span><span>.</span><span>checkStackOverflow</span><span>(); </span><span>err</span><span> !=</span><span> nil</span><span> {</span></span> <span><span> return</span><span> nil</span><span>, </span><span>err</span></span> <span><span> }</span></span> <span><span> return</span><span> nil</span><span>, </span><span>nil</span></span> <span><span>}</span></span> </code></pre> <h2>Virtual Machine</h2><p>Next, we will define our VM as a Go&rsquo;s <code>struct</code> type.</p><pre><code><span><span>type</span><span> JumpTable</span><span> [</span><span>1</span><span> &lt;&lt;</span><span> 16</span><span>]</span><span>opFunc</span></span> <span><span>type</span><span> vm</span><span> struct</span><span> {</span></span> <span><span> stack</span><span> []</span><span>uint64</span><span> // stack</span></span> <span><span> pc</span><span> uint64</span><span> // program counter</span></span> <span><span> sp</span><span> uint64</span><span> // stack pointer</span></span> <span><span> bytecode</span><span> []</span><span>byte</span><span> // compiled bytecode</span></span> <span><span> memory</span><span> []</span><span>byte</span><span> // memory</span></span> <span><span> jumpTable</span><span> JumpTable</span></span> <span><span>}</span></span></code></pre> <ol> <li><code>stack []uint64</code> <ul> <li>Represents the stack used by the VM</li> <li>Each element in the stack is 64 bits in size (<code>uint64</code>)</li> </ul> </li> <li><code>pc uint64</code> <ul> <li>Stands for the program counter</li> <li>Indicates the current position of the VM within the bytecode</li> </ul> </li> <li><code>sp uint64</code> <ul> <li>Represents the stack pointer</li> <li>Points to the top of the stack, indicating the next available location for pushing data</li> <li>Changes dynamically as the VM executes instructions that manipulate the stack</li> </ul> </li> <li><code>bytecode []byte</code> <ul> <li>Holds the compiled bytecode of the program</li> <li>Bytecode consists of low-level instructions that the VM executes</li> </ul> </li> <li><code>memory []byte</code> <ul> <li>Represents the memory space available to the VM</li> <li>Used for storing data required during program execution, such as variables and constants</li> </ul> </li> <li><code>jumpTable JumpTable</code> <ul> <li>A mapping table for each opcode to its corresponding opcode function (<code>opFunc</code>)</li> <li>During execution, the VM uses this table to efficiently look up the operation function associated with each opcode in the bytecode</li> </ul> </li> </ol> <h2>Stack Operation</h2><p>The crucial component for stack operations is the stack itself and its pointer, commonly referred to as the <strong>Stack Pointer (SP</strong>). The stack pointer is an internal pointer that keeps track of the top of the stack, which is the location for the next item to be placed on.</p><p>Each time an item is pushed onto the stack, the stack pointer increments; conversely, it decrements every time an item is popped off.</p><p>For Example: Imagine a scenario in a VM where the stack starts empty and the following operations (opcodes) occur:</p><ol> <li><strong>PUSH8 0x3</strong> - places the value 3 at the current stack pointer location and increments the pointer.</li> <li><strong>PUSH8 0x8</strong> - places the value 8 at the new stack pointer location, incrementing the pointer again.</li> <li><strong>ADD</strong> - pops the top two values, adds them, and pushes the result back onto the stack, adjusting the stack pointer appropriately.</li> </ol><p>Here&rsquo;s how the stack and the stack pointer change with each operation:</p><ul> <li><strong>Initial State</strong>: Stack is empty, SP = 0</li> <li><strong>After PUSH8 0x3</strong>: Stack = [3], SP = 1</li> <li><strong>After PUSH8 0x8</strong>: Stack = [3, 8], SP = 2</li> <li><strong>After ADD</strong>: Stack = [11], SP = 1</li> </ul> <h2>Compiling Instruction into Byte Code</h2><p>Instructions for the VM will be specified in the following string format</p><pre><code><span><span>// This is comment line</span></span> <span><span>PUSH8 0x48656C6C6F20576F</span></span> <span><span>PUSH1 0x00</span></span> <span><span>STORE8</span></span> <span><span>PUSH8 0x726C642100000000</span></span> <span><span>PUSH1 0x08</span></span> <span><span>STORE8</span></span> <span><span>PUSH1 0x0C</span></span> <span><span>PUSH1 0x00</span></span> <span><span>RETURN</span></span></code></pre><p>Each line represents a single instruction using the defined <code>opcode</code>. Only <code>PUSH1</code> and <code>PUSH8</code> require an operand for the data to be pushed into the stack in hexadecimal representation.</p><p>Any line that starts with <code>//</code> or is empty will be ignored by the compiler.</p><p>Following the the compiled bytecode in hexadecimal representation.</p><pre><code><span><span>02 48 65 6C 6C 6F 20 57 6F 01 00 09 02 72 6C 64 21 00 00 00 00 01 08 09 01 0C 01 00 0A</span></span></code></pre><p>Which can be mapped into the instruction as followed</p><pre><code><span><span>// This is comment line</span></span> <span><span>PUSH8 0x48656C6C6F20576F - 02 48 65 6C 6C 6F 20 57 6F</span></span> <span><span>PUSH1 0x00 - 01 00</span></span> <span><span>STORE8 - 09</span></span> <span><span>PUSH8 0x726C642100000000 - 02 72 6C 64 21 00 00 00 00</span></span> <span><span>PUSH1 0x08 - 01 08</span></span> <span><span>STORE8 - 09</span></span> <span><span>PUSH1 0x0C - 01 0C</span></span> <span><span>PUSH1 0x00 - 01 00</span></span> <span><span>RETURN - 00 0A</span></span></code></pre> <h2>Execution of Byte Code</h2><p><strong>Program Counter (PC)</strong> is used to track the execution of bytecode instructions. The PC points to the address of the current instruction that the VM is executing.</p><p>As each instruction is processed, the PC is incremented to point to the next instruction, ensuring that operations are carried out in the correct sequence.</p><pre><code><span><span>02 48 65 6C 6C 6F 20 57 6F 01 00 09 02 72 6C 64 21 00 00 00 00 01 08 09 01 0C 01 00 0A</span></span></code></pre><p>Here&rsquo;s how the Program Counter (PC) operates through the sequence of bytecode from earlier example:</p><ol> <li><strong>PC = 0</strong>: <ul> <li><strong>Reads opcode <code>02</code> (PUSH8)</strong>, indicating the operation to push 8 bytes into the stack.</li> <li><strong>Increment PC by 1</strong> to start reading data for this opcode.</li> <li><strong>Data read</strong>: <code>48 65 6C 6C 6F 20 57 6F</code> from PC=1 to PC=8.</li> <li><strong>Update PC by 8</strong> after reading 8 bytes of data, now PC = 9.</li> </ul> </li> <li><strong>PC = 9</strong>: <ul> <li><strong>Reads opcode <code>01</code> (PUSH1)</strong>, indicating the operation to push 1 byte into the stack.</li> <li><strong>Increment PC by 1</strong> to read the data for this opcode.</li> <li><strong>Data read</strong>: <code>00</code> at PC=10.</li> <li><strong>Update PC by 1</strong> after reading the byte, now PC = 11.</li> </ul> </li> <li><strong>PC = 11</strong>: <ul> <li><strong>Reads opcode <code>09</code> (STORE8)</strong>, indicating the operation to pop items from the stack and store them.</li> <li><strong>Increment PC by 1</strong> to move to the next part of the instruction or the next opcode.</li> </ul> </li> <li><strong>Continues through the sequence</strong>: As each opcode is processed, the PC is incremented by 1 to read the opcode, and then further incremented as per the number of bytes the opcode processes. This ensures each instruction is executed in the correct order.</li> </ol> <h2>Memory Operation</h2><p>In the VM, the <code>memory []byte</code> field is a byte array that represents the VM&rsquo;s memory space, designed to store data throughout the lifecycle of the VM&rsquo;s operation.</p><p>Using the following example, let&rsquo;s visualize the changes to the VM&rsquo;s memory:</p><pre><code><span><span>PUSH8 0x48656C6C6F20576F // Push "Hello Wo" onto the stack</span></span> <span><span>PUSH1 0x08 // Push the memory offset 8 onto the stack</span></span> <span><span>STORE8 // Store 8 bytes at memory offset 8</span></span></code></pre> <ol> <li><code>PUSH8 0x48656C6C6F20576F</code> <ul> <li>This instruction pushes the 8-byte value corresponding to the ASCII string &ldquo;Hello Wo&rdquo; onto the stack. The hexadecimal <code>0x48656C6C6F20576F</code> directly translates to the string &ldquo;Hello Wo&rdquo;.</li> <li><strong>Effect on Stack</strong>: <ul> <li>Stack before operation: <code>[]</code></li> <li>Stack after operation: <code>[0x48656C6C6F20576F]</code></li> <li>The stack now contains one item, the 8-byte string &ldquo;Hello Wo&rdquo;.</li> </ul> </li> </ul> </li> <li><code>PUSH1 0x08</code> <ul> <li>This instruction pushes a 1-byte value <code>0x08</code> onto the stack. In this context, <code>0x08</code> represents the memory offset where the previously pushed data (&ldquo;Hello Wo&rdquo;) will be stored.</li> <li><strong>Effect on Stack</strong>: <ul> <li>Stack before operation: <code>[0x48656C6C6F20576F]</code></li> <li>Stack after operation: <code>[0x48656C6C6F20576F, 0x08]</code></li> <li>The stack now has two items: the &ldquo;Hello Wo&rdquo; data and the memory offset <code>0x08</code>.</li> </ul> </li> </ul> </li> <li><code>STORE8</code> <ul> <li>This instruction takes the two items from the top of the stack: the 8-byte data (&ldquo;Hello Wo&rdquo;) and the 1-byte memory offset (<code>0x08</code>). It stores the 8-byte data at the specified memory offset in the VM&rsquo;s memory.</li> <li><strong>Effect on Memory and Stack</strong>: <ul> <li>Memory before operation: <code>[00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ...]</code></li> <li>Memory after operation: <code>[00 00 00 00 00 00 00 00 48 65 6C 6C 6F 20 57 6F ...]</code></li> <li>The bytes <code>48 65 6C 6C 6F 20 57 6F</code> (representing &ldquo;Hello Wo&rdquo;) are stored starting at memory offset <code>0x08</code>.</li> <li>Stack after operation: <code>[]</code></li> <li>The stack is cleared of both items used by the <code>STORE8</code> operation.</li> </ul> </li> </ul> </li> </ol> <h2>Program Output</h2><p>The opcode <code>RETURN</code> provides a mechanism for returning data from the VM&rsquo;s memory to the calling code . It reads the specified memory offset and size from the stack and returns the data as a byte array.</p><p>Let&rsquo;s consider the sequence in the provided example, culminating in the use of the <code>RETURN</code> opcode:</p><pre><code><span><span>PUSH8 0x48656C6C6F20576F</span></span> <span><span>PUSH1 0x00</span></span> <span><span>STORE8</span></span> <span><span>PUSH8 0x726C642100000000</span></span> <span><span>PUSH1 0x08</span></span> <span><span>STORE8</span></span> <span><span>// Memory at this point</span></span> <span><span>// [48 65 6C 6C 6F 20 57 6F 72 6C 64 21 00 00 00 00 00 ...]</span></span> <span><span>PUSH1 0x0C // Pushes the size of the data to return</span></span> <span><span>PUSH1 0x00 // Pushes the starting memory offset for the return</span></span> <span><span>RETURN</span></span></code></pre> <ol> <li><code>PUSH1 0x0C</code> <ul> <li>This pushes <code>0x0C</code> onto the stack, which represents the size of the data (12 bytes) to be returned. This size indicates how many bytes following the specified offset should be returned.</li> </ul> </li> <li><code>PUSH1 0x00</code> <ul> <li>This pushes <code>0x00</code> onto the stack, indicating the memory offset from which the return should begin.</li> </ul> </li> <li><code>RETURN</code> <ul> <li>This opcode functions by retrieving the offset (<code>0x00</code>) and size (<code>0x0C</code>) from the stack. It then accesses the VM&rsquo;s memory starting from this offset and reads the specified number of bytes to form the returned byte array.</li> <li>The returned data would consist of the first 12 bytes of memory, corresponding to the sequence <code>[48 65 6C 6C 6F 20 57 6F 72 6C 64 21]</code>, which is the ASCII representation of &ldquo;Hello World!&rdquo;.</li> </ul> </li> </ol> <h2>Running the Virtual Machine</h2><p>The source code for the virtual machine (VM) is available at <a href="https://github.com/PhakornKiong/go-vm">GitHub - PhakornKiong/go-vm</a>.</p><p>To operate the VM and execute instructions, use the command <code>go run main.go -i examples/addition</code> to run an example instruction set from the <code>examples</code> folder.</p><p>For output customization, the <code>-t</code> flag allows you to specify the output data type, determining how the results are formatted and displayed.</p></article> </section>]]></description><pubDate>Mon, 15 Sep 2025 13:06:44 +0530</pubDate></item><item><link>https://purplesyringa.moe/blog/falsehoods-programmers-believe-about-null-pointers/</link><title>Falsehoods programmers believe about null pointers (purplesyringa.moe)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhekur/falsehoods_programmers_believe_about_null_pointers/</guid><comments>https://www.reddit.com/r/programming/comments/1nhekur/falsehoods_programmers_believe_about_null_pointers/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 15 min | <a href='https://www.reddit.com/r/programming/comments/1nhekur/falsehoods_programmers_believe_about_null_pointers/'>Post permalink</a></p></section><section class='preview-image'><img src='https://purplesyringa.moe/blog/falsehoods-programmers-believe-about-null-pointers/og.png' /></section><section class='parsed-content'><div><p><time>January 30, 2025</time><a href="https://news.ycombinator.com/item?id=42894220"> Hacker News</a><a href="https://www.reddit.com/r/programming/comments/1ieagxg/falsehoods_programmers_believe_about_null_pointers/"> Reddit</a></p><blockquote><p><em>Added on February 1</em>: This article assumes you know what UB is and why you shouldn&rsquo;t trigger it, very basic knowledge of how CPUs work, and an ability to take exact context into account without overgeneralizing specifics. These falsehoods are misconceptions because they don&rsquo;t apply globally, not because their inverse applies globally. If any of that is a problem to you, reading this will do more harm than good to your software engineering capabilities, and I&rsquo;d advise against interacting with this post. Check out the comments on Reddit for what can go wrong if you attempt that regardless.</p></blockquote><p>Null pointers look simple on the surface, and that&rsquo;s why they&rsquo;re so dangerous. As compiler optimizations, intuitive but incorrect simplifications, and platform-specific quirks have piled on, the odds of making a wrong assumption have increased, leading to the proliferation of bugs and vulnerabilities.</p><p>This article explores common misconceptions about null pointers held by many programmers, starting with simple fallacies and working our way up to the weirdest cases. Some of them will be news only to beginners, while others may lead experts down the path of meticulous fact-checking. Without further ado, let&rsquo;s dive in.</p><p><strong>Dereferencing a null pointer immediately crashes the program.</strong></p><p>Everyone&rsquo;s first attempt to dereference a null pointer in C, C++, or Rust results either in <code>STATUS_ACCESS_VIOLATION</code> or a dreaded <code>Segmentation fault (core dumped)</code> message, which gives this misconception some credibility. However, higher-level languages and libraries like <a href="https://chromium.googlesource.com/crashpad/crashpad">Crashpad</a> can handle the error and print a nice message and a backtrace before the crash. This is implemented by installing a <a href="https://learn.microsoft.com/en-us/windows/win32/debug/vectored-exception-handling">vectored exception handler</a> on Windows and a <a href="https://en.wikipedia.org/wiki/C_signal_handling">signal handler</a> on Unix-like platforms.</p><p><strong>Dereferencing a null pointer eventually leads to program termination.</strong></p><p>While dereferencing a null pointer is a Bad Thing, it is by no means unrecoverable. Vectored exception and signal handlers can resume the program (perhaps from a different code location) instead of bringing the process down. For example, Go translates nil pointer dereferences to panics, which can be caught in user code with <a href="https://go.dev/blog/defer-panic-and-recover">recover</a>, and Java translates them to <code>NullPointerException</code>, which can also be caught by user code like any other exception.</p><p>In both cases, asking for forgiveness (dereferencing a null pointer and then recovering) instead of permission (checking if the pointer is null before dereferencing it) is an optimization. Comparing all pointers with null would slow down execution when the pointer <em>isn&rsquo;t</em> null, i.e. in the majority of cases. In contrast, signal handling is zero-cost until the signal is generated, which happens exceedingly rarely in well-written programs.</p><p><strong>Dereferencing a null pointer always causes a signal, an exception, or is otherwise rejected by hardware.</strong></p><p>Let&rsquo;s ignore undefined behavior for now and assume that the dereference is not optimized out.</p><p>Before virtual memory was a thing, almost all memory was accessible. For example, x86 in real mode stored interrupt tables at addresses from <code>0</code> to <code>1024</code>. From the hardware point of view, dereferencing a null pointer is no different from dereferencing other pointers, and as such, it simply accessed memory at address <code>0</code>.</p><p>This is still the case on many embedded platforms. Dereferencing a null pointer is still considered UB, so if, for whatever reason, you need to access address <code>0</code>, there are two major ways to do this:</p><ol><li>You can write the relevant code in assembly, which does not have UB.</li><li>If the hardware ignores the topmost bits of the address, you can access <code>0x80000000</code> (or similar) from C instead.</li></ol><p><strong>On modern conventional platforms, dereferencing a null pointer always causes a signal, an exception, or is otherwise rejected by hardware.</strong></p><p>Linux supports a <a href="https://man7.org/linux/man-pages/man2/personality.2.html">personality flag</a> called <code>MMAP_PAGE_ZERO</code> for compatibility with programs developed for System V. Running a program under <code>setarch -Z</code> executes it with address <code>0</code> to <code>4096</code> (or whatever your page size is) mapped to a page of zeroes. Alternatively, you can use <code>mmap</code> to place memory at address <code>0</code> by hand. Many years ago, Wine used this trick (among others, like patching LDT) to run DOS applications without DOSBox.</p><p>This no longer works by default for security reasons. One man&rsquo;s treasure is another man&rsquo;s trash: if the kernel accidentally dereferences a null pointer while the memory at address <code>0</code> is mapped, it might interpret user-supplied data as a kernel data structure, which facilitates exploits. However, you can still enable this explicitly by running <code>sudo sysctl vm.mmap_min_addr=0</code>.</p><p>Despite this, there&rsquo;s a very modern and common platform that still maps memory at address <code>0</code>. It&rsquo;s WebAssembly. Isolation within a wasm container is unnecessary, so this does not ease security exploits, and as such, dereferencing a null pointer still works here.</p><p><strong>Dereferencing a null pointer always triggers &ldquo;UB&rdquo;.</strong></p><p>This one&rsquo;s tricky. The standard does say this triggers Undefined Behavior, but what this phrase <em>means</em> has significantly changed over time.</p><p>In ye olden times, the C standard was considered guidelines rather than a ruleset, <em>undefined behavior</em> was closer to <em>implementation-defined behavior</em> than dark magic, and optimizers were stupid enough to make that distinction irrelevant. On a majority of platforms, dereferencing a null pointer compiled and behaved exactly like dereferencing a value at address <code>0</code>.</p><p>For all intents and purposes, UB as we understand it today with spooky action at a distance didn&rsquo;t exist.</p><p>For example, the <a href="https://stackoverflow.com/questions/58843458/hp-ux-cc-uses-a-default-setting-to-allow-null-dereferences-is-that-possible-in">HP-UX C compiler</a> had a CLI option to map a page of zeroes at address <code>0</code>, so that <code>*(int*)NULL</code> would return <code>0</code>. Certain programs relied on this behavior and had to be patched to run correctly on modern operating systems &ndash; or be executed with a personality flag.</p><hr><p>Now we enter the cursed territory.</p><p><strong>The null pointer has address <code>0</code>.</strong></p><p>The C standard does not require the null pointer to have address <code>0</code>. The only requirement it imposes is for <code>(void*)x</code> to evaluate to a null pointer, where <code>x</code> is <em>a compile-time constant equal to zero</em>. Such patterns can easily be matched in compile time, so null pointers can have addresses other than <code>0</code>. Similarly, casting a pointer to a boolean (as in <code>if (p)</code> and <code>!p</code>) is required to produce <code>false</code> for null pointers, not for zero pointers.</p><p>This is not a hypothetical: <a href="https://c-faq.com/null/machexamp.html">some real architectures</a> and C interpreters use non-zero null pointers. <code>fullptr</code> is not really a joke.</p><p>If you&rsquo;re wondering, Rust and other modern languages usually don&rsquo;t support this case.</p><p><strong>The null pointer has address <code>0</code> on modern platforms.</strong></p><p>On GPU architectures like <a href="https://reviews.llvm.org/D26196">AMD GCN</a> and <a href="https://what.thedailywtf.com/topic/8661/sometimes-checking-for-null-pointers-is-a-mistake-nvidia-cuda/13">NVIDIA Fermi</a>, <code>0</code> points to accessible memory. At least on AMD GCN, the null pointer is represented as <code>-1</code>. (I&rsquo;m not sure if that holds for Fermi, but that would be reasonable.)</p><p><strong>Since <code>(void*)0</code> is a null pointer, <code>int x = 0; (void*)x</code> must be a null pointer, too.</strong></p><p>In <code>int x = 0; (void*)x</code>, <code>x</code> is not a constant expression, so the standard does not require it to produce a null pointer. Runtime integer-to-pointer casts are often no-ops, so adding <code>if (x == 0) x = ACTUAL_NULL_POINTER_ADDRESS;</code> to every cast would be very inefficient, and generating a null pointer conditional on optimizations seeing through runtime values would be unnecessarily inconsistent.</p><p>Obviously, <code>void *p; memset(&amp;p, 0, sizeof(p)); p</code> is not guaranteed to produce a null pointer either.</p><p><strong>On platforms where the null pointer has address <code>0</code>, C objects may not be placed at address <code>0</code>.</strong></p><p>A pointer to an object is not a null pointer, even if it has the same address.</p><p>If you know what pointer provenance is, pointers with the same bitwise representation behaving differently shouldn&rsquo;t be news to you:</p><pre><code><span>int</span> x[<span>1</span>]; <span>int</span> y = ; <span>int</span> *p = x + <span>1</span>; <span>// This may evaluate to true</span> <span>if</span> (p == &amp;y) { <span>// But this will be UB even though p and &amp;y are equal</span> *p; } </code></pre><p>Similarly, objects can be placed at address <code>0</code> even though pointers to them will be indistinguishable from <code>NULL</code> in runtime:</p><pre><code><span>int</span> tmp = <span>123</span>; <span>// This can be placed at address 0</span> <span>int</span> *p = &amp;tmp; <span>// Just a pointer to 0, does not originate from a constant zero</span> <span>int</span> *q = <span>NULL</span>; <span>// A null pointer because it originates from a constant zero</span> <span>// p and q will have the same bitwise representation, but...</span> <span>int</span> x = *p; <span>// produces 123</span> <span>int</span> y = *q; <span>// UB</span> </code></pre><p><strong>On platforms where the null pointer has address <code>0</code>, <code>int x = 0; (void*)x</code> is a null pointer.</strong></p><p>The result of an integer-to-pointer conversion is implementation-defined. While a null pointer is an obvious candidate, this can also produce an invalid pointer or even a dereferenceable pointer to an object at address <code>0</code>. Certain compilers <a href="https://c-faq.com/.xx/q5.19.html">encouraged</a> this pattern for accessing memory at address <code>0</code> soundly:</p><pre><code><span>int</span> *p = (<span>void</span>*); <span>// Must produce a NULL pointer</span> <span>int</span> x = *p; <span>// UB</span> <span>int</span> zero = ; <span>int</span> *q = (<span>void</span>*)zero; <span>// May produce a dereferenceable pointer on some compilers</span> <span>int</span> y = *q; <span>// Not necessarily UB</span> </code></pre><p>This is mostly a C legacy: most languages don&rsquo;t differentiate between runtime and compile-time integer-to-pointer casts and will exhibit consistent behavior.</p><p><strong>On platforms where the null pointer has address <code>0</code>, <code>int x = 0; (void*)x</code> will compare equal to <code>NULL</code>.</strong></p><p>In C, pointers to objects are documented to compare as unequal to <code>NULL</code>, even if the object is at address <code>0</code>. In other words, knowing the addresses of pointers is not enough to compare them. This is one of the rare cases where provenance affects program execution in a way that does not cause UB.</p><p>The following asserts hold:</p><pre><code><span>extern</span> <span>int</span> tmp; <span>// Suppose this is at address 0</span> <span>int</span> *p = &amp;tmp; assert(p != <span>NULL</span>); <span>// Pointer to object compares unequal to NULL</span> <span>int</span> *q = (<span>void</span>*)(<span>uintptr_t</span>)p; assert(p == q); <span>// Round-tripping produces a possibly invalid, but equal pointer</span> assert(q != <span>NULL</span>); <span>// By transitivity</span> <span>int</span> x = ; <span>int</span> *r = (<span>void</span>*)x; <span>// This is still round-tripping, lack of data dependency on p is irrelevant</span> assert(r != <span>NULL</span>); </code></pre><p>As provenance is not accessible in runtime, such comparisons can only be resolved in compile time. So if a pointer to an object might cross an FFI boundary or be passed to complex code, that object can&rsquo;t be realistically placed at address <code>0</code>.</p><p>Even if there is no object at address <code>0</code>, <code>int x = 0; (void*)x</code> is still allowed to produce a pointer that compares unequal to <code>NULL</code>, as the conversion is implementation-defined.</p><p>In Rust, objects are not allowed to be placed at address <code>0</code> explicitly.</p><p><strong>On platforms where the null pointer has address <code>0</code>, null pointers are stored as zeroes.</strong></p><p>The address of a pointer as revealed by integer casts and the bitwise representation of a pointer don&rsquo;t have to be equal, much like casting an integer to a float does not retain the bits.</p><p>Segmented addressing is a common example, but pointer authentication is a more modern instance of this effect. On ARM, the top byte of a pointer can be configured to store a cryptographic signature, which is then verified at dereference. Pointers inside <a href="https://github.com/swiftlang/llvm-project/blob/65e6c0eccdc1b63a0598b735dabaccf0d575a6b4/clang/docs/PointerAuthentication.rst#ptrauth-qualifier">__ptr_auth</a> regions are signed, storing the signature in addition to the address. Apple decided against signing null pointers, as this would make their values unpredictable during compile time. Still, this was a deliberate decision rather than an implication of the standard.</p><p>CHERI is even weirder. CHERI pointers store <eq><math><mn>128</mn></math></eq>-bit capabilities in addition to the <eq><math><mn>64</mn></math></eq>-bit address we&rsquo;re used to to protect against UAF and OOB accesses. Any pointer with address <code>0</code> is considered a null pointer, so there are effectively <eq><math><msup><mn>2</mn><mn>128</mn></msup></math></eq>-ish different null pointers, only one of which is all-zero. (This also means that comparing pointers for equality can yield different results than comparing their binary representations.)</p><p>If you extend the definition of pointers to include pointers to class members, this gets even more realistic. Pointers to members are, in effect, offsets to fields (at least if we aren&rsquo;t taking methods into account), and <code>0</code> is a valid offset, so <code>(int Class::*)nullptr</code> is usually stored as <code>-1</code>.</p><p>Null pointers are even more cursed than pointers in general, and provenance already makes pointers quite complicated. Being aware of edge cases like these is valuable to prevent accidentally non-portable code and interpret other people&rsquo;s code correctly.</p><p>But if this sounds like an awful lot to keep in mind all the time, you&rsquo;re missing the point. Tailoring rules and programs to new environments as more platforms emerged and optimizing compilers got smarter is what got us into this situation in the first place.</p><p>Many people call C a &ldquo;portable assembler&rdquo;. This is emphatically not the case. C <em>looks</em> close to hardware, but in reality this language has its own abstract machine and operational semantics. Optimization passes, code-generating backends, and libraries need to speak a platform-independent language to work in tandem, and that language is not &ldquo;whatever hardware does&rdquo;. Instead of translating what you&rsquo;d like the hardware to perform to C literally, treat C as a higher-level language, because it <em>is</em> one.</p><p>Python does not suffer from horrible memory safety bugs and non-portable behavior not only because it&rsquo;s an interpreted language, but also because software engineers don&rsquo;t try to outsmart the compiler or the runtime. Consider applying the same approach to C.</p><ul><li>Do you <em>need</em> to <code>memset</code> this structure, or will <code>= {0}</code> do the trick?</li><li>Why are you casting pointers to <code>size_t</code>? Use <code>uintptr_t</code> instead.</li><li>Why are you even round-tripping through integers? Use <code>void*</code> as an untyped/unaligned pointer type.</li><li>Instead of crafting branchless code like <code>(void*)((uintptr_t)p * flag)</code> by hand, let the compiler optimize <code>flag ? p : NULL</code> for you.</li><li>Can you store flags next to the pointer instead of abusing its low bits? If not, can you insert flags with <code>(char*)p + flags</code> instead of <code>(uintptr_t)p | flags</code>?</li></ul><p>If your spider sense tingles, consult the C standard, then your compiler&rsquo;s documentation, then ask compiler developers. Don&rsquo;t assume there are no long-term plans to change the behavior and certainly don&rsquo;t trust common sense.</p><p>When all else fails, do the next best thing: document the assumptions. This will make it easier for users to understand the limits of your software, for developers to port your application to a new platform, and for you to debug unexpected problems.</p><p><em>Next up: an architecture that stores memory addresses in IEEE-754 floats.</em></p></div></section>]]></description><pubDate>Mon, 15 Sep 2025 12:07:47 +0530</pubDate></item><item><link>https://asibahi.github.io/thoughts/a-gentle-introduction-to-z3/</link><title>A Dumb Introduction to z3. Exploring the world of constraint solvers with very simple examples. (asibahi.github.io)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nh38nn/a_dumb_introduction_to_z3_exploring_the_world_of/</guid><comments>https://www.reddit.com/r/programming/comments/1nh38nn/a_dumb_introduction_to_z3_exploring_the_world_of/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 39 min | <a href='https://www.reddit.com/r/programming/comments/1nh38nn/a_dumb_introduction_to_z3_exploring_the_world_of/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><p>Recently I have come across a nice article: <a href="https://buttondown.com/hillelwayne/archive/many-hard-leetcode-problems-are-easy-constraint/">Many Hard Leetcode Problems are Easy Constraint Problems</a>, and I figured, I really should learn how to use these things! What else do I really have to do? I have had use for solvers (or as they are commonly called: theorem provers) <a href="https://asibahi.github.io/thoughts/the-hanging-gardens-problem/">In a previous article</a>, but then I tried to prove the things with good old algorithms. I looked at <code>z3</code> at the time, but found the whole concept a bit too opaque. Now however, it seemed a bit easier to get into.</p><p>To be clear, as of writing these words, I have only been looking at <code>z3</code> reading material for two days. I am in no way an expert, and I have not written anything more complex than a solver for the change counter problem (the first example in the article listed above). So I am writing this really knowing nothing about the underlying theory, theorem provers, or whatever the hell "unification" is. There is a good chance you know more about this than I do.</p><p>There are <code>z3</code> bindings in many popular languages. I will be using <a href="https://docs.rs/z3/latest/z3/"><code>z3</code>'s Rust bindings</a>, because I am more comfortable in Rust than, say, Python or JavaScript. The examples I worked with to understand <code>z3</code> however, can be found in two nice documents:</p><ol> <li><a href="https://z3prover.github.io/papers/programmingz3.html">First is in Python</a></li> <li><a href="https://microsoft.github.io/z3guide/programming/Z3%20JavaScript%20Examples/">Second in JavaScript</a></li> </ol> <h2>What are Solvers?</h2><p>Solvers are a class of .. tools? libraries? where you input some rules and constraints and have the tool just .. solve it for you. It is not going to be a faster or more optimized solution than a custom made algorithm, but it is much easier to change the rules on the fly.</p><p>There are many real world uses. They are often used for scheduling and resource allocation problems. Consider the common scenario of a school schedule: Mary cannot work on Tuesdays because she needs to take care of her father; John lives far so he cannot give classes before 10; Class 3-A is full of nerds so their math hours are double; city council regulates no outdoor activity after 12; Susan and Sarah hate each other so you should not have them teach the same class; etc. You can either have two teachers work on it for a week, or just pop it in a solver!</p><p>The <a href="https://www.minizinc.org">MiniZinc homepage</a> (another popular solver) has a couple of nice examples: a seating chart, rostering, vehicle routing, grid coloring.</p><p>On that note, you might wonder: why did I go with <code>z3</code> when MiniZinc has a more colorful homepage and is actually referenced by the article linked at the start of this article? The answer is because <code>z3</code> has bindings in Rust. That is pretty much it.</p><h2>A Note on Terminology</h2><p>Documentation on <code>z3</code> and its API use a lot of jargon,<sup><a href="https://asibahi.github.io#fn-jargon">1</a></sup>which makes the whole thing really difficult to wade into without a previous background. I will explain things as I understand when I get to them, but two things really stand out.</p><p>The first is the word <code>Sort</code>. You see this in the context of arrays and function declarations (we will get to those, I hope). But it has nothing to do with .. well .. sorting. <code>Sort</code> is just the jargon word for <em>types</em>.</p><p>The second one is <strong>constants</strong>. They are not what a normal person would call constants: they are actually the knobs the solvers use to solve problems. There are two types of constants: <em>free</em>, which are what one would call variables; and <em>interpreted</em>, which is when you'd type an integer literal and clever type machinations turns it into a constant in the solver.</p><p>Note that also solvers do not work within the regular type system of the programming language. They have their own types (sorry, sorts), and operations that may or may not map nicely to the language's types and operations. Much of the actual code you are writing is about expressing things in the target solver's language. <code>z3</code> uses a language called "SMT-LIB2" (henceforth called <code>smt2</code>), apparently. And you can actually write your constraints immediately in said language and have the library consume it. Much of what the bindings is take your code and translate internally to this language before feeding it to the solver.<sup><a href="https://asibahi.github.io#fn-dsl">2</a></sup></p><hr> <h2>A Simple Equation</h2><p>Let's start with what might be the simplest, dumbest equation. Solve for <code>x</code>:</p><pre><code><span>x + 4 = 7 </span></code></pre><p>Yes, a child (literally) can solve this. But it is nice. Here is the Rust program to solve it.</p><pre><code><span>use </span><span>z3::{Solver, ast::Int}; </span><span>fn </span><span>main</span><span>() { </span><span>let</span><span> solver </span><span>= </span><span>Solver::new(); </span><span> </span><span>// define the variable. </span><span>let</span><span> x </span><span>= </span><span>Int::new_const(</span><span>"x"</span><span>); </span><span> </span><span>// define the equation </span><span> solver.</span><span>assert</span><span>((</span><span>&amp;</span><span>x </span><span>+ </span><span>4</span><span>).</span><span>eq</span><span>(</span><span>7</span><span>)); </span><span> </span><span>// run the solver </span><span>_ =</span><span> solver.</span><span>check</span><span>(); </span><span>let</span><span> model </span><span>=</span><span> solver.</span><span>get_model</span><span>().</span><span>unwrap</span><span>(); </span><span> println!(</span><span>"</span><span>{model:?}</span><span>"</span><span>); </span><span>} </span></code></pre><p>This prints out the solution. <code>x</code> equals three. Who would have guessed?</p><pre><code><span>x -&gt; </span><span>3 </span></code></pre><p>The Rust bindings have some nice ergonomics here. You can simply do <code>&amp;x + 4</code> and it would do all the bookkeeping behind closed doors to transform the <code>4</code> (and the <code>7</code>) into an interpreted constant and have them inserted into the internal model.</p><p>The reason you have to pass in a string in <code>new_const</code> is that this is the name given to the variable in <code>smt2</code>. It does not have to be <code>"x"</code>, it can be anything. Why do the bindings not autogenerate the name for you? Who knows.</p><p>If you print the solver (as in <code>println!("{solver:?}");</code>), you will get the following output in the <code>smt2</code>.</p><pre><code><span>(</span><span>declare-fun </span><span>x </span><span>() </span><span>Int</span><span>) </span><span>(</span><span>assert</span><span> (</span><span>=</span><span> (</span><span>+</span><span> x </span><span>4</span><span>) </span><span>7</span><span>)) </span></code></pre><p>Note that the variable you declared is declared as a <em>function</em>. A free constant is basically a function that takes no input and gives an output (here of <del>type</del> sort <code>Int</code>). The solver finds which version of the function satisfies the assertions. This also explains the arrow in <code>x -&gt; 3</code> earlier. <code>x</code> <em>evaluates to</em> 3.</p><hr> <h2>A Jump in Complexity</h2><p>In school, jumping from solving equations with a single variable to equations with two variables was a real jump on complexity. Everything was doubled! Here is a pair of equations we will try to solve next:</p><pre><code><span>x + y = 17 </span><span>y = 2 * x </span></code></pre><p>Here is the program. I am going to print the result of <code>solver.check()</code> first, tho. I just made up those numbers!</p><pre><code><span>use </span><span>z3::{Solver, ast::Int}; </span><span>fn </span><span>main</span><span>() { </span><span>let</span><span> solver </span><span>= </span><span>Solver::new(); </span><span> </span><span>// define the variable. </span><span>let</span><span> x </span><span>= </span><span>Int::new_const(</span><span>"x"</span><span>); </span><span>let</span><span> y </span><span>= </span><span>Int::new_const(</span><span>"y"</span><span>); </span><span> </span><span>// define the equation </span><span> solver.</span><span>assert</span><span>((</span><span>&amp;</span><span>x </span><span>+ &amp;</span><span>y).</span><span>eq</span><span>(</span><span>17</span><span>)); </span><span> solver.</span><span>assert</span><span>((</span><span>&amp;</span><span>x </span><span>* </span><span>2</span><span>).</span><span>eq</span><span>(</span><span>&amp;</span><span>y)); </span><span> println!(</span><span>"</span><span>{solver:?}</span><span>"</span><span>); </span><span> </span><span>let</span><span> c </span><span>=</span><span> solver.</span><span>check</span><span>(); </span><span> println!(</span><span>"; </span><span>{c:?}</span><span>"</span><span>); </span><span>} </span></code></pre><p>This prints out the following:</p><pre><code><span>(</span><span>declare-fun </span><span>y </span><span>() </span><span>Int</span><span>) </span><span>(</span><span>declare-fun </span><span>x </span><span>() </span><span>Int</span><span>) </span><span>(</span><span>assert</span><span> (</span><span>=</span><span> (</span><span>+</span><span> x y) </span><span>17</span><span>)) </span><span>(</span><span>assert</span><span> (</span><span>=</span><span> (</span><span>*</span><span> x </span><span>2</span><span>) y)) </span><span>; Unsat </span></code></pre><p>Oh it is <code>Unsat</code>. Unsatisfiable. Bummer. This means this cannot be solved as defined.</p><p>Let's try changing the type to <code>Real</code>.<sup><a href="https://asibahi.github.io#fn-float">3</a></sup>The <code>Real</code> type does not have the same nice ergonomics as <code>Int</code> apparently, so the code will look slightly uglier. This is the new updated code.</p><pre><code><span>use </span><span>z3::{Solver, ast::Real}; </span><span>fn </span><span>main</span><span>() { </span><span>let</span><span> solver </span><span>= </span><span>Solver::new(); </span><span> </span><span>// define the variable. </span><span>let</span><span> x </span><span>= </span><span>Real::new_const(</span><span>"x"</span><span>); </span><span>let</span><span> y </span><span>= </span><span>Real::new_const(</span><span>"y"</span><span>); </span><span> </span><span>let</span><span> seventeen </span><span>= </span><span>Real::from_rational(</span><span>17</span><span>, </span><span>1</span><span>); </span><span>let</span><span> two </span><span>= </span><span>Real::from_rational(</span><span>2</span><span>, </span><span>1</span><span>); </span><span> </span><span>// define the equation </span><span> solver.</span><span>assert</span><span>((</span><span>&amp;</span><span>x </span><span>+ &amp;</span><span>y).</span><span>eq</span><span>(</span><span>&amp;</span><span>seventeen)); </span><span> solver.</span><span>assert</span><span>((</span><span>&amp;</span><span>x </span><span>* &amp;</span><span>two).</span><span>eq</span><span>(</span><span>&amp;</span><span>y)); </span><span> println!(</span><span>"</span><span>{solver:?}</span><span>"</span><span>); </span><span> </span><span>let</span><span> c </span><span>=</span><span> solver.</span><span>check</span><span>(); </span><span> println!(</span><span>"; </span><span>{c:?}</span><span>"</span><span>); </span><span>} </span></code></pre><p>Which prints</p><pre><code><span>(</span><span>declare-fun </span><span>y </span><span>() </span><span>Real</span><span>) </span><span>(</span><span>declare-fun </span><span>x </span><span>() </span><span>Real</span><span>) </span><span>(</span><span>assert</span><span> (</span><span>=</span><span> (</span><span>+</span><span> x y) </span><span>17</span><span>.</span><span>)) </span><span>(</span><span>assert</span><span> (</span><span>=</span><span> (</span><span>*</span><span> x </span><span>2</span><span>.</span><span>) y)) </span><span>; Sat </span></code></pre><p>Excellent! Using <code>get_model()</code> and printing the model as before gives us the following answer, presented as a nice rational number.</p><pre><code><span>y -&gt; (/ </span><span>34</span><span>.</span><span>0 3</span><span>.</span><span>) </span><span>x -&gt; (/ </span><span>17</span><span>.</span><span>0 3</span><span>.</span><span>) </span></code></pre><p>To actually extract the values programmatically, instead of debug printing <code>model</code>, requires some song and dance with the API, but it is simple, really. This is what it would look like.</p><pre><code><span>// to avoid panicking on unsatisfiable models </span><span>if let </span><span>z3::SatResult::Sat </span><span>=</span><span> solver.</span><span>check</span><span>() { </span><span>let</span><span> model </span><span>=</span><span> solver.</span><span>get_model</span><span>().</span><span>unwrap</span><span>(); </span><span> </span><span>// do not ask me what the `true` is for. I don't know. </span><span>let</span><span> x </span><span>=</span><span> model.</span><span>eval</span><span>(</span><span>&amp;</span><span>x, </span><span>true</span><span>).</span><span>unwrap</span><span>().</span><span>approx_f64</span><span>(); </span><span>let</span><span> y </span><span>=</span><span> model.</span><span>eval</span><span>(</span><span>&amp;</span><span>y, </span><span>true</span><span>).</span><span>unwrap</span><span>().</span><span>approx_f64</span><span>(); </span><span> println!(</span><span>"x: </span><span>{x:.3}\t</span><span>y: </span><span>{y:.3}</span><span>"</span><span>); </span><span>} </span></code></pre><p>Which prints</p><pre><code><span>x: 5.667 y: 11.333 </span></code></pre><p>Nice. Isn't this grand?</p><hr> <h2>Multiple Solutions</h2><p>As I am sure you know from your high school math, some equations have multiple solutions. Here is a simple one.</p><pre><code><span>x * x = 4 </span></code></pre><p>The Rust bindings have a nice method for getting multiple solutions out of a solver, simply called <code>solutions</code>. It works similarly to <code>model.eval()</code> above, and takes the same parameters with the same output. Here is the complete program. (I am going back to <code>Int</code> because I am not cool enough for <code>Real</code> numbers.)</p><pre><code><span>use </span><span>z3::{Solver, ast::Int}; </span><span>fn </span><span>main</span><span>() { </span><span>let</span><span> solver </span><span>= </span><span>Solver::new(); </span><span>let</span><span> x </span><span>= </span><span>Int::new_const(</span><span>"x"</span><span>); </span><span> solver.</span><span>assert</span><span>((</span><span>&amp;</span><span>x </span><span>* &amp;</span><span>x).</span><span>eq</span><span>(</span><span>4</span><span>)); </span><span> println!(</span><span>"</span><span>{solver:?}</span><span>"</span><span>); </span><span> </span><span>// This terminates when `check` does not return `Sat` </span><span>for </span><span>(idx, s) </span><span>in</span><span> solver.</span><span>solutions</span><span>(x, </span><span>true</span><span>).</span><span>enumerate</span><span>() { </span><span>let</span><span> s </span><span>=</span><span> s.</span><span>as_i64</span><span>().</span><span>unwrap</span><span>(); </span><span> println!(</span><span>";</span><span>{}</span><span>:</span><span>\t{s}</span><span>"</span><span>, idx </span><span>+ </span><span>1</span><span>); </span><span> } </span><span>} </span></code></pre><p>Which prints:</p><pre><code><span>(</span><span>declare-fun </span><span>x </span><span>() </span><span>Int</span><span>) </span><span>(</span><span>assert</span><span> (</span><span>=</span><span> (</span><span>*</span><span> x x) </span><span>4</span><span>)) </span><span>;1: -2 </span><span>;2: 2 </span></code></pre><p>I am not clear really on how to get multiple solutions with the regular <code>check</code> followed by <code>get_model</code> method, but this one is easy enough to use. Also, some problems might have infinitely many solutions, so it is advisable to use <code>take</code> with the <code>solutions</code> iterator. To demonstrate, I will use the circle equation.</p><pre><code><span>x * x + y * y = 25 </span></code></pre><p>Here is the straightforward script followed by the printed out result. Note that the <code>fresh_const</code> API creates a unique name for every invocation.</p><pre><code><span>use </span><span>z3::{Solver, ast::Int}; </span><span>fn </span><span>main</span><span>() { </span><span>let</span><span> solver </span><span>= </span><span>Solver::new(); </span><span> </span><span>let</span><span> x </span><span>= </span><span>Int::fresh_const(</span><span>"x"</span><span>); </span><span>let</span><span> y </span><span>= </span><span>Int::fresh_const(</span><span>"x"</span><span>); </span><span> </span><span>let</span><span> area </span><span>= &amp;</span><span>x </span><span>* &amp;</span><span>x </span><span>+ &amp;</span><span>y </span><span>* &amp;</span><span>y; </span><span> solver.</span><span>assert</span><span>(area.</span><span>eq</span><span>(</span><span>25</span><span>)); </span><span> println!(</span><span>"</span><span>{solver:?}</span><span>"</span><span>); </span><span> </span><span>for </span><span>(idx, (x,y)) </span><span>in</span><span> solver.</span><span>solutions</span><span>((x,y), </span><span>true</span><span>).</span><span>enumerate</span><span>() { </span><span>let</span><span> x </span><span>=</span><span> x.</span><span>as_i64</span><span>().</span><span>unwrap</span><span>(); </span><span>let</span><span> y </span><span>=</span><span> y.</span><span>as_i64</span><span>().</span><span>unwrap</span><span>(); </span><span> println!(</span><span>"; </span><span>{}</span><span>:</span><span>\t{x:&gt;2}</span><span>,{y:?2} "</span><span>, idx </span><span>+ </span><span>1</span><span>); </span><span> } </span><span>} </span></code></pre><pre><code><span>(</span><span>declare-fun </span><span>x!1 </span><span>() </span><span>Int</span><span>) </span><span>(</span><span>declare-fun </span><span>x!0 </span><span>() </span><span>Int</span><span>) </span><span>(</span><span>assert</span><span> (</span><span>=</span><span> (</span><span>+</span><span> (</span><span>*</span><span> x!</span><span> x!</span><span>) (</span><span>*</span><span> x!</span><span>1</span><span> x!</span><span>1</span><span>)) </span><span>25</span><span>)) </span><span>; 1: 0, 5 </span><span>; 2: -5, 0 </span><span>; 3: -3,-4 </span><span>; 4: -4,-3 </span><span>; 5: 0,-5 </span><span>; 6: 3,-4 </span><span>; 7: 4,-3 </span><span>; 8: 5, 0 </span><span>; 9: 3, 4 </span><span>; 10: 4, 3 </span><span>; 11: -3, 4 </span><span>; 12: -4, 3 </span></code></pre><p>This goes without saying, but if I used the <code>Real</code> type in this example it would generate infinite solutions.</p><hr> <h2>Coin Change Problem</h2><p>The Coin Change problem is a simple one: given a list of denominations and a total, find the smallest number of coins that add up to said total. Emphasis on <em>smallest</em>. Unlike previous problems, this is an optimization problem. We are looking for a solution that satisfies specific criteria instead of just <em>a</em> solution. Conveniently enough, <code>z3</code> provides an <code>Optimize</code> object which we can use to optimize.</p><p>Let us set up the parameters of the problem in plain language. The denominations we have are 1, 5, and 10. We need to give 37 money in the least amount of coins. This is simple enough that we can know the solution is three 10 coins, one 5 coin, and two 1 coins. Let's see if we can get the same result. As usual, code followed by output:</p><pre><code><span>use </span><span>z3::{Optimize, ast::Int}; </span><span>fn </span><span>main</span><span>() { </span><span>let</span><span> opt </span><span>= </span><span>Optimize::new(); </span><span> </span><span>let</span><span> c1 </span><span>= </span><span>Int::new_const(</span><span>"c1"</span><span>); </span><span>let</span><span> c5 </span><span>= </span><span>Int::new_const(</span><span>"c5"</span><span>); </span><span>let</span><span> c10 </span><span>= </span><span>Int::new_const(</span><span>"c10"</span><span>); </span><span> </span><span>let</span><span> total </span><span>= </span><span>(</span><span>&amp;</span><span>c1 </span><span>* </span><span>1</span><span>) </span><span>+ </span><span>(</span><span>&amp;</span><span>c5 </span><span>* </span><span>5</span><span>) </span><span>+ </span><span>(</span><span>&amp;</span><span>c10 </span><span>* </span><span>10</span><span>); </span><span>let</span><span> count </span><span>= &amp;</span><span>c1 </span><span>+ &amp;</span><span>c5 </span><span>+ &amp;</span><span>c10; </span><span> opt.</span><span>assert</span><span>(</span><span>&amp;</span><span>total.</span><span>eq</span><span>(</span><span>37</span><span>)); </span><span> opt.</span><span>minimize</span><span>(</span><span>&amp;</span><span>count); </span><span> println!(</span><span>"</span><span>{opt:?}</span><span>"</span><span>); </span><span> </span><span>if let </span><span>z3::SatResult::Sat </span><span>=</span><span> opt.</span><span>check</span><span>(</span><span>&amp;</span><span>[]) { </span><span>let</span><span> model </span><span>=</span><span> opt.</span><span>get_model</span><span>().</span><span>unwrap</span><span>(); </span><span>let</span><span> c1 </span><span>=</span><span> model.</span><span>eval</span><span>(</span><span>&amp;</span><span>c1, </span><span>true</span><span>).</span><span>unwrap</span><span>(); </span><span>let</span><span> c5 </span><span>=</span><span> model.</span><span>eval</span><span>(</span><span>&amp;</span><span>c5, </span><span>true</span><span>).</span><span>unwrap</span><span>(); </span><span>let</span><span> c10 </span><span>=</span><span> model.</span><span>eval</span><span>(</span><span>&amp;</span><span>c10, </span><span>true</span><span>).</span><span>unwrap</span><span>(); </span><span> println!(</span><span>"; c1: </span><span>{c1:?}</span><span>, c5: </span><span>{c5:?}</span><span>, c10: </span><span>{c10:?}</span><span>"</span><span>); </span><span> } </span><span>else </span><span>{ </span><span> println!(</span><span>"; woe for us"</span><span>); </span><span> } </span><span>} </span></code></pre><pre><code><span>(</span><span>declare-fun </span><span>c10 </span><span>() </span><span>Int</span><span>) </span><span>(</span><span>declare-fun </span><span>c5 </span><span>() </span><span>Int</span><span>) </span><span>(</span><span>declare-fun </span><span>c1 </span><span>() </span><span>Int</span><span>) </span><span>(</span><span>assert</span><span> (</span><span>=</span><span> (</span><span>+</span><span> (</span><span>*</span><span> c1 </span><span>1</span><span>) (</span><span>*</span><span> c5 </span><span>5</span><span>) (</span><span>*</span><span> c10 </span><span>10</span><span>)) </span><span>37</span><span>)) </span><span>(minimize (</span><span>+</span><span> c1 c5 c10)) </span><span>(</span><span>check-sat</span><span>) </span><span>; c1: 37, c5: 0, c10: 0 </span></code></pre><p>Oops. This cannot be right.</p><p>I do not really understand why the answer is so nonsensical here. The problem is that <code>Int</code> really spans the entire natural integers range, so it is accounting for negative amounts of coins. This still does not explain how the optimal solution given is 37 coins. (If you can explain, please let me know.)</p><p>The solution for this<sup><a href="https://asibahi.github.io#fn-mystery">4</a></sup>is to constrain the amount of coins to be non-negative. So let's do that. Add these assertions somewhere before <code>check</code>, and Bob's your uncle.</p><pre><code><span>opt.</span><span>assert</span><span>(</span><span>&amp;</span><span>c1.</span><span>ge</span><span>(</span><span>)); </span><span>opt.</span><span>assert</span><span>(</span><span>&amp;</span><span>c5.</span><span>ge</span><span>(</span><span>)); </span><span>opt.</span><span>assert</span><span>(</span><span>&amp;</span><span>c10.</span><span>ge</span><span>(</span><span>)); </span></code></pre><pre><code><span>(</span><span>declare-fun </span><span>c1 </span><span>() </span><span>Int</span><span>) </span><span>(</span><span>declare-fun </span><span>c5 </span><span>() </span><span>Int</span><span>) </span><span>(</span><span>declare-fun </span><span>c10 </span><span>() </span><span>Int</span><span>) </span><span>(</span><span>assert</span><span> (</span><span>&gt;=</span><span> c1 </span><span>)) </span><span>(</span><span>assert</span><span> (</span><span>&gt;=</span><span> c5 </span><span>)) </span><span>(</span><span>assert</span><span> (</span><span>&gt;=</span><span> c10 </span><span>)) </span><span>(</span><span>assert</span><span> (</span><span>=</span><span> (</span><span>+</span><span> (</span><span>*</span><span> c1 </span><span>1</span><span>) (</span><span>*</span><span> c5 </span><span>5</span><span>) (</span><span>*</span><span> c10 </span><span>10</span><span>)) </span><span>37</span><span>)) </span><span>(minimize (</span><span>+</span><span> c1 c5 c10)) </span><span>(</span><span>check-sat</span><span>) </span><span>; c1: 2, c5: 1, c10: 3 </span></code></pre><p>That's more like it. Now let's try with different denominations. Something like 10. 9, and 1. Note that the optimal solution for 37 would be: one 10 coin, three 9 coins, and no 1 coins. The greedy solution would fail to catch that. Here is the output of printing the optimizer and the result.</p><pre><code><span>(</span><span>declare-fun </span><span>c1 </span><span>() </span><span>Int</span><span>) </span><span>(</span><span>declare-fun </span><span>c9 </span><span>() </span><span>Int</span><span>) </span><span>(</span><span>declare-fun </span><span>c10 </span><span>() </span><span>Int</span><span>) </span><span>(</span><span>assert</span><span> (</span><span>&gt;=</span><span> c1 </span><span>)) </span><span>(</span><span>assert</span><span> (</span><span>&gt;=</span><span> c9 </span><span>)) </span><span>(</span><span>assert</span><span> (</span><span>&gt;=</span><span> c10 </span><span>)) </span><span>(</span><span>assert</span><span> (</span><span>=</span><span> (</span><span>+</span><span> (</span><span>*</span><span> c1 </span><span>1</span><span>) (</span><span>*</span><span> c9 </span><span>9</span><span>) (</span><span>*</span><span> c10 </span><span>10</span><span>)) </span><span>37</span><span>)) </span><span>(minimize (</span><span>+</span><span> c1 c9 c10)) </span><span>(</span><span>check-sat</span><span>) </span><span>; c1: 0, c9: 3, c10: 1 </span></code></pre><p>Success!</p><h2><code>push</code> and <code>pop</code></h2><p>Currently, the total 37 is hardcoded. But what if I want the answers for a number of different totals? Thankfully, you do not need to build the optimizer from scratch for every total. Instead, use the magical functions <code>push</code> and <code>pop</code>. The first one essentially creates a bookmark in the stack of assertions. The second removes everything above said bookmark, and the bookmark. It is simple really. Here are the solutions from 30 to 39, because why not.</p><p>Here is the full <code>main</code>. I will spare you the output.</p><pre><code><span>let</span><span> opt </span><span>= </span><span>Optimize::new(); </span><span>let</span><span> c1 </span><span>= </span><span>Int::new_const(</span><span>"c1"</span><span>); </span><span>let</span><span> c9 </span><span>= </span><span>Int::new_const(</span><span>"c9"</span><span>); </span><span>let</span><span> c10 </span><span>= </span><span>Int::new_const(</span><span>"c10"</span><span>); </span><span>opt.</span><span>assert</span><span>(</span><span>&amp;</span><span>c1.</span><span>ge</span><span>(</span><span>)); </span><span>opt.</span><span>assert</span><span>(</span><span>&amp;</span><span>c9.</span><span>ge</span><span>(</span><span>)); </span><span>opt.</span><span>assert</span><span>(</span><span>&amp;</span><span>c10.</span><span>ge</span><span>(</span><span>)); </span><span>let</span><span> total </span><span>= </span><span>(</span><span>&amp;</span><span>c1 </span><span>* </span><span>1</span><span>) </span><span>+ </span><span>(</span><span>&amp;</span><span>c9 </span><span>* </span><span>9</span><span>) </span><span>+ </span><span>(</span><span>&amp;</span><span>c10 </span><span>* </span><span>10</span><span>); </span><span>let</span><span> count </span><span>= &amp;</span><span>c1 </span><span>+ &amp;</span><span>c9 </span><span>+ &amp;</span><span>c10; </span><span>opt.</span><span>minimize</span><span>(</span><span>&amp;</span><span>count); </span><span>println!(</span><span>"; total</span><span>\t</span><span>count</span><span>\t</span><span>c1</span><span>\t</span><span>c9</span><span>\t</span><span>c10"</span><span>); </span><span>for</span><span> t </span><span>in </span><span>(</span><span>30</span><span>u32..</span><span>).</span><span>take</span><span>(</span><span>10</span><span>) { </span><span> print!(</span><span>"; </span><span>{t}\t</span><span>"</span><span>); </span><span> opt.</span><span>push</span><span>(); </span><span> opt.</span><span>assert</span><span>(</span><span>&amp;</span><span>total.</span><span>eq</span><span>(t)); </span><span> </span><span>if let </span><span>SatResult::Sat </span><span>=</span><span> opt.</span><span>check</span><span>(</span><span>&amp;</span><span>[]) { </span><span>let</span><span> model </span><span>=</span><span> opt.</span><span>get_model</span><span>().</span><span>unwrap</span><span>(); </span><span>let</span><span> c1 </span><span>=</span><span> model.</span><span>eval</span><span>(</span><span>&amp;</span><span>c1, </span><span>true</span><span>).</span><span>unwrap</span><span>().</span><span>as_u64</span><span>().</span><span>unwrap</span><span>(); </span><span>let</span><span> c9 </span><span>=</span><span> model.</span><span>eval</span><span>(</span><span>&amp;</span><span>c9, </span><span>true</span><span>).</span><span>unwrap</span><span>().</span><span>as_u64</span><span>().</span><span>unwrap</span><span>(); </span><span>let</span><span> c10 </span><span>=</span><span> model.</span><span>eval</span><span>(</span><span>&amp;</span><span>c10, </span><span>true</span><span>).</span><span>unwrap</span><span>().</span><span>as_u64</span><span>().</span><span>unwrap</span><span>(); </span><span> </span><span>let</span><span> count </span><span>=</span><span> c1 </span><span>+</span><span> c9 </span><span>+</span><span> c10; </span><span> println!(</span><span>"</span><span>{count}\t{c1:?}\t{c9:?}\t{c10:?}</span><span>"</span><span>); </span><span> } </span><span>else </span><span>{ </span><span> println!(</span><span>"; woe for us"</span><span>); </span><span> } </span><span> opt.</span><span>pop</span><span>(); </span><span>// no RAII for you </span><span>} </span></code></pre><p>Note that the <code>push</code> and <code>pop</code> API is available for <code>Solver</code> as well. At any rate, back to solving.</p><hr> <h2>Sudoku</h2><p>This is a significant jump in complexity, so bear with me. We are going to solve a Sudoku.<sup><a href="https://asibahi.github.io#fn-js">5</a></sup>So let's write the constraints first. We can use Rust's arrays or <code>Vec</code> to organize our <code>z3.Int</code>s and check their constraints. First, this is the puzzle we are solving:</p><pre><code><span>....94.3. </span><span>...51...7 </span><span>.89....4. </span><span>......2.8 </span><span>.6.2.1.5. </span><span>1.2...... </span><span>.7....52. </span><span>9...65... </span><span>.4.97.... </span></code></pre><p>I will forgo the steps to turn that into a <code>[[Option<u8>;9];9]</u8></code>. Instead the code below will get that info from a <code>get_puzzle()</code> function.</p><pre><code><span>let</span><span> solver </span><span>= </span><span>Solver::new(); </span><span>// Note that we're using Rust arrays here. The solver does not really know about them. </span><span>let</span><span> grid: [[</span><span>_</span><span>; </span><span>9</span><span>]; </span><span>9</span><span>] </span><span>= </span><span> array::from_fn(|i| array::from_fn(|j| Int::new_const(format!(</span><span>"x</span><span>{i}{j}</span><span>"</span><span>)))); </span><span>// each cell contains a value 1&lt;=x&lt;=9 </span><span>grid.</span><span>iter</span><span>().</span><span>flatten</span><span>().</span><span>for_each</span><span>(|i| { </span><span> solver.</span><span>assert</span><span>(i.</span><span>ge</span><span>(</span><span>1</span><span>) </span><span>&amp;</span><span> i.</span><span>le</span><span>(</span><span>9</span><span>)); </span><span>}); </span><span>// each row contains a digit only once </span><span>for</span><span> row </span><span>in &amp;</span><span>grid { </span><span> solver.</span><span>assert</span><span>(Ast::distinct(row)); </span><span>} </span><span>// each column contains a digit only once </span><span>for</span><span> idx </span><span>in </span><span>..</span><span>9 </span><span>{ </span><span>let mut</span><span> col </span><span>= </span><span>Vec</span><span>::with_capacity(</span><span>9</span><span>); </span><span> grid.</span><span>iter</span><span>().</span><span>for_each</span><span>(|r| col.</span><span>push</span><span>(r[idx].</span><span>clone</span><span>())); </span><span> solver.</span><span>assert</span><span>(Ast::distinct(</span><span>&amp;</span><span>col)) </span><span>} </span><span>// each 3x3 contains a digit at most once </span><span>for</span><span> x_s </span><span>in </span><span>(</span><span>..</span><span>9</span><span>).</span><span>step_by</span><span>(</span><span>3</span><span>) { </span><span>for</span><span> y_s </span><span>in </span><span>(</span><span>..</span><span>9</span><span>).</span><span>step_by</span><span>(</span><span>3</span><span>) { </span><span>let mut</span><span> square </span><span>= </span><span>Vec</span><span>::with_capacity(</span><span>9</span><span>); </span><span>for</span><span> x </span><span>in </span><span>(x_s</span><span>..</span><span>).</span><span>take</span><span>(</span><span>3</span><span>) { </span><span>for</span><span> y </span><span>in </span><span>(y_s</span><span>..</span><span>).</span><span>take</span><span>(</span><span>3</span><span>) { </span><span>// very nested loop </span><span> square.</span><span>push</span><span>(grid[x][y].</span><span>clone</span><span>()); </span><span> } </span><span> } </span><span> solver.</span><span>assert</span><span>(Ast::distinct(</span><span>&amp;</span><span>square)) </span><span> } </span><span>} </span><span>// Finally, assert that each cell equals a provided clue in the given puzzle </span><span>get_puzzle</span><span>().</span><span>iter</span><span>().</span><span>flatten</span><span>().</span><span>zip</span><span>(grid.</span><span>iter</span><span>().</span><span>flatten</span><span>()).</span><span>for_each</span><span>(|(clue, variable)| { </span><span>if let </span><span>Some</span><span>(clue) </span><span>=</span><span> clue { </span><span> solver.</span><span>assert</span><span>(variable.</span><span>eq</span><span>(</span><span>*</span><span>clue)); </span><span> } </span><span>}); </span><span>eprintln!(</span><span>"</span><span>{solver:?}</span><span>"</span><span>); </span></code></pre><p>Printing the solver after each step lets you debug whether you have your constraints correctly. The printout is over 200 lines long, so let's skip that. All we have to do next is to check the value of each cell in <code>grid</code>.</p><pre><code><span>if let </span><span>SatResult::Sat </span><span>=</span><span> solver.</span><span>check</span><span>() { </span><span>let</span><span> model </span><span>=</span><span> solver.</span><span>get_model</span><span>().</span><span>unwrap</span><span>(); </span><span>for</span><span> row </span><span>in</span><span> grid { </span><span>for</span><span> int </span><span>in</span><span> row { </span><span>let</span><span> result </span><span>=</span><span> model.</span><span>eval</span><span>(</span><span>&amp;</span><span>int, </span><span>true</span><span>).</span><span>unwrap</span><span>(); </span><span> print!(</span><span>"</span><span>{result:?}</span><span>"</span><span>); </span><span> } </span><span> println!(); </span><span> } </span><span>} </span><span>else </span><span>{ </span><span> println!(</span><span>"Unsolvable"</span><span>) </span><span>} </span></code></pre><p>And this prints out the result. You can verify for yourself whether this is correct or not. Maybe try other puzzles. Or add more constraints. <a href="https://www.youtube.com/watch?v=yKf9aUIxdb4">You can even try the Miracle Sudoku</a>.</p><pre><code><span>715894632 </span><span>234516897 </span><span>689723145 </span><span>493657218 </span><span>867231954 </span><span>152489763 </span><span>376148529 </span><span>928365471 </span><span>541972386 </span></code></pre><p>One thing of note here: which is how <em>dumb</em> the solver is. Note that if you print out the solver, there is no notion of rows and columns and squares. It does not know any Sudoku tricks like X-wings and what have you. All the data is organized on the Rust side of things, and what is given to the solver is "these two variables cannot be the same" over and over and over again. And it just .. tells you what the rest of them are.</p><p>Another thing that is not obvious at first glance, is that it does not check if there is a unique solution. The puzzle may be badly constructed and have multiple solutions, and it will happily give you one, or two, or how many you ask for. It does, however, check if it is unsolvable!</p><hr> <h2>Page Layout</h2><p>One of the famous examples of using solvers in production is .. layouting. You have a number of elements and you want to arrange them on a page, or a browser window, or whatever. So let's do a rudimentary version of that.<sup><a href="https://asibahi.github.io#fn-simon">6</a></sup></p><p>The page we are layouting has an arbitrary size of 190mm width by 270mm tall. We are to put three boxes on the page of varying sizes and rules. I am just spitballing the sizes here: first box is 105mm by 140mm; second is 85 by 135, third is 120 by 110. They should not overlap, and like .. that's it?</p><pre><code><span>let</span><span> page_width </span><span>= </span><span>190</span><span>; </span><span>let</span><span> page_height </span><span>= </span><span>270</span><span>; </span><span>let</span><span> solver </span><span>= </span><span>Solver::new(); </span><span>let</span><span> box_dims </span><span>= </span><span>[(</span><span>105</span><span>, </span><span>140</span><span>), (</span><span>85</span><span>, </span><span>135</span><span>), (</span><span>120</span><span>, </span><span>110</span><span>)]; </span><span>let</span><span> box_locs </span><span>=</span><span> box_dims.</span><span>map</span><span>(|b| { </span><span>let</span><span> left </span><span>= </span><span>Int::fresh_const(</span><span>"x"</span><span>); </span><span>let</span><span> top </span><span>= </span><span>Int::fresh_const(</span><span>"y"</span><span>); </span><span> </span><span>let</span><span> right </span><span>= &amp;</span><span>left </span><span>+</span><span> b.</span><span>; </span><span>let</span><span> bottom </span><span>= &amp;</span><span>top </span><span>+</span><span> b.</span><span>1</span><span>; </span><span> </span><span>// assert the boxes fit within the page </span><span> solver.</span><span>assert</span><span>(</span><span>&amp;</span><span>left.</span><span>ge</span><span>(</span><span>)); </span><span> solver.</span><span>assert</span><span>(</span><span>&amp;</span><span>top.</span><span>ge</span><span>(</span><span>)); </span><span> solver.</span><span>assert</span><span>(</span><span>&amp;</span><span>right.</span><span>le</span><span>(page_width)); </span><span> solver.</span><span>assert</span><span>(</span><span>&amp;</span><span>bottom.</span><span>le</span><span>(page_height)); </span><span> </span><span>// assert each box aligns to lines. line height is 10mm </span><span> solver.</span><span>assert</span><span>(</span><span>&amp;</span><span>bottom.</span><span>rem</span><span>(</span><span>10</span><span>).</span><span>eq</span><span>(</span><span>)); </span><span> (left, top, right, bottom) </span><span>}); </span><span>// assert boxes do not overlap </span><span>for</span><span> i </span><span>in </span><span>..</span><span>box_locs.</span><span>len</span><span>() { </span><span>for</span><span> j </span><span>in</span><span> i </span><span>+ </span><span>1</span><span>..</span><span>box_locs.</span><span>len</span><span>() { </span><span>let</span><span> fst_box </span><span>= &amp;</span><span>box_locs[i]; </span><span>let</span><span> snd_box </span><span>= &amp;</span><span>box_locs[j]; </span><span>// tuple structure: (left, top, right, bottom) </span><span>let</span><span> cond_1 </span><span>=</span><span> fst_box.</span><span>0.</span><span>ge</span><span>(</span><span>&amp;</span><span>snd_box.</span><span>2</span><span>); </span><span>// fst left &gt;= snd right </span><span>let</span><span> cond_2 </span><span>=</span><span> fst_box.</span><span>1.</span><span>ge</span><span>(</span><span>&amp;</span><span>snd_box.</span><span>3</span><span>); </span><span>// fst top &gt;= snd bottom </span><span>let</span><span> cond_3 </span><span>=</span><span> fst_box.</span><span>2.</span><span>le</span><span>(</span><span>&amp;</span><span>snd_box.</span><span>); </span><span>// fst right &lt;= snd left </span><span>let</span><span> cond_4 </span><span>=</span><span> fst_box.</span><span>3.</span><span>le</span><span>(</span><span>&amp;</span><span>snd_box.</span><span>1</span><span>); </span><span>// fst bottom &lt;= snd top </span><span> solver.</span><span>assert</span><span>(Bool::or(</span><span>&amp;</span><span>[cond_1, cond_2, cond_3, cond_4])); </span><span> } </span><span>} </span><span>// println!("{solver:?}"); </span><span>if let </span><span>SatResult::Sat </span><span>=</span><span> solver.</span><span>check</span><span>() { </span><span>let</span><span> model </span><span>=</span><span> solver.</span><span>get_model</span><span>().</span><span>unwrap</span><span>(); </span><span> </span><span>for </span><span>(idx, b) </span><span>in</span><span> box_locs.</span><span>into_iter</span><span>().</span><span>enumerate</span><span>() { </span><span> print!(</span><span>"box </span><span>{}\t</span><span>"</span><span>, idx </span><span>+ </span><span>1</span><span>); </span><span>let</span><span> left </span><span>=</span><span> model.</span><span>eval</span><span>(</span><span>&amp;</span><span>b.</span><span>, </span><span>true</span><span>).</span><span>unwrap</span><span>(); </span><span>let</span><span> top </span><span>=</span><span> model.</span><span>eval</span><span>(</span><span>&amp;</span><span>b.</span><span>1</span><span>, </span><span>true</span><span>).</span><span>unwrap</span><span>(); </span><span>let</span><span> right </span><span>=</span><span> model.</span><span>eval</span><span>(</span><span>&amp;</span><span>b.</span><span>2</span><span>, </span><span>true</span><span>).</span><span>unwrap</span><span>(); </span><span>let</span><span> bottom </span><span>=</span><span> model.</span><span>eval</span><span>(</span><span>&amp;</span><span>b.</span><span>3</span><span>, </span><span>true</span><span>).</span><span>unwrap</span><span>(); </span><span> println!(</span><span>"</span><span>{left}</span><span>, </span><span>{top}</span><span>, </span><span>{right}</span><span>, </span><span>{bottom}</span><span>"</span><span>); </span><span> } </span><span>} </span><span>else </span><span>{ </span><span> println!(</span><span>"Unsolvable"</span><span>); </span><span>} </span></code></pre><p>This prints out this neat solution:</p><pre><code><span>box 1 85, 0, 190, 140 </span><span>box 2 0, 5, 85, 140 </span><span>box 3 0, 140, 120, 250 </span></code></pre><p>Turned out easier than I thought. Hah.</p><hr> <h2>Epilogue</h2><p>Obviously, all these examples are rather simple. Figuring out how to model the problem in the form of boolean rules and constraints is almost all the challenge. There are some limitations, too: <code>z3</code> cannot solve equations of the sort <code>2^x == 3</code>; it cannot call external functions to get values (though there are ways to work around that).</p><p>There is plenty of stuff I have not shown. <code>Array</code>s, which are nothing like programming language arrays and more mappings from one domain to another. <code>BV</code>, bit vectors, which allow bitwise operations on their values like <code>and</code> and <code>or</code> and bit shifting. (Which are the key to solving <a href="https://asibahi.github.io/thoughts/the-hanging-gardens-problem/">the Hanging Gardens Problem</a>.) <code>Set</code>s and <code>Seq</code>s and <code>String</code>s and regexes and stuff I have not really looked into. Unfortunately, most resources on the web are a bit heavy on theory, and are not targeted to stupid coders like myself.</p><p>Until later.</p><hr> </div></section>]]></description><pubDate>Mon, 15 Sep 2025 02:49:17 +0530</pubDate></item><item><link>https://newsletter.scalablethread.com/p/why-event-driven-systems-are-hard</link><title>Why Event-Driven Systems are Hard? (newsletter.scalablethread.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1ngwj0l/why_eventdriven_systems_are_hard/</guid><comments>https://www.reddit.com/r/programming/comments/1ngwj0l/why_eventdriven_systems_are_hard/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 21 min | <a href='https://www.reddit.com/r/programming/comments/1ngwj0l/why_eventdriven_systems_are_hard/'>Post permalink</a></p></section><section class='preview-image'><img src='https://substackcdn.com/image/fetch/$s_!yqHc!,w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f23e44c-2a3c-416c-82ed-55a143001014_1456x1048.png' /></section><section class='parsed-content'><div><p><span>An </span><em>event</em><span> is just a small message that says, "Hey, something happened!" For example, </span><code>UserClickedButton</code><span>, </span><code>PaymentProcessed</code><span>, or </span><code>NewOrderPlaced</code><span>. Services subscribe to the events they care about and react accordingly. This event-driven approach makes systems resilient and flexible. However, building and managing these systems at a large scale is surprisingly hard.</span></p><p>Imagine you and your friend have a secret code to pass notes. One day, you decide to add a new symbol to the code to mean something new. If you start using it without telling your friend, your new notes will confuse them. This is exactly what happens in event-driven systems.</p><p><span>For example, an </span><code>OrderPlaced</code><span> event might look like this:</span></p><p><span>Now imagine another service reads this event to send a confirmation email. Then, six months later, you add a new field: </span><code>shippingAddress</code><span>. You update the producer. The event becomes:</span></p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!rBfd!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee987fa9-a617-4f5a-83dc-285b7cc3abbe_476x194.png"></a></figure></div><p><span>The problem is that other services, like the </span><code>OrderConfirmationEmailService</code><span>, might still be expecting the old version 1 format. When they receive this new message, they won't know what to do with the </span><code>shippingAddress </code><span>field. Worse, if a field they relied on was removed, they would simply crash.</span></p><p>This forces teams to carefully manage how schemas evolve. Common strategies include:</p><ul><li><p><strong>Backward Compatibility:</strong><span> New schemas can be read by services expecting the old schema. This usually means you can only add new, optional fields. You can't rename or remove existing ones.</span></p></li><li><p><strong>Forward Compatibility:</strong><span> Services expecting a new schema can still read messages written in an old one. This is harder to achieve and often requires setting default values for missing fields.</span></p></li><li><p><strong>Schema Registry:</strong><span> This is like a central dictionary for all your event "secret codes." Before a service sends a message, it checks with the registry to make sure the format is valid and compatible. It prevents services from sending out "confusing notes."</span></p></li></ul><p>Without strict rules for changing message formats, a simple update can cause a cascade of failures throughout a large system.</p><p>In a traditional, non-event-driven system, when a user clicks a button, one piece of code calls another, which calls another, in a straight line. If something breaks, you can look at the error log and see the entire sequence of calls, like following a single piece of string from start to finish.</p><p><span>In an event-driven system, that single string is cut into dozens of tiny pieces. The </span><code>OrderService</code><span> publishes an </span><code>OrderPlaced</code><span> event. The </span><code>PaymentService</code><span>, </span><code>ShippingService</code><span>, and </span><code>NotificationService</code><span> all pick it up and do their own work independently. They might, in turn, publish their own events.</span></p><p>Now, imagine a customer calls saying they placed an order but never got a confirmation email. Where did it go wrong?</p><ul><li><p><span>Did the </span><code>OrderService</code><span> fail to publish the event?</span></p></li><li><p><span>Did the </span><code>NotificationService</code><span> not receive it?</span></p></li><li><p>Did it receive the event but fail to connect to the email server?</p></li></ul><p>Debugging this can be difficult as you can't see the whole picture at once.</p><p><span>To solve this, we use </span><strong>distributed tracing</strong><span>. When the very first event is created, we attach a unique ID to it, called a </span><strong>Correlation ID</strong><span>. Every service that processes this event or creates a new event as a result must copy that same ID onto its own work.</span></p><p>When you need to investigate a problem, you can search for this one correlation ID across all the logs of all your services. This allows you to stitch the story back together and see the journey of that single request across the entire distributed system.</p><p>Events can disappear. Not because of bugs &mdash; because of infrastructure issues like network failure, a service crashing, or the message broker itself having a problem.</p><p><span>The core promise of many event systems is </span><a href="https://newsletter.scalablethread.com/i/146810780/at-least-once-guarantee">at-least-once delivery</a><span>. This means the system will do everything it can to make sure your event gets delivered. If a service that is supposed to receive an event is temporarily down, the message broker will hold onto the message and try again later.</span></p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!sGmD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc173afd2-6a2e-414d-a7ed-8af50160ab18_3183x2926.png"></a></figure></div><p><span>But what if a service has a persistent bug and crashes every time it tries to process a specific message? The broker will keep trying to redeliver it, and the service will keep crashing, until the broker retry limit is reached. To handle this, we use a </span><a href="https://newsletter.scalablethread.com/i/152780978/using-dead-letter-queues">Dead-Letter Queue (DLQ)</a><span>. After a few failed delivery attempts, the message broker moves the crash causing message to the DLQ. This stops the cycle of crashing and allows the service to continue processing other, valid messages. Engineers can then inspect the DLQ later to debug the problematic message.</span></p><p>The guarantee of "at-least-once delivery" creates a new, tricky problem: what if a message is delivered more than once? This can happen if a service processes an event but crashes before it can tell the message broker, "I'm done!" The broker, thinking the message was never handled, will deliver it again when the service restarts.</p><p><span>If the event was </span><code>IncreaseItemCountInCart</code><span>, receiving it twice is a big problem. The customer who wanted one item now has two in their cart. If it was </span><code>ChargeCreditCard</code><span>, they get charged twice.</span></p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!BlHa!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7318f031-5ce1-43d4-870a-dcd0a94748ec_3038x1084.png"></a></figure></div><p><span>To prevent this, services must be </span><a href="https://newsletter.scalablethread.com/p/how-to-build-idempotent-apis">idempotent</a><span>. We can achieve idempotency by having the service keep a record of the event IDs it has already processed. When a new event comes in, the service first checks its records.</span></p><ol><li><p>Has it seen this event ID before?</p></li><li><p>If yes, it simply ignores the duplicate and tells the broker, "Yep, I'm done."</p></li><li><p>If no, it processes the event and then saves the event ID to its records before telling the broker it's done.</p></li></ol><p>This ensures that even if a message is delivered 100 times, the action is only performed once.</p><p><span>In a simple application with one database, when you write data, it's there instantly. If you change your shipping address, the very next screen you load will show the new address. This is called </span><a href="https://newsletter.scalablethread.com/i/146489166/strict-consistency-model">strong consistency</a><span>.</span></p><p><span>Event-driven systems give up this guarantee for the sake of scalability and resilience. They operate on a model of </span><a href="https://newsletter.scalablethread.com/i/146489166/eventual-consistency-model">eventual consistency</a><span>. For example, when a user updates their address, the </span><code>CustomerService</code><span> updates its own database and publishes an </span><code>AddressUpdated</code><span> event. The </span><code>ShippingService</code><span> and </span><code>BillingService</code><span> subscribe to this event. But it might take a few hundred milliseconds for them to receive the event and update their own data (This example is to provide some context, but ideally, the address should be stored at one place and the id of that record should be passed around in the events).</span></p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!gE2i!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ec02ee-bcdf-4832-8ccc-0adfa1aec02b_2812x2100.png"></a></figure></div><p>Designing for eventual consistency means the system must be built to handle this temporary state of disagreement. This might involve:</p><ul><li><p>Designing user interfaces that account for the delay.</p></li><li><p>Adding logic to services to double-check critical data if needed.</p></li><li><p>Accepting that for some non-critical data, a small delay is acceptable.</p></li></ul><p><em>If you enjoyed this article, please hit the &#10084;&#65039; like button.</em></p><p><em>If you think someone else will benefit from this, please &#128257; share this post.</em></p></div><div class="gallery"><p><img src="https://substackcdn.com/image/fetch/$s_!H04Y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec4178cf-ceb4-41b5-984a-39babd83c5d5_447x174.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!rBfd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee987fa9-a617-4f5a-83dc-285b7cc3abbe_476x194.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!sGmD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc173afd2-6a2e-414d-a7ed-8af50160ab18_3183x2926.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!BlHa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7318f031-5ce1-43d4-870a-dcd0a94748ec_3038x1084.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!gE2i!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ec02ee-bcdf-4832-8ccc-0adfa1aec02b_2812x2100.png"></p></div></section>]]></description><pubDate>Sun, 14 Sep 2025 22:28:39 +0530</pubDate></item><item><link>https://strategizeyourcareer.com/p/how-software-engineers-make-productive-decisions</link><title>How Software Engineers Make Productive Decisions (without slowing the team down) (strategizeyourcareer.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nglax1/how_software_engineers_make_productive_decisions/</guid><comments>https://www.reddit.com/r/programming/comments/1nglax1/how_software_engineers_make_productive_decisions/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 25 min | <a href='https://www.reddit.com/r/programming/comments/1nglax1/how_software_engineers_make_productive_decisions/'>Post permalink</a></p></section><section class='preview-image'><img src='https://substackcdn.com/image/fetch/$s_!mn9w!,w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74bbd26c-5055-4674-b33d-e18156d02bea_4813x3950.png' /></section><section class='parsed-content'><div><p>Most teams don&rsquo;t get stuck because problems are impossible. They get stuck because every choice is treated like it&rsquo;s irreversible. In reality, lots of calls are two-way doors: you can walk through, check the room, and walk back out. Save the caution for the true one-way doors: data migrations, security posture, customer-visible changes with real blast radius.</p><p>When I&rsquo;m unsure, I run a fast, risk-aware filter. If the downside is small, the change is reversible, or I can mitigate quickly, I ship with guardrails. That&rsquo;s how you move fast without being sloppy.</p><p><strong>&#11088; In this post, you'll learn:</strong></p><ul><li><p>How to tell if a decision is reversible or not</p></li><li><p>The 3 questions I ask before slowing down</p></li><li><p>How to move fast without being sloppy</p></li><li><p>Why speed compounds into career growth</p></li></ul><p>Not every door leads to a cliff, some just swing back open. Two-way doors are things like toggling a feature flag, shipping a non-consumed response field, or swapping an internal library behind an abstraction. If it goes sideways, you flip the switch or roll back.</p><p>One-way doors are different. Think data migrations, schema changes, or decisions that can silently corrupt data or take a core service down. At my job, when a migration touches your database and could risk data loss, I&rsquo;d slow down on purpose: rehearsal in non-prod environments, snapshot plans, read-only windows if needed, and crisp rollback playbooks.</p><p>The productivity benefit is knowing the difference before you start. Over-investing in reversible decisions burns time and morale. Under-investing in high-stakes calls burns trust and customer goodwill.</p><p>When a decision lands on your lap, take 1-2 minutes and ask:</p><p>Is the effect invisible, annoying, or catastrophic? User-visible errors, security regressions, and data integrity issues are &ldquo;slow-down&rdquo; territory. On the other hand, shipping a field the client doesn&rsquo;t yet consume is low risk. I&rsquo;ve green-lit rollouts like this with smoke tests + feature flag, skipping a day or two of heavy testing because there was effectively no customer impact and rollback was trivial.</p><p>Reversal options change everything. If I can roll back in ~10 minutes because I have alarms, canary checks, and a pre-wired rollback, I bias toward speed. When reversal is painful (e.g., a destructive migration), I do design notes, peer review, and a rehearsal.</p><p>Sometimes you can&rsquo;t prevent every issue, but you can limit the blast radius. Canaries, partial rollouts, and scoped feature flags mean we learn quickly without harming many users. A line I actually use with stakeholders:</p><p>&ldquo;Do we need to focus on prevention here, or can we move forward and mitigate fast with a small blast radius if something is wrong? If mitigation is fast and contained, let&rsquo;s go.&rdquo;</p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!4ove!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe27276b0-b814-4dd0-8330-adb762162b0b_865x401.png"></a></figure></div><p>If your situation is somewhere in the middle, pick the stricter option to err on the safe side.</p><p>Flags are the safest way to keep a single-branch mainline in production. Merge early, merge often, even incomplete work, because the flag hides it. That enables smaller PRs, faster reviews, and quicker rollback. In my experience, for many features, the client hasn&rsquo;t started working on the changes on their end, which makes these deployments extremely low risk because they aren&rsquo;t consuming your new changes yet.</p><p><strong>Checklist for feature flags:</strong></p><ul><li><p>Default-off flag per risk domain (UI, backend path, integration).</p></li><li><p>One-line rationale in the PR (&ldquo;why now, why safe&rdquo;).</p></li><li><p>Smoke tests for both on/off states.</p></li><li><p>Exit plan: when and how to delete the flag.</p></li></ul><p>Even without a feature flag, speed is safe if your observability and rollback are tight:</p><ol><li><p><strong>Before:</strong><span> canary tests succeeding, metrics emitted.</span></p></li><li><p><strong>After:</strong><span> alarms on errors, latency, saturation, and key business metrics.</span></p></li><li><p><strong>Abort:</strong><span> scripted rollback (or deploy previous artifact) within ~10 minutes.</span></p></li></ol><p>I&rsquo;ve shipped features knowing that if anything trips alarms, the change is reverted quickly. That confidence changes the cost/benefit calculus.</p><ul><li><p>Timebox reversible decisions to 30-60 minutes of research. Make a call, document trade-offs, and move.</p></li><li><p>Slow down for one-way doors: destructive DB changes, non-backward compatible API changes, payment logic. Do some shadow testing to properly mimic production.</p></li></ul><p><strong>Two-minute safety-net before shipping:</strong></p><ul><li><p>Write a one-line rationale.</p></li><li><p>Identify the kill switch (flag or rollback).</p></li><li><p>Ping the right stakeholder if risk &gt; medium.</p></li><li><p>Confirm alarms cover the critical path.</p></li></ul><p>I added a new field to an HTTP response that clients weren&rsquo;t consuming yet. A full regression would have cost 1-2 days. Instead, I agreed with my team to:</p><ul><li><p>Shipped behind a feature flag.</p></li><li><p>Ran smoke tests on the endpoint.</p></li><li><p>Set alarms and a canary to verify no unexpected 4xx/5xx patterns.</p></li><li><p>Communicated &ldquo;proceed unless blocked.&rdquo;</p></li></ul><p>No customer impact, tiny blast radius, trivial rollback. That&rsquo;s a textbook two-way door.</p><p>For a risky database migration (possible customer data loss if wrong, service down for 1+ hours), we did the opposite:</p><ul><li><p>Wrote a design doc with trade-offs and risk analysis.</p></li><li><p>Investigated the migration in staging with production-like data.</p></li><li><p>Booked a change window, took snapshots, confirmed restore steps.</p></li><li><p>Assigned an on-call with a printed execution and rollback playbook.</p></li></ul><p>One-way doors require us to slow down, for good reason.</p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!MQj5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F582106f5-720b-4aa0-8ca8-4941b6bfe3cd_1241x433.png"></a></figure></div><p>You don&rsquo;t grow by being right once. You grow by making many decisions and handling the wrong ones well. Use the 3-question filter:</p><ol><li><p>Impact if wrong</p></li><li><p>Ease of reversal</p></li><li><p>Fast mitigation with small blast radius</p></li></ol><p>Turn as many calls as possible into two-way doors with flags, canaries, alarms, and quick rollbacks. Slow down only for the truly irreversible. That&rsquo;s how software engineers make decisions&mdash;fast, but not sloppy.</p><p>Book a 15-minute huddle where you: state the problem, options, trade-offs, risk level, and your recommendation. Close with: &ldquo;I&rsquo;ll proceed unless you see blockers.&rdquo; Silence becomes alignment, and you avoid approval ping-pong.</p><p>You don&rsquo;t need a five-page RFC for every call. A micro-ADR keeps history without ceremony:</p><pre><code><code># ADR: Add field (server response) - Context: Mobile clients currently ignore this field, used by future device rollout. - Decision: Ship behind flag, smoke test, canary 5%, alarms on 4xx/5xx and latency. - Alternatives: Delay until client change, ship without flag. - Consequences: If wrong, toggle flag off, rollback build, cleanup flag in 2 weeks. - Author: <your-name> | Date: 2025-09-13 </your-name></code></code></pre><p>This takes two minutes and prevents &ldquo;why did we do this?&rdquo; archaeology months later.</p><p><strong>How do I know if a decision is reversible?</strong><span> If you can turn it off, roll it back quickly, or hide it (flag) without customer harm, it&rsquo;s reversible. If it threatens data integrity, security, or customer trust and can&rsquo;t be undone cleanly, treat it as a one-way door.</span></p><p><strong>When should I write a full RFC vs. a micro-ADR?</strong><span> Use a micro-ADR for low/medium-risk calls to keep momentum. Use an RFC or longer design doc for high-risk, hard-to-reverse changes (migrations, auth, billing).</span></p><p><strong>What&rsquo;s a good rollback target?</strong><span> Aim for ~10 minutes from alarm to safe state for medium-risk changes. For high-risk changes, rehearse rollback in staging and ensure snapshot/restore times are known.</span></p><p><strong>How do I communicate speed safely?</strong><span> Frame decisions in risk terms: &ldquo;Impact small, reversible in 10 minutes, mitigation plan ready.&rdquo; If mitigation is fast and the blast radius is tiny, bias toward speed.</span></p><p><strong>Do feature flags add tech debt?</strong><span> Only if you don&rsquo;t clean them up. Track flags, add an &ldquo;expiry&rdquo; note in your micro-ADR, and remove them once the decision is proven.</span></p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!M2Z1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe0e0bed-a699-429b-94ee-b29c768c5323_800x60.png"></a></figure></div><div><figure><a href="https://substackcdn.com/image/fetch/$s_!iggC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb37fb50-b8f5-4348-9675-1129f39e6926_800x60.png"></a></figure></div><p>These are some great articles I&rsquo;ve read last week:</p><ul><li><p><a href="https://read.highgrowthengineer.com/p/operating-principles-to-staff-part-1?r=2tt34d">Operating Principles That Guided Me to Staff Engineer (Part 1: Driving Impact)</a><span> by </span></p><span>. I was happy to read this post about Jordan&rsquo;s promotion. Don&rsquo;t wait for tickets, hunt for problems and solve them early. That&rsquo;s how you create visible impact and grow faster.</span></li><li><p><a href="https://newsletter.eng-leadership.com/p/how-to-use-ai-to-improve-teamwork?r=2tt34d">How to Use AI to Improve Teamwork in Engineering Teams</a><span> by </span></p><span> and </span><span> . AI won&rsquo;t fix teamwork on its own, but with trust and autonomy in place it removes friction and lets teams move faster.</span></li><li><p><a href="https://growthalgorithm.dev/p/if-you-write-80-less-code-as-tech?r=2tt34d">If You Write 80% Less Code as Tech Lead</a><span> by </span></p><span>. As a tech lead, your leverage comes from enabling others: teach through reviews, own key components, and carve time for prototypes.</span></li><li><p><a href="https://thehustlingengineer.substack.com/p/genai-for-engineers-part-1-the-foundations?r=2tt34d">GenAI for Engineers (Part 1: The Foundations)</a><span> by </span></p><span> . I wrote recently about context engineering. LLMs are just prediction machines. To build production-ready systems, we need to focus on the context.</span></li></ul><div><figure><a href="https://substackcdn.com/image/fetch/$s_!Wk_Z!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cd056dd-a3ed-4cd0-8a5c-88321c90bbc6_800x60.png"></a></figure></div><p><em>P.S. This may interest you:</em></p><ol><li><p><em><span>Are you in doubt whether the paid version of the newsletter is for you? </span><strong><a href="https://strategizeyourcareer.com/about">Discover the benefits here</a></strong></em></p></li><li><p><span>Could you take one minute to answer a quick, anonymous survey to make me improve this newsletter? </span><strong><a href="https://forms.gle/2A5QCTAevJJuHaro9">Take the survey here</a></strong></p></li><li><p><em><span>Are you a brand looking to advertise to engaged engineers and leaders? </span><strong><a href="https://www.passionfroot.me/fransoto">Book your slot now</a></strong></em></p></li></ol><p><span>Give a like &#10084;&#65039; to this post if you found it useful, and share it with a friend to </span><a href="http://strategizeyourcareer.com/leaderboard">get referral rewards</a></p></div><div class="gallery"><p><img src="https://substackcdn.com/image/fetch/$s_!mn9w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74bbd26c-5055-4674-b33d-e18156d02bea_4813x3950.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!4ove!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe27276b0-b814-4dd0-8330-adb762162b0b_865x401.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!MQj5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F582106f5-720b-4aa0-8ca8-4941b6bfe3cd_1241x433.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!M2Z1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe0e0bed-a699-429b-94ee-b29c768c5323_800x60.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!iggC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb37fb50-b8f5-4348-9675-1129f39e6926_800x60.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!Wk_Z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cd056dd-a3ed-4cd0-8a5c-88321c90bbc6_800x60.png"></p></div></section>]]></description><pubDate>Sun, 14 Sep 2025 13:24:39 +0530</pubDate></item><item><link>https://theaxolot.wordpress.com/2025/09/10/be-an-agnostic-programmer/</link><title>Be An Agnostic Programmer (theaxolot.wordpress.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1ng1yn4/be_an_agnostic_programmer/</guid><comments>https://www.reddit.com/r/programming/comments/1ng1yn4/be_an_agnostic_programmer/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 7 min | <a href='https://www.reddit.com/r/programming/comments/1ng1yn4/be_an_agnostic_programmer/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys! Back with another article on a topic that&#39;s been stewing in the back of my mind for a while. Please enjoy!</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://theaxolot.wordpress.com/wp-content/uploads/2024/05/cropped-1677184869904.jpg?w=200' /></section><section class='parsed-content'><div><p>If civil engineering is a mature field, then software development is a baby. An ugly baby, but perhaps a late bloomer. The so-called &ldquo;best practices&rdquo; of our industry are a chaotic nebula of differing, often incompatible, perspectives.</p><p><em>&ldquo;If you don&rsquo;t follow TDD, you&rsquo;re not a professional.&rdquo;</em></p><p><em>&ldquo;If you think OOP leads to overengineering, you&rsquo;re doing it wrong.&rdquo;</em></p><p>&ldquo;<em>100% code coverage</em> <em>is the gold standard.&rdquo;</em></p><p>But I&rsquo;m not here to discuss individual practices in detail. Instead, I want to zoom out and look at how we got here in the first place. Strap in, because this is gonna get philosophical.</p><p><strong>Margaret Hamilton </strong>coined the term &ldquo;software engineering&rdquo; to lend the discipline a legitimacy when the moon landing was in the works. It seems to have worked. Although nowadays, the term is so broad that a simple React web app also qualifies.</p><p>When you include the word &ldquo;engineering&rdquo; in your title, people think your discipline is this scientific, rigorous, methodological process that yields the best answer based on collective historical experience. And to the layman, it might seem so. After all, software runs on machines, and machines fall under &ldquo;engineering.&rdquo;</p><p>But let&rsquo;s not kid ourselves.</p><p>Software development isn&rsquo;t a science.</p><p>It&rsquo;s not an art, either. It&rsquo;s a mix of both, and that&rsquo;s why I love it.</p><p>The science is in the logic of your program and the architecture of your system. It&rsquo;s in how well you can prove the correctness of your code (invariants, assertions, tests, etc). And it&rsquo;s in how you investigate and deduce your way to the root causes of issues.</p><p>But there&rsquo;s a human element, too.</p><p>With every sizable new system or feature, you explore while your design isn&rsquo;t yet crystallized, and that gives room to creativity. You don&rsquo;t always know how your interfaces (APIs, classes, namespaces, modules, etc) will end up until you have a good amount implementation in front of you (part of my dislike of by-the-book TDD).</p><p>And just like a UI/UX designer, you need empathy and foresight to ensure a good experience for future maintainers, even if you have to break conventions sometimes. If that&rsquo;s not art, then what is?</p><blockquote><p>Know the rules well, so you can break them effectively</p></blockquote><p>But you can&rsquo;t break the rules without a diverse set of tools in your toolbox. Whenever I see someone proselytize a specific programming paradigm as a one-size fits all &ldquo;best practice&rdquo;, it&rsquo;s because they don&rsquo;t get this.</p><p>Programming attracts people who are enamored with logic in action. Unfortunately, it also attracts people who can be obsessive about it, to the point where they fallaciously believe that programming is a discipline with hard rules like other kinds of engineering. And when that happens, you end up exclusively subscribing to a paradigm your were taught as &ldquo;the correct way,&rdquo; and it becomes difficult to think outside of that box.</p><p>The classic example is Object-Oriented Programming. Many developers had their first exposure to programming through this paradigm, and were taught little else. So when they went into the workforce, they constantly ran into problems that didn&rsquo;t map neatly onto OOP principles. So what did they do? They forced them to, like a square peg when all you have are round holes.</p><p>I was lucky in that I was exposed to many programming styles during my early education, such that I don&rsquo;t feel constrained to adhering to a single one. Does that make me unprincipled? No, it makes me versatile. Git gud.</p><p>Some endure a hard journey of unlearning these rigid beliefs. But others double down and become dogmatists, unable to concede to the drawbacks of their favorite paradigm lest they invalidate their whole identity as developers. And some, though far fewer, become grifters who profit off the air of authority that being &ldquo;principled&rdquo; gives them.</p><p>Programming books are a major culprit, because they hold the ultimate air of authority. Honest authors will:</p><ul> <li>Acknowledge when something is more their opinion than fact</li> <li>Highlight the drawbacks of their suggested methodologies as well as their benefits</li> <li>Give leeway for breaking of their rules in service of the ultimate goal of code: to be maintained by humans (until AI takes over, I mean)</li> </ul><p>But dishonest authors will:</p><ul> <li>Intersperse personal opinions with facts to subtly convert readers</li> <li>Treat their preferred paradigm like a product to be sold (e.g downplaying the flaws)</li> <li>Assert objective superiority of their methodologies in all scenarios</li> <li>Cherry-pick or misrepresent studies that confirm what they&rsquo;re pushing and ignore those that contradict</li> </ul><p>(Note: Isn&rsquo;t it interesting how we rarely cite studies when debating the best programming guidelines? But rather we always appeal to principles and/or psychology?).</p><p>But the biggest culprit of all, in my opinion, is a lack of confidence.</p><p>Every discipline has beginners that ask questions like, &ldquo;What&rsquo;s the best way to do X?&rdquo; or &ldquo;Is it okay if I don&rsquo;t do X?&rdquo;. In fields involving creativity, the most common answer is, &ldquo;it depends.&rdquo;</p><p>People still give general recommendations, but the experts are confident enough in their skills to pick and choose which guidelines to follow, yet humble enough to acknowledge they can&rsquo;t push hard rules.</p><p>Expert software developers, in particular, discuss trade-offs instead of asserting best practices with snappy soundbites. Because every situation is unique. That&rsquo;s what makes the job so difficult, yet so rewarding.</p><p>But single-minded adherents are terrified of tapping into their creative side, unless they&rsquo;re heavily constrained. They&rsquo;re terrified of improvising and not knowing exactly what to do in every situation. But most of all, they&rsquo;re terrified that there isn&rsquo;t an objective way to get a correct answer to their problems.</p><p>&ldquo;Everything has to adhere to a principle, or else how do I know I&rsquo;m doing the right thing?&rdquo;</p><p>Experience, intuition, and being comfortable with uncertainty.</p><p>P.S</p><p>I&rsquo;m not saying there aren&rsquo;t any best practices at all. What I&rsquo;m saying is that the more high-level a practice is, the more leeway you need to exercise with how rigidly you adhere to it. Simply because that&rsquo;s the nature of creative fields. If I had to roughly rank the following kinds of software development guidelines from 1 to 5, where 1 is the most low-level, and 5 is most high-level, it would look like this:</p><ul> <li>(1) Code style</li> <li>(2) Module dependency structure</li> <li>(3) Development paradigm (TDD, OOP)</li> <li>(4) System architecture practices</li> <li>(5) Work-planning (Agile, Scrum, etc)</li> </ul><p>Notice how the higher you go, the less agreement you&rsquo;ll find on what best practices are. Here&rsquo;s another ranking with creative writing:</p><ul> <li>(1) Spelling &amp; grammar</li> <li>(2) Sentence variation and prose</li> <li>(3) Chapter/Scene structure</li> <li>(4) Plot beats</li> <li>(5) Overall story and theme</li> </ul><p>Obviously unlike stories, software is never finished, but you get my point. The higher-level you go, the more complicating factors and cases there are to consider. There&rsquo;s actually a similar kind of debate in the writing world, where you have &ldquo;plotters&rdquo; who plan their story before writing, and &ldquo;pantsers&rdquo; who go by the seat of their pants and explore their plot as they write, but that&rsquo;s a whole other topic.</p><p>Anyway, thanks for reading.</p></div></section>]]></description><pubDate>Sat, 13 Sep 2025 22:01:41 +0530</pubDate></item><item><link>https://open.substack.com/pub/allvpv/p/gits-hidden-simplicity?r=6ehrq6&amp;</link><title>Gitâ€™s hidden simplicity: whatâ€™s behind every commit (open.substack.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nfzfuo/gits_hidden_simplicity_whats_behind_every_commit/</guid><comments>https://www.reddit.com/r/programming/comments/1nfzfuo/gits_hidden_simplicity_whats_behind_every_commit/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 12 min | <a href='https://www.reddit.com/r/programming/comments/1nfzfuo/gits_hidden_simplicity_whats_behind_every_commit/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Itâ€™s time to learn some Git internals.</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://substackcdn.com/image/fetch/$s_!mPYn!,w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16171831-6ba2-4abc-8d7d-ec29ea5a06d2_2678x1826.png' /></section><section class='parsed-content'><div><article><div><p>Many programmers would admit this: our knowledge of Git tends to be pretty&hellip; superficial. &ldquo;Oops, what happened? Screw that, I&rsquo;ll cherry pick my commits and start again on a fresh branch&rdquo;.</p><p>I&rsquo;ve been there. I knew the basic use cases. I even thought I was pretty experienced after a hundred or so resolved merge conflicts. But the confidence or fluency somehow wasn&rsquo;t coming. It was a hunch: learned scenarios, commands from Stack Overflow or ChatGPT, trivia-like knowledge without a solid base.</p><p><span>In software engineering, you don&rsquo;t need to have </span><em>all the knowledge</em><span>: you just need to</span><em> quickly identify and fetch the missing bits of knowledge</em><span>. My goal is to give you that low-level grounding to sharpen your intuition. Git isn&rsquo;t really complicated in its principles!</span></p><p>Disclaimer: I am not a Git expert either. Let&rsquo;s learn together.</p><p><span>Do you know how </span><em>commit</em><em>hashes</em><span> are generated? I have to admit, I thought for a while that those hashes were somehow randomized. After all, I can run </span><code>git commit --amend</code><span>, change nothing, and still get the same commit, but with a new hash, right? Likewise, </span><code>cherry-pick</code><span>ing the same commit onto another branch gives me yet another hash.</span></p><p>Boy, I couldn&rsquo;t be more wrong. The commit hash is literally just a SHA-1 checksum of the information that constitutes the commit. So two identical commits have identical hashes. Let&rsquo;s look what a commit consists of. Run the following command:</p><pre><code><code>$ git --no-replace-objects cat-file commit HEAD</code></code></pre><p><em>(In case you don&rsquo;t know: HEAD resolves to the commit you currently checked out).</em><span> Let&rsquo;s call the output of this command the </span><code>payload</code><span>. For example, the </span><code>payload</code><span> might be:</span></p><pre><code><code>tree a55ff598781e0c7870fa5c87154a7b731b1c3336 parent c1f4476718c232f4fd8d24cf6249e42995734abc author Przemys&#322;aw Kusiak <mail> 1757612521 +0200 committer Przemys&#322;aw Kusiak <mail> 1757612563 +0200 nushell: short `git status` (`-s`) by default, remove ambiguity, scale factor</mail></mail></code></code></pre><p><span>That&rsquo;s it. That&rsquo;s the full commit. Then prepend the following </span><strong>null-terminated</strong><span> string to the </span><code>payload</code><span>: &ldquo;</span><code>commit 298</code><span>&rdquo;, where </span><code>298</code><span> is the size of the payload in bytes. Compute a SHA-1 over the result and boom: you&rsquo;ve got a Git commit hash! Try it yourself:</span></p><pre><code>$ git --no-replace-objects cat-file commit HEAD &gt; payload $ printf "commit %s\0" $(wc -c &lt; payload) &gt; payload_with_header $ cat payload &gt;&gt; payload_with_header $ sha1sum payload_with_header</code></pre><p>Now compare the output to the actual commit hash:</p><pre><code>$ git rev-parse HEAD</code></pre><p><span>It works. So simple. Now, let&rsquo;s ponder what the </span><code>payload</code><span> contains:</span></p><ol><li><p><code>tree</code><span> &ndash; the hash of a tree object. More on trees later; for now, think of it as a snapshot of all files in the repo.</span></p></li><li><p><code>parent</code><span> &ndash; a hash of parent commit(s).</span></p></li><li><p><code>author</code><span>, </span><code>committer</code><span> &ndash; self-explanatory, but notice that they include date (seconds since the Unix epoch) and time zone; </span><a href="https://stackoverflow.com/a/6755848">in several scenarios it&rsquo;s possible that the author is not the committer</a><span>.</span></p></li><li><p>the commit message.</p></li></ol><p><span>We are not hashing the </span><strong>diff</strong><span> a commit introduces.</span><strong> </strong><span>Rather, the commit </span><strong>header</strong><span>, together with the referenced </span><strong>tree</strong><span> and </span><strong>parent</strong><span>, determines the hash.</span></p><p><span>And now it&rsquo;s easy to see what happens when you run </span><code>git commit --amend</code><span> and change &ldquo;nothing&rdquo;. Something still changes: the date in the </span><code>committer</code><span> field! (Note that </span><code>git show</code><span> doesn&rsquo;t display the </span><code>committer</code><span>; the date you see comes from the </span><code>author</code><span> field). But if you are fast enough to amend within the same second as the original commit, the commit hash remains unchanged!</span></p><p><span>And on a </span><code>cherry-pick</code><span>, the </span><code>parent</code><span> field changes, and usually, though not always, the </span><code>tree</code><span> field as well.</span></p><blockquote><p><span>If you&rsquo;re a careful reader, you might wonder what the </span><code>parent</code><span> field is for the </span><strong>first</strong><span> commit in a repo, and for a </span><strong>merge</strong><span> commit. What do you think? Grab a repo and verify.</span></p></blockquote><p>We saw that a commit references a tree. Let&rsquo;s check what it really is:</p><pre><code>$ tree_hash=a55ff598781e0c7870fa5c87154a7b731b1c3336 $ git cat-file tree $tree_hash</code></pre><p><span>Oops, the </span><code>payload</code><span> isn&rsquo;t human-readable text; it&rsquo;s binary data. But just like with commits, if you prepend &ldquo;</span><code>tree<payloadsize><nul></nul></payloadsize></code><span>&rdquo; to the </span><code>payload</code><span> bytes, you can compute the tree&rsquo;s hash from the result!</span></p><p>Fortunately, Git lets you pretty-print a tree&rsquo;s contents:</p><pre><code>$ git cat-file -p $tree_hash 100644 blob b9768f0236f3d932e680f1edfca69f2d8de776b8 .gitconfig 100644 blob d960f12b4f187ee82d7a1ac545e6452ebb9c2d5b .gitignore 100644 blob 2bb1c65b1090c881adc201d78ea2654d575146ea README.md 100644 blob a26fd7ac25e457c22af2f2436aac581b50b0558a bashrc 040000 tree 4572efa73b2d3d822ef76b6771a2dc4f9a22772a bin 100755 blob b9956764ddc570a78d5daa825c6b0ad4cafbc26e bootstrap.sh 040000 tree ad5b3107519883dad04997e0e1161ddbb392fc63 keyboard 040000 tree eea34f8abc6358d88ca654774dd00d8bca32fa58 lumber 040000 tree cfea0128ab25dfd83ec43f035adfe71ab1e18583 neovide 040000 tree b2504f0e9c082f5a04d07c3eb41116fecb821e7d nushell 040000 tree 6eb525af45a6f346c34a9add71600c6b8a5c9729 nvim 100644 blob 287ee75ab7c9fea8995c9219e8f90b08ba457134 screen.png</code></pre><p><span>A </span><code>tree</code><span> is just like a directory: it references other files (blobs) and directories (trees) nested inside it. It looks a bit like </span><code>ls</code><span> output. The first column records, of course, the Unix file permissions.</span></p><p><span>Nothing more, nothing less than the raw file content &ndash; no metadata. And yes, prepend null-terminated &ldquo;</span><code>blob <file_size></file_size></code><span>&rdquo; to the bytes, run </span><code>sha1sum</code><span>, and you&rsquo;ll get the blob&rsquo;s hash!</span></p><p><span>No extra metadata such as file modification time: that can be inferred from commit history. </span><strong>A simple and immutable structure:</strong><span> you can&rsquo;t change a commit without changing its hash.</span></p><p>And if you think about it, you will notice that it is a&hellip;</p><p><span>There are three types of nodes in this graph: </span><strong>commits</strong><span>, </span><strong>trees</strong><span>, and </span><strong>blobs</strong><span>. And four types of edges:</span></p><ul><li><p><strong>commit &rarr; commit</strong><span> &ndash; parent relationship; a commit has zero or more parents (usually one).</span></p></li><li><p><strong>commit &rarr; tree</strong><span> &ndash; each commit points to exactly one tree (a snapshot of files and folders).</span></p></li><li><p><strong>tree &rarr; tree</strong><span> &ndash; subdirectory relationship.</span></p></li><li><p><strong>tree &rarr; blob</strong><span> &ndash; files contained in a directory.</span></p></li></ul><p><span>Interestingly, the graph fragment reachable from a </span><strong>tree</strong><span> node doesn&rsquo;t have to form a strict tree. For example, a single blob can be referenced by multiple parents.</span></p><p><span>As you probably know, a branch is just a </span><code>ref</code><span> pointing to a commit hash. If you run this in your repo root,</span></p><pre><code>$ ls .git/refs/heads/</code></pre><p><span>you&rsquo;ll see all local branches as file names, each file just a few bytes, with the referenced commit&rsquo;s hash inside. Likewise, </span><code>.git/refs/remotes/origin/</code><span> directory contains pointers to the remote-tracking branches.</span></p><p><span>So you can think of branches as labels for commit histories. If you commit on </span><code>main</code><span>:</span></p><ul><li><p><span>the new commit will have the hash pointed to by </span><code>main</code><span> as its </span><code>parent</code><span> field;</span></p></li><li><p><span>then the </span><code>main</code><span> branch label will be updated to point to the new commit&rsquo;s hash.</span></p></li></ul><p><span>And the </span><code>.git/HEAD</code><span> file contains the name of the current branch &ndash; or commit hash, if you&rsquo;re in a detached state. This special pointer tells Git what is currently checked out.</span></p><p>I hope this clarifies your mental model and clears some of the mystery around Git. The building blocks are simple. Now you shouldn&rsquo;t have a problem answering questions such as:</p><ol><li><p>How are Git commit hashes generated? Why does rebasing produce different commit hashes?</p></li><li><p>Can a remote-tracking branch update without your local branch updating?</p></li><li><p>Which data structure represents the repository? What are the node and edge types in this DAG, and how do they relate?</p></li></ol><p>In the next articles, I plan to cover more advanced concepts, such as Git object storage, garbage collection, and how the default merge strategy works.</p><p>If you have a little more time and want to keep going, I recommend a few resources:</p><ul><li><p><a href="https://git-scm.com/book/en/v2">Pro Git Book</a><span>: very practical, but it doesn&rsquo;t lack depth; look at the </span><strong><a href="https://git-scm.com/book/en/v2/Git-Internals-Plumbing-and-Porcelain">Git Internals</a></strong><span> section</span><strong>.</strong></p></li><li><p><a href="https://eagain.net/articles/git-for-computer-scientists/">Git for Computer Scientists</a><span> by Tommi Virtanen; short and sweet: this is where I got the DAG analogy.</span></p></li></ul></div></article></div><div><h4>Discussion about this post</h4></div><div class="gallery"><p><img src="https://substackcdn.com/image/fetch/$s_!mPYn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16171831-6ba2-4abc-8d7d-ec29ea5a06d2_2678x1826.png"></p></div></section>]]></description><pubDate>Sat, 13 Sep 2025 20:21:31 +0530</pubDate></item><item><link>https://open.substack.com/pub/verbosemode/p/on-staying-sane-as-a-developer?r=31x3tz&amp;&amp;</link><title>On Staying Sane as a Developer (open.substack.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nfy4xg/on_staying_sane_as_a_developer/</guid><comments>https://www.reddit.com/r/programming/comments/1nfy4xg/on_staying_sane_as_a_developer/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 6 min | <a href='https://www.reddit.com/r/programming/comments/1nfy4xg/on_staying_sane_as_a_developer/'>Post permalink</a></p></section><section class='preview-image'><img src='https://images.unsplash.com/photo-1603880920705-3fcc96d6e602?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxOHx8Y2hhb3N8ZW58MHx8fHwxNzU3Njg3NTE0fDA&ixlib=rb-4.1.0&q=80&w=1080' /></section><section class='parsed-content'><div><p><span>Chaos is part of a developer&rsquo;s life. Priorities shift, tasks multiply, and the sense of being &ldquo;done&rdquo; is rare. </span><a href="https://verbosemode.dev/p/im-changing-my-writing-schedule-for">I had planned to publish one post per month in 2025</a><span>, yet here we are&mdash;</span><a href="https://verbosemode.dev/p/why-kotlins-result-type-falls-short">only one post in February so far</a><span>.</span></p><p><span>Not because I ran out of ideas. I wanted to write only </span><em>quality</em><span> posts, things that felt worth sharing, like my earlier one on Kotlin&rsquo;s </span><code>Result</code><span> that came out of a big refactor. Then I started a new job, and life quickly filled with busyness.</span></p><p>That small story is just another example of how unpredictable and messy this work can be. Which is why I want to share the habits that help me keep a bit of sanity in the middle of engineering chaos.</p><p><span>I </span><strong>try</strong><span> to do this every morning: sit down, look at my calendar, and write down the three most important tasks of the day. The goal is to bring my brain into &ldquo;work mode.&rdquo; Combine this with a coffee, and voil&agrave;&mdash;you&rsquo;ve got a simple morning routine.</span></p><p><span>If I recall correctly, I got this idea from the book </span><strong><a href="https://chrisbailey.com/hyperfocus/">Hyperfocus</a></strong><span>.</span></p><p>Morning routines are trendy right now. You&rsquo;ll find thousands of videos of people&mdash;famous or not&mdash;explaining why their morning routine is the best one ever. To be honest, sometimes I wish I had one of those strict one-hour routines with sports, meditation, and everything else. But reality bites: I don&rsquo;t want to wake up before everyone else just to cram all that in, and I simply don&rsquo;t have the time every morning.</p><p>Still, these 10 minutes of focus help me immensely to start the day right.</p><p>If you&rsquo;re familiar with Cal Newport, you might know his concept of a &ldquo;shutdown routine&rdquo;: closing open loops by moving tasks into a trusted system so they don&rsquo;t linger in your mind after work.</p><p>Open loops are everywhere in software engineering. Some tasks&mdash;like reproducing a bug&mdash;can take minutes, hours, or even days, and during that time they stick in your head. I often found myself thinking about these things well into the evening.</p><p><span>My solution is simple: an end-of-day </span><strong>brain dump</strong><span>. I jot down, often unstructured, everything that comes to mind in my notebook:</span></p><ul><li><p>What I accomplished today</p></li><li><p>What went well and what didn&rsquo;t</p></li><li><p>Any disagreements or frustrations</p></li><li><p>Thoughts on tricky tasks or ongoing bugs</p></li></ul><p><span>Some days, it&rsquo;s just a couple of bullet points. Other days, it can fill several pages. The goal is always the same: </span><strong>get everything work-related out of my head.</strong></p><p>If something pops into my mind later, I write it down immediately. This has significantly reduced the sleepless nights spent replaying problem X over and over.</p><p>It also helps me transition from &ldquo;work&rdquo; to &ldquo;private life&rdquo;&mdash;a crucial boundary when working from home.</p><p>We&rsquo;ve all been in retros, trying to recall what we worked on over the past few weeks. Sometimes, we can barely remember what we did the day before.</p><p>To make things easier, I&rsquo;ve adopted a small trick: I don&rsquo;t stop work with everything perfectly neat. Instead, I deliberately leave a test failing or a piece of code unfinished.</p><p>That way, when I sit down the next day, I know exactly where to pick things up. It jump-starts my brain and makes it easier to re-engage with the task.</p><p>You&rsquo;ve probably heard of &ldquo;Definition of Done&rdquo; (DoD)&mdash;a Scrum buzzword meant to ensure a user story is truly complete. In my experience, these team-wide DoDs often fade into obscurity.</p><p><span>But having a </span><strong>personal</strong><span> DoD can be a game changer. Mine applies to pull requests, and before opening one, I mentally check off a short list, for example:</span></p><ul><li><p>Are all tests green?</p></li><li><p>Have I removed debug logs or quick hacks?</p></li><li><p>Is this the simplest yet most effective solution?</p></li><li><p>Did I test this on Dev?</p></li><li><p>(If applicable) Did I check performance for new queries?</p></li></ul><p><span>This feels less like Scrum jargon and more like the </span><a href="https://en.wikipedia.org/wiki/Pointing_and_calling">Pointing and Calling</a><span> method&mdash;simple but effective.</span></p><p>For me, early mornings are the quietest&mdash;and most productive&mdash;parts of the day. That&rsquo;s when I tackle the hardest problems, aka &ldquo;eat the frog.&rdquo;</p><p>To protect this time, I&rsquo;ve blocked two slots in my calendar:</p><ul><li><p><strong>08:30&ndash;10:00</strong></p></li><li><p><strong>10:30&ndash;12:15</strong></p></li></ul><p>These are reserved for deep work. I try to protect them as much as possible, but I&rsquo;m not dogmatic about it&mdash;if a meeting really needs to happen then, I&rsquo;ll allow it.</p><p>The first block begins right after my short morning routine. It&rsquo;s consistently my most productive window of the day.</p><p>One of the most important lessons I&rsquo;ve learned is that there will always be open tasks. You are never truly &ldquo;done.&rdquo; That&rsquo;s why it&rsquo;s essential to have tools and habits that help you stay sane amidst the chaos.</p><p>In this post, I shared some of mine. Hopefully, they inspire you&mdash;or at least give you a starting point for developing your own.</p><p>For further reading:</p><ul><li><p><em><a href="https://calnewport.com/writing/#books">Deep Work</a></em><span> by Cal Newport (Shutdown, Time Blocking)</span></p></li><li><p><em><a href="https://chrisbailey.com/hyperfocus/">Hyperfocus</a></em><span> by Chris Bailey (Three Most Important Things)</span></p></li><li><p>Unfortunately, I can&rsquo;t find the original post where I first read about &ldquo;Leave things broken for tomorrow.&rdquo;</p></li></ul><p>If this post sparked a thought, I&rsquo;d love to hear from you&mdash;drop me a line or leave a comment &#128172;. Always curious about how others tackle the chaos!</p></div></section>]]></description><pubDate>Sat, 13 Sep 2025 19:26:19 +0530</pubDate></item><item><link>https://ekxide.io/blog/iceoryx2-0-7-release/</link><title>Announcing iceoryx2 v0.7: Fast and Robust Inter-Process Communication (IPC) Library for Rust, Python, C++, and C (ekxide.io)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nfvdvk/announcing_iceoryx2_v07_fast_and_robust/</guid><comments>https://www.reddit.com/r/programming/comments/1nfvdvk/announcing_iceoryx2_v07_fast_and_robust/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 11 min | <a href='https://www.reddit.com/r/programming/comments/1nfvdvk/announcing_iceoryx2_v07_fast_and_robust/'>Post permalink</a></p></section><section class='preview-image'><img src='https://www.ekxide.com/logo/ekxide-black.png' /></section><section class='parsed-content'><div><h3>What Is iceoryx2</h3><p><strong>iceoryx2</strong> is a service-based communication library designed to build robust and efficient decentralized systems. It enables ultra low-latency communication between processes &mdash; similar to Unix domain sockets or message queues, but significantly faster and easier to use.</p><p>It includes language bindings for C, C++, Python, and Rust and it runs on Linux, macOS, Windows, FreeBSD and QNX.</p><p>iceoryx2 supports messaging patterns such as publish-subscribe, events, request-response stream and the introduced blackboard pattern - a key-value repository in shared memory. It is robust and comes with a decentralized architecture without the need for a central broker.</p><p>Check out the <a href="https://github.com/eclipse-iceoryx/iceoryx2/tree/main/benchmarks">iceoryx2 benchmarks</a> and try them out on your platform!</p><h3>Release v0.7.0</h3><p>With the new v0.7.0 release of iceoryx2, we&rsquo;re adding full Python bindings and a network tunnel. That means you can now send data not just between processes on the same machine, but also across hosts - iceoryx2 handles the communication details for you.</p><p>We&rsquo;ve also finished the first version of the <a href="https://ekxide.github.io/iceoryx2-book/main/">iceoryx2 Book</a>, which not only helps you get started quickly but also provides deep background on the overall architecture.</p><p>Debugging, introspection, and record-and-replay of data are now possible with the command-line client. On top of that, we&rsquo;ve introduced a new messaging pattern called <strong>blackboard</strong>, a key-value repository in shared memory.</p><p>And one more thing: iceoryx2 now runs on <strong>QNX 7.1</strong>. There are plenty of smaller enhancements too. Check out the full release notes here: <a href="https://github.com/eclipse-iceoryx/iceoryx2/releases/tag/v0.7.0">iceoryx2 v0.7.0 release</a></p><ul> <li><a href="https://ekxide.io/blog/iceoryx2-0-7-release/#the-iceoryx2-book">The iceoryx2 Book</a></li> <li><a href="https://ekxide.io/blog/iceoryx2-0-7-release/#python-language-bindings">Python Language Bindings</a></li> <li><a href="https://ekxide.io/blog/iceoryx2-0-7-release/#network-communication">Network Communication</a></li> <li><a href="https://ekxide.io/blog/iceoryx2-0-7-release/#command-line-client-debugging-and-introspection">Command Line Client: Debugging And Introspection</a></li> <li><a href="https://ekxide.io/blog/iceoryx2-0-7-release/#record-and-replay">Record And Replay</a></li> <li><a href="https://ekxide.io/blog/iceoryx2-0-7-release/#messaging-pattern-blackboard">Messaging Pattern: Blackboard</a></li> <li><a href="https://ekxide.io/blog/iceoryx2-0-7-release/#new-supported-platforms">New Supported Platforms</a></li> <li><a href="https://ekxide.io/blog/iceoryx2-0-7-release/#other-feature-highlights">Other Feature Highlights</a></li> </ul> <h4>The iceoryx2 Book</h4><p>We&rsquo;ve put effort into the <a href="https://ekxide.github.io/iceoryx2-book/main/">iceoryx2 book</a>, which serves as a starting point for every user. Whether you&rsquo;re looking for a quick introduction, a getting-started tutorial, or a deeper dive into the architecture of iceoryx2.</p><p>If you&rsquo;d like to contribute, by writing an article, creating a tutorial on a specific topic, or reporting an issue, we&rsquo;d be glad to receive your pull request: <a href="https://github.com/ekxide/iceoryx2-book">https://github.com/ekxide/iceoryx2-book</a></p><p><strong>Remark:</strong> The iceoryx2 book replaces the previous ReadTheDocs documentation and provides references for C, C++, Python, and Rust.</p><h4>Python Language Bindings</h4><p>We&rsquo;ve ported the iceoryx2 API to Python and added an extensive set of <a href="https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/python">examples on GitHub</a>.</p><p>If you&rsquo;re new to iceoryx2, start with the <em>Getting Started</em> tutorial <a href="https://ekxide.github.io/iceoryx2-book/main/getting-started/robot-nervous-system/index.html">A Robot Nervous System</a> in the iceoryx2 book. It walks through all the major features across every supported language. You can switch between languages while reading, which makes it easy to build complex systems that mix Python, C, C++, and Rust - without paying the serialization overhead.</p><p>A highlight is the publish&ndash;subscribe cross-language example, where Python, C, C++, and Rust processes exchange data directly in shared memory. Check it out:</p><ul> <li><a href="https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/python/publish_subscribe_cross_language">Python Publish&ndash;Subscribe Cross-Language</a></li> <li><a href="https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/c/publish_subscribe_cross_language">C Publish&ndash;Subscribe Cross-Language</a></li> <li><a href="https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/cxx/publish_subscribe_cross_language">C++ Publish&ndash;Subscribe Cross-Language</a></li> <li><a href="https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/rust/publish_subscribe_cross_language">Rust Publish&ndash;Subscribe Cross-Language</a></li> </ul><p>And this isn&rsquo;t limited to publish-subscribe, the same zero-copy, cross-language communication works with all messaging patterns, including request&ndash;response and events.</p><h4>Network Communication</h4><p>Until now, iceoryx2 focused on inter-process communication. With the new <strong>network tunnel</strong>, you can communicate between hosts as well. No complicated setup, no configuration headaches, just run the CLI:</p><pre><code><span>cargo</span><span> install iceoryx2-cli </span><span>iox2</span><span> tunnel zenoh </span></code></pre><p>Do that on each host, and your publish&ndash;subscribe and event communication is instantly available across machines.</p><p>The network tunnel, that connects iceoryx2 instances across the network, is still in development, so some patterns, like request&ndash;response and blackboard, aren&rsquo;t supported yet. But it&rsquo;s also the first step toward a full <strong>gateway</strong>: the idea is that you&rsquo;ll use the native iceoryx2 API while the gateway connects to other protocols in the background.</p><p>Picture this: you start a gRPC, MQTT, or DDS gateway, and suddenly your iceoryx2 application can talk to anything, or let others talk to you, without changing a single line of code.</p><p>That&rsquo;s the vision. And with the tunnel, you can already connect iceoryx2 applications across hosts today by just running <code>iox2 tunnel zenoh</code>.</p><h4>Command Line Client: Debugging And Introspection</h4><p>We are continuously improving the <code>iox2</code> command-line client to make it a versatile tool for working with iceoryx2. It allows direct interaction with iceoryx2 from the shell, which is especially useful for debugging.</p><p>Let&rsquo;s start with events. Open two terminals and run the listener example in one of them:</p><pre><code><span>cargo</span><span> run</span><span> --example</span><span> event_listener </span></code></pre><p>Now you can send event notifications to this process and wake it up explicitly:</p><pre><code><span>cargo</span><span> install iceoryx2-cli </span><span>iox2</span><span> service notify</span><span> --event-id</span><span> 1</span><span> --num</span><span> 2</span><span> --interval-in-ms</span><span> 1500 MyEventName </span></code></pre><p>You can also wait for events directly on the command line, or send and receive publish-subscribe samples. More details and examples can be found in the <a href="https://ekxide.github.io/iceoryx2-book/main/getting-started/robot-nervous-system/command-line-tools-and-debugging.html">getting started article on the command line</a> in the iceoryx2 book.</p><h4>Record And Replay</h4><p>The <code>iox2</code> command-line client also supports <strong>record and replay</strong>. This is useful when you want to capture real data and feed it back into the system during development - for example, creating an endlessly repeating stream of sensor data.</p><p>Let&rsquo;s walk through it with the publish-subscribe example. Start the publisher in one terminal:</p><pre><code><span>cargo</span><span> run</span><span> --example</span><span> publish_subscribe_publisher </span></code></pre><p>In another terminal, record the data for 10 seconds and write it to <code>record.dat</code>:</p><pre><code><span>iox2</span><span> service record</span><span> --timeout-in-sec</span><span> 10</span><span> --output</span><span> record.dat </span><span>"</span><span>My/Funk/ServiceName</span><span>" </span></code></pre><p>The output file is human-readable by default. <strong>Tip:</strong> Payloads are stored in hex. This makes it easy to edit them manually and then inject modified data back into the system to test edge cases. Next, stop the publisher and start the subscriber:</p><pre><code><span>cargo</span><span> run</span><span> --example</span><span> publish_subscribe_subscriber </span></code></pre><p>Now replay the recording twice:</p><pre><code><span>iox2</span><span> service replay</span><span> --input</span><span> record.dat</span><span> --repetitions</span><span> 2 </span></code></pre><p>A more detailed walkthrough is available in the <a href="https://ekxide.github.io/iceoryx2-book/main/getting-started/robot-nervous-system/command-line-tools-and-debugging.html#recording-and-replaying-data">getting started article on the command line</a> in the iceoryx2 book.</p><h4>Messaging Pattern: Blackboard</h4><p>The publish-subscribe pattern can reach its limits when a single publisher needs to serve hundreds or even thousands of subscribers. This often happens when the system requires a global state - for example, in a simulation where every entity&rsquo;s position and movement must be tracked, or when maintaining a global configuration across multiple components.</p><p>With the newly introduced <strong>blackboard messaging pattern</strong>, iceoryx2 provides a shared-memory key-value repository that every process can access efficiently. A single process can maintain the global state in a thread-safe way, while all others can read it without overhead.</p><p>The iceoryx2 book contains a <a href="https://ekxide.github.io/iceoryx2-book/main/getting-started/robot-nervous-system/blackboard.html">getting started article on the blackboard</a>, and you can also explore some hands-on code in the <a href="https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples">examples folder on GitHub</a>.</p><p>For deeper background on the concept itself, see the article <a href="https://ekxide.io/blog/advanced-messaging-patterns-blackboard/">Advanced Messaging Patterns &ndash; Blackboard</a>.</p><p><strong>Remark:</strong> The blackboard messaging pattern is currently available only in Rust. C, C++, and Python bindings will be added in the next release.</p><h4>New Supported Platforms</h4><p>For Yocto users, we now provide a <a href="https://github.com/eclipse-iceoryx/meta-iceoryx2">dedicated Yocto layer on GitHub</a>.</p><p>We have also added support for <strong>QNX 7.1</strong>. In the open-source repository, QNX is available as a tier-3 platform due to license restrictions. If you require tier-1 support, please reach out to us.</p><p>In addition, there is a <strong>VxWorks</strong> proof of concept. Contact us for details.</p><h4>Other Feature Highlights</h4><p><strong>Thread-safe service types</strong> Until now, iceoryx2 ports were not thread-safe, which made async use cases tricky. This release introduces <code>ipc_threadsafe::Service</code> and <code>local_threadsafe::Service</code>. These variants are optimized for different contexts, and the <a href="https://github.com/eclipse-iceoryx/iceoryx2/blob/main/examples/rust/service_types/">service variant example on GitHub</a> introduces all of them, how you can specialize them and shows how to pick the right one.</p><p><strong>Service discovery</strong> We added a request-response service to obtain the full list of all running services in the system. The list can be kept up to date by subscribing to a publish-subscribe service. See the <a href="https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/rust/discovery_service">discovery example</a>.</p><p><strong>Graceful shutdown</strong> Client-server connections can now be shut down gracefully while the client is receiving a response stream. The API provides <code>PendingResponse::set_disconnect_hint()</code> and <code>ActiveRequest::has_disconnect_hint()</code>.</p><h3>Roadmap: What&rsquo;s Next?</h3><p><a href="https://github.com/eclipse-iceoryx/iceoryx2/blob/main/ROADMAP.md">iceoryx2 Roadmap</a>.</p><ul> <li><code>no_std</code> support for embedded use cases</li> <li>Blackboard Messaging Pattern Language Bindings for C, C++ and Python</li> <li>QNX 8.0</li> <li>network tunnel: support for request-response and blackboard</li> <li>(Moonshot) Go language bindings</li> </ul> <h3>Thank You</h3><p>We want to thank our community. Your ideas, discussions, and collaborative spirit help shape iceoryx2 every day. Even frustrating bugs become less painful when tackled with humor and openness.</p><p>Also a big thank you to the iceoryx team, which was relentless in implementing all those features.</p><p>And finally, a big thank you to our customers who share our vision:</p><p><strong>To create an open-source, certifiable base and communication library that can be trusted in mission-critical systems.</strong></p><ul> <li><a href="https://www.reddit.com/r/programming/comments/1nfvdvk/announcing_iceoryx2_v07_fast_and_robust/">Discuss on Reddit</a></li> <li><a href="https://programming.dev/post/37369902">Discuss on programming.dev</a></li> <li><a href="https://github.com/eclipse-iceoryx/iceoryx2">Project iceoryx2 on GitHub</a></li> <li><a href="https://github.com/ekxide/rmw_iceoryx2">Project iceoryx2 RMW for ROS on GitHub</a></li> <li><a href="https://crates.io/crates/iceoryx2">Project on crates.io</a></li> </ul> </div></section>]]></description><pubDate>Sat, 13 Sep 2025 17:13:37 +0530</pubDate></item><item><link>https://blog.rust-lang.org/2025/09/12/crates-io-phishing-campaign/</link><title>crates.io phishing campaign | Rust Blog (blog.rust-lang.org)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nfsfml/cratesio_phishing_campaign_rust_blog/</guid><comments>https://www.reddit.com/r/programming/comments/1nfsfml/cratesio_phishing_campaign_rust_blog/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/programming/comments/1nfsfml/cratesio_phishing_campaign_rust_blog/'>Post permalink</a></p></section><section class='preview-image'><img src='https://www.rust-lang.org/static/images/rust-social-wide.jpg' /></section><section class='parsed-content'><p>Sept. 12, 2025 &middot; Rust Security Response WG, crates.io team </p><div><p>We received multiple reports of a phishing campaign targeting crates.io users (from the <code>rustfoundation.dev</code> domain name), mentioning a compromise of our infrastructure and asking users to authenticate to limit damage to their crates.</p><p>These emails are malicious and come from a domain name not controlled by the Rust Foundation (nor the Rust Project), seemingly with the purpose of stealing your GitHub credentials. We have no evidence of a compromise of the crates.io infrastructure.</p><p>We are taking steps to get the domain name taken down and to monitor for suspicious activity on crates.io. Do not follow any links in these emails if you receive them, and mark them as phishing with your email provider.</p><p>If you have any further questions please reach out to <a href="https://blog.rust-lang.orgmailto:security@rust-lang.org">security@rust-lang.org</a> and <a href="https://blog.rust-lang.orgmailto:help@crates.io">help@crates.io</a>.</p></div></section>]]></description><pubDate>Sat, 13 Sep 2025 14:10:48 +0530</pubDate></item><item><link>https://www.cerbos.dev/blog/productivity-paradox-of-ai-coding-assistants</link><title>The productivity paradox of AI coding assistants (cerbos.dev)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nf9buo/the_productivity_paradox_of_ai_coding_assistants/</guid><comments>https://www.reddit.com/r/programming/comments/1nf9buo/the_productivity_paradox_of_ai_coding_assistants/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 13 min | <a href='https://www.reddit.com/r/programming/comments/1nf9buo/the_productivity_paradox_of_ai_coding_assistants/'>Post permalink</a></p></section><section class='preview-image'><img src='https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/The_productivity_paradox_of_AI_coding_assistants_1fb36a40a2.png' /></section><section class='parsed-content'><div><p>Our development team at Cerbos is split into two camps. One side uses AI coding help like Cursor or Claude Code and sees it as the fastest way to ship. The other side has the typical &ldquo;meh&rdquo; reaction you often see on Reddit, arguing that AI assistance is mostly a racket.</p><p>Some of us lean on AI coding to push side projects faster into the delivery pipeline. These are not core product features but experiments and MVP-style initiatives. For bringing that kind of work to its first version, the speed-up is real.</p><p>AI coding assistants promise less boilerplate, fewer doc lookups, and quicker iteration. From my perspective, they deliver on that promise when building MVPs, automations, and hobby projects. Outside of those use cases, the picture changes. You may feel like you are moving quickly, but getting code production ready often takes longer.</p><p>Let&rsquo;s dig into the data.</p><ol> <li><a href="https://www.cerbos.dev#dopamine-vs.-reality">Dopamine vs. reality</a></li> <li><a href="https://www.cerbos.dev#the-quality-problem">The quality problem</a></li> <li><a href="https://www.cerbos.dev#so-where-is-the-magical-10x-productivity-boost?">So where is the magical 10x productivity boost?</a></li> <li><a href="https://www.cerbos.dev#security-is-where-the-gap-shows-most-clearly">Security is where the gap shows most clearly</a></li> <li><a href="https://www.cerbos.dev#how-ai-assistants-create-new-attack-surfaces">How AI assistants create new attack surfaces</a></li> <li><a href="https://www.cerbos.dev#the-70%25-problem">The 70% problem</a></li> <li><a href="https://www.cerbos.dev#business-opinion-vs.-developer-opinion">Business opinion vs. developer opinion</a></li> </ol> <h2><strong>Dopamine vs. reality</strong> <a></a></h2><p>AI coding assistants feel productive because they give instant feedback. You type a prompt and code drops in right away. That loop feels like progress, the same reward you get from closing a ticket or fixing a failing test. The problem is that dopamine rewards activity in the editor, not working code in production.</p><p>The METR randomized <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/">trial</a> in July 2025 with experienced open source developers showed how strong this illusion is. Half the group had AI tools, the other half coded without them. Participants mainly used Cursor Pro with Claude 3.5 and 3.7 Sonnet, which we use internally as well. Developers using AI were on average 19% slower. Yet they were convinced they had been faster.</p><ul> <li>Before starting, they predicted AI would make them 24% faster.</li> <li>After finishing, even with slower results, they still believed AI had sped them up by ~20%<strong>.</strong></li> </ul><p>The chart below from the study makes the point clearly. The green dots show developer expectations and self-reports, while the red dots show actual performance. AI coding &ldquo;felt faster&rdquo;, but in reality, it slowed experienced developers down.</p><p>Marcus Hutchins described this perfectly in his <a href="https://malwaretech.com/2025/08/every-reason-why-i-hate-ai.html">essay</a>:</p><p><em>&ldquo;LLMs inherently hijack the human brain&rsquo;s reward system&hellip; LLMs give the same feeling of achievement one would get from doing the work themselves, but without any of the heavy lifting.&rdquo;</em></p><p>That gap between perception and reality is the productivity placebo. It also shows up in the 2025 Stack Overflow Developer <a href="https://survey.stackoverflow.co/2025/ai#ai-agents">Survey</a>. Only 16.3% of developers said AI made them more productive to a great extent. The largest group, 41.4%, said it had little or no effect. Most developers were in the middle, reporting &ldquo;somewhat&rdquo; better output. That lines up with the METR findings. AI feels faster, but the measurable gains are marginal or even negative.</p><h2><strong>The quality problem</strong> <a></a></h2><p>Speed is one thing, but quality is another. Developers who have used AI assistants in long sessions see the same pattern: output quality gets worse the more context you add. The model starts pulling in irrelevant details from earlier prompts, and accuracy drops. This effect is often called context rot.</p><p>More context is not always better. In theory, a bigger context window should help, but in practice, it often distracts the model. The result is bloated or off-target code that looks right but does not solve the problem you are working on.</p><p>In the Stack Overflow Developer Survey of more than 90,000 developers, 66% said the most common frustration with AI assistants is that the code is &ldquo;almost right, but not quite.&rdquo; Another 45.2% pointed to time spent debugging AI-generated code.</p><p>That lines up with what most of us see in practice. As one of my teammates from the dev team mentioned in our discussion: &ldquo;A lot of the positive sentiments you&rsquo;d read about online are likely because the problems these people are solving are repetitive, boilerplate-y problems solved a gazillion times before. I don&rsquo;t trust the code output from an agent... It&rsquo;s dangerous and (in the rare case that the code is effective) an instant tech-debt factory. That said, I do sometimes find it a useful way of asking questions about a codebase, a powerful grokking tool if you will&rdquo;. I can&rsquo;t agree more.</p><h2><strong>So where is the magical 10x productivity boost?</strong> <a></a></h2><p>The claim that AI makes developers 10x more productive gets repeated pretty often. But the math does not hold up. A 10x boost means what used to take three months now takes a week and a half. Anyone who has actually shipped complex software knows that it is impossible.</p><p>The bottlenecks are not typing speed. They are design reviews, PR queues, test failures, context switching, and waiting on deployments.</p><p>Let&rsquo;s look at some interesting research. In 2023, GitHub and Microsoft ran <a href="https://arxiv.org/abs/2302.06590">a controlled experiment</a> where developers were asked to implement a small HTTP server in JavaScript. Developers using Copilot finished the task <strong>55.8% faster</strong> than the control group. The setup was closer to a benchmark exercise than day-to-day work, and most of the gains came from less experienced devs who leaned on the AI for scaffolding. And, obviously, those were vendor-run experiments.</p><p>METR tested the opposite scenario. Senior engineers worked in large OSS repositories that they already knew well. In that environment, the minutes saved on boilerplate were wiped out by time spent reviewing, fixing, or discarding AI output. As one of my teammates put it, you&rsquo;re not actually saving time with AI coding; you&rsquo;re just trading less typing for more time reading and untangling code.</p><p>Even when AI enables parallelism, one more research shows the cost is more juggling, more reviews, not less time to ship. In July 2025, Faros AI <a href="https://go.faros.ai/ai-engineering">analyzed</a> telemetry from over 10,000 developers across 1,255 teams. They found that teams with high AI adoption interacted with 9% more tasks and 47% more pull requests per day. Developers were juggling more parallel workstreams because AI could scaffold multiple tasks at once.</p><p>Historically, context switching is a negative indicator, correlated with cognitive overload and reduced focus. Faros points out that developers spend more time orchestrating and validating AI contributions across streams. That extra juggling cancels out much of the speed-up you get in typing.</p><p>Not all findings are negative. In 2024, <a href="https://economics.mit.edu/sites/default/files/inline-files/draft_copilot_experiments.pdf">researchers</a> from MIT, Harvard, and Microsoft ran large-scale field experiments across three companies: Microsoft, Accenture, and a Fortune 100 firm. The sample covered 4,867 professional developers working on production code.</p><p>With access to AI coding tools, developers completed 26.08% more tasks on average compared to the control group. Junior and newer hires adopted the tools more readily and showed the largest productivity boost:</p><ul> <li>Senior developers, especially those already familiar with the codebase and stack, saw little or no measurable speed-up.</li> <li>The boost was strongest in situations where devs lacked prior context and used the AI to scaffold, fill in boilerplate, or cut down on docs lookups.</li> </ul><p>These gains &#128070; were good but not near 10x.</p><h2><strong>Security is where the gap shows most clearly</strong> <a></a></h2><p>With the core focus of our company on <a href="https://www.cerbos.dev/product-cerbos-hub">permission management</a> for humans and machines, we naturally look at AI coding assistance through a security lens.</p><p>Older data from 2023 found that developers using assistants shipped more vulnerabilities because they trusted the output too much (<a href="https://arxiv.org/pdf/2211.03622">Stanford research</a>). Obviously, in 2025, fewer developers would trust AI-generated code.</p><p>However, Apiiro&rsquo;s 2024 research is actually very alarming. It showed AI-generated code introduced <strong>322% more privilege escalation paths</strong> and <strong>153% more design flaws</strong> compared to human-written code.</p><p>Apiiro&rsquo;s 2024 research also found:</p><ul> <li><p>AI-assisted commits were merged into production 4x faster than regular commits, which meant insecure code bypassed normal review cycles.</p></li> <li><p>Projects using assistants showed a 40% increase in secrets exposure, mostly hard-coded credentials and API keys generated in scaffolding code. Accidentally pasting API keys, tokens, or configs into an AI assistant is one of the top risks. Even if rotated later, those secrets are now in someone else&rsquo;s logs.</p></li> <li><p>AI-generated changes were linked to a 2.5x higher rate of critical vulnerabilities (CVSS 7.0+) flagged later in scans.</p></li> <li><p>Review complexity went up significantly: PRs with AI code required 60% more reviewer comments on security issues.</p></li> </ul><p>And it is not just security; it is <a href="https://www.cerbos.dev/blog/staying-compliant">compliance</a> too. If code, credentials, or production data leave your environment through an AI assistant, you cannot guarantee deletion or control over where that data ends up. For organizations under SOC2, ISO, GDPR, or HIPAA, that can mean stepping outside policy or outright violations. This is exactly the kind of blind spot CISOs worry about.</p><h2><strong>How AI assistants create new attack surfaces</strong> <a></a></h2><p>AI coding assistants don&rsquo;t just generate code. They also bring new runtimes, plugins, and extensions into the developer workflow. That extra surface means more places where things can go wrong and attackers have already started exploiting them:</p><ul> <li><p>In July 2025, Google&rsquo;s Gemini CLI shipped with a bug that let attackers trigger arbitrary code execution on a dev machine. The tool that was supposed to speed up coding workflows basically turned into a local RCE vector.</p></li> <li><p>A year earlier, the Amazon Q extension in VS Code (August 2024) carried a poisoned update. Hidden prompts in the release told the assistant to delete local files and even shut down AWS EC2 instances. Because the extension shipped with broad local and cloud permissions, the malicious instructions executed without barriers.</p></li> </ul><p>These incidents highlight specific failures, but the bigger issue is structural. Coding assistants expand the software supply chain and increase the number of privileged connections that can be abused.</p><p>The diagram below, from Jim Gumbley and Lilly Ryan&rsquo;s piece on <a href="https://martinfowler.com/articles/exploring-gen-ai/software-supply-chain-attack-surface.html">Martin Fowler</a>, maps this new attack surface. It shows how agents, MCP servers, file systems, CI/CD, and LLM backends are all interconnected, each link a potential entry point for context poisoning or privilege escalation.</p><h2><strong>The 70% problem</strong> <a></a></h2><p>I loved Addy Osmani&rsquo;s piece in <a href="https://newsletter.pragmaticengineer.com/p/how-ai-will-change-software-engineering">Pragmatic Engineer</a> because it nailed what many developers see. AI can get you 70% of the way, but the last 30% is the hard part. The assistant scaffolds a feature, but production readiness means edge cases, architecture fixes, tests, and cleanup:</p><p>For juniors, 70% feel magical. For seniors, the last 30% is often slower than writing it clean from the start. That is why METR&rsquo;s experienced developers were slower with AI; they already knew the solution, and the assistant just added friction.</p><p>This is the difference between a demo and production. AI closes the demo gap quickly, but shipping to production still belongs to humans. A demo only has to run once. Production code has to run a million times without breaking. Humans at least know what they want, even if they misunderstand requirements. An LLM has no intent, which is why the final 30% always falls apart.</p><p>Our team also flagged another issue. Patterns you learn while vibe coding with these tools often break with every model update. There is no stable base to build on. Engineering needs determinism, not shifting patterns that collapse the moment the model retrain.</p><h2><strong>Business opinion vs. developer opinion</strong> <a></a></h2><p>The story of the &ldquo;10x engineer&rdquo; has always been more appealing in boardrooms than in code reviews. Under pressure to do more with less, it is tempting for leadership to see AI as a multiplier that could let one team ship the work of ten. The current AI hype plays straight into that narrative.</p><p>Developers, though, know where the real bottlenecks are. No AI collapses design discussions, sprint planning, meetings, or QA cycles. It does not erase tech debt or magically handle system dependencies. The reality is incremental speed-ups in boilerplate, not 10x multipliers across the delivery pipeline.</p><p>Engineers and EMs both need the same thing &mdash; software that is secure, reliable, and production-ready. AI can play a role in getting there, but only when expectations are grounded in how development actually works.</p><hr><p>Shameless plug: If you are working on IAM and permission management, our product <strong><a href="https://www.cerbos.dev/product-cerbos-hub">Cerbos Hub</a></strong> handles fine-grained authorization for humans and machines. Enforce contextual and continuous access control across apps, APIs, services, workloads, MCP servers, and AI agents &mdash; all from one place.</p></div><div class="gallery"><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_coding_research_1_930ab08028.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_agents_productivity_90254da6a3.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_tools_frustraton_a93d1fb96a.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/Context_switching_with_AI_0b5971d327.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_coding_security_risks_fda1644620.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_tools_supply_chain_risks_95a478012a.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_productivity_issues_d2047e21b0.png"></p></div></section>]]></description><pubDate>Fri, 12 Sep 2025 22:57:32 +0530</pubDate></item><item><link>https://labs.iximiuz.com/tutorials/container-filesystem-from-scratch</link><title>How Containers Work: Building a Docker-like Container From Scratch (labs.iximiuz.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nf96ir/how_containers_work_building_a_dockerlike/</guid><comments>https://www.reddit.com/r/programming/comments/1nf96ir/how_containers_work_building_a_dockerlike/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 67 min | <a href='https://www.reddit.com/r/programming/comments/1nf96ir/how_containers_work_building_a_dockerlike/'>Post permalink</a></p></section><section class='preview-image'><img src='https://labs.iximiuz.com/content/files/tutorials/container-filesystem-from-scratch/__static__/container-rootfs-full-rev2.png' /></section><section class='parsed-content'><div><p>One of the superpowers of containers is their isolated <strong>filesystem view</strong> - from inside a container it can look like a full Linux distro, often different from the host. Run <code>docker run nginx</code>, and Nginx lands in its familiar Debian userspace no matter what Linux flavor your host runs. But how is that illusion built?</p><p>In this post, we'll assemble a tiny but realistic, Docker-like container using only stock Linux tools: <code>unshare</code>, <code>mount</code>, and <code>pivot_root</code>. No runtime magic and (almost) no cut corners. Along the way, you'll see why the <strong>mount namespace</strong> is the bedrock of container isolation, while other namespaces, such as <strong>PID</strong>, <strong>cgroup</strong>, <strong>UTS</strong>, and even <strong>network</strong>, play rather complementary roles.</p><p>By the end - especially if you pair this with the <a href="https://labs.iximiuz.com/tutorials/container-networking-from-scratch">container networking tutorial</a> - you'll be able to spin up fully featured, Docker-style containers using nothing but standard Linux commands. The ultimate goal of every aspiring container guru.</p><h2>Prerequisites</h2><ul><li>Some prior familiarity with Docker (or Podman, or the like) containers</li><li>Basic Linux knowledge (shell scripting, general namespace awareness)</li><li>Filesystem fundamentals (single directory hierarchy, mount table, bind mount, etc.)</li></ul><h2>Visualizing the end result</h2><p>The diagram below shows what filesystem isolation looks like when Docker creates a new container. It's all right if the drawing feels overwhelming. With the help of the hands-on exercises in this tutorial, we'll build a comprehensive mental model of how containers work, so when we revisit the diagram in the closing section, it'll look much more digestible.</p><div><p><i>Click to enlarge</i></p></div><h2>What exactly does Mount Namespace isolate?</h2><p>Let's do a quick experiment. In <span>Terminal 1</span>, start a new shell session in its own mount namespace:</p><div><pre><code><span><span>sudo</span><span> unshare</span><span> --mount</span><span> bash </span></span></code></pre></div><p>Now in <span>Terminal 2</span>, create a file somewhere on the host's filesystem:</p><div><pre><code><span><span>echo</span><span> "Hello from host's mount namespace"</span><span> |</span><span> sudo</span><span> tee</span><span> /opt/marker.txt </span></span></code></pre></div><p>Surprisingly or not, when you try locating this file in the newly created mount namespace using the <span>Terminal 1</span> tab, it'll be there:</p><p>So what exactly did we just isolate with <code>unshare --mount</code>? &#129300;</p><p>The answer is - a <strong>mount table</strong>. Here is how to verify it. From <span>Terminal 1</span>, mount something:</p><div><pre><code><span><span>sudo</span><span> mount</span><span> --bind</span><span> /tmp</span><span> /mnt </span></span></code></pre></div><p>&#128161; The above command uses a <a href="https://labs.iximiuz.com/challenges/storage-bind-mount">bind mount</a> for simplicity, but a <a href="https://labs.iximiuz.com/challenges/storage-simple-mount">regular mount</a> (of a block device) would do, too.</p><p>Now if you list the contents of the <code>/mnt</code> folder in <span>Terminal 1</span>, you should see the files of the <code>/tmp</code> folder:</p><div><pre><code>total 12 drwx------ 3 root root 4096 Sep 11 14:16 file1 drwx------ 3 root root 4096 Sep 11 14:16 file2 ... </code></pre></div><p>But at the same time, the <code>/mnt</code> folder remained empty in the host mount namespace. If you run the same <code>ls</code> command from <span>Terminal 2</span>, you'll see no files:</p><p>Finally, the filesystem "views" started diverging between namespaces. However, we could only achieve it by creating a new mount point.</p><div><p><i>Mount namespaces, visualized</i></p></div><p>From <a href="https://man7.org/linux/man-pages/man7/mount_namespaces.7.html">the mount namespace man page</a>:</p><blockquote><p>Mount namespaces provide isolation of the list of mounts seen by the processes in each namespace instance. Thus, the processes in each of the mount namespace instances will see distinct single directory hierarchies.</p></blockquote><p>Compare the mount tables by running <code>findmnt</code> from <span>Terminal 1</span> and <span>Terminal 2</span>:</p><div><div><p>Host namespace</p><p>New namespace</p></div><div><pre><code><span><span>TARGET SOURCE FSTYPE OPTIONS </span></span><span><span>/ /dev/vda ext4 rw,... </span></span><span><span>&#9500;&#9472;/dev devtmpfs devtmpfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/dev/shm tmpfs tmpfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/dev/pts devpts devpts rw,... </span></span><span><span>&#9474; &#9492;&#9472;/dev/mqueue mqueue mqueue rw,... </span></span><span><span>&#9500;&#9472;/proc proc proc rw,... </span></span><span><span>&#9500;&#9472;/sys sysfs sysfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/sys/kernel/security securityfs securityfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/sys/fs/cgroup cgroup2 cgroup2 rw,... </span></span><span><span>&#9474; ... </span></span><span><span>&#9492;&#9472;/run tmpfs tmpfs rw,... </span></span><span><span> &#9500;&#9472;/run/lock tmpfs tmpfs rw,... </span></span><span><span> &#9492;&#9472;/run/user/1001 tmpfs tmpfs rw,... </span></span></code></pre></div></div><p>In hindsight, it should probably make sense - after all, we are playing with a <em>mount</em> namespace (and there is no such thing as <em>filesystem</em> namespaces, for better or worse).</p><p>&#128161; <strong>Interesting fact:</strong> Mount namespaces were the first namespace type added to Linux, appearing in Linux 2.4, ca. 2002.</p><div><p>&#128161; <strong>Pro Tip:</strong> You can quickly check the current mount namespace of a process using the following command:</p><div><pre><code><span><span>readlink</span><span> /proc/</span><span>$PID</span><span>/ns/mnt </span></span></code></pre></div><p>Different inode numbers in the output will indicate different namespaces. Try running <code>readlink /proc/self/ns/mnt</code> from <span>Terminal 1</span> and <span>Terminal 2</span>.</p></div><h2>What the heck is Mount Propagation?</h2><p>Before we jump to how exactly mount namespaces are applied by <del>Docker</del> an OCI runtime (e.g., <a href="https://labs.iximiuz.com/challenges/start-container-with-runc">runc</a>) to create containers, we need to learn about one more important (and related) concept - <strong>mount propagation</strong>.</p><p>&#9888;&#65039; Make sure to exit the namespaced shell in <span>Terminal 1</span> before proceeding with the commands in this section.</p><p>If you tried to re-do the experiment from the previous section using the <code>unshare()</code> system call instead of the <code>unshare</code> CLI command, the results might look different.</p><div><p><span>unshare_lite.go</span></p><pre><code><span><span>package</span><span> main </span></span><span><span>import</span><span> "os" </span></span><span><span>import</span><span> "os/exec" </span></span><span><span>import</span><span> "syscall" </span></span><span><span>func</span><span> main</span><span>() { </span></span><span><span> if</span><span> err</span><span> :=</span><span> syscall</span><span>.</span><span>Unshare</span><span>(</span><span>syscall</span><span>.</span><span>CLONE_NEWNS</span><span>); </span><span>err</span><span> !=</span><span> nil</span><span> { </span></span><span><span> panic</span><span>(</span><span>err</span><span>) </span></span><span><span> } </span></span><span><span> cmd</span><span> :=</span><span> exec</span><span>.</span><span>Command</span><span>(</span><span>"bash"</span><span>) </span></span><span><span> cmd</span><span>.</span><span>Stdin</span><span> =</span><span> os</span><span>.</span><span>Stdin </span></span><span><span> cmd</span><span>.</span><span>Stdout</span><span> =</span><span> os</span><span>.</span><span>Stdout </span></span><span><span> cmd</span><span>.</span><span>Stderr</span><span> =</span><span> os</span><span>.</span><span>Stderr </span></span><span><span> cmd</span><span>.</span><span>Env</span><span> =</span><span> os</span><span>.</span><span>Environ</span><span>() </span></span><span><span> cmd</span><span>.</span><span>Run</span><span>() </span></span><span><span>} </span></span></code></pre></div><p>Build the above improvised <code>unshare_lite</code> program with:</p><div><pre><code><span><span>go</span><span> build</span><span> -o</span><span> unshare_lite</span><span> unshare_lite.go </span></span></code></pre></div><p>And run it from <span>Terminal 1</span>:</p><p>Then mount something:</p><p>This time, the results of the <code>ls -l /mnt</code> will look identical in <span>Terminal 1</span> and <span>Terminal 2</span>. <strong>Thus, the mount namespace alone may not be enough to provide the mount table isolation.</strong></p><p>If you compare the mount tables by running <code>findmnt</code> from <span>Terminal 1</span> and <span>Terminal 2</span>, they will look the same:</p><div><div><p>Host namespace</p><p>New namespace</p></div><div><pre><code><span><span>TARGET SOURCE FSTYPE OPTIONS </span></span><span><span>/ /dev/vda ext4 rw,... </span></span><span><span>&#9500;&#9472;/dev devtmpfs devtmpfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/dev/shm tmpfs tmpfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/dev/pts devpts devpts rw,... </span></span><span><span>&#9474; &#9492;&#9472;/dev/mqueue mqueue mqueue rw,... </span></span><span><span>&#9500;&#9472;/proc proc proc rw,... </span></span><span><span>&#9500;&#9472;/sys sysfs sysfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/sys/kernel/security securityfs securityfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/sys/fs/cgroup cgroup2 cgroup2 rw,... </span></span><span><span>&#9474; ... </span></span><span><span>&#9500;&#9472;/run tmpfs tmpfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/run/lock tmpfs tmpfs rw,... </span></span><span><span>&#9474; &#9492;&#9472;/run/user/1001 tmpfs tmpfs rw,... </span></span><span><span>&#9492;&#9472;/mnt /dev/vda[/tmp] ext4 rw,... </span></span></code></pre></div></div><p>When you <em>unshare</em> a new mount namespace, it gets a full copy of the mount table of the caller process. However, changes to the caller's mount table <em>may be propagated</em> to the new mount table and vice versa.</p><p>But why? &#129300;</p><p>Today, containers can easily be the main "consumer" of mount namespaces. However, the applicability of mount namespaces is not limited to containerization use cases. For example, they can be used to provide per-user views of the filesystem.</p><p>The original implementation of mount namespaces came out too strict, and <a href="https://lwn.net/Articles/689856/">it led to tedious repetitive work for system administrators</a>. To alleviate the problem, the kernel was extended with the mechanism of <a href="https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt">shared subtrees</a>, which in particular introduced <strong>mount event propagation</strong> between <em>peer groups</em> (of mount points).</p><div><p><i>Mount event propagation, visualized</i></p></div><p>For instance, if multiple users on the system were using separate mount namespaces to isolate their root filesystems, <strong>without mount event propagation</strong>, mounting a new shared volume would require N <code>mount</code> operations, where N is equal to the number of users. While <strong>with mount event propagation</strong>, system administrators need to mount the volume only once, and the change will be replicated in all <em>peer groups</em>, even across different mount namespaces.</p><p>&#129299; Neither kernel documentation nor the mount namespace man page use the term <strong>mount propagation</strong> - instead, they refer to it as <strong>propagation type</strong> (of a mount point). However, the term <strong>mount propagation</strong> seems to be <a href="https://lwn.net/Articles/690679/">commonly used in the industry</a>, including in the Docker (<a href="https://docs.docker.com/engine/storage/bind-mounts/#configure-bind-propagation">example</a>) and Kubernetes (<a href="https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation">example</a>) documentation.</p><p>Mount event propagation is exactly what we've just observed when we tried using the <code>unshare</code> system call directly from a Go program: when the <code>/tmp</code> folder was bind-mounted to the <code>/mnt</code> folder in the new mount namespace, the original namespace received a <em>mount event</em> and replicated the change creating a similar <code>/tmp:/mnt</code> mount.</p><p>Hmm... Why didn't it happen when we used the standard <code>unshare</code> command-line tool? &#129300;</p><p>The <code>unshare</code> CLI tool does slightly more than just the <code>unshare()</code> system call. You can sneak a peek under the hood of the <code>unshare</code> CLI with the following <code>strace</code> trick (from a <span>fresh terminal</span>):</p><div><pre><code><span><span>sudo</span><span> strace</span><span> unshare</span><span> --mount</span><span> bash </span></span></code></pre></div><p>When you cut through the noise of the trace, you'll spot these three important system calls done in a sequence:</p><div><pre><code><span><span>... </span></span><span><span>unshare(CLONE_NEWNS) = 0 </span></span><span><span>mount("none", "/", NULL, MS_REC|MS_PRIVATE, NULL) = 0 </span></span><span><span>execve("/usr/bin/bash", ["bash"], 0x7fff03d0e038 /* 19 vars */) = 0 </span></span><span><span>... </span></span></code></pre></div><p>Right <em>after</em> creating a new mount namespace and <em>before</em> executing the <code>bash</code> binary, the <code>unshare</code> command also changed the mount propagation type of the root mount point. The above <code>mount()</code> call is equivalent to the following <code>mount</code> command:</p><p>...which means that in the new mount namespace, the root mount and <em>all its sub-mounts</em> (<code>MS_REC</code> and <code>r</code> in <code>rprivate</code> stand for <em>recursive</em>) become completely isolated from the outside world - mounting new filesystems inside the mount namespace won't be noticeable in the caller's (i.e., the host's, in our case) mount namespace and vice versa.</p><p>&#128161; Mount propagation type is a property of a mount point. Since each mount point belongs to the corresponding mount namespace, the mount propagation type is also a namespace-specific property. For instance, the root mount <code>/</code> can have a <code>shared</code> mount propagation type in one namespace and <code>private</code> in another.</p><div><p>No mount event propagation between namespaces:</p><div><pre><code><span><span>sudo</span><span> unshare</span><span> --mount</span><span> --propagation</span><span> private </span></span><span><span>findmnt</span><span> -o</span><span> TARGET,SOURCE,FSTYPE,PROPAGATION </span></span></code></pre></div><div><pre><code>TARGET SOURCE FSTYPE PROPAGATION / /dev/vda ext4 private &#9500;&#9472;/dev devtmpfs devtmpfs private &#9474; &#9500;&#9472;/dev/shm tmpfs tmpfs private &#9474; &#9500;&#9472;/dev/pts devpts devpts private &#9474; &#9492;&#9472;/dev/mqueue mqueue mqueue private &#9500;&#9472;/proc proc proc private &#9474; &#9492;&#9472;/proc/sys/fs/binfmt_misc systemd-1 autofs private &#9474; &#9492;&#9472;/proc/sys/fs/binfmt_misc binfmt_misc binfmt_misc private &#9500;&#9472;/sys sysfs sysfs private &#9474; &#9500;&#9472;/sys/kernel/security securityfs securityfs private &#9474; &#9500;&#9472;/sys/fs/selinux selinuxfs selinuxfs private &#9474; ... &#9492;&#9472;/run tmpfs tmpfs private &#9500;&#9472;/run/lock tmpfs tmpfs private &#9492;&#9472;/run/user/1001 tmpfs tmpfs private </code></pre></div></div><p>Why does mount propagation matter for us? Two reasons:</p><ul><li><code>pivot_root</code>, the modern <code>chroot</code> alternative most container runtimes rely on, comes with its own requirements for the mount propagation type of the involved mount points (we'll see it in the next section).</li><li>Some applications may want to mount filesystems on the host while running in a container and some others may need to spot the host (or peer containers) mounting filesystems in runtime (e.g., <a href="https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation"><code>HostToContainer</code> and <code>Bidirectional</code> mount propagations in Kubernetes</a>). More on it later.</li></ul><h2>A naive attempt to isolate container filesystem</h2><p>Mount namespaces and propagation are great, but how is all this stuff used in containers? Let's try creating a simple container to see this machinery in action.</p><p>&#9888;&#65039; Make sure to exit the namespaced shell in <span>Terminal 1</span> before proceeding with the commands in this section.</p><h3>Preparing container rootfs</h3><p>First off, we'll need to prepare the future root filesystem. From the host's standpoint, each container's rootfs is just a regular folder with some files inside:</p><div><pre><code><span><span>sudo</span><span> mkdir</span><span> -p</span><span> /opt/container-1/rootfs </span></span></code></pre></div><p>For this experiment, we can "borrow" the Alpine filesystem <a href="https://labs.iximiuz.com/tutorials/extracting-container-image-filesystem">by extracting the <code>alpine:3</code> image</a> into the directory we just created:</p><div><pre><code><span><span>crane</span><span> export</span><span> alpine:3</span><span> |</span><span> sudo</span><span> tar</span><span> -xvC</span><span> /opt/container-1/rootfs </span></span></code></pre></div><p>If you compare the contents of the <code>/opt/container-1/rootfs</code> and the host's <code>/</code> folders, they will look surprisingly similar:</p><div><div><pre><code><span><span>tree</span><span> -L</span><span> 1</span><span> /opt/container-1/rootfs </span></span></code></pre></div><div><pre><code>/opt/container-1/rootfs/ &#9500;&#9472;&#9472; bin &#9500;&#9472;&#9472; dev &#9500;&#9472;&#9472; etc &#9500;&#9472;&#9472; home &#9500;&#9472;&#9472; lib ... &#9500;&#9472;&#9472; tmp &#9500;&#9472;&#9472; usr &#9492;&#9472;&#9472; var 18 directories, 0 files </code></pre></div></div><p>However, upon closer inspection, you'll see that it's two different Linux distributions:</p><div><div><pre><code><span><span>cat</span><span> /opt/container-1/rootfs/etc/os-release </span></span></code></pre></div><div><pre><code>NAME="Alpine Linux" ID=alpine VERSION_ID=3.22.1 PRETTY_NAME="Alpine Linux v3.22" HOME_URL="https://alpinelinux.org/" BUG_REPORT_URL="https://gitlab.alpinelinux.org/alpine/aports/-/issues" </code></pre></div></div><h3>Switching to new rootfs (pivot_root)</h3><p>The <a href="https://man7.org/linux/man-pages/man2/pivot_root.2.html"><code>pivot_root(new_root, put_old)</code> syscall</a> changes the root mount <strong>in the mount namespace of the calling process</strong>. More precisely, it moves the current root mount of the caller to the directory <code>put_old</code> and makes <code>new_root</code> the new root mount.</p><p>What it practically means is that by calling <code>pivot_root("/opt/container-1/rootfs")</code> in a new mount namespace, we'll switch to the new root filesystem.</p><p>&#128161; From a layman's standpoint, <code>pivot_root</code> is a safer version of <code>chroot</code> - similar effect but no risk of breakouts via forgotten symlinks to the old root filesystem or the double-chroot trick.</p><p>The <code>pivot_root()</code> call comes with a number of restrictions, in particular:</p><ul><li>The <code>new_root</code> path must be a mount point, but can't be <code>/</code> (well, attempting <code>pivot_root("/")</code> wouldn't make much sense anyway).</li><li>The <strong>propagation type</strong> of the parent mount of <code>new_root</code> and the parent mount of the current root directory must not be <code>shared</code>.</li><li>If <code>put_old</code> is an existing mount point, its <strong>propagation type</strong> must not be <code>shared</code>.</li></ul><p>Expectedly, we only want to perform such a disruptive operation from a separate mount namespace (otherwise, we'd damage the host), and the last two restrictions ensure that <code>pivot_root</code> never propagates any mount table changes to another mount namespace:</p><div><pre><code><span><span>sudo</span><span> unshare</span><span> --mount</span><span> bash </span></span></code></pre></div><p>Now let's try satisfying the <code>pivot_root</code>'s requirements. The propagation type of the <code>/</code> mount (the parent mount of the current root directory) should not be <code>shared</code>. The above <code>unshare</code> command has likely already set it to <code>private</code> but being explicit won't hurt:</p><p>In our case, the <code>/opt/container-1/rootfs</code> folder is not a mount point (it's a regular folder somewhere in the host's filesystem), but we can easily make it a mount point by bind mounting the path onto itself (using a recursive bind mount because hypothetically the container rootfs folder itself can contain sub-mounts):</p><div><pre><code><span><span>mount</span><span> --rbind</span><span> /opt/container-1/rootfs</span><span> /opt/container-1/rootfs </span></span></code></pre></div><p>Lastly, ensuring that the propagation type of the <code>new_root</code> itself isn't <code>shared</code>:</p><div><pre><code><span><span>mount</span><span> --make-rprivate</span><span> /opt/container-1/rootfs </span></span></code></pre></div><p>Now we're ready to <del>choort</del> pivot the root filesystem:</p><div><pre><code><span><span>cd</span><span> /opt/container-1/rootfs </span></span><span><span>mkdir</span><span> .oldroot </span></span></code></pre></div><p>...and immediately after that, switch to a shell from the new rootfs because the current <code>bash</code> process may get broken in subtle ways after a <code>pivot_root</code> into a completely different Linux distro (this part is only needed for our demo example - real-world container runtimes usually don't have this issue because they communicate with the kernel directly, using syscalls instead of shell commands):</p><p>Interestingly, after the <code>pivot_root</code> operation, <a href="https://github.com/opencontainers/runtime-spec/blob/main/config-linux.md#rootfs-mount-propagation">container runtimes are free to set the propagation type of the new root filesystem to pretty much any value</a> (<code>shared</code>, <code>slave</code>, <code>private</code>, and even <code>unbindable</code>):</p><p>&#128161; Propagation type of the container root filesystem should not be confused with the propagation type of bind mounts and volumes in Docker and Kubernetes respectively (see below). This is an advanced setting that is often not even exposed through the user-facing APIs of the higher-level container runtimes, and the most typical use case for it is nested containers (e.g., <a href="https://hub.docker.com/_/docker">DinD</a>).</p><p>Finally, since you probably don't want the original root filesystem to be accessible in the container, the <code>.oldroot</code> can (and should) be removed right after the <code>pivot_root</code> call:</p><div><pre><code><span><span>umount</span><span> -l</span><span> .oldroot</span><span> # -l stands for "lazy" because the fs can be busy </span></span><span><span>rm</span><span> -rf</span><span> .oldroot </span></span></code></pre></div><p>Yay! We've just pivoted into a new container. Let's look around:</p><div><pre><code>ls -l / total 68 drwxr-xr-x 2 root root 4096 Jul 15 10:42 bin drwxr-xr-x 2 root root 4096 Sep 7 12:40 dev drwxr-xr-x 17 root root 4096 Jul 15 10:42 etc drwxr-xr-x 2 root root 4096 Jul 15 10:42 home ... drwxr-xr-x 11 root root 4096 Jul 15 10:42 var </code></pre></div><div><pre><code>NAME="Alpine Linux" ID=alpine VERSION_ID=3.22.1 PRETTY_NAME="Alpine Linux v3.22" HOME_URL="https://alpinelinux.org/" BUG_REPORT_URL="https://gitlab.alpinelinux.org/alpine/aports/-/issues" </code></pre></div><p>So far so good! But if you try listing processes, the output will be empty (which of course can't be true):</p><p>And the <code>df</code> command also seems broken:</p><div><pre><code>Filesystem Size Used Available Use% Mounted on df: /proc/mounts: No such file or directory </code></pre></div><h2>Preparing a complete container filesystem</h2><p>The <code>df</code>'s error message contained a hint - the <code>/proc</code> folder is empty in the new mount namespace:</p><p>Hmm... How come?</p><p>Well, apparently, not every part of the container root filesystem comes from its image!</p><p>Similarly to the host, where <a href="https://docs.kernel.org/filesystems/proc.html"><code>/proc</code> is populated by the corresponding kernel pseudo filesystem</a>, container's <code>/proc</code> needs to be set up separately. And the same goes for <code>/dev</code> and <code>/sys</code> virtual filesystems.</p><p>On top of that, some special files like <code>/etc/hosts</code>, <code>/etc/hostname</code>, or <code>/etc/resolv.conf</code> should be crafted for each container individually because the corresponding files in the image (if present) can only contain generic values (e.g., <code>localhost</code>) while Docker typically sets the hostname of a container to a prefix of its random ID and derives the <code>resolv.conf</code> from the eponymous file on the host.</p><h3>Populating /proc pseudo filesystem</h3><p>Populating the <code>/proc</code> pseudo filesystem is as simple as:</p><p>&#128161; In reality, container runtimes usually populate the <code>/proc</code> filesystem <em>before</em> the <code>pivot_root</code> call, so the command would look like <code>mount -t proc proc $ROOTFS/proc</code>.</p><p>However, if you run the above command right away, the <code>/proc</code> filesystem in the container will look exactly the same as the one on the host. In particular, it means that the <code>ps</code> command will start showing the full list of processes on the server, which is usually undesirable in a container.</p><p>This is where the <strong>PID namespace</strong> comes into play. We need to go a few steps back and adjust the <code>unshare</code> command to create not just the mount but also a new PID namespace, so that the container's topmost process would become PID 1 and the process hierarchy in the container would start from it:</p><div><pre><code><span><span># DO NOT RUN ME </span></span><span><span>sudo</span><span> unshare</span><span> --mount</span><span> --pid</span><span> --fork</span><span> bash </span></span></code></pre></div><p>But let's not do it just yet...</p><p>&#128161; The extra <code>--fork</code> flag above doesn't create any new namespaces, but rather makes <code>unshare</code> create a new process instead of exec'ing the <code>bash</code> command directly. This is a requirement to make the <code>--pid</code> flag actually have the effect on the unshared command because it's the first child that gets placed into the new PID namespace, not the process that called <code>unshare(CLONE_NEWPID)</code> itself.</p><h3>Populating /dev pseudo filesystem</h3><p>Another special folder is <code>/dev</code>. On the host, it's typically provided by the <code>devtmpfs</code> and a number of subordinate virtual filesystems (from a <span>fresh terminal</span>):</p><div><pre><code>TARGET SOURCE FSTYPE OPTIONS / /dev/vda ext4 rw,relatime,stripe=4 &#9500;&#9472;/dev devtmpfs devtmpfs rw,relatime,size=4068368k,nr_inodes=1017092,mode=755 &#9474; &#9500;&#9472;/dev/shm tmpfs tmpfs rw,nosuid,nodev &#9474; &#9500;&#9472;/dev/pts devpts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 &#9474; &#9500;&#9472;/dev/mqueue mqueue mqueue rw,nosuid,nodev,noexec,relatime ... </code></pre></div><p>However, containers usually get a more limited version of the <code>/dev</code> folder, backed by a regular <code>tmpfs</code>. Here is how it can be populated from inside the new mount namespace (back from <span>Terminal 1</span>):</p><div><pre><code><span><span>mkdir</span><span> -p</span><span> /dev </span></span><span><span>mount</span><span> -t</span><span> tmpfs</span><span> -o</span><span> nosuid,strictatime,mode=0755,size=65536k</span><span> tmpfs</span><span> /dev </span></span></code></pre></div><p>&#128161; In reality, container runtimes usually populate the <code>/dev</code> filesystem <em>before</em> the <code>pivot_root</code> call, so the command would look like <code>mount -t tmpfs ... $ROOTFS/dev</code>.</p><p>After mounting the <code>/dev</code> tmpfs, you'd need to create special character devices such as <code>/dev/null</code>, <code>/dev/zero</code>, <code>/dev/random</code>, etc. Here is how you can do it using the <code>mknod</code> command:</p><div><pre><code><span><span>mknod</span><span> -m</span><span> 666</span><span> /dev/null</span><span> c</span><span> 1</span><span> 3 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> /dev/zero</span><span> c</span><span> 1</span><span> 5 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> /dev/full</span><span> c</span><span> 1</span><span> 7 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> /dev/random</span><span> c</span><span> 1</span><span> 8 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> /dev/urandom</span><span> c</span><span> 1</span><span> 9 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> /dev/tty</span><span> c</span><span> 5</span></span><span></span><span><span>chown</span><span> root:root</span><span> "/dev/{null,zero,full,random,urandom,tty} </span></span></code></pre></div><p>Then, mount the subordinate filesystems (<code>/dev/shm</code>, <code>/dev/pts</code>, and <code>/dev/mqueue</code>):</p><div><pre><code><span><span>mkdir</span><span> -p</span><span> /dev/{shm,pts,mqueue} </span></span><span><span>mount</span><span> -t</span><span> tmpfs</span><span> -o</span><span> nosuid,nodev,noexec,mode=1777,size=</span><span>67108864</span><span> tmpfs</span><span> /dev/shm </span></span><span><span>mount</span><span> -t</span><span> devpts</span><span> -o</span><span> newinstance,ptmxmode=0666,mode=</span><span>0620</span><span> devpts</span><span> /dev/pts </span></span><span><span>mount</span><span> -t</span><span> mqueue</span><span> -o</span><span> nosuid,nodev,noexec</span><span> mqueue</span><span> /dev/mqueue </span></span></code></pre></div><p>And lastly, set up some well-known symlinks:</p><div><pre><code><span><span>ln</span><span> -sf</span><span> /proc/self/fd</span><span> /dev/fd </span></span><span><span>ln</span><span> -sf</span><span> /proc/self/fd/0</span><span> /dev/stdin </span></span><span><span>ln</span><span> -sf</span><span> /proc/self/fd/1</span><span> /dev/stdout </span></span><span><span>ln</span><span> -sf</span><span> /proc/self/fd/2</span><span> /dev/stderr </span></span><span><span>ln</span><span> -sf</span><span> /proc/kcore</span><span> /dev/core </span></span></code></pre></div><h3>Populating /sys pseudo filesystem</h3><p>The most limited of the containers' pseudo filesystems is probably <a href="https://www.kernel.org/doc/Documentation/filesystems/sysfs.txt"><code>/sys</code></a>. It's usually mounted read-only and contains only a few nodes:</p><div><pre><code><span><span>mount</span><span> -t</span><span> sysfs</span><span> -o</span><span> ro,nosuid,nodev,noexec</span><span> sysfs</span><span> /sys </span></span></code></pre></div><p>&#128161; In reality, container runtimes usually populate the <code>/sys</code> filesystem <em>before</em> the <code>pivot_root</code> call, so the command would look like <code>mount -t sysfs ... $ROOTFS/sys</code>.</p><p>A prominent part of the <code>/sys</code> filesystem is the <a href="https://labs.iximiuz.com/tutorials/controlling-process-resources-with-cgroups/">virtual cgroup filesystem</a>. Since a few years ago, Docker and other popular container runtimes started fully isolating the container's cgroup hierarchy by default. Similarly to the <code>/proc</code> filesystem that works best in combination with a new PID namespace, a new <strong>cgroup namespace</strong> can be used to make the <code>cgroup2</code> mount rooted at the host's cgroupfs node that corresponds to the container's topmost process. Thus, the <code>unshare</code> command <strong>would need</strong> one more flag, <code>--cgroup</code>:</p><div><pre><code><span><span># DO NOT RUN ME </span></span><span><span>sudo</span><span> unshare</span><span> --mount</span><span> --pid</span><span> --fork</span><span> --cgroup</span><span> bash </span></span></code></pre></div><p>To mount the <code>cgroup2</code> filesystem, you can use the following command:</p><div><pre><code><span><span>mkdir</span><span> -p</span><span> /sys/fs/cgroup </span></span><span><span>mount</span><span> -t</span><span> cgroup2</span><span> -o</span><span> ro,nosuid,nodev,noexec</span><span> cgroup2</span><span> /sys/fs/cgroup </span></span></code></pre></div><h3>Hardening pseudo filesystems</h3><p>While it is not strictly necessary for a demo, real-world container root filesystems usually go through an extra round of hardening. For instance, Docker typically marks a few parts of the <code>/proc</code> filesystem as <em>read-only</em> and <em>masks</em> others, making them completely inaccessible to the containerized app.</p><p>Here is how you can get a list of sensitive locations that are made read-only by Docker (from a <span>fresh terminal</span>):</p><div><pre><code><span><span>docker</span><span> container</span><span> inspect</span><span> \ </span></span><span><span> $(</span><span>docker</span><span> run</span><span> --rm</span><span> -d</span><span> alpine:3</span><span> sleep</span><span> 5</span><span>) </span><span>\ </span></span><span><span> --format</span><span> '{{join .HostConfig.ReadonlyPaths "\n"}}' </span></span></code></pre></div><div><pre><code>/proc/bus /proc/fs /proc/irq /proc/sys /proc/sysrq-trigger </code></pre></div><p>You can make any file or folder read-only by binding it to itself and remounting it using the <code>ro</code> option:</p><div><pre><code><span><span>RO_PATH</span><span>=</span><span>/proc/bus</span><span> # or /proc/fs, /proc/irq, etc. </span></span><span><span>if</span><span> [[ </span><span>-e</span><span> "</span><span>$RO_PATH</span><span>"</span><span> ]]; </span><span>then </span></span><span><span> mount</span><span> --bind</span><span> "</span><span>$RO_PATH</span><span>"</span><span> "</span><span>$RO_PATH</span><span>" </span></span><span><span> mount</span><span> -o</span><span> remount,bind,ro</span><span> "</span><span>$RO_PATH</span><span>" </span></span><span><span>fi </span></span></code></pre></div><p>Similarly, here is how you can get a list of locations that are typically made completely inaccessible (through masking) to the containerized app:</p><div><pre><code><span><span>docker</span><span> container</span><span> inspect</span><span> \ </span></span><span><span> $(</span><span>docker</span><span> run</span><span> --rm</span><span> -d</span><span> alpine:3</span><span> sleep</span><span> 5</span><span>) </span><span>\ </span></span><span><span> --format</span><span> '{{join .HostConfig.MaskedPaths "\n"}}' </span></span></code></pre></div><div><pre><code>/proc/asound /proc/acpi /proc/interrupts /proc/kcore /proc/keys /proc/latency_stats /proc/timer_list /proc/timer_stats /proc/sched_debug /proc/scsi /sys/firmware /sys/devices/virtual/powercap </code></pre></div><p>Masking of folders and regular files differs. To mask a folder, a read-only <code>tmpfs</code> filesystem can be mounted over it, and to mask a regular file, the <code>/dev/null</code> device can be bound to its path.</p><div><pre><code><span><span>MASKED_FILE</span><span>=</span><span>/proc/asound</span><span> # or /proc/interrupts, /proc/kcore, etc. </span></span><span><span>mount</span><span> --bind</span><span> /dev/null</span><span> $MASKED_FILE </span></span><span><span>MASKED_DIR</span><span>=</span><span>/proc/acpi</span><span> # or /proc/scsi, etc. </span></span><span><span>mount</span><span> -t</span><span> tmpfs</span><span> -o</span><span> ro</span><span> tmpfs</span><span> $MASKED_DIR </span></span></code></pre></div><p>&#128161; The above read-only and masked paths are Docker's <strong>defaults for non-privileged containers</strong>, while the OCI Runtime Spec defines only the hardening mechanism and not the exact locations (see <a href="https://github.com/opencontainers/runtime-spec/blob/e3c8d12d94cdd269a145a263ace7457f56c74eff/config-linux.md#masked-paths">Masked Paths</a> and <a href="https://github.com/opencontainers/runtime-spec/blob/e3c8d12d94cdd269a145a263ace7457f56c74eff/config-linux.md#readonly-paths">Readonly Paths</a>).</p><h3>Preparing special /etc files</h3><p>Some of the regular files in the container rootfs also require special treatment:</p><ul><li><code>/etc/hosts</code></li><li><code>/etc/hostname</code></li><li><code>/etc/resolv.conf</code></li></ul><p>Inspecting these files in the <code>/opt/container-1/rootfs</code> folder right after extracting the Alpine rootfs into it would reveal why:</p><div><pre><code><span><span>cat</span><span> /opt/container-1/rootfs/etc/{hosts,hostname,resolv.conf} </span></span></code></pre></div><div><pre><code># -- /opt/container-1/rootfs/etc/hosts 127.0.0.1 localhost localhost.localdomain ::1 localhost localhost.localdomain # -- /opt/container-1/rootfs/etc/hostname localhost # -- /opt/container-1/rootfs/etc/resolv.conf cat: /opt/container-1/rootfs/resolv.conf: No such file or directory </code></pre></div><p>The above are some generic values that come directly from the <code>alpine:3</code> image, which wouldn't make much sense in any particular container. At the same time, these files would look very different when inspected from a running <code>alpine:3</code> container:</p><div><pre><code><span><span>docker</span><span> run</span><span> --rm</span><span> alpine:3</span><span> cat</span><span> /etc/{hosts,hostname,resolv.conf} </span></span></code></pre></div><div><pre><code># -- /etc/hosts 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback 172.17.0.2 2f26e97ae70c # -- /etc/hostname 2f26e97ae70c # -- /etc/resolv.conf # Generated by Docker Engine. # This file can be edited; Docker Engine will not make further changes once it # has been modified. nameserver 168.119.149.157 nameserver 8.8.8.8 nameserver 1.1.1.1 # Based on host file: '/etc/resolv.conf' (legacy) # Overrides: [] </code></pre></div><p>Thus, Docker (or one of its underlying runtimes) replaces the generic <code>/etc/hosts</code>, <code>/etc/hostname</code>, and <code>/etc/resolv.conf</code> files from the image with container-specific variants.</p><p>We can do it, too! Our container has no network interfaces (modulo <code>loopback</code>), but it can still have a proper hostname set (from the host's <span>terminal</span>):</p><div><pre><code><span><span>cat</span><span> &lt;&lt;</span><span>EOF</span><span> |</span><span> sudo</span><span> tee</span><span> /opt/container-1/hosts </span></span><span><span>127.0.0.1 localhost container-1 </span></span><span><span>::1 localhost ip6-localhost ip6-loopback </span></span><span><span>EOF </span></span></code></pre></div><div><pre><code><span><span>cat</span><span> |</span><span> sudo</span><span> tee</span><span> /opt/container-1/hostname</span><span> &lt;&lt;</span><span>EOF </span></span><span><span>container-1 </span></span><span><span>EOF </span></span></code></pre></div><div><pre><code><span><span>sudo</span><span> cp</span><span> /etc/resolv.conf</span><span> /opt/container-1/resolv.conf </span></span></code></pre></div><p>&#128161; The <code>/etc/resolv.conf</code> file is usually based on the host's <code>/etc/resolv.conf</code> file, and then potentially adjusted to the container's needs.</p><p>The most interesting part is how these files are placed into the container's rootfs. Instead of just overwriting the files from the image, container runtimes usually mount the container-specific variants of these files on top of the original ones, effectively masking them:</p><div><pre><code><span><span>sudo</span><span> mount</span><span> --bind</span><span> /opt/container-1/hosts</span><span> /opt/container-1/rootfs/etc/hosts </span></span><span><span>sudo</span><span> mount</span><span> --bind</span><span> /opt/container-1/hostname</span><span> /opt/container-1/rootfs/etc/hostname </span></span><span><span>sudo</span><span> mount</span><span> --bind</span><span> /opt/container-1/resolv.conf</span><span> /opt/container-1/rootfs/etc/resolv.conf </span></span></code></pre></div><p>Last but not least, for the container to have its own hostname, the container needs to use a new <strong>network</strong> and <strong>UTS namespaces</strong>, so the <code>unshare</code> command would need to have two more flags (<code>--uts</code> and <code>--net</code>):</p><div><pre><code><span><span># DO NOT RUN ME </span></span><span><span>sudo</span><span> unshare</span><span> --mount</span><span> --pid</span><span> --fork</span><span> --cgroup</span><span> --uts</span><span> --net</span><span> bash </span></span></code></pre></div><p>&#128161; If we forget to use a new <strong>UTS namespace</strong>, setting the hostname in the new container will overwrite the host's hostname, which is something we definitely don't want. And without a new <strong>network namespace</strong>, the container simply cannot have its own hostname, because then it technically <a href="https://labs.iximiuz.com/tutorials/container-networking-from-scratch">has the same network stack as the host (which in particular includes the hostname)</a>.</p><p>Finally, we're ready to prepare a fully isolated container filesystem!</p><h2>Creating a container from scratch (end-to-end example)</h2><p>With all the above lessons learned, let's try creating our second container, this time applying all the necessary namespaces and rootfs adjustments.</p><div><p>&#128161; The below commands are based on the <a href="https://github.com/opencontainers/runc/blob/b27d6f3f1af9a56f2770c8ec6e1a1ff986ca9c09/libcontainer/rootfs_linux.go">real container preparation steps taken by the runc runtime</a> obtained with the following <code>strace</code> trick:</p><div><pre><code><span><span># Terminal 1 </span></span><span><span>sudo</span><span> strace</span><span> -f</span><span> -qqq</span><span> -e</span><span> \ </span></span><span><span> trace=/clone,/exec,/unshare,/mount,/mknod,/mkdir,/link,/chdir,/root</span><span> \ </span></span><span><span> -p</span><span> $(</span><span>pgrep</span><span> containerd</span><span>) </span></span></code></pre></div><div><pre><code><span><span># Terminal 2 </span></span><span><span>docker</span><span> run</span><span> alpine:3</span><span> sleep</span><span> 9999 </span></span></code></pre></div></div><h3>Step 1: Prepare rootfs files</h3><p>&#9888;&#65039; Make sure to exit the namespaced shell in <span>Terminal 1</span> before proceeding with the commands in this section.</p><p>The second container will be stored in the <code>/opt/container-2</code> directory:</p><div><pre><code><span><span>CONTAINER_DIR</span><span>=</span><span>/opt/container-2 </span></span><span><span>ROOTFS_DIR</span><span>=</span><span>${</span><span>CONTAINER_DIR</span><span>}</span><span>/rootfs </span></span></code></pre></div><p>Similar to the first container, we'll use the <code>alpine:3</code> image to "borrow" the rootfs files:</p><div><pre><code><span><span>sudo</span><span> mkdir</span><span> -p</span><span> $ROOTFS_DIR </span></span><span><span>crane</span><span> export</span><span> alpine:3</span><span> |</span><span> sudo</span><span> tar</span><span> -xvC</span><span> $ROOTFS_DIR </span></span></code></pre></div><p>This time, we'll create the <code>/etc/hosts</code>, <code>/etc/hostname</code>, and <code>/etc/resolv.conf</code> files beforehand (but store them outside of the rootfs dir for now):</p><div><pre><code><span><span>cat</span><span> &lt;&lt;</span><span>EOF</span><span> |</span><span> sudo</span><span> tee</span><span> $CONTAINER_DIR</span><span>/hosts </span></span><span><span>127.0.0.1 localhost container-2 </span></span><span><span>::1 localhost ip6-localhost ip6-loopback </span></span><span><span>EOF </span></span></code></pre></div><div><pre><code><span><span>cat</span><span> &lt;&lt;</span><span>EOF</span><span> |</span><span> sudo</span><span> tee</span><span> $CONTAINER_DIR</span><span>/hostname </span></span><span><span>container-2 </span></span><span><span>EOF </span></span></code></pre></div><div><pre><code><span><span>sudo</span><span> cp</span><span> /etc/resolv.conf</span><span> $CONTAINER_DIR</span><span>/resolv.conf </span></span></code></pre></div><h3>Step 2: Create namespaces</h3><p>Create all the required namespaces with the <code>unshare</code> command (<strong>mount</strong>, <strong>PID</strong>, <strong>cgroup</strong>, <strong>UTS</strong>, and <strong>network</strong>):</p><div><pre><code><span><span>sudo</span><span> unshare</span><span> --mount</span><span> --pid</span><span> --fork</span><span> --cgroup</span><span> --uts</span><span> --net</span><span> bash </span></span></code></pre></div><div><p>&#128161; <a href="https://man7.org/linux/man-pages/man7/namespaces.7.html">Other possible namespaces are</a>:</p><ul><li><code>ipc</code> - this namespace has no impact on the rootfs creation, so we're skipping it for brevity</li><li><code>time</code> - (optional) not used by Docker or other mainstream container runtimes yet</li><li><code>user</code> - (optional) rootless containers is an advanced topic that deserves its own tutorial</li></ul></div><h3>Step 3: Isolate new mount namespace</h3><div><p>From now on, all commands are executed as <code>root</code> and in the new namespaces, so we're skipping the <code>sudo</code> prefix, and the <code>CONTAINER_DIR</code> and <code>ROOTFS_DIR</code> variables may need to be re-set:</p><div><pre><code><span><span>CONTAINER_DIR</span><span>=</span><span>/opt/container-2 </span></span><span><span>ROOTFS_DIR</span><span>=</span><span>${</span><span>CONTAINER_DIR</span><span>}</span><span>/rootfs </span></span></code></pre></div></div><p>First, we need to make sure that no mount events are propagated back to the host's mount namespace:</p><p>Then, we need to make sure that the root filesystem itself is a mount point:</p><div><pre><code><span><span>mount</span><span> --rbind</span><span> $ROOTFS_DIR</span><span> $ROOTFS_DIR </span></span></code></pre></div><p>...and that the propagation type of the root filesystem isn't <code>shared</code>:</p><div><pre><code><span><span>mount</span><span> --make-private</span><span> $ROOTFS_DIR </span></span></code></pre></div><h3>Step 4: Prepare /proc pseudo filesystem</h3><p>Mount <code>/proc</code> pseudo filesystem:</p><div><pre><code><span><span>mkdir</span><span> -p</span><span> $ROOTFS_DIR</span><span>/proc </span></span><span><span>mount</span><span> -t</span><span> proc</span><span> proc</span><span> $ROOTFS_DIR</span><span>/proc </span></span></code></pre></div><div><p>&#9888;&#65039; <strong>Security Caveat:</strong> In untrusted rootfs, <code>$ROOTFS_DIR/<path></path></code> can be a symlink pointing outside of <code>$ROOTFS_DIR</code>. This can make the above and many of the below operations corrupt the host system.</p><p>Real-world container runtimes typically use the <a href="https://man7.org/linux/man-pages/man2/openat2.2.html"><code>openat2()</code> syscall with the <code>RESOLVE_NO_SYMLINKS</code> flag</a> to first open the target file or directory ensuring it's not a symlink, and then use <code>mount</code> (or other filesystem operations) on an open file descriptor instead of a textual filename. The latter helps to avoid <a href="https://en.wikipedia.org/wiki/Time-of-check_to_time-of-use">TOCTTOU vulnerabilities</a> when the <code>$ROOTFS_DIR</code> contents are changed <em>while</em> the container is being created.</p><p>However, in a demo context it should be relatively safe to operate with regular filenames. So, we'll do it the simpler way for brevity.</p></div><h3>Step 5: Prepare /dev pseudo filesystem</h3><p>Mount <code>/dev</code> pseudo filesystem as a regular <code>tmpfs</code>:</p><div><pre><code><span><span>mount</span><span> -t</span><span> tmpfs</span><span> \ </span></span><span><span> -o</span><span> nosuid,strictatime,mode=0755,size=65536k</span><span> tmpfs</span><span> \ </span></span><span><span> $ROOTFS_DIR</span><span>/dev </span></span></code></pre></div><p>Create the standard character devices (<code>/dev/null</code>, <code>/dev/zero</code>, <code>/dev/random</code>, etc.):</p><div><pre><code><span><span>mknod</span><span> -m</span><span> 666</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/null"</span><span> c</span><span> 1</span><span> 3 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/zero"</span><span> c</span><span> 1</span><span> 5 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/full"</span><span> c</span><span> 1</span><span> 7 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/random"</span><span> c</span><span> 1</span><span> 8 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/urandom"</span><span> c</span><span> 1</span><span> 9 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/tty"</span><span> c</span><span> 5</span></span><span></span><span><span>chown</span><span> root:root</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/"{null,zero,full,random,urandom,tty} </span></span></code></pre></div><p>Create typical symlinks:</p><div><pre><code><span><span>ln</span><span> -sf</span><span> /proc/self/fd</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/fd" </span></span><span><span>ln</span><span> -sf</span><span> /proc/self/fd/0</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/stdin" </span></span><span><span>ln</span><span> -sf</span><span> /proc/self/fd/1</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/stdout" </span></span><span><span>ln</span><span> -sf</span><span> /proc/self/fd/2</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/stderr" </span></span><span><span>ln</span><span> -sf</span><span> /proc/kcore</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/core" </span></span></code></pre></div><p>Create subordinate filesystems (<code>/dev/pts</code>, <code>/dev/shm</code>, <code>/dev/mqueue</code>):</p><div><pre><code><span><span>mkdir</span><span> -p</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/pts" </span></span><span><span>mount</span><span> -t</span><span> devpts</span><span> \ </span></span><span><span> -o</span><span> newinstance,ptmxmode=0666,mode=</span><span>0620</span><span> devpts</span><span> \ </span></span><span><span> $ROOTFS_DIR</span><span>/dev/pts </span></span><span><span>ln</span><span> -sf</span><span> /dev/pts/ptmx</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/ptmx" </span></span></code></pre></div><div><pre><code><span><span>mkdir</span><span> -p</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/mqueue" </span></span><span><span>mount</span><span> -t</span><span> mqueue</span><span> \ </span></span><span><span> -o</span><span> nosuid,nodev,noexec</span><span> mqueue</span><span> \ </span></span><span><span> $ROOTFS_DIR</span><span>/dev/mqueue </span></span></code></pre></div><div><pre><code><span><span>mkdir</span><span> -p</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/shm" </span></span><span><span>mount</span><span> -t</span><span> tmpfs</span><span> \ </span></span><span><span> -o</span><span> nosuid,nodev,noexec,mode=1777,size=</span><span>67108864</span><span> tmpfs</span><span> \ </span></span><span><span> $ROOTFS_DIR</span><span>/dev/shm </span></span></code></pre></div><h3>Step 6: Prepare /sys pseudo filesystem</h3><p>Mount a read-only <code>/sys</code> pseudo filesystem:</p><div><pre><code><span><span>mkdir</span><span> -p</span><span> "</span><span>$ROOTFS_DIR</span><span>/sys" </span></span><span><span>mount</span><span> -t</span><span> sysfs</span><span> \ </span></span><span><span> -o</span><span> ro,nosuid,nodev,noexec</span><span> sysfs</span><span> \ </span></span><span><span> $ROOTFS_DIR</span><span>/sys </span></span></code></pre></div><p>Mount the subordinate <code>cgroup2</code> filesystem as <code>/sys/fs/cgroup</code>:</p><div><pre><code><span><span>mkdir</span><span> -p</span><span> "</span><span>$ROOTFS_DIR</span><span>/sys/fs/cgroup" </span></span><span><span>mount</span><span> -t</span><span> cgroup2</span><span> \ </span></span><span><span> -o</span><span> ro,nosuid,nodev,noexec</span><span> cgroup2</span><span> \ </span></span><span><span> $ROOTFS_DIR</span><span>/sys/fs/cgroup </span></span></code></pre></div><h3>Step 7: Bind hostname, hosts, and resolv.conf files</h3><p>Bind the container-specific <code>hostname</code>, <code>hosts</code>, and <code>resolv.conf</code> files from <code>/opt/container-2</code>, masking the original files in the rootfs' <code>/etc</code> directory:</p><div><pre><code><span><span>for</span><span> p</span><span> in</span><span> hostname</span><span> hosts</span><span> resolv.conf </span></span><span><span>do </span></span><span><span> touch</span><span> $ROOTFS_DIR</span><span>/etc/</span><span>$p </span></span><span><span> mount</span><span> --bind</span><span> "</span><span>$CONTAINER_DIR</span><span>/</span><span>$p</span><span>"</span><span> $ROOTFS_DIR</span><span>/etc/</span><span>$p </span></span><span><span>done </span></span></code></pre></div><h3>Step 8: Pivot into the new rootfs</h3><p>Finally, pivot into the fully prepared root filesystem:</p><div><pre><code><span><span>cd</span><span> $ROOTFS_DIR </span></span><span><span>mkdir</span><span> -p</span><span> .oldroot </span></span><span><span>pivot_root</span><span> .</span><span> .oldroot </span></span></code></pre></div><div><p>This is not something a real runtime would do, but since we use a shell, it's better to exec into the target container's shell as soon as possible after the <code>pivot_root</code> call:</p></div><p>Configure the propagation type of the container's root filesystem (setting it arbitrarily to <code>slave</code>, <a href="https://github.com/opencontainers/runtime-spec/blob/383cadbf08c0be925b62a4532e99a538249797e6/config-linux.md#rootfs-mount-propagation">but the OCI Runtime Specification supports <code>private</code> and even <code>shared</code></a>):</p><p>And lastly, getting rid of the link to the old root filesystem:</p><div><pre><code><span><span>umount</span><span> -l</span><span> .oldroot </span></span><span><span>rmdir</span><span> .oldroot </span></span></code></pre></div><p>Set the hostname of the container using the value from the container's <code>/etc/hostname</code> file:</p><div><pre><code><span><span>hostname</span><span> $(</span><span>cat</span><span> /etc/hostname</span><span>) </span></span></code></pre></div><h3>Step 9: Harden container filesystem</h3><p>Making a good part of the <code>/proc</code> filesystem read-only:</p><div><pre><code><span><span>for</span><span> d</span><span> in</span><span> bus</span><span> fs</span><span> irq</span><span> sys</span><span> sysrq-trigger </span></span><span><span>do </span></span><span><span> if</span><span> [ </span><span>-e</span><span> "/proc/</span><span>$d</span><span>"</span><span> ]; </span><span>then </span></span><span><span> mount</span><span> --bind</span><span> "/proc/</span><span>$d</span><span>"</span><span> "/proc/</span><span>$d</span><span>" </span></span><span><span> mount</span><span> -o</span><span> remount,bind,ro</span><span> "/proc/</span><span>$d</span><span>" </span></span><span><span> fi </span></span><span><span>done </span></span></code></pre></div><p>Masking sensitive paths in the <code>/proc</code> and <code>/sys</code> filesystems:</p><div><pre><code><span><span>for</span><span> p</span><span> in</span><span> \ </span></span><span><span> /proc/asound</span><span> \ </span></span><span><span> /proc/interrupts</span><span> \ </span></span><span><span> /proc/kcore</span><span> \ </span></span><span><span> /proc/keys</span><span> \ </span></span><span><span> /proc/latency_stats</span><span> \ </span></span><span><span> /proc/timer_list</span><span> \ </span></span><span><span> /proc/timer_stats</span><span> \ </span></span><span><span> /proc/sched_debug</span><span> \ </span></span><span><span> /proc/acpi</span><span> \ </span></span><span><span> /proc/scsi</span><span> \ </span></span><span><span> /sys/firmware </span></span><span><span>do </span></span><span><span> if</span><span> [ </span><span>-d</span><span> "</span><span>$p</span><span>"</span><span> ]; </span><span>then </span></span><span><span> # Masking a folder </span></span><span><span> mount</span><span> -t</span><span> tmpfs</span><span> -o</span><span> ro</span><span> tmpfs</span><span> $p </span></span><span><span> elif</span><span> [ </span><span>-f</span><span> "</span><span>$p</span><span>"</span><span> ]; </span><span>then </span></span><span><span> # Masking a regular file </span></span><span><span> mount</span><span> --bind</span><span> /dev/null</span><span> $p </span></span><span><span> fi </span></span><span><span>done </span></span></code></pre></div><h3>Step 10: Execute target application</h3><p>At this point, the containerized environment is ready to be used. Feel free to look around using the <code>ps</code>, <code>ls</code>, <code>mount</code>, <code>df</code>, <code>hostname</code>, and any other commands you can think of, and then <code>exec</code> the containerized application:</p><div><pre><code><span><span>APP</span><span>=</span><span>${</span><span>APP</span><span>:-/</span><span>bin</span><span>/</span><span>sh</span><span>} </span></span><span><span>exec</span><span> $APP </span></span></code></pre></div><h2>Bonus: Sharing host files and folders with containers</h2><p>One of the very common Docker use cases, especially during local development, is sharing files and folders from the host into the container via <a href="https://docs.docker.com/engine/storage/bind-mounts/">bind mounts</a> like this:</p><div><pre><code><span><span># Traditional -v|--volume flag </span></span><span><span>docker</span><span> run</span><span> -v</span><span> ./data:/data</span><span> redis </span></span><span><span># More modern but equivalent --mount form </span></span><span><span>docker</span><span> run</span><span> --mount</span><span> type='bind,src=./data,dst=/data'</span><span> redis </span></span></code></pre></div><p>In the previous section(s), we saw that regular files located on the host can be bind mounted into the future container's root filesystem. This is exactly how <del>Docker</del> runc and similar container runtimes inject the customized <code>/etc/hosts</code>, <code>/etc/hostname</code>, and <code>/etc/resolv.conf</code> files.</p><p>But the exact same technique can be used to inject any other files or folders from the host into the container.</p><p>The <code>strace -p $(pgrep containerd)</code> command that we used to reverse engineer the rootfs preparation steps will reveal that the bind mounts of the <code>-v|--volume</code> flag happen right after the pseudo filesystems preparation and just before the mounts of the <code>/etc/hosts</code>, <code>/etc/hostname</code>, and <code>/etc/resolv.conf</code> files.</p><p>And it's a good thing we invested some time in learning about the <em>mount event propagation</em> mechanism - Docker allows <a href="https://docs.docker.com/engine/storage/bind-mounts/#configure-bind-propagation">configuring the propagation type for bind mounts</a>, so the following command should not look like a magic spell anymore:</p><div><pre><code><span><span>docker</span><span> run</span><span> -v</span><span> .:/project:ro,rshared</span><span> ... </span></span></code></pre></div><p>In the above example, if the containerized application would mount a sub-folder under <code>/project</code>, it would be visible on the host as well (and vice versa). However, the default propagation type of a Docker bind mount is <code>rprivate</code>, so don't be surprised if you don't see sub-mounts showing up.</p><h2>Bonus: Adding support for data volumes</h2><p>While Docker docs position <a href="https://docs.docker.com/engine/storage/volumes/">volumes</a> as a distinct concept, under the hood, they are just bind mounts, but with a few extra features like naming, lifecycle management, and various data source drivers support:</p><div><pre><code><span><span># Traditional -v|--volume flag </span></span><span><span>docker</span><span> run</span><span> --volume</span><span> redis-data:/data</span><span> redis </span></span><span><span># More modern but equivalent --mount form </span></span><span><span>docker</span><span> run</span><span> --mount</span><span> type='volume,src=redis-data,dst=/data'</span><span> redis </span></span></code></pre></div><p>Instead of arbitrary folders on the host, volume data is always stored in <code>/var/lib/docker/volumes/CONTAINER_ID/_data</code>, and you can list all existing volumes with the <code>docker volume ls</code> command, or create new ones with <code>docker volume create</code>, or even purge them with <code>docker volume rm</code>. But at the end of the day, you're just listing, creating, or removing <code>_data</code> folders in the <code>/var/lib/docker/volumes</code> directory.</p><p>Interesting that Docker always sets mount propagation for volumes to <code>rprivate</code> (for bind mounts you could tweak it), while Kubernetes, despite relying on the exact same runc (or the like) runtime under the hood, allows more flexible mount propagation configuration (<a href="https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation"><code>HostToContainer</code>, <code>Bidirectional</code>, etc.</a>).</p><p>So, in Docker, <strong>bind mounts vs. volumes</strong> is more of a semantic difference (and induced artificial constraints on the data location and propagation type) than an actual technical difference.</p><h2>Where do union filesystems come into play?</h2><p>One of the things we didn't talk about in this article is <em>union filesystems</em> like <code>overlayfs</code> - simply because despite popular belief, <strong>they're not mandatory for containers.</strong></p><p>As we just proved with the above demo, it's possible to create a fully-fledged container without relying on a union filesystem at all. Docker uses <a href="https://docs.docker.com/engine/storage/drivers/overlayfs-driver/"><code>overlay2</code></a> (or <a href="https://docs.docker.com/engine/storage/drivers/">an alternative</a>) storage driver to unpack layered container images into "flat" local folders. However, this is only an optimization, mainly focused on the disk space efficiency - as we just saw, it's possible to extract a container image filesystem into a regular folder with <code>crane export</code> (or a similar command), and the container runtime (e.g., runc) will happily use it as a root filesystem.</p><h2>Summarizing</h2><p>At the heart of containers lies the <strong>mount namespace</strong>. That's not an accident - Linux has long treated the filesystem as the central interface for managing processes, devices, and resources. Once you start assembling a root filesystem for a container, it quickly becomes clear that other namespaces - <strong>PID</strong>, <strong>cgroup</strong>, <strong>UTS</strong>, and <strong>network</strong> - are interconnected and much needed to complete the task.</p><p>This is why walking through the rootfs exercise isn't just an impressive low-level demo you could give at a conference. It's a way to build a comprehensive mental model of how containers work. And with that model in place, higher-level topics like bind mounts, volumes, mount propagation, and persistence in Docker or Kubernetes stop feeling like special cases - they become natural extensions of the same foundation.</p><p>Ah, and if you made it this far, take another look at the diagram from the opening part - it should make much more sense now!</p><div><p><i>Click to enlarge</i></p></div><h2>Resources</h2><ul><li><a href="https://man7.org/linux/man-pages/man7/namespaces.7.html">namespaces(7) &mdash; Linux manual page</a></li><li><a href="https://man7.org/linux/man-pages/man7/mount_namespaces.7.html">mount_namespaces(7) &mdash; Linux manual page</a></li><li><a href="https://man7.org/linux/man-pages/man2/mount.2.html">mount(2) &mdash; Linux manual page</a></li><li><a href="https://man7.org/linux/man-pages/man2/pivot_root.2.html">pivot_root(2) &mdash; Linux manual page</a></li><li><a href="https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt">Shared Subtrees - Linux kernel documentation</a></li><li><a href="https://lwn.net/Articles/689856/">Mount namespaces and shared subtrees - LWN.net</a></li><li><a href="https://lwn.net/Articles/690679/">Mount namespaces, mount propagation, and unbindable mounts - LWN.net</a></li><li><a href="https://github.com/opencontainers/runtime-spec/blob/383cadbf08c0be925b62a4532e99a538249797e6/config.md#mounts">Mounts - OCI Runtime Specification</a></li><li><a href="https://github.com/opencontainers/runc/blob/b27d6f3f1af9a56f2770c8ec6e1a1ff986ca9c09/libcontainer/rootfs_linux.go">rootfs_linux.go - runc source code</a></li><li><a href="https://docs.docker.com/engine/storage/bind-mounts/">Bind mounts - Docker documentation</a></li><li><a href="https://docs.docker.com/engine/storage/volumes/">Volumes - Docker documentation</a></li><li><a href="https://docs.docker.com/engine/storage/drivers/overlayfs-driver/">OverlayFS storage driver - Docker documentation</a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation">Mount propagation - Kubernetes documentation</a></li><li><a href="https://brauner.io/2023/02/28/mounting-into-mount-namespaces.html">Mounting into mount namespaces - Christian Brauner's blog</a></li><li><a href="https://jpetazzo.github.io/2015/01/13/docker-mount-dynamic-volumes/">Attach a volume to a container while it is running - J&eacute;r&ocirc;me Petazzoni's blog</a></li><li><a href="https://sid-agrawal.ca/linux,/docker,/mount,/namespaces,/mount_namespaces/2024/11/26/docker-mounts.html">Understanding the various mounts setup by a Docker container - Sid Agrawal's blog</a></li></ul><h2>Practice</h2></div></section>]]></description><pubDate>Fri, 12 Sep 2025 22:51:50 +0530</pubDate></item><item><link>https://www.crunchydata.com/blog/get-excited-about-postgres-18</link><title>Get Excited About Postgres 18 (crunchydata.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nf90qr/get_excited_about_postgres_18/</guid><comments>https://www.reddit.com/r/programming/comments/1nf90qr/get_excited_about_postgres_18/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 9 min | <a href='https://www.reddit.com/r/programming/comments/1nf90qr/get_excited_about_postgres_18/'>Post permalink</a></p></section><section class='preview-image'><img src='https://imagedelivery.net/lPM0ntuwQfh8VQgJRu0mFg/24623c31-81a1-4004-3cbf-cb8511db4500/public' /></section><section class='parsed-content'><div><p>Postgres 18 will be released in just a couple weeks! Here&rsquo;s some details on the most important and exciting features.</p><h2><a href="https://www.crunchydata.com#asynchronous-io">Asynchronous i/o</a></h2><p>Postgres 18 is adding asynchronous i/o. This means faster reads for many use cases. This is also part of a bigger series of performance improvements planned for future Postgres, part of which may be multi-threading. Expect to see more on this in coming versions.</p><p><strong>What is async I/O?</strong></p><p>When <a href="https://www.crunchydata.com/blog/postgres-data-flow">data</a> isn&rsquo;t in the shared memory buffers already, Postgres reads from disk, and <a href="https://www.crunchydata.com/blog/understanding-postgres-iops">I/O is needed to retrieve data</a>. Synchronous I/O means that each individual request to the disk is waited on for completion before moving on to something else. For busy databases with a lot of activity, this can be a bottleneck.</p><p>Postgres 18 will introduce asynchronous I/O, allowing workers to optimize idle time and improve system throughput by batching reads. Currently, Postgres relies on the operating system for intelligent I/O handling, expecting OS or storage read-ahead for sequential scans and using features like Linux's posix_fadvise for other read types like Bitmap Index Scans. Moving this work into the database with asynchronous I/O will provide a more predictable and better-performing method for batching operations at the database level. Additionally, a new system view, pg_aios, will be available to provide data about the asynchronous I/O system.</p><p>Postgres writes will continue to be synchronous - since this is needed for ACID compliance.</p><p>If async i/o seems confusing, think of it like ordering food at a restaurant. In a synchronous model, you would place your order and stand at the counter, waiting, until your food is ready before you can do anything else. In an asynchronous model, you place your order, receive a buzzer, and are free to go back to your table and chat with friends until the buzzer goes off, signaling that your food is ready to be picked up.</p><p>Async I/O will affect:</p><ul><li>sequential scans</li><li>bitmap heap scans (following the bitmap index scan)</li><li>some maintenance operations like VACUUM.</li></ul><p>By default Postgres will turn on <strong>io_method = worker</strong>. By default there are 3 workers and this can be adjusted up for systems with larger CPU workers. I haven&rsquo;t seen any reliable recommendations on this, so stay tuned for more on that from our team soon.</p><p>For Postgres running on Linux 5.1+ you can utilize the io_uring system calls and have the invocations made via the actual backends rather than having separate processes with the optional <strong>io_method = io_uring</strong>.</p><h2><a href="https://www.crunchydata.com#uuid-v7">UUID v7</a></h2><p>UUIDs are getting a bit of an overhaul in this version by moving to v7.</p><p>UUIDs are randomly generated strings which are globally unique and often used for primary keys. UUIDs are popular in modern applications for a couple reasons:</p><ul><li>They&rsquo;re unique: You can use keys generated from more than one place.</li><li>Decoupled:Your application can generate a primary key <em>before</em> sending the data to the database.</li><li>URL obscurity: If your URLs use primary keys (e.g., .../users/5), other URLs are easy to guess (.../users/6, .../users/7). With a UUID (.../users/f47ac10b-58cc-4372-a567-0e02b2c3d479), it's impossible to guess other IDs.</li></ul><p>A new standard for UUID v7 came out in mid-2024 via a series of standards updates. UUIDv4 was the prior version of uuid with native Postgres support. But sorting and indexing in large tables had performance issues due to the relative randomness, leading to fragmented indexes and bad locality.&nbsp; UUIDv7 helps with the sort and indexing issues. It is still random but that first 48 bits (12 characters) are a timestamp, and the remaining bits are random; this gives better locality for data inserted around the same time and thus better indexability.</p><p>The timestamp part is a hexadecimal value (i.e. compressed decimal). So for example a uuid that begins with <code>01896d6e4a5d6</code> (hex) would represent the <code>2707238289622</code> (decimal) and that is the number of milliseconds since 1970.</p><p>This is how the DDL will look for uuid v7:</p><pre><code>CREATE TABLE user_actions ( action_id UUID PRIMARY KEY DEFAULT uuidv7(), user_id BIGINT NOT NULL, action_description TEXT, action_time TIMESTAMPTZ NOT NULL DEFAULT NOW() ); CREATE INDEX idx_action_id ON user_actions (action_id); </code></pre><h2><a href="https://www.crunchydata.com#b-tree-skip-scans">B-tree skip scans</a></h2><p>There&rsquo;s a nice performance bump coming in Postgres 18 for some multi-column B-tree indexes.</p><p>In Postgres, if you have an index on columns (<code>status</code>, <code>date</code>) in a table, this index can be used to match queries which query both <code>status</code> and <code>date</code> fields, or just <code>status</code>.</p><p>In Postgres 17 and below, this same index cannot be used to answer queries against just the <code>date</code> field; you would have to have that column indexed separately or the database would resort to a sequence scan + filter approach if there were no appropriate indexes for that table.</p><p>In Postgres 18, in many cases it can automatically use this multi-column index for queries touching only the <code>date</code> field.&nbsp; Known as a skip scan, this lets the system "skip" over portions of the index.</p><p>This works when queries don&rsquo;t use the leading columns in the conditions and the omitted column has a low cardinality, like a small number of distinct values. The optimization works by:</p><ol><li>Identifying all the distinct values in the omitted leading column(s).</li><li>Effectively transform the query to add the conditions to match the leading values.</li><li>The resulting query is able to use existing infrastructure to optimize lookups across multiple leading columns, effectively skipping any pages in the index scan which do not match both conditions.</li></ol><p>For example, if we had a sales table with columns <code>status</code> and <code>date</code>, we might have a multi-column index:</p><pre><code>CREATE INDEX idx_status_date ON sales (status, date); </code></pre><p>An example query could have a where clause that doesn&rsquo;t include status.</p><pre><code>SELECT * FROM sales WHERE date = '2025-01-01'; </code></pre><p>Nothing in the query plan tells you this is a skip scan, so you&rsquo;ll end up with a normal Index scan like this, showing you the index conditions.</p><pre><code> QUERY PLAN ------------------------------------------------------------- Index Only Scan using idx_status_date on sales (cost=0.29..21.54 rows=4 width=8) Index Cond: (date = '2025-01-01'::date) (2 rows) </code></pre><p>Before 18, a full table scan would be done, since the leading column of the index is not included, but with skip scan Postgres can use the same index for this index scan.</p><p>In Postgres 18, because status has a low cardinality and just a few values, a compound index scan can be done. Note that this optimization only works for queries which use the <code>=</code> operator, so it will not work with inequalities or ranges.</p><p>This all happens behind-the-scenes in the Postgres planner so you don&rsquo;t need to turn it on. The idea is that it will benefit analytics use cases where filters and conditions often change and aren&rsquo;t necessarily related to existing indexes.</p><p>The query planner will decide if using a skip scan is worthwhile, based on the table's statistics and the number of distinct values in the columns being skipped.</p><h2><a href="https://www.crunchydata.com#generated-columns-on-the-fly">Generated columns on-the-fly</a></h2><p>PostgreSQL 18 introduces virtual generated columns. Previously, generated columns were always stored on disk. This meant for generated columns, values were computed at the time of an insert or update and adding a bit of write overhead.</p><p>In PostgreSQL 18, virtual generated columns are now the default type for generated columns. if you define a generated column without explicitly specifying STORED, it will be created as a virtual generated column.</p><pre><code>CREATE TABLE user_profiles ( user_id SERIAL PRIMARY KEY, settings JSONB, username VARCHAR(100) GENERATED ALWAYS AS (settings -&gt;&gt; 'username') VIRTUAL ); </code></pre><p>This is a great update for folks using JSON data, queries can be simplified and data changes or normalization can be done on the fly as needed.</p><p>Note that virtual generated columns are not indexable - since they&rsquo;re not stored on disk. For <a href="https://www.crunchydata.com/blog/indexing-jsonb-in-postgres">indexing of JSONB</a>, use the stored version or expression index.</p><h2><a href="https://www.crunchydata.com#oauth-20">OAUTH 2.0</a></h2><p>Good news for folks that use Okta, Keycloak, and other managed authentication services, Postgres is now compatible with OAUTH 2.0. This is specified in the main host based authentication configuration (pg_hba.conf) file.</p><p>The Oauth system uses bearer tokens where the client application presents a token instead of a password to prove identity. The token is an opaque string and its format is determined by the authorization server. This feature removes the need to store passwords in the database. It also allows for more robust security measures like multi-factor authentication (MFA) and single sign-on (SSO) to be managed by external identity providers.</p><h2><a href="https://www.crunchydata.com#postgres-versions-are-packed-with-other-improvements">Postgres versions are packed with other improvements</a></h2><p>Postgres 18 comes with a staggering 3,000 commits from more than 200 authors. While many of these are features, there are numerous additions and optimizations under the hood to the Postgres query planner and other parts of the system that are behind the scenes. Even if you don&rsquo;t utilize optional features, there&rsquo;s still performance benefits (uh ... asyc i/o is a biggie), bug fixes, and security patches that make upgrading on a regular cadence a good idea.</p></div><div class="gallery"><p><img src="https://imagedelivery.net/lPM0ntuwQfh8VQgJRu0mFg/2bf43dd0-9a3a-4535-55c0-5f18a9a9a200/public"></p><p><img src="https://imagedelivery.net/lPM0ntuwQfh8VQgJRu0mFg/6d5ed16d-2a24-4ff4-4a6c-fd42773e4b00/public"></p></div></section>]]></description><pubDate>Fri, 12 Sep 2025 22:45:30 +0530</pubDate></item><item><link>https://buttondown.com/hillelwayne/archive/many-hard-leetcode-problems-are-easy-constraint/</link><title>Many Hard Leetcode Problems are Easy Constraint Problems (buttondown.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nf8hyo/many_hard_leetcode_problems_are_easy_constraint/</guid><comments>https://www.reddit.com/r/programming/comments/1nf8hyo/many_hard_leetcode_problems_are_easy_constraint/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 7 min | <a href='https://www.reddit.com/r/programming/comments/1nf8hyo/many_hard_leetcode_problems_are_easy_constraint/'>Post permalink</a></p></section><section class='preview-image'><img src='https://assets.buttondown.email/images/63337f78-7138-4b21-87a0-917c0c5b1706.jpg?w=960&fit=max' /></section><section class='parsed-content'><div><date> September 10, 2025 </date> <h2> Use the right tool for the job. </h2><p>In my first interview out of college I was asked the change counter problem:</p><blockquote><p>Given a set of coin denominations, find the minimum number of coins required to make change for a given number. IE for USA coinage and 37 cents, the minimum number is four (quarter, dime, 2 pennies).</p></blockquote><p>I implemented the simple greedy algorithm and immediately fell into the trap of the question: the greedy algorithm only works for "well-behaved" denominations. If the coin values were <code>[10, 9, 1]</code>, then making 37 cents would take 10 coins in the greedy algorithm but only 4 coins optimally (<code>10+9+9+9</code>). The "smart" answer is to use a dynamic programming algorithm, which I didn't know how to do. So I failed the interview.</p><p>But you only need dynamic programming if you're writing your own algorithm. It's really easy if you throw it into a constraint solver like <a href="https://www.minizinc.org/">MiniZinc</a> and call it a day. </p><div><pre><code>int: total; array[int] of int: values = [10, 9, 1]; array[index_set(values)] of var 0..: coins; constraint sum (c in index_set(coins)) (coins[c] * values[c]) == total; solve minimize sum(coins); </code></pre></div><p>You can try this online <a href="https://play.minizinc.dev/">here</a>. It'll give you a prompt to put in <code>total</code> and then give you successively-better solutions:</p><div><pre><span></span><code>coins = [0, 0, 37]; ---------- coins = [0, 1, 28]; ---------- coins = [0, 2, 19]; ---------- coins = [0, 3, 10]; ---------- coins = [0, 4, 1]; ---------- coins = [1, 3, 0]; ---------- </code></pre></div><p>Lots of similar interview questions are this kind of mathematical optimization problem, where we have to find the maximum or minimum of a function corresponding to constraints. They're hard in programming languages because programming languages are too low-level. They are also exactly the problems that constraint solvers were designed to solve. Hard leetcode problems are easy constraint problems.<sup><a href="https://buttondown.com#fn:leetcode">1</a></sup>Here I'm using MiniZinc, but you could just as easily use Z3 or OR-Tools or whatever your favorite generalized solver is.</p><h3>More examples</h3><p>This was a question in a different interview (which I thankfully passed):</p><blockquote><p>Given a list of stock prices through the day, find maximum profit you can get by buying one stock and selling one stock later.</p></blockquote><p>It's easy to do in O(n^2) time, or if you are clever, you can do it in O(n). Or you could be not clever at all and just write it as a constraint problem:</p><div><pre><code>array[int] of int: prices = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8]; var int: buy; var int: sell; var int: profit = prices[sell] - prices[buy]; constraint sell &gt; buy; constraint profit &gt; 0; solve maximize profit; </code></pre></div><p>Reminder, link to trying it online <a href="https://play.minizinc.dev/">here</a>. While working at that job, one interview question we tested out was:</p><blockquote><p>Given a list, determine if three numbers in that list can be added or subtracted to give 0? </p></blockquote><p>This is a satisfaction problem, not a constraint problem: we don't need the "best answer", any answer will do. We eventually decided against it for being too tricky for the engineers we were targeting. But it's not tricky in a solver; </p><div><pre><span></span><code>include "globals.mzn"; array[int] of int: numbers = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8]; array[index_set(numbers)] of var {0, -1, 1}: choices; constraint sum(n in index_set(numbers)) (numbers[n] * choices[n]) = 0; constraint count(choices, -1) + count(choices, 1) = 3; solve satisfy; </code></pre></div><p>Okay, one last one, a problem I saw last year at <a href="https://chicagopython.github.io/algosig/">Chipy AlgoSIG</a>. Basically they pick some leetcode problems and we all do them. I failed to solve <a href="https://leetcode.com/problems/largest-rectangle-in-histogram/description/">this one</a>:</p><blockquote><p>Given an array of integers heights representing the histogram's bar height where the width of each bar is 1, return the area of the largest rectangle in the histogram.</p></blockquote><p>The "proper" solution is a tricky thing involving tracking lots of bookkeeping states, which you can completely bypass by expressing it as constraints:</p><div><pre><code>array[int] of int: numbers = [2,1,5,6,2,3]; var 1..length(numbers): x; var 1..length(numbers): dx; var 1..: y; constraint x + dx &lt;= length(numbers); constraint forall (i in x..(x+dx)) (y &lt;= numbers[i]); var int: area = (dx+1)*y; solve maximize area; output ["(\(x)-&gt;\(x+dx))*\(y) = \(area)"] </code></pre></div><p>There's even a way to <a href="https://docs.minizinc.dev/en/2.9.3/visualisation.html">automatically visualize the solution</a> (using <code>vis_geost_2d</code>), but I didn't feel like figuring it out in time for the newsletter.</p><h3>Is this better?</h3><p>Now if I actually brought these questions to an interview the interviewee could ruin my day by asking "what's the runtime complexity?" Constraint solvers runtimes are unpredictable and almost always than an ideal bespoke algorithm because they are more expressive, in what I refer to as the <a href="https://buttondown.com/hillelwayne/archive/the-capability-tractability-tradeoff/">capability/tractability tradeoff</a>. But even so, they'll do way better than a <em>bad</em> bespoke algorithm, and I'm not experienced enough in handwriting algorithms to consistently beat a solver.</p><p>The real advantage of solvers, though, is how well they handle new constraints. Take the stock picking problem above. I can write an O(n&sup2;) algorithm in a few minutes and the O(n) algorithm if you give me some time to think. Now change the problem to</p><blockquote><p>Maximize the profit by buying and selling up to <code>max_sales</code> stocks, but you can only buy or sell one stock at a given time and you can only hold up to <code>max_hold</code> stocks at a time?</p></blockquote><p>That's a way harder problem to write even an inefficient algorithm for! While the constraint problem is only a tiny bit more complicated:</p><div><pre><span></span><code>include "globals.mzn"; int: max_sales = 3; int: max_hold = 2; array[int] of int: prices = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8]; array [1..max_sales] of var int: buy; array [1..max_sales] of var int: sell; array [index_set(prices)] of var 0..max_hold: stocks_held; var int: profit = sum(s in 1..max_sales) (prices[sell[s]] - prices[buy[s]]); constraint forall (s in 1..max_sales) (sell[s] &gt; buy[s]); constraint profit &gt; 0; constraint forall(i in index_set(prices)) (stocks_held[i] = (count(s in 1..max_sales) (buy[s] &lt;= i) - count(s in 1..max_sales) (sell[s] &lt;= i))); constraint alldifferent(buy ++ sell); solve maximize profit; output ["buy at \(buy)\n", "sell at \(sell)\n", "for \(profit)"]; </code></pre></div><p>Most constraint solving examples online are puzzles, like <a href="https://docs.minizinc.dev/en/stable/modelling2.html#ex-sudoku">Sudoku</a> or "<a href="https://docs.minizinc.dev/en/stable/modelling2.html#ex-smm">SEND + MORE = MONEY</a>". Solving leetcode problems would be a more interesting demonstration. And you get more interesting opportunities to teach optimizations, like symmetry breaking.</p><p><em>If you're reading this on the web, you can subscribe <a href="https://buttondown.com/hillelwayne">here</a>. Updates are once a week. My main website is <a href="https://www.hillelwayne.com">here</a>.</em></p><p><em>My new book, </em>Logic for Programmers<em>, is now in early access! Get it <a href="https://leanpub.com/logic/">here</a>.</em></p></div></section>]]></description><pubDate>Fri, 12 Sep 2025 22:25:19 +0530</pubDate></item><item><link>https://www.youtube.com/watch?v=fdUKJ-4y2zo</link><title>â€œI Got Pwnedâ€: npm maintainer of Chalk &amp;amp; Debug speaks on the massive supply-chain attack (youtube.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nf6df3/i_got_pwned_npm_maintainer_of_chalk_debug_speaks/</guid><comments>https://www.reddit.com/r/programming/comments/1nf6df3/i_got_pwned_npm_maintainer_of_chalk_debug_speaks/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/programming/comments/1nf6df3/i_got_pwned_npm_maintainer_of_chalk_debug_speaks/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey Everyone,<br/>This week I posted our discovery of finding that a popular open-source projects, including debug and chalk had been breached. I&#39;m happy to say the Josh (Qix) the maintainer that was compromised agreed to sit down with me and discuss his experience, it was a very candid conversation but one I think was important to have.</p><p>Below are some of the highlight and takeaways from the conversation, since the â€œhow could this happen?â€ question is still circulating.</p><p><strong>Was MFA on the account?</strong></p><p>â€œThere was definitely MFAâ€¦ but timed one-time passwords are not phishing resistant. They can be man in the middle. Thereâ€™s no cryptographic checks, no domain association, nothing like U2F would have.â€</p><p>The attackers used a fake NPM login flow and captured his TOTP, allowing them to fully impersonate him. Josh called out not enabling phishing-resistant MFA (FIDO2/U2F) as his biggest technical mistake.</p><p><strong>The scale of the blast radius</strong></p><p>Charlie (our researcher) spotted the issue while triaging suspicious packages:</p><p>â€œFirst I saw the debug packageâ€¦ then I saw chalk and error-exâ€¦ and I knew a significant portion of the JS ecosystem would be impacted.â€</p><p>Wiz later reported that <strong>99% of cloud environments used at least one affected package</strong>.</p><p>â€œThe fact it didnâ€™t do anything was the bullet we dodged. It ran in CI/CD, on laptops, servers, enterprise machines. It could have done anything.â€</p><p>Wiz also reported that 10% of cloud environments they analyzed had the malware inside them. There were some &#39;hot takes&#39; on the internet that, in fact this was not a big deal and some said it was a win for security. Josh shared that this was not a win and the only reason we got away with it was because how ineffective the attackers were. The malicious packages were downloaded 2.5 million times in the 2 hour window they were live.</p><p><strong>Ecosystem-level shortcomings</strong></p><p>Josh was frank about registry response times and missing safeguards:</p><p>â€œThere was a huge process breakdown during this attack with NPM. Extremely slow to respond. No preemptive â€˜switch to U2Fâ€™ push despite billions of downloads. I had no recourse except filing a ticket through their public form.&quot;</p><p>Josh also gave some advice for anyone going through this in the future which is to be open and transparent, the internet largely agreed Josh handled this in the best way possible (short of not getting phished in the first place )</p><p>â€œIf you screw up, own it. In open source, being transparent and immediate saves a lot of peopleâ€™s time and money. Vulnerability (the human kind) goes a long way.â€</p></div><!-- SC_ON --></section><section class='embedded-media'><iframe width="267" height="200" src="https://www.youtube.com/embed/fdUKJ-4y2zo?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Yep, I Got Pwned: A Candid Chat With The Chalk &amp; Debug Maintainer"></iframe></section>]]></description><pubDate>Fri, 12 Sep 2025 21:02:55 +0530</pubDate></item><item><link>https://lwn.net/Articles/1034966/</link><title>The Challenge of Maintaining Curl (lwn.net)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nesqco/the_challenge_of_maintaining_curl/</guid><comments>https://www.reddit.com/r/programming/comments/1nesqco/the_challenge_of_maintaining_curl/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 4 min | <a href='https://www.reddit.com/r/programming/comments/1nesqco/the_challenge_of_maintaining_curl/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><div><p>By <b>Jonathan Corbet</b><br>August 29, 2025</p><hr><p><a href="https://lwn.net/Archives/ConferenceByYear/#2025-Open_Source_Summit_Europe">OSS EU</a> </p></div><p>Keynote sessions at Open Source Summit events tend not to allow much time for detailed talks, and the 2025 <a href="https://events.linuxfoundation.org/open-source-summit-europe/">Open Source Summit Europe</a> did not diverge from that pattern. Even so, Daniel Stenberg, the maintainer of the <a href="https://curl.se/">curl</a> project, managed to cram a lot into the 15&nbsp;minutes given to him. Like the maintainers of many other projects, Stenberg is feeling some stress, and the problems appear to be getting worse over time. </p><p><a href="https://lwn.net/Articles/1034968/"><img src="https://static.lwn.net/images/conf/2025/osseu/DanielStenberg-sm.png" alt="[Daniel Stenberg]" title="Daniel Stenberg"></a> Curl, he began, is "<q>a small project with a big impact</q>". It began in 1996 with all of 100&nbsp;lines of code; it has since grown to 180,000 lines that have been contributed by 1,400 authors. In any given month, there are 20-25 developers who are actively contributing to curl. The project has exactly one full-time employee &mdash; that being Stenberg himself. </p><p>The program is widely used, having been deployed in at least one-billion devices. Just about anything that occasionally connects to the net, he said, uses curl to do it. But using curl is different from supporting its development. As an example, he put up a slide listing the 47 car brands that use curl in their products; he followed it with a slide listing the brands that contribute to curl. The second slide, needless to say, was empty. (A version of both slides can be seen on <a href="https://mastodon.social/@bagder/115025727082593712">this page</a>). </p><p>Companies tend to assume that somebody else is paying for the development of open-source software, so they do not have to contribute. He emphasized that he has released curl under a free license, so there is no <i>legal</i> problem with what these companies are doing. But, he suggested, these companies might want to think a bit more about the future of the software they depend on. </p><p>Open-source software is the best choice, he said, but maintaining it is a tough job. Most projects out there have a single maintainer, and that person is often doing the work in their spare time, without funding. Maintenance involves a lot of tasks, including taking care of security, reviewing patches, writing documentation, keeping the web site going, administering the mailing list, and a long list of other tasks. Occasionally, if a little time is left over, it might also be possible to do a bit of feature development. That is a lot for one person to keep up with. </p><p>Companies have a certain tendency to make things worse. He put up an excerpt of a message from Apple support, referring a customer to the curl project for help with their (Apple) device. He has received demands from companies for information on the project's development and security practices, often with tight deadlines for a response. He typically replies by sending back a support contract; that usually results in never hearing from the company again, he said. More recently, he has been getting demands from European companies seeking information on the curl project's Cyber Resilience Act compliance practices. </p><p>Some communications are rather less humorous than that; one email came with a subject reading "<q>I will slaughter you</q>". He gets emails from people who found his address in the license notices shipped with their automobiles asking for support. But he also gets nice thank-you emails at times. </p><p>Problematic email takes other forms as well. There is an increasing crowd of people who ask a large language model to "<q>find a problem in curl, make it sound terrible</q>", then send the result, which is never correct, to the project, thinking that they are somehow helping. Dealing with these useless problem reports takes an increasing amount of time. </p><p>Recently, the curl project, like many operators of web sites, has been contending with distributed denial-of-service attacks by scrapers run by AI companies. He put up a link to <a href="https://lwn.net/Articles/1008897/">LWN's article on this problem</a> for those who are unfamiliar with it. The curl site consumes a massive amount of bandwidth every month, but only 0.01% of that is source downloads. Most of the rest is bot traffic. That, too, adds to the difficulty of maintaining the project. </p><p>He concluded the brief talk with one last email; it was from an 11-year-old child who had found curl useful in some project they were working on. It included an expression of gratitude that, Stenberg said, was truly heartwarming. </p><p>[Thanks to the Linux Foundation, LWN's travel sponsor, for supporting our travel to this event.]<br></p><table> <tr><th>Index entries for this article</th></tr> <tr><td><a href="https://lwn.net/Archives/ConferenceIndex/">Conference</a></td><td><a href="https://lwn.net/Archives/ConferenceIndex/#Open_Source_Summit_Europe-2025">Open Source Summit Europe/2025</a></td></tr> </table><br> <hr> </div></section>]]></description><pubDate>Fri, 12 Sep 2025 08:56:40 +0530</pubDate></item><item><link>https://fabiensanglard.net/floating_point_visually_explained/</link><title>Floating Point Visually Explained (fabiensanglard.net)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nesp2g/floating_point_visually_explained/</guid><comments>https://www.reddit.com/r/programming/comments/1nesp2g/floating_point_visually_explained/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 4 min | <a href='https://www.reddit.com/r/programming/comments/1nesp2g/floating_point_visually_explained/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><br><center> </center><p>August 29, 2017</p><p>Floating Point Visually Explained</p><hr><div><p>While I was writing the Wolfenstein 3D book<a href="https://fabiensanglard.net#footnote_1"><sup>[1]</sup></a>, I wanted to demonstrate how much of a handicap it was to work without floating points. My attempts at understanding floating points using canonical<a href="https://fabiensanglard.net#footnote_2"><sup>[2]</sup></a> articles<a href="https://fabiensanglard.net#footnote_3"><sup>[3]</sup></a> were met with resistance from my brain. </p><p>I tried to find a different way. Something far from $(-1)^S * 1.M * 2^{(E-127)}$ and </p><p>its mysterious exponent/mantissa. Possibly a drawing since they seem to flow through my brain better.</p><p>I ended up with what follows and I decided to include it in the book. I am not claiming this is my invention but I have never seen floating points explained this way so far. I hope it will helps a few people like me who are a bit allergic to mathematic notations. </p></div><p>How Floating Point are usually explained</p><hr><p>In the C language, floats are 32-bit container following the IEEE 754 standard. Their purpose is to store and allow operations on approximation of real numbers. The way I have seen them explained so far is as follow. The 32 bits are divided in three sections: </p><ul> <li>1 bit S for the sign </li><li>8 bits E for the exponent </li><li>23 bits for the mantissa </li></ul><div><p><img src="https://fabiensanglard.netfloating_point_layout.svg" width="1135.6868" height="63.758217"><span><i><small>Floating Point internals.</small></i></span> <img src="https://fabiensanglard.netfloating_point_math.svg" width="1136.25" height="38.75"><span><i><small>The three sections of a floating point number.</small></i></span> So far, so good. Now, how numbers are interpreted is usually explained with the formula:</p></div><p>$$ (-1)^S * 1.M * 2^{(E-127)} $$ </p><span><i><small>How everybody hates floating point to be explained to them.</small></i></span><p>This is usually where I flip the table. Maybe I am allergic to mathematic notation but something just doesn't click when I read it. It feels like learning to <a href="https://fabiensanglard.net01.jpg">draw a owl</a>.<br> </p><blockquote> Floating-point arithmetic is considered an esoteric subject by many people.<p>- David Goldberg</p></blockquote><p>A different way to explain...</p><hr><div><p>Although correct, this way of explaining floating point will leaves some of us completely clueless. Fortunately, there is a different way to explain it. Instead of Exponent, think of a Window between two consecutive power of two integers. Instead of a Mantissa, think of an Offset within that window.</p><p><img src="https://fabiensanglard.netfloating_point_intuitive.svg"><br> <span><i><small>The three sections of a floating Point number.</small></i></span> The window tells within which two consecutive power-of-two the number will be: [0.5,1], [1,2], [2,4], [4,8] and so on (up to [$2^{127}$,$2^{128}$]). The offset divides the window in $ 2^{23} = 8388608 $ buckets. With the window and the offset you can approximate a number. The window is an excellent mechanism to protect from overflowing. Once you have reached the maximum in a window (e.g [2,4]), you can "float" it right and represent the number within the next window (e.g [4,8]). It only costs a little bit of precision since the window becomes twice as large.</p><p>The next figure illustrates how the number 6.1 would be encoded. The window must start at 4 and span to next power of two, 8. The offset is about half way down the window.</p><p><span><i><small>Value 6.1 approximated with floating point.</small></i></span></p></div><p>Precision</p><hr><p>How much precision is lost when the window covers a wider range? Let's take an example with window [1,2] where the 8388608 offsets cover a range of 1 which gives a precision of $\frac{(2-1)}{8388608}=0.00000011920929$. In the window [2048,4096] the 8388608 offsets cover a range of $4096 - 2048 = 2048$ which gives a precision of $ \frac{4096-2048}{8388608}=0.0002 $.<br> </p><p>An other example</p><hr><p>Let's take an other example with the detailed calculations of the floating point representation of a number we all know well: 3.14.<br> </p><ul> <li>The number 3.14 is positive $\rightarrow S=0$. </li><li>The number 3.14 is between the power of two 2 and 4 so the floating window must start at $2^1$ $\rightarrow E=128$ (see formula where window is $2^{(E-127)}$). </li><li>Finally there are $2^{23}$ offsets available to express where 3.14 falls within the interval [2-4]. It is at $\frac{3.14 -2 }{4 - 2} = 0.57$ within the interval which makes the offset $ M = 2^{23}*0.57 = 4781507$ </li></ul><p>Which in binary translates to: </p><ul> <li> S = 0 = 0b </li><li> E = 128 = 10000000b </li><li> M = 4781507 = 10010001111010111000011b </li></ul><p><img src="https://fabiensanglard.netfloating_point_layout_pi.svg"><br> <span><i><small>3.14 floating point binary representation.</small></i></span> The value 3.14 is therefore approximated to 3.1400001049041748046875. The corresponding value with the ugly formula:<br> </p><p>$$ 3.14 = (-1)^0 * 1.57 * 2^{(128-127)} $$ </p><br><div><p>And finally the graphic representation with window and offset:</p><p><img src="https://fabiensanglard.netfloating_point_window_pi.svg"><br> <span><i><small>3.14 window and offset.</small></i></span></p></div><p><br> I hope that helped :) ! </p><p>References</p><hr> <hr> <center>*</center></div></section>]]></description><pubDate>Fri, 12 Sep 2025 08:54:47 +0530</pubDate></item><item><link>https://43081j.com/2025/09/bloat-of-edge-case-libraries</link><title>The bloat of edge-case first libraries (43081j.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1neezti/the_bloat_of_edgecase_first_libraries/</guid><comments>https://www.reddit.com/r/programming/comments/1neezti/the_bloat_of_edgecase_first_libraries/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 15 min | <a href='https://www.reddit.com/r/programming/comments/1neezti/the_bloat_of_edgecase_first_libraries/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><article><p>This is just some of what I&rsquo;ve been pondering recently - particularly in terms of how we ended up with such overly-granular dependency trees.</p><p>I think we&rsquo;ve ended up with many libraries in our ecosystem which are edge-case-first, the opposite to what I&rsquo;d expect. I&rsquo;ll give a few examples and some thoughts around this, mostly in the hope we can start to trim some of it away.</p><h2>The problem</h2><p>I believe a lot of the questionably small libraries hiding in our deep dependency trees are a result of over-engineering for inputs and edge cases we&rsquo;ve probably never seen.</p><p>For example, say we&rsquo;re building a <code>clamp</code> function:</p><div><pre><code><span>export</span> <span>function</span> <span>clamp</span><span>(</span><span>value</span><span>:</span> <span>number</span><span>,</span> <span>min</span><span>:</span> <span>number</span><span>,</span> <span>max</span><span>:</span> <span>number</span><span>):</span> <span>number</span> <span>{</span> <span>return</span> <span>Math</span><span>.</span><span>min</span><span>(</span><span>Math</span><span>.</span><span>max</span><span>(</span><span>value</span><span>,</span> <span>min</span><span>),</span> <span>max</span><span>);</span> <span>}</span> </code></pre></div><p>Pretty simple!</p><p>What if someone passes nonsensical ranges? Let&rsquo;s handle that.</p><div><pre><code><span>export</span> <span>function</span> <span>clamp</span><span>(</span><span>value</span><span>:</span> <span>number</span><span>,</span> <span>min</span><span>:</span> <span>number</span><span>,</span> <span>max</span><span>:</span> <span>number</span><span>):</span> <span>number</span> <span>{</span> <span>if</span> <span>(</span><span>min</span> <span>&gt;</span> <span>max</span><span>)</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>min must be less than or equal to max</span><span>'</span><span>);</span> <span>}</span> <span>return</span> <span>Math</span><span>.</span><span>min</span><span>(</span><span>Math</span><span>.</span><span>max</span><span>(</span><span>value</span><span>,</span> <span>min</span><span>),</span> <span>max</span><span>);</span> <span>}</span> </code></pre></div><p><strong>This is probably as far as I&rsquo;d go.</strong> But let&rsquo;s over-engineer - what if someone passes a number-like string?</p><div><pre><code><span>export</span> <span>function</span> <span>clamp</span><span>(</span><span>value</span><span>:</span> <span>number</span> <span>|</span> <span>string</span><span>,</span> <span>min</span><span>:</span> <span>number</span> <span>|</span> <span>string</span><span>,</span> <span>max</span><span>:</span> <span>number</span> <span>|</span> <span>string</span><span>):</span> <span>number</span> <span>{</span> <span>if</span> <span>(</span><span>typeof</span> <span>value</span> <span>===</span> <span>'</span><span>string</span><span>'</span> <span>&amp;&amp;</span> <span>Number</span><span>.</span><span>isNaN</span><span>(</span><span>Number</span><span>(</span><span>value</span><span>)))</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>value must be a number or a number-like string</span><span>'</span><span>);</span> <span>}</span> <span>if</span> <span>(</span><span>typeof</span> <span>min</span> <span>===</span> <span>'</span><span>string</span><span>'</span> <span>&amp;&amp;</span> <span>Number</span><span>.</span><span>isNaN</span><span>(</span><span>Number</span><span>(</span><span>min</span><span>)))</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>min must be a number or a number-like string</span><span>'</span><span>);</span> <span>}</span> <span>if</span> <span>(</span><span>typeof</span> <span>max</span> <span>===</span> <span>'</span><span>string</span><span>'</span> <span>&amp;&amp;</span> <span>Number</span><span>.</span><span>isNaN</span><span>(</span><span>Number</span><span>(</span><span>max</span><span>)))</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>max must be a number or a number-like string</span><span>'</span><span>);</span> <span>}</span> <span>if</span> <span>(</span><span>Number</span><span>(</span><span>min</span><span>)</span> <span>&gt;</span> <span>Number</span><span>(</span><span>max</span><span>))</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>min must be less than or equal to max</span><span>'</span><span>);</span> <span>}</span> <span>return</span> <span>Math</span><span>.</span><span>min</span><span>(</span><span>Math</span><span>.</span><span>max</span><span>(</span><span>value</span><span>,</span> <span>min</span><span>),</span> <span>max</span><span>);</span> <span>}</span> </code></pre></div><p>At this point, it seems clear to me we&rsquo;ve just poorly designed our function. It solely exists to clamp numbers, so why would we accept strings?</p><p>But hey, let&rsquo;s go further! What if other libraries also want to accept such loose inputs? Let&rsquo;s extract this into a separate library:</p><div><pre><code><span>import</span> <span>isNumber</span> <span>from</span> <span>'</span><span>is-number</span><span>'</span><span>;</span> <span>export</span> <span>function</span> <span>clamp</span><span>(</span><span>value</span><span>:</span> <span>number</span> <span>|</span> <span>string</span><span>,</span> <span>min</span><span>:</span> <span>number</span> <span>|</span> <span>string</span><span>,</span> <span>max</span><span>:</span> <span>number</span> <span>|</span> <span>string</span><span>):</span> <span>number</span> <span>{</span> <span>if</span> <span>(</span><span>!</span><span>isNumber</span><span>(</span><span>value</span><span>))</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>value must be a number or a number-like string</span><span>'</span><span>);</span> <span>}</span> <span>if</span> <span>(</span><span>!</span><span>isNumber</span><span>(</span><span>min</span><span>))</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>min must be a number or a number-like string</span><span>'</span><span>);</span> <span>}</span> <span>if</span> <span>(</span><span>!</span><span>isNumber</span><span>(</span><span>max</span><span>))</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>max must be a number or a number-like string</span><span>'</span><span>);</span> <span>}</span> <span>if</span> <span>(</span><span>Number</span><span>(</span><span>min</span><span>)</span> <span>&gt;</span> <span>Number</span><span>(</span><span>max</span><span>))</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>min must be less than or equal to max</span><span>'</span><span>);</span> <span>}</span> <span>return</span> <span>Math</span><span>.</span><span>min</span><span>(</span><span>Math</span><span>.</span><span>max</span><span>(</span><span>value</span><span>,</span> <span>min</span><span>),</span> <span>max</span><span>);</span> <span>}</span> </code></pre></div><p><strong>Whoops!</strong> We&rsquo;ve just created the infamous <a href="https://www.npmjs.com/package/is-number"><code>is-number</code></a> library!</p><h2>How it should be</h2><p>This, in my opinion, is poor technical design we&rsquo;ve all ended up dealing with over the years. Carrying the baggage of these overly-granular libraries that exist to handle edge cases we&rsquo;ve probably never encountered.</p><p>I think it should have been:</p><div><pre><code><span>export</span> <span>function</span> <span>clamp</span><span>(</span><span>value</span><span>:</span> <span>number</span><span>,</span> <span>min</span><span>:</span> <span>number</span><span>,</span> <span>max</span><span>:</span> <span>number</span><span>):</span> <span>number</span> <span>{</span> <span>return</span> <span>Math</span><span>.</span><span>min</span><span>(</span><span>Math</span><span>.</span><span>max</span><span>(</span><span>value</span><span>,</span> <span>min</span><span>),</span> <span>max</span><span>);</span> <span>}</span> </code></pre></div><p><em>Maybe</em> with some <code>min &lt;= max</code> validation, but even that is debatable. At this point, you may as well inline the <code>Math.min(Math.max(...))</code> expression instead of using a dependency.</p><p><strong>We should be able to define our functions to accept the inputs they are designed for, and not try to handle every possible edge case.</strong></p><p>There are two things at play here:</p><ul> <li>Data types</li> <li>Values</li> </ul><p>A well designed library would assume the right <strong>data types</strong> have been passed in, but may validate that the <strong>values</strong> make sense (e.g. <code>min</code> is less than or equal to <code>max</code>).</p><p>These over-engineered libraries have decided to implement <em>both</em> at runtime - essentially run-time type checking and value validation. One could argue that this is just a result of building in the pre-TypeScript era, but that still doesn&rsquo;t justify the overly specific <em>value validation</em> (e.g. the real <code>is-number</code> also checks that it is finite).</p><h2>What we shouldn&rsquo;t do</h2><p>We shouldn&rsquo;t build edge-case-first libraries, i.e. those which solve for edge cases we have yet to encounter or are unlikely to ever encounter.</p><h2>Example: <code>is-arrayish</code> (76M downloads/week)</h2><p>The <code>is-arrayish</code> library determines if a value is an <code>Array</code> or behaves like one.</p><p>There will be some edge cases where this matters a lot, where we want to accept something we can index into but don&rsquo;t care if it is a real <code>Array</code> or not.</p><p>However, the common use case clearly will not be that and we could&rsquo;ve just used <code>Array.isArray()</code> all along.</p><h2>Example: <code>is-number</code> (90M downloads/week)</h2><p>The <code>is-number</code> library determines if a value is a positive, finite number or number-like string (maybe we should name it <code>is-positive-finite-number</code> to be more accurate).</p><p>Again, there will be edge cases where we want to deal with number-like strings or we want to validate that a number is within a range (e.g. finite).</p><p>The common use case will not be this. The common use case will be that we want to check <code>typeof n === 'number'</code> and be done with it.</p><p>For those edge cases where we want to <em>additionally</em> validate what kind of number it is, we could use a library (but one which exists for the validation, not for the type check).</p><h2>Example: <code>pascalcase</code> (9.7M downloads/week)</h2><p>The <code>pascalcase</code> library transforms text to PascalCase.</p><p>It has 1 dependency (<code>camelcase</code>) and accepts a variety of input types:</p><ul> <li>strings</li> <li>null</li> <li>undefined</li> <li>arrays of strings</li> <li>functions</li> <li>arbitrary objects with <code>toString</code> methods</li> </ul><p>In reality, almost every user will be passing a <code>string</code>.</p><h2>Example: <code>is-regexp</code> (10M downloads/week)</h2><p>The <code>is-regexp</code> library checks if a value is a <code>RegExp</code> object, and supports cross-realm values.</p><p>In reality, almost every user will be passing a <code>RegExp</code> object, and not one from another realm.</p><p>For context, cross-realm values can happen when you retrieve a value from an <code>iframe</code> or VM for example:</p><div><pre><code><span>const</span> <span>iframe</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'</span><span>iframe</span><span>'</span><span>);</span> <span>iframe</span><span>.</span><span>contentWindow</span><span>.</span><span>RegExp</span> <span>===</span> <span>RegExp</span><span>;</span> <span>// false</span> <span>const</span> <span>iframeRegex</span> <span>=</span> <span>iframe</span><span>.</span><span>contentWindow</span><span>.</span><span>someRegexp</span><span>;</span> <span>iframeRegex</span> <span>instanceof</span> <span>RegExp</span><span>;</span> <span>// false</span> <span>isRegex</span><span>(</span><span>iframeRegex</span><span>);</span> <span>// true</span> </code></pre></div><p>This is indeed useful, and I do support this myself in chai (which I maintain). However, this is an edge case most libraries don&rsquo;t need to care about.</p><h2>What we should do</h2><p>We should build libraries which solve the common use case and make assumptions about the input types they will be given.</p><h2>Example: scule (1.8M downloads/week)</h2><p><a href="https://www.npmjs.com/package/scule">scule</a> is a library for transforming casing of text (e.g. camel case, etc).</p><p>It only accepts inputs it is designed for (strings and arrays of strings) and has zero dependencies.</p><p>In most of the functions it exports, it assumes valid input data types.</p><h2>Example: dlv (14.9M downloads/week)</h2><p><a href="https://www.npmjs.com/package/dlv">dlv</a> is a library for deep property access.</p><p>It only accepts strings and arrays of strings as the path to access, and assumes this (i.e. does no validation).</p><h2>Validation is important</h2><p>Validation is important, and I want to be clear that I&rsquo;m not saying we should stop validating our data.</p><p>However, we should usually be validating the data in the project that owns it (e.g. at the app level), and not in every library that later consumes it as input.</p><p>Deep dependencies applying validation like this actually shift the burden from where it belongs (at data boundaries) to deep in the dependency tree.</p><p>Often at this point, it is invisible to the consumer of the library.</p><p>How many people are passing values into <code>is-number</code> (via other libraries), not realising it will prevent them from using negative numbers and <code>Infinity</code>?</p><h2>A note on overly-granular libraries</h2><p>This post isn&rsquo;t about overly-granular libraries in general, but I&rsquo;d like to briefly mention them for visibility.</p><p>An overly-granular library is one where someone took a useful library and split it up into an almost atomic-level of granularity.</p><p>Some examples:</p><ul> <li><code>shebang-regex</code> - 2LOC, does the same as <code>startsWith('#!')</code>, <strong>86M downloads/week</strong></li> <li><code>is-whitespace</code> - 7LOC, checks if a string is only whitespace, <strong>1M downloads/week</strong></li> <li><code>is-npm</code> - 8LOC, checks <code>npm_config_user_agent</code> or <code>npm_package_json</code> are set, <strong>7M downloads/week</strong></li> </ul><p>This is a personal preference some maintainers clearly prefer. The thought seems to be that by having atomic libraries, you can easily build your next library mostly from the existing building blocks you have.</p><p>I don&rsquo;t really agree with this and think downloading a package for <code>#!</code> 86 million times a week is a bit much.</p><h2>What can be done about this?</h2><p>The <a href="https://e18e.dev">e18e</a> community is already tackling a lot of this by contributing performance improvements across the ecosystem, including removing and replacing dependencies with more modern, performant ones.</p><p>Through these efforts, there&rsquo;s already a useful <a href="https://e18e.dev/guide/replacements.html">list of replacements</a> and an <a href="https://github.com/es-tooling/eslint-plugin-depend/">ESLint plugin</a>.</p><h2>As a maintainer</h2><p>If you&rsquo;re maintaining a library, it would be worth reviewing your dependencies to see if:</p><ul> <li>Any are replaceable by native functionality these days (e.g. <code>Array.isArray</code>)</li> <li>Any are replaceable by smaller, less granular and/or more performant alternatives (e.g. <code>scule</code> instead of <code>pascalcase</code>)</li> <li>Any are redundant if you make more assumptions about input types</li> </ul><p>Tools like <a href="https://npmgraph.js.org/">npmgraph</a> can help you visualise your dependency tree to make this task easier.</p><p>Also, being stricter around input types will allow you to reduce a lot of code and dependencies.</p><p>If you can assume the data being passed in is the correct type, you can leave validation up to the consumer.</p><h2>As a user</h2><p>Keep a close eye on your dependencies (both deep and direct), and what alternatives are available to your direct dependencies.</p><p>Often, it is easy to stick with a dependency from long ago and forget to re-visit it one day in case there is a better way. Many of these packages are possible natively, or have more modern alternatives.</p><p>Useful tools:</p><ul> <li><a href="https://npmgraph.js.org/">npmgraph</a> for visualising your dependency tree</li> <li><a href="https://node-modules.dev/">node-modules.dev</a> for visualising your dependencies and lots of useful meta data</li> <li><a href="https://docs.github.com/en/code-security/getting-started/dependabot-quickstart-guide">Dependabot</a> for keeping your dependencies up to date</li> </ul><p>On the topic of data, it is also worth ensuring validation happens at data boundaries rather than being delegated to various dependencies. Try to validate the type and value up front, before passing into dependencies.</p><h2>Conclusion</h2><p>Most of these libraries exist to handle edge cases that do certainly exist. However, <strong>we are all paying the cost of that rather than only those who need to support those edge cases</strong>.</p><p>This is the wrong way around. Libraries should implement the main use case, and alternatives (or plugins) can exist to provide the edge cases the minority needs.</p><p>We should all be more aware of what is in our dependency tree, and should push for more concise, lighter libraries.</p></article> </div></section>]]></description><pubDate>Thu, 11 Sep 2025 22:59:35 +0530</pubDate></item><item><link>https://reiner.org/hashed-sorting</link><title>Hashed sorting is typically faster than hash tables1 (reiner.org)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1ne6jtd/hashed_sorting_is_typically_faster_than_hash/</guid><comments>https://www.reddit.com/r/programming/comments/1ne6jtd/hashed_sorting_is_typically_faster_than_hash/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 23 min | <a href='https://www.reddit.com/r/programming/comments/1ne6jtd/hashed_sorting_is_typically_faster_than_hash/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><p>Problem statement: count the unique values in a large array of mostly-unique uint64s. Two standard approaches are:</p><ul> <li>Insert into a hash table and return the number of entries.</li> <li>Sort the array, then count positions that differ from their predecessor.</li> </ul><p>Hash tables win the interview (<span><span><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>)</mo></mrow><annotation>O(n)</annotation></semantics></math></span></span> vs <span><span><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mi>log</mi><mo>&#8289;</mo><mi>n</mi><mo>)</mo></mrow><annotation>O(n \log n)</annotation></semantics></math></span></span>), but sorting is typically faster in a well-tuned implementation. This problem and its variants are the inner loop of <a href="https://reiner.org#applications">some of the world&rsquo;s biggest CPU workloads</a>.</p><h2>Benchmark highlights</h2><p>Here is performance on M2 Pro<a href="https://reiner.org#fn2"><sup>2</sup></a>, comparing our best-tuned hash table and our best-tuned sorting implementation. We also include Rust&rsquo;s default implementations (<code>sort_unstable()</code>, and <code>HashSet</code> with <code>foldhash::fast</code>) as a reference point for high-quality general-purpose implementations:</p><table> <colgroup><col> <col> <col> <col> <col> </colgroup><thead> <tr> <th>Data size</th> <th>Baseline hash table</th> <th>Baseline sorting</th> <th>Tuned hash table</th> <th>Tuned sorting</th> </tr> </thead> <tbody> <tr> <td>8 KiB</td> <td>3.8&#8239;&micro;s</td> <td>5.1&#8239;&micro;s</td> <td><strong>1.6&#8239;&micro;s</strong></td> <td>6.5&#8239;&micro;s</td> </tr> <tr> <td>256 KiB</td> <td>219&#8239;&micro;s</td> <td>264&#8239;&micro;s</td> <td>193&#8239;&micro;s</td> <td><strong>92&#8239;&micro;s</strong></td> </tr> <tr> <td>8 MiB</td> <td>8.1&#8239;ms</td> <td>12.0&#8239;ms</td> <td>7.1&#8239;ms</td> <td><strong>3.9&#8239;ms</strong></td> </tr> <tr> <td>256 MiB</td> <td>875&#8239;ms</td> <td>464&#8239;ms</td> <td>269&#8239;ms</td> <td><strong>185&#8239;ms</strong></td> </tr> <tr> <td>2 GiB</td> <td>7.6&#8239;s</td> <td>4.3&#8239;s</td> <td>2.6&#8239;s</td> <td><strong>1.9&#8239;s</strong></td> </tr> </tbody> </table><p>Tuned sorting beats our best hash table by ~1.5&times; on non-tiny sizes, and is up to 4&times; faster than the excellent <a href="https://www.youtube.com/watch?v=ncHmEUmJZf4">&ldquo;Swiss Table&rdquo;</a> hash tables that ship with Rust&rsquo;s standard library.</p><p><a href="https://github.com/reinerp/hashed-sorting-benchmark">Benchmark code is available</a>.</p><h2>Why does sorting win?</h2><p>Memory bandwidth: <em>even though sorting makes multiple passes through memory, each pass uses bandwidth far more efficiently than a hash table&rsquo;s single pass.</em></p><p>Once the dataset outgrows CPU caches (often around ~1 MiB on a single core, CPU-dependent), both hashing and sorting become limited by cache-line fetch bandwidth to main memory. Cache lines are typically 64 bytes, and the CPU fetches the entire line if you touch even a single byte.</p><p>Hash tables waste bandwidth: each 8-byte key access pulls a full 64-byte cache line. So a hash table must incur at least 128 bytes of traffic per uint64 processed: 64 bytes read and 64 bytes written.</p><p>For sorting we use a radix sort that splits into 1024 buckets per pass, needing just 3 passes for <span><span><math><semantics><mrow><msup><mn>2</mn><mn>30</mn></msup></mrow><annotation>2^{30}</annotation></semantics></math></span></span> elements<a href="https://reiner.org#fn3"><sup>3</sup></a>. Each pass reads and writes the entire array once, and the accesses have enough spatial locality that the <em>whole</em> cache line is used productively, unlike in hash tables. So processing a single uint64 takes only 48 bytes of memory traffic: 8 bytes &times; 3 passes of reads, 8 bytes &times; 3 passes of writes.</p><p>This analysis would suggests a 2.7&times; speedup vs.&nbsp;hash tables: 128 bytes vs.&nbsp;48 bytes of memory traffic per uint64. The measured speedup is only ~1.5&times;, primarily (as far as I can tell) because it&rsquo;s harder to make the CPU cache system do exactly what you want for radix sort than for hash tables<a href="https://reiner.org#fn4"><sup>4</sup></a>. Hash tables get closer to their ideal than radix sort; radix sort&rsquo;s ideal is better enough that it still wins.</p><h2>Making radix sort robust</h2><p>Although radix sort is often the fastest sorting algorithm on random data, its performance degrades on data that isn&rsquo;t uniformly spread out over the key space. One pass sorts one byte of the key into 256 buckets; if only a small subset of the 256 buckets are used by keys in practice&mdash;for example, if the top byte of a uint64 is always zero&mdash;then a lot of the work on that pass of radix sort may be wasted.</p><p>To show this, I benchmarked sorting on (a) random uint64s and (b) uint64s where the random bits are &ldquo;spread out&rdquo;: even bits random, odd bits zero, like <span><span><math><semantics><mrow><mn>0</mn><mi>a</mi><mn>0</mn><mi>b</mi><mn>0</mn><mi>c</mi><mn>0</mn><mi>d</mi><mo>&hellip;</mo></mrow><annotation>0a0b0c0d\ldots</annotation></semantics></math></span></span>. For the spread-out numbers, each radix pass uses only 16 of 256 buckets, hurting efficiency. Radix sort is ~2&times; faster than quicksort on random numbers, but ~1.5&times; slower on spread-out numbers:</p><table> <thead> <tr> <th>Data size</th> <th>Quicksort</th> <th>Radix sort</th> </tr> </thead> <tbody> <tr> <td>8 KiB (random)</td> <td>5.1&#8239;&micro;s</td> <td><strong>4.8&#8239;&micro;s</strong></td> </tr> <tr> <td>256 KiB (random)</td> <td>262&#8239;&micro;s</td> <td><strong>111&#8239;&micro;s</strong></td> </tr> <tr> <td>8 MiB (random)</td> <td>11.9&#8239;ms</td> <td><strong>5.4&#8239;ms</strong></td> </tr> <tr> <td>256 MiB (random)</td> <td>466&#8239;ms</td> <td><strong>267&#8239;ms</strong></td> </tr> <tr> <td>2 GiB (random)</td> <td>4.2&#8239;s</td> <td><strong>2.8&#8239;s</strong></td> </tr> <tr> <td>8 KiB (spread-out)</td> <td><strong>5.2&#8239;&micro;s</strong></td> <td>6.3&#8239;&micro;s</td> </tr> <tr> <td>256 KiB (spread-out)</td> <td><strong>262&#8239;&micro;s</strong></td> <td>459&#8239;&micro;s</td> </tr> <tr> <td>8 MiB (spread-out)</td> <td><strong>12.2&#8239;ms</strong></td> <td>17.4&#8239;ms</td> </tr> <tr> <td>256 MiB (spread-out)</td> <td><strong>462&#8239;ms</strong></td> <td>628&#8239;ms</td> </tr> <tr> <td>2 GiB (spread-out)</td> <td>4.2&#8239;s</td> <td><strong>3.2&#8239;s</strong></td> </tr> </tbody> </table><p>To avoid these slowdowns, we borrow an idea from hash tables: sort by <code>hash(key)</code> rather than <code>key</code>. For counting uniques we don&rsquo;t care about final order, only grouping. Of course, this will change the sort order, but given that we&rsquo;re only interested in counting uniques rather than the sort order per se, this doesn&rsquo;t matter.</p><p>Better yet, use an invertible (<a href="https://en.wikipedia.org/wiki/Bijection">bijective</a>) hash to transform keys in place. That avoids storing both key <em>and</em> hash, or recomputing the hash each pass. Many widely used hash functions are invertible on <code>uint64</code>; I used <a href="https://github.com/reinerp/hashed-sorting-benchmark/blob/174a64335e879ad367a5393892ebd8461529af58/src/hashers.rs#L19">Murmur3</a> and a cheaper variant, <a href="https://github.com/reinerp/hashed-sorting-benchmark/blob/174a64335e879ad367a5393892ebd8461529af58/src/hashers.rs#L43">MulSwapMul</a>.</p><p>With a reasonably fast hasher, the cost of hashing is cheap and it fixes bad distributions. Using MulSwapMul, performance looks like this mostly regardless of data distribution:</p><table> <thead> <tr> <th>Data size</th> <th>Hashed radix sort</th> </tr> </thead> <tbody> <tr> <td>8 KiB</td> <td>6.5&#8239;&micro;s</td> </tr> <tr> <td>256 KiB</td> <td>92&#8239;&micro;s</td> </tr> <tr> <td>8 MiB</td> <td>3.9&#8239;ms</td> </tr> <tr> <td>256 MiB</td> <td>185&#8239;ms</td> </tr> <tr> <td>2 GiB</td> <td>1.9&#8239;s</td> </tr> </tbody> </table><p>This is the &ldquo;best algorithm&rdquo; shown at the top of this article: hash with MulSwapMul, then radix sort using <a href="https://axelle.me/2022/04/19/diverting-lsd-sort/">diverting LSD</a> <a href="https://en.wikipedia.org/wiki/Radix_sort">radix sort</a>. I fuse hashing with the first pass of radix sort, and counting with the last pass of radix sort, for a small additional speedup.</p><h2>When should I choose hash tables versus hashed radix sort?</h2><p>Some hash-table uses cannot be reformulated as hashed radix sort; the latter is more restrictive. Converting hash-table lookups/inserts to hashed radix sort fundamentally requires <em>batching</em>: you must issue many lookups without needing results until much later. Sometimes you can turn a one-pass algorithm (traverse a data structure and look up as you go) into two passes of the data structure: first gather keys from the data structure; do the radix sort; then traverse the data structure again to write results back. In some scenarios, such as <a href="https://en.wikipedia.org/wiki/Hash_consing">hash consing</a>, <a href="https://en.wikipedia.org/wiki/Common_subexpression_elimination">common subexpression elimination</a>, or parser lookup tables, the requirement for batching is a dealbreaker: sorting isn&rsquo;t viable.</p><p>Where batching <em>is</em> viable, hashed radix sorts are typically viable:</p><ul> <li>Any key types usable with hash tables work with hashed radix sorts: apply the same hash.</li> <li>If you are <em>constructing</em> a hash table, the analog is building a sorted array by radix sort. If you are <em>querying</em> an existing table, the analog is: sort the queries, then do a linear-time merge with the existing sorted array of keys.</li> <li>Radix sort <a href="https://reiner.org#appendix-4-parallelism">parallelizes at least as well</a> as hash tables, if not better.</li> </ul><p>If hashed radix sorts are viable for your problem, will they be faster? The main determining factor seems to be the number of <em>repeat accesses per unique key</em>. If you perform many more inserts/lookups than there are unique keys, hash tables tend to win; if accesses are on the same order as unique keys&mdash;most keys touched only a few times&mdash;radix sort tends to win. This is because hash tables use O(unique keys) memory, while radix sort uses O(accesses); fitting into a smaller footprint often yields better memory-system performance.</p><p>On my benchmarks, hash tables start to pull ahead when keys are accessed ~30 times on average. The threshold rises with problem size:</p><table> <colgroup><col> <col> <col> <col> <col> <col> <col> </colgroup><thead> <tr> <th>Size</th> <th>8 accesses per key<br>HashSet</th> <th>8 accesses per key<br>Hashed sorting</th> <th>32 accesses per key<br>HashSet</th> <th>32 accesses per key<br>Hashed sorting</th> <th>128 accesses per key<br>HashSet</th> <th>128 accesses per key<br>Hashed sorting</th> </tr> </thead> <tbody> <tr> <td>8 KiB</td> <td><strong>1.9&#8239;&micro;s</strong></td> <td>6.1&#8239;&micro;s</td> <td><strong>1.9&#8239;&micro;s</strong></td> <td>6.6&#8239;&micro;s</td> <td><strong>1.7&#8239;&micro;s</strong></td> <td>7.2&#8239;&micro;s</td> </tr> <tr> <td>256 KiB</td> <td>125&#8239;&micro;s</td> <td><strong>119&#8239;&micro;s</strong></td> <td><strong>120 us</strong></td> <td>141&#8239;&micro;s</td> <td><strong>149&#8239;&micro;s</strong></td> <td>157&#8239;&micro;s</td> </tr> <tr> <td>8 MiB</td> <td>6.7&#8239;ms</td> <td><strong>6.1&#8239;ms</strong></td> <td><strong>5.7&#8239;ms</strong></td> <td>7.0&#8239;ms</td> <td><strong>5.1&#8239;ms</strong></td> <td>7.1&#8239;ms</td> </tr> <tr> <td>256 MiB</td> <td>414&#8239;ms</td> <td><strong>246&#8239;ms</strong></td> <td><strong>240&#8239;ms</strong></td> <td><strong>242&#8239;ms</strong></td> <td><strong>200&#8239;ms</strong></td> <td>267&#8239;ms</td> </tr> <tr> <td>2 GiB</td> <td>3.9&#8239;s</td> <td><strong>2.6&#8239;s</strong></td> <td>3.2&#8239;s</td> <td><strong>2.7&#8239;s</strong></td> <td><strong>2.5&#8239;s</strong></td> <td>2.7&#8239;s</td> </tr> </tbody> </table> <h2>Why does it matter?</h2><p>Several high-performance systems have an inner loop equivalent to this; I&rsquo;ve personally worked on two.</p><p>First, extremely sparse unstructured matrix multiplication with e.g.&nbsp;sparsity of one in a billion and one scalar per nonzero. This applies to <a href="https://www.bigdatawire.com/2014/07/17/inside-sibyl-googles-massively-parallel-machine-learning-platform/">Google&rsquo;s Sibyl</a>, which in 2015 was one of Google&rsquo;s largest workloads and consumed several percent of fleetwide CPU. I and many others collectively spent years optimizing these inner loops.</p><p>Second, the <a href="https://www-labs.iro.umontreal.ca/~simul/testu01/tu01.html">BigCrush test suite for random number generators</a> counts duplicates among n-grams of generated numbers to find anomalies.</p><p>In my experience the default for problems like these is hash tables. The surprise to me, once I ran these benchmarks, is that radix sort can beat them by a substantial margin.</p><hr> <h2>Appendix 1: tuning radix sort</h2><p>There&rsquo;s a rich literature and set of libraries for fast radix sort. I primarily relied on and recommend <a href="https://axelle.me/2022/04/19/diverting-lsd-sort/">diverting LSD sort</a>, <a href="https://arxiv.org/pdf/2207.14334">diverting fast radix</a>, and the Rust library <a href="https://docs.rs/voracious_radix_sort/latest/voracious_radix_sort/">voracious_radix_sort</a>. The main characteristics of high-performance implementations like these are listed below:</p><p>Use a <em>diverting</em> radix sort, which only performs a few passes of radix sort before falling back to insertion sort. If <span><span><math><semantics><mrow><mtext>radix</mtext></mrow><annotation>\text{radix}</annotation></semantics></math></span></span> is the number of buckets per pass (e.g., 1024), then after <span><span><math><semantics><mrow><mi>p</mi></mrow><annotation>p</annotation></semantics></math></span></span> passes the array has been sorted into <span><span><math><semantics><mrow><msup><mtext>radix</mtext><mi>p</mi></msup></mrow><annotation>\text{radix}^p</annotation></semantics></math></span></span> buckets, with each bucket still needing sorting. After enough passes the buckets are tiny (average size &lt;1), so you stop doing radix passes and fix up locally with insertion sort. This makes diverting radix sort <span><span><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><msub><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mtext>radix</mtext></msub><mi>n</mi><mo>)</mo></mrow><annotation>O(n \log_{\text{radix}} n)</annotation></semantics></math></span></span>, versus non-diverting radix sort&rsquo;s <span><span><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>&sdot;</mo><mi>w</mi><mo>)</mo></mrow><annotation>O(n \cdot w)</annotation></semantics></math></span></span> where <span><span><math><semantics><mrow><mi>w</mi></mrow><annotation>w</annotation></semantics></math></span></span> is word length. For large keys like uint64 you can typically save more than half the passes.</p><p>Form the histograms for all <span><span><math><semantics><mrow><mi>p</mi></mrow><annotation>p</annotation></semantics></math></span></span> passes of radix sort in a single histogramming sweep, rather than one by one before each pass. This saves bandwidth: you only do <span><span><math><semantics><mrow><mi>p</mi><mo>+</mo><mn>1</mn></mrow><annotation>p+1</annotation></semantics></math></span></span> instead of <span><span><math><semantics><mrow><mn>2</mn><mi>p</mi></mrow><annotation>2p</annotation></semantics></math></span></span> sweeps through memory.</p><p>Beyond the above techniques standard in the literature, I did two other optimizations.</p><p>First, I fused the hashing pass (before the sort) into the histogramming pass (first step of the sort) to save memory bandwidth.</p><p>I fall back to inlined insertion sort rather than a function call into the standard library. Calling into standard library sort, which needs to perform many branches on size to figure out that this is a very small array, adds a lot of overhead for buckets whose average size is 1. Inlined insertion sort finishes very quickly for small arrays.</p><h2>Appendix 2: other memory-bandwidth-efficient sorts</h2><p>We prefer radix sort because it makes <span><span><math><semantics><mrow><msub><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mn>1024</mn></msub><mo>(</mo><mi>n</mi><mo>)</mo></mrow><annotation>\log_{1024}(n)</annotation></semantics></math></span></span> passes through memory rather than quicksort/mergesort&rsquo;s <span><span><math><semantics><mrow><msub><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mn>2</mn></msub><mo>(</mo><mi>n</mi><mo>)</mo></mrow><annotation>\log_{2}(n)</annotation></semantics></math></span></span> passes. Another way to reduce passes is <em>multi-way mergesort</em>: merge <span><span><math><semantics><mrow><mi>k</mi></mrow><annotation>k</annotation></semantics></math></span></span> arrays at once instead of 2, best implemented with a <a href="https://en.wikipedia.org/wiki/K-way_merge_algorithm#Tournament_Tree">tournament tree</a>.</p><p>I implemented and benchmarked this. In my implementation it substantially underperforms quicksort and radix sort. Even though memory bandwidth usage is better, the instruction usage seems to be worse. However, especially here, where the literature appears sparser, it&rsquo;s possible that my implementation was poorly tuned.</p><h2>Appendix 3: tuning hash tables</h2><p>Our tuned hash tables outperform baseline &ldquo;Swiss Table&rdquo; hash tables by up to 3&times; on this workload, largely because we can optimize for our specific case (huge tables, uint64 keys, prioritizing runtime time over memory footprint) whereas Swiss Tables must balance many use cases (strings, small tables, memory footprint).</p><p>Our tuned hash table is different than Swiss Tables in the following ways.</p><p><em>Single table vs.&nbsp;metadata+data.</em> Swiss Tables keep a &ldquo;metadata&rdquo; table (1-byte tags derived from 7-bit hash codes) in addition to the real data table. They probe metadata first (using SIMD instructions to probe 8&ndash;16 keys at once), then they probe the data table on a match. This allows efficient probing even when probe sequences are long, it has the disadvantage of requiring <em>two</em> cache misses per lookup: one for the metadata table, one for the data table. By contrast, we only use a data table: probing uint64 keys is already plenty fast. This allows us to pay just one cache miss per lookup<a href="https://reiner.org#fn5"><sup>5</sup></a>.</p><p>Swiss Table probing is not cacheline-aligned. A probe sequence may start in the middle of a cache line and continue to the next before finishing the current line, wasting bandwidth. Our tuned table also starts probing mid-line, but when we hit the end of the cache line we wrap to the beginning and probe the rest of the cache line before advancing to the next.<a href="https://reiner.org#fn6"><sup>6</sup></a></p><p>Swiss Tables pack their data more densely than our table does: they target a max load factor of 78.5%, whereas we target 50%. This is a pure time-vs-memory tradeoff: Swiss Tables value memory more, we value time more. By using a lower load factor, we keep probe distances shorter, and increase the probability that a lookup can succeed with just one cache miss.</p><p>We use <a href="https://doc.rust-lang.org/std/intrinsics/fn.prefetch_read_data.html">prefetching instructions</a> when accessing the table; Rust&rsquo;s Swiss Tables implementation does not offer such functionality. Prefetching for hash tables is advanced user functionality, because users need to restructure their loops (and typically split into a loop prologue, loop body, loop epilogue) to support prefetching. Compared to any other API offered by Rust&rsquo;s HashSet, this functionality is hugely more niche, and perhaps appropriately is not included in the standard library. That said, if you are willing to put in the effort to restructure your loops and tune them, prefetching can offer a huge speedup.</p><p>Unlike the above differences, the hash function itself is the same between our tuned hash table and Swiss Tables: we use MulSwapMul in both. This is similar speed to <code>foldhash::fast</code> and hugely faster than Rust&rsquo;s default <code>SipHash</code>.</p><p>All of these differences together add up to a ~3&times; speedup over Swiss Tables on this workload:</p><table> <thead> <tr> <th>Data size</th> <th>Swiss table</th> <th>Tuned hash table</th> </tr> </thead> <tbody> <tr> <td>8 KiB</td> <td>3.8&#8239;&micro;s</td> <td>1.6&#8239;&micro;s</td> </tr> <tr> <td>256 KiB</td> <td>219&#8239;&micro;s</td> <td>193&#8239;&micro;s</td> </tr> <tr> <td>8 MiB</td> <td>8.1&#8239;ms</td> <td>7.1&#8239;ms</td> </tr> <tr> <td>256 MiB</td> <td>875&#8239;ms</td> <td>269&#8239;ms</td> </tr> <tr> <td>2 GiB</td> <td>7.6&#8239;s</td> <td>2.6&#8239;s</td> </tr> </tbody> </table> <h2>Appendix 4: parallelism</h2><p>In many high-performance applications we want parallelism. Both radix sort and hash tables parallelize efficiently: for radix sort, parallelize within each pass and synchronize at pass boundaries; for hash tables, use fine-grained locking such as a lock per cacheline inside the table itself.</p><p>From a theoretical analysis I expect both radix sort and hash tables to parallelize very well, and the advantage of radix sort to be sustained even in the parallel context. I benchmarked a few of the top Rust libraries for parallel radix sort and parallel hash tables, but I didn&rsquo;t build any custom tuned versions myself.</p><p>For radix sort, I found <a href="https://docs.rs/voracious_radix_sort/latest/voracious_radix_sort/">voracious_radix_sort</a> to have excellent parallel performance; it starts beating the best single-threaded implementation once the data exceeded 8MiB, and I suspect it could be improved further by porting over some of the single-threaded tuning work I did.</p><table> <thead> <tr> <th>Data size</th> <th>Best sequential sorting</th> <th>Parallel sorting (8 cores)</th> </tr> </thead> <tbody> <tr> <td>8 KiB</td> <td>6.5&#8239;&micro;s</td> <td>109&#8239;&micro;s</td> </tr> <tr> <td>256 KiB</td> <td><strong>92&#8239;&micro;s</strong></td> <td>386&#8239;&micro;s</td> </tr> <tr> <td>8 MiB</td> <td><strong>3.9&#8239;ms</strong></td> <td>4.5&#8239;ms</td> </tr> <tr> <td>256 MiB</td> <td>185&#8239;ms</td> <td><strong>61&#8239;ms</strong></td> </tr> <tr> <td>2 GiB</td> <td>1.9&#8239;s</td> <td><strong>440&#8239;ms</strong></td> </tr> </tbody> </table><p>For hash tables, I didn&rsquo;t find an implementation that outperforms my best single-threaded implementation. This surprises me: in other contexts I&rsquo;ve seen near-linear parallel speedups with a lock-per&ndash;cache-line design, and I suspect it&rsquo;s possible here too. I didn&rsquo;t spend long tuning this, and it&rsquo;s possible I was using the existing libraries wrong or outside of their target workload profile.</p><section> <hr> <ol> <li><p>For batch algorithms with balanced lookups and inserts.<a href="https://reiner.org#fnref1">&#8617;&#65038;</a></p></li> <li><p>I also ran on a large AMD Zen 4 machine (AMD EPYC 9B14), and got similar results:</p><table> <colgroup><col> <col> <col> <col> <col> </colgroup><thead> <tr> <th>Data size</th> <th>Baseline hash table</th> <th>Baseline sorting</th> <th>Tuned hash table</th> <th>Tuned sorting</th> </tr> </thead> <tbody> <tr> <td>8 KiB</td> <td>3.6&#8239;&micro;s</td> <td>6.1&#8239;&micro;s</td> <td><strong>2.7&#8239;&micro;s</strong></td> <td>5.4&#8239;&micro;s</td> </tr> <tr> <td>256 KiB</td> <td>172&#8239;&micro;s</td> <td>302&#8239;&micro;s</td> <td><strong>98.2&#8239;&micro;s</strong></td> <td>123&#8239;&micro;s</td> </tr> <tr> <td>8 MiB</td> <td>12.5&#8239;ms</td> <td>14.6&#8239;ms</td> <td>9.0&#8239;ms</td> <td><strong>4.2&#8239;ms</strong></td> </tr> <tr> <td>256 MiB</td> <td>1.2&#8239;s</td> <td>585&#8239;ms</td> <td>378&#8239;ms</td> <td><strong>186&#8239;ms</strong></td> </tr> <tr> <td>2 GiB</td> <td>10.3&#8239;s</td> <td>5.2&#8239;s</td> <td>3.1&#8239;s</td> <td><strong>1.5&#8239;s</strong></td> </tr> </tbody> </table> <a href="https://reiner.org#fnref2">&#8617;&#65038;</a></li> <li><p>Traditional radix sorts wouldn&rsquo;t take 3 passes, they&rsquo;d take 8: one per byte of an 8-byte key. That&rsquo;s wasteful: if you&rsquo;re processing only <span><span><math><semantics><mrow><msup><mn>2</mn><mn>32</mn></msup></mrow><annotation>2^{32}</annotation></semantics></math></span></span> elements, then after 4 passes the array is almost fully sorted (each bucket has, on average, one element), so that&rsquo;s a good time to fall back to a simpler, cache-local algorithm such as insertion sort. The best form of this approach seems to be <a href="https://axelle.me/2022/04/19/diverting-lsd-sort/">Diverting LSD radix sort</a>.<a href="https://reiner.org#fnref3">&#8617;&#65038;</a></p></li> <li><p>In one pass, radix sort maintains 1024 write pointers (one per bucket). We write a uint64 to one of the write pointers, then advance the write pointer by 8 bytes. To hit ideal cache performance, you want the current cache line for each pointer to be kept in L1 cache (&ldquo;hot&rdquo;). Then, when the write pointer advances to the next cache line, we want the previous cache line to be flushed to main memory and then keep the next cache line hot. This requires the CPU to keep 1024 specific cache lines hot, and then quickly evicted and replaced with different cachelines when the pointers advance beyond the current line. Current CPUs typically have capacity for around 2048 cache lines in L1 cache, so in principle this is possible. Unfortunately, CPU caches are hardware-managed rather than software-managed, so we can&rsquo;t just <em>tell</em> the CPU to do this: instead, we rely on its cache eviction policy heuristics to do the job. Sometimes they will make mistakes, causing us to waste memory traffic; the fact that our cacheline demand is a large fraction of the total CPU cache size makes mistakes more likely.</p><p>For comparison, on hash tables there&rsquo;s much less guesswork required from the cache system. To hit the ideal cache system performance, the primary goal is to ensure that many cache misses can run in parallel so that we&rsquo;re cache-miss-throughput-bound (memory bandwidth) rather than cache-miss-latency-bound (memory latency). We achieve this by issuing cacheline prefetch instructions 64 iterations in advance of performing the lookup, so that we can have 64 cacheline fetches outstanding in parallel. This comes much closer to ideal than for radix sort because (a) there&rsquo;s CPU prefetch instructions that let us <em>tell</em> the cache system what we want to happen and (b) we only need 64 cache lines hot (out of an available 2048) rather than 1024, so mistakes are much less likely.<a href="https://reiner.org#fnref4">&#8617;&#65038;</a></p></li> <li><p>Meta&rsquo;s <a href="https://engineering.fb.com/2019/04/25/developer-tools/f14/">F14 table</a> places 14 metadata slots and 14 data slots side by side on the same pair of cache lines, achieving both locality <em>and</em> SIMD probing. At that sizing it still needs two (adjacent) misses; a 7-slot variant would be ideal for uint64 keys.<a href="https://reiner.org#fnref5">&#8617;&#65038;</a></p></li> <li><p>A simpler way to make probing cacheline-aligned would be to <em>start</em> all probe sequences at the beginning of a cache line. However, this increases average probe length: two different keys are more likely to start at the same probe location, since there are fewer probe sequence starting locations available.<a href="https://reiner.org#fnref6">&#8617;&#65038;</a></p></li> </ol> </section> </div></section>]]></description><pubDate>Thu, 11 Sep 2025 17:12:47 +0530</pubDate></item><item><link>https://eclipse.dev/eclipse/markdown/?f=news/4.37/index.md</link><title>Eclipse 4.37 Released (eclipse.dev)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1ne4hb2/eclipse_437_released/</guid><comments>https://www.reddit.com/r/programming/comments/1ne4hb2/eclipse_437_released/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/programming/comments/1ne4hb2/eclipse_437_released/'>Post permalink</a></p></section>]]></description><pubDate>Thu, 11 Sep 2025 15:13:53 +0530</pubDate></item><item><link>https://gizmodo.com/microsoft-goes-back-to-basic-open-sources-bill-gates-code-2000654010</link><title>Microsoft Goes Back to BASIC, Open-Sources Bill Gates' Code (gizmodo.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1ndpzfz/microsoft_goes_back_to_basic_opensources_bill/</guid><comments>https://www.reddit.com/r/programming/comments/1ndpzfz/microsoft_goes_back_to_basic_opensources_bill/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/programming/comments/1ndpzfz/microsoft_goes_back_to_basic_opensources_bill/'>Post permalink</a></p></section><section class='preview-image'><img src='https://gizmodo.com/app/uploads/2020/08/z1olh9qw8p81mdbspql3.jpg' /></section><section class='parsed-content'><div><p>In the era of vibe coding, when even professionals are pawning off their programming work on AI tools, Microsoft is throwing it all the way back to the language that launched a billion devices. On Wednesday, the company <a href="https://opensource.microsoft.com/blog/2025/09/03/microsoft-open-source-historic-6502-basic/">announced</a> that it would make the source code for Microsoft BASIC for the 6502 Version 1.1 publicly available and open-source. The code is now <a href="https://github.com/microsoft/BASIC-M6502">uploaded to GitHub</a> under an MIT license (with a cheeky commit time stamp of &ldquo;48 years ago&rdquo;).</p><p>Microsoft called the code&mdash;written by the company&rsquo;s founder, Bill Gates, and its second-ever employee, Ric Weiland&mdash;&rdquo;one of the most historically significant pieces of software from the early personal computer era.&rdquo; It&rsquo;s pretty simple, clocking in at just 6,955 lines of assembly language, but that simplicity was key to its becoming so foundational to just about everything.</p><p>The <a href="https://spectrum.ieee.org/chip-hall-of-fame-mos-technology-6502-microprocessor">MOS 6502 processor</a>, which ran the code, was inexpensive and accessible compared to contemporary alternatives, and variations of the chip would eventually find their way into the Atari 2600, Nintendo Entertainment System, and Commodore computers. In fact, the story goes that Microsoft licensed its 6502 BASIC to Commodore for a flat fee of $25,000, which turned out to be a great deal for Commodore, which shipped millions of computers running the code.</p><p>Per Microsoft, the company&rsquo;s first product was a BASIC interpreter for the Intel 8080, which was written by Gates and co-founder Paul Allen. The version the company dropped on GitHub is actually an updated version of BASIC, which contains bug fixes implemented by Gates and Commodore engineer John Feagans. While it&rsquo;s called 1.1 on GitHub, Microsoft said it initially shipped as BASIC V2.</p><p>It&rsquo;s kind of a big deal for Microsoft to finally open-source the entirety of the code, which was previously only available in bits and pieces. Without Microsoft&rsquo;s official blessing to make this code public, it was possible that the original documentation, as well as the legal permission needed to use the code, would have been lost to history. Now it&rsquo;s possible for the code to be preserved, played with, and better understood.</p><p>As <a href="https://arstechnica.com/gadgets/2025/09/microsoft-open-sources-bill-gates-6502-basic-from-1978/">Ars Technica points out</a>, the assembly code can&rsquo;t be run on modern devices directly, but is still functional in emulators and field-programmable gate array (FPGA) implementations that allow researchers and programmers to explore old code and mine it for everything from just understanding how it works to understanding how programmers of the past approached efficient design practices.</p><p>BASIC 5502 joins <a href="https://github.com/microsoft/GW-BASIC">GW-BASIC</a>, <a href="https://github.com/microsoft/MS-DOS">MS-DOS</a>, and the <a href="https://github.com/option8/Altair-BASIC">Altair BASIC</a> on the list of code that Microsoft has open-sourced in recent years.</p></div></section>]]></description><pubDate>Thu, 11 Sep 2025 02:23:02 +0530</pubDate></item></channel></rss>
