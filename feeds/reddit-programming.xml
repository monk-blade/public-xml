<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><atom:link href="http://192.168.1.132/?platform=reddit&amp;subreddit=programming&amp;averagePostsPerDay=5&amp;content&amp;view=rss" rel="self" type="application/rss+xml"/><title>/r/programming</title><description>Hot posts in /r/programming (roughly 5 posts per day)</description><link>https://www.reddit.com/r/programming/</link><language>en-us</language><lastBuildDate>Sun, 14 Sep 2025 23:09:46 +0000</lastBuildDate><generator>Upvote RSS</generator><image><url>https://styles.redditmedia.com/t5_2fwo/styles/communityIcon_1bqa1ibfp8q11.png</url><title>/r/programming</title><link>https://www.reddit.com/r/programming/</link></image><item><link>https://newsletter.scalablethread.com/p/why-event-driven-systems-are-hard</link><title>Why Event-Driven Systems are Hard? (newsletter.scalablethread.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1ngwj0l/why_eventdriven_systems_are_hard/</guid><comments>https://www.reddit.com/r/programming/comments/1ngwj0l/why_eventdriven_systems_are_hard/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 21 min | <a href='https://www.reddit.com/r/programming/comments/1ngwj0l/why_eventdriven_systems_are_hard/'>Post permalink</a></p></section><section class='preview-image'><img src='https://substackcdn.com/image/fetch/$s_!yqHc!,w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f23e44c-2a3c-416c-82ed-55a143001014_1456x1048.png' /></section><section class='parsed-content'><div><p><span>An </span><em>event</em><span> is just a small message that says, "Hey, something happened!" For example, </span><code>UserClickedButton</code><span>, </span><code>PaymentProcessed</code><span>, or </span><code>NewOrderPlaced</code><span>. Services subscribe to the events they care about and react accordingly. This event-driven approach makes systems resilient and flexible. However, building and managing these systems at a large scale is surprisingly hard.</span></p><p>Imagine you and your friend have a secret code to pass notes. One day, you decide to add a new symbol to the code to mean something new. If you start using it without telling your friend, your new notes will confuse them. This is exactly what happens in event-driven systems.</p><p><span>For example, an </span><code>OrderPlaced</code><span> event might look like this:</span></p><p><span>Now imagine another service reads this event to send a confirmation email. Then, six months later, you add a new field: </span><code>shippingAddress</code><span>. You update the producer. The event becomes:</span></p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!rBfd!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee987fa9-a617-4f5a-83dc-285b7cc3abbe_476x194.png"></a></figure></div><p><span>The problem is that other services, like the </span><code>OrderConfirmationEmailService</code><span>, might still be expecting the old version 1 format. When they receive this new message, they won't know what to do with the </span><code>shippingAddress </code><span>field. Worse, if a field they relied on was removed, they would simply crash.</span></p><p>This forces teams to carefully manage how schemas evolve. Common strategies include:</p><ul><li><p><strong>Backward Compatibility:</strong><span> New schemas can be read by services expecting the old schema. This usually means you can only add new, optional fields. You can't rename or remove existing ones.</span></p></li><li><p><strong>Forward Compatibility:</strong><span> Services expecting a new schema can still read messages written in an old one. This is harder to achieve and often requires setting default values for missing fields.</span></p></li><li><p><strong>Schema Registry:</strong><span> This is like a central dictionary for all your event "secret codes." Before a service sends a message, it checks with the registry to make sure the format is valid and compatible. It prevents services from sending out "confusing notes."</span></p></li></ul><p>Without strict rules for changing message formats, a simple update can cause a cascade of failures throughout a large system.</p><p>In a traditional, non-event-driven system, when a user clicks a button, one piece of code calls another, which calls another, in a straight line. If something breaks, you can look at the error log and see the entire sequence of calls, like following a single piece of string from start to finish.</p><p><span>In an event-driven system, that single string is cut into dozens of tiny pieces. The </span><code>OrderService</code><span> publishes an </span><code>OrderPlaced</code><span> event. The </span><code>PaymentService</code><span>, </span><code>ShippingService</code><span>, and </span><code>NotificationService</code><span> all pick it up and do their own work independently. They might, in turn, publish their own events.</span></p><p>Now, imagine a customer calls saying they placed an order but never got a confirmation email. Where did it go wrong?</p><ul><li><p><span>Did the </span><code>OrderService</code><span> fail to publish the event?</span></p></li><li><p><span>Did the </span><code>NotificationService</code><span> not receive it?</span></p></li><li><p>Did it receive the event but fail to connect to the email server?</p></li></ul><p>Debugging this can be difficult as you can't see the whole picture at once.</p><p><span>To solve this, we use </span><strong>distributed tracing</strong><span>. When the very first event is created, we attach a unique ID to it, called a </span><strong>Correlation ID</strong><span>. Every service that processes this event or creates a new event as a result must copy that same ID onto its own work.</span></p><p>When you need to investigate a problem, you can search for this one correlation ID across all the logs of all your services. This allows you to stitch the story back together and see the journey of that single request across the entire distributed system.</p><p>Events can disappear. Not because of bugs &mdash; because of infrastructure issues like network failure, a service crashing, or the message broker itself having a problem.</p><p><span>The core promise of many event systems is </span><a href="https://newsletter.scalablethread.com/i/146810780/at-least-once-guarantee">at-least-once delivery</a><span>. This means the system will do everything it can to make sure your event gets delivered. If a service that is supposed to receive an event is temporarily down, the message broker will hold onto the message and try again later.</span></p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!sGmD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc173afd2-6a2e-414d-a7ed-8af50160ab18_3183x2926.png"></a></figure></div><p><span>But what if a service has a persistent bug and crashes every time it tries to process a specific message? The broker will keep trying to redeliver it, and the service will keep crashing, until the broker retry limit is reached. To handle this, we use a </span><a href="https://newsletter.scalablethread.com/i/152780978/using-dead-letter-queues">Dead-Letter Queue (DLQ)</a><span>. After a few failed delivery attempts, the message broker moves the crash causing message to the DLQ. This stops the cycle of crashing and allows the service to continue processing other, valid messages. Engineers can then inspect the DLQ later to debug the problematic message.</span></p><p>The guarantee of "at-least-once delivery" creates a new, tricky problem: what if a message is delivered more than once? This can happen if a service processes an event but crashes before it can tell the message broker, "I'm done!" The broker, thinking the message was never handled, will deliver it again when the service restarts.</p><p><span>If the event was </span><code>IncreaseItemCountInCart</code><span>, receiving it twice is a big problem. The customer who wanted one item now has two in their cart. If it was </span><code>ChargeCreditCard</code><span>, they get charged twice.</span></p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!BlHa!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7318f031-5ce1-43d4-870a-dcd0a94748ec_3038x1084.png"></a></figure></div><p><span>To prevent this, services must be </span><a href="https://newsletter.scalablethread.com/p/how-to-build-idempotent-apis">idempotent</a><span>. We can achieve idempotency by having the service keep a record of the event IDs it has already processed. When a new event comes in, the service first checks its records.</span></p><ol><li><p>Has it seen this event ID before?</p></li><li><p>If yes, it simply ignores the duplicate and tells the broker, "Yep, I'm done."</p></li><li><p>If no, it processes the event and then saves the event ID to its records before telling the broker it's done.</p></li></ol><p>This ensures that even if a message is delivered 100 times, the action is only performed once.</p><p><span>In a simple application with one database, when you write data, it's there instantly. If you change your shipping address, the very next screen you load will show the new address. This is called </span><a href="https://newsletter.scalablethread.com/i/146489166/strict-consistency-model">strong consistency</a><span>.</span></p><p><span>Event-driven systems give up this guarantee for the sake of scalability and resilience. They operate on a model of </span><a href="https://newsletter.scalablethread.com/i/146489166/eventual-consistency-model">eventual consistency</a><span>. For example, when a user updates their address, the </span><code>CustomerService</code><span> updates its own database and publishes an </span><code>AddressUpdated</code><span> event. The </span><code>ShippingService</code><span> and </span><code>BillingService</code><span> subscribe to this event. But it might take a few hundred milliseconds for them to receive the event and update their own data (This example is to provide some context, but ideally, the address should be stored at one place and the id of that record should be passed around in the events).</span></p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!gE2i!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ec02ee-bcdf-4832-8ccc-0adfa1aec02b_2812x2100.png"></a></figure></div><p>Designing for eventual consistency means the system must be built to handle this temporary state of disagreement. This might involve:</p><ul><li><p>Designing user interfaces that account for the delay.</p></li><li><p>Adding logic to services to double-check critical data if needed.</p></li><li><p>Accepting that for some non-critical data, a small delay is acceptable.</p></li></ul><p><em>If you enjoyed this article, please hit the &#10084;&#65039; like button.</em></p><p><em>If you think someone else will benefit from this, please &#128257; share this post.</em></p></div><div class="gallery"><p><img src="https://substackcdn.com/image/fetch/$s_!H04Y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec4178cf-ceb4-41b5-984a-39babd83c5d5_447x174.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!rBfd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee987fa9-a617-4f5a-83dc-285b7cc3abbe_476x194.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!sGmD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc173afd2-6a2e-414d-a7ed-8af50160ab18_3183x2926.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!BlHa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7318f031-5ce1-43d4-870a-dcd0a94748ec_3038x1084.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!gE2i!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ec02ee-bcdf-4832-8ccc-0adfa1aec02b_2812x2100.png"></p></div></section>]]></description><pubDate>Sun, 14 Sep 2025 22:28:39 +0530</pubDate></item><item><link>https://strategizeyourcareer.com/p/how-software-engineers-make-productive-decisions</link><title>How Software Engineers Make Productive Decisions (without slowing the team down) (strategizeyourcareer.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nglax1/how_software_engineers_make_productive_decisions/</guid><comments>https://www.reddit.com/r/programming/comments/1nglax1/how_software_engineers_make_productive_decisions/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 25 min | <a href='https://www.reddit.com/r/programming/comments/1nglax1/how_software_engineers_make_productive_decisions/'>Post permalink</a></p></section><section class='preview-image'><img src='https://substackcdn.com/image/fetch/$s_!mn9w!,w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74bbd26c-5055-4674-b33d-e18156d02bea_4813x3950.png' /></section><section class='parsed-content'><div><p>Most teams don&rsquo;t get stuck because problems are impossible. They get stuck because every choice is treated like it&rsquo;s irreversible. In reality, lots of calls are two-way doors: you can walk through, check the room, and walk back out. Save the caution for the true one-way doors: data migrations, security posture, customer-visible changes with real blast radius.</p><p>When I&rsquo;m unsure, I run a fast, risk-aware filter. If the downside is small, the change is reversible, or I can mitigate quickly, I ship with guardrails. That&rsquo;s how you move fast without being sloppy.</p><p><strong>&#11088; In this post, you'll learn:</strong></p><ul><li><p>How to tell if a decision is reversible or not</p></li><li><p>The 3 questions I ask before slowing down</p></li><li><p>How to move fast without being sloppy</p></li><li><p>Why speed compounds into career growth</p></li></ul><p>Not every door leads to a cliff, some just swing back open. Two-way doors are things like toggling a feature flag, shipping a non-consumed response field, or swapping an internal library behind an abstraction. If it goes sideways, you flip the switch or roll back.</p><p>One-way doors are different. Think data migrations, schema changes, or decisions that can silently corrupt data or take a core service down. At my job, when a migration touches your database and could risk data loss, I&rsquo;d slow down on purpose: rehearsal in non-prod environments, snapshot plans, read-only windows if needed, and crisp rollback playbooks.</p><p>The productivity benefit is knowing the difference before you start. Over-investing in reversible decisions burns time and morale. Under-investing in high-stakes calls burns trust and customer goodwill.</p><p>When a decision lands on your lap, take 1-2 minutes and ask:</p><p>Is the effect invisible, annoying, or catastrophic? User-visible errors, security regressions, and data integrity issues are &ldquo;slow-down&rdquo; territory. On the other hand, shipping a field the client doesn&rsquo;t yet consume is low risk. I&rsquo;ve green-lit rollouts like this with smoke tests + feature flag, skipping a day or two of heavy testing because there was effectively no customer impact and rollback was trivial.</p><p>Reversal options change everything. If I can roll back in ~10 minutes because I have alarms, canary checks, and a pre-wired rollback, I bias toward speed. When reversal is painful (e.g., a destructive migration), I do design notes, peer review, and a rehearsal.</p><p>Sometimes you can&rsquo;t prevent every issue, but you can limit the blast radius. Canaries, partial rollouts, and scoped feature flags mean we learn quickly without harming many users. A line I actually use with stakeholders:</p><p>&ldquo;Do we need to focus on prevention here, or can we move forward and mitigate fast with a small blast radius if something is wrong? If mitigation is fast and contained, let&rsquo;s go.&rdquo;</p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!4ove!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe27276b0-b814-4dd0-8330-adb762162b0b_865x401.png"></a></figure></div><p>If your situation is somewhere in the middle, pick the stricter option to err on the safe side.</p><p>Flags are the safest way to keep a single-branch mainline in production. Merge early, merge often, even incomplete work, because the flag hides it. That enables smaller PRs, faster reviews, and quicker rollback. In my experience, for many features, the client hasn&rsquo;t started working on the changes on their end, which makes these deployments extremely low risk because they aren&rsquo;t consuming your new changes yet.</p><p><strong>Checklist for feature flags:</strong></p><ul><li><p>Default-off flag per risk domain (UI, backend path, integration).</p></li><li><p>One-line rationale in the PR (&ldquo;why now, why safe&rdquo;).</p></li><li><p>Smoke tests for both on/off states.</p></li><li><p>Exit plan: when and how to delete the flag.</p></li></ul><p>Even without a feature flag, speed is safe if your observability and rollback are tight:</p><ol><li><p><strong>Before:</strong><span> canary tests succeeding, metrics emitted.</span></p></li><li><p><strong>After:</strong><span> alarms on errors, latency, saturation, and key business metrics.</span></p></li><li><p><strong>Abort:</strong><span> scripted rollback (or deploy previous artifact) within ~10 minutes.</span></p></li></ol><p>I&rsquo;ve shipped features knowing that if anything trips alarms, the change is reverted quickly. That confidence changes the cost/benefit calculus.</p><ul><li><p>Timebox reversible decisions to 30-60 minutes of research. Make a call, document trade-offs, and move.</p></li><li><p>Slow down for one-way doors: destructive DB changes, non-backward compatible API changes, payment logic. Do some shadow testing to properly mimic production.</p></li></ul><p><strong>Two-minute safety-net before shipping:</strong></p><ul><li><p>Write a one-line rationale.</p></li><li><p>Identify the kill switch (flag or rollback).</p></li><li><p>Ping the right stakeholder if risk &gt; medium.</p></li><li><p>Confirm alarms cover the critical path.</p></li></ul><p>I added a new field to an HTTP response that clients weren&rsquo;t consuming yet. A full regression would have cost 1-2 days. Instead, I agreed with my team to:</p><ul><li><p>Shipped behind a feature flag.</p></li><li><p>Ran smoke tests on the endpoint.</p></li><li><p>Set alarms and a canary to verify no unexpected 4xx/5xx patterns.</p></li><li><p>Communicated &ldquo;proceed unless blocked.&rdquo;</p></li></ul><p>No customer impact, tiny blast radius, trivial rollback. That&rsquo;s a textbook two-way door.</p><p>For a risky database migration (possible customer data loss if wrong, service down for 1+ hours), we did the opposite:</p><ul><li><p>Wrote a design doc with trade-offs and risk analysis.</p></li><li><p>Investigated the migration in staging with production-like data.</p></li><li><p>Booked a change window, took snapshots, confirmed restore steps.</p></li><li><p>Assigned an on-call with a printed execution and rollback playbook.</p></li></ul><p>One-way doors require us to slow down, for good reason.</p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!MQj5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F582106f5-720b-4aa0-8ca8-4941b6bfe3cd_1241x433.png"></a></figure></div><p>You don&rsquo;t grow by being right once. You grow by making many decisions and handling the wrong ones well. Use the 3-question filter:</p><ol><li><p>Impact if wrong</p></li><li><p>Ease of reversal</p></li><li><p>Fast mitigation with small blast radius</p></li></ol><p>Turn as many calls as possible into two-way doors with flags, canaries, alarms, and quick rollbacks. Slow down only for the truly irreversible. That&rsquo;s how software engineers make decisions&mdash;fast, but not sloppy.</p><p>Book a 15-minute huddle where you: state the problem, options, trade-offs, risk level, and your recommendation. Close with: &ldquo;I&rsquo;ll proceed unless you see blockers.&rdquo; Silence becomes alignment, and you avoid approval ping-pong.</p><p>You don&rsquo;t need a five-page RFC for every call. A micro-ADR keeps history without ceremony:</p><pre><code><code># ADR: Add field (server response) - Context: Mobile clients currently ignore this field, used by future device rollout. - Decision: Ship behind flag, smoke test, canary 5%, alarms on 4xx/5xx and latency. - Alternatives: Delay until client change, ship without flag. - Consequences: If wrong, toggle flag off, rollback build, cleanup flag in 2 weeks. - Author: <your-name> | Date: 2025-09-13 </your-name></code></code></pre><p>This takes two minutes and prevents &ldquo;why did we do this?&rdquo; archaeology months later.</p><p><strong>How do I know if a decision is reversible?</strong><span> If you can turn it off, roll it back quickly, or hide it (flag) without customer harm, it&rsquo;s reversible. If it threatens data integrity, security, or customer trust and can&rsquo;t be undone cleanly, treat it as a one-way door.</span></p><p><strong>When should I write a full RFC vs. a micro-ADR?</strong><span> Use a micro-ADR for low/medium-risk calls to keep momentum. Use an RFC or longer design doc for high-risk, hard-to-reverse changes (migrations, auth, billing).</span></p><p><strong>What&rsquo;s a good rollback target?</strong><span> Aim for ~10 minutes from alarm to safe state for medium-risk changes. For high-risk changes, rehearse rollback in staging and ensure snapshot/restore times are known.</span></p><p><strong>How do I communicate speed safely?</strong><span> Frame decisions in risk terms: &ldquo;Impact small, reversible in 10 minutes, mitigation plan ready.&rdquo; If mitigation is fast and the blast radius is tiny, bias toward speed.</span></p><p><strong>Do feature flags add tech debt?</strong><span> Only if you don&rsquo;t clean them up. Track flags, add an &ldquo;expiry&rdquo; note in your micro-ADR, and remove them once the decision is proven.</span></p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!M2Z1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe0e0bed-a699-429b-94ee-b29c768c5323_800x60.png"></a></figure></div><div><figure><a href="https://substackcdn.com/image/fetch/$s_!iggC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb37fb50-b8f5-4348-9675-1129f39e6926_800x60.png"></a></figure></div><p>These are some great articles I&rsquo;ve read last week:</p><ul><li><p><a href="https://read.highgrowthengineer.com/p/operating-principles-to-staff-part-1?r=2tt34d">Operating Principles That Guided Me to Staff Engineer (Part 1: Driving Impact)</a><span> by </span></p><span>. I was happy to read this post about Jordan&rsquo;s promotion. Don&rsquo;t wait for tickets, hunt for problems and solve them early. That&rsquo;s how you create visible impact and grow faster.</span></li><li><p><a href="https://newsletter.eng-leadership.com/p/how-to-use-ai-to-improve-teamwork?r=2tt34d">How to Use AI to Improve Teamwork in Engineering Teams</a><span> by </span></p><span> and </span><span> . AI won&rsquo;t fix teamwork on its own, but with trust and autonomy in place it removes friction and lets teams move faster.</span></li><li><p><a href="https://growthalgorithm.dev/p/if-you-write-80-less-code-as-tech?r=2tt34d">If You Write 80% Less Code as Tech Lead</a><span> by </span></p><span>. As a tech lead, your leverage comes from enabling others: teach through reviews, own key components, and carve time for prototypes.</span></li><li><p><a href="https://thehustlingengineer.substack.com/p/genai-for-engineers-part-1-the-foundations?r=2tt34d">GenAI for Engineers (Part 1: The Foundations)</a><span> by </span></p><span> . I wrote recently about context engineering. LLMs are just prediction machines. To build production-ready systems, we need to focus on the context.</span></li></ul><div><figure><a href="https://substackcdn.com/image/fetch/$s_!Wk_Z!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cd056dd-a3ed-4cd0-8a5c-88321c90bbc6_800x60.png"></a></figure></div><p><em>P.S. This may interest you:</em></p><ol><li><p><em><span>Are you in doubt whether the paid version of the newsletter is for you? </span><strong><a href="https://strategizeyourcareer.com/about">Discover the benefits here</a></strong></em></p></li><li><p><span>Could you take one minute to answer a quick, anonymous survey to make me improve this newsletter? </span><strong><a href="https://forms.gle/2A5QCTAevJJuHaro9">Take the survey here</a></strong></p></li><li><p><em><span>Are you a brand looking to advertise to engaged engineers and leaders? </span><strong><a href="https://www.passionfroot.me/fransoto">Book your slot now</a></strong></em></p></li></ol><p><span>Give a like &#10084;&#65039; to this post if you found it useful, and share it with a friend to </span><a href="http://strategizeyourcareer.com/leaderboard">get referral rewards</a></p></div><div class="gallery"><p><img src="https://substackcdn.com/image/fetch/$s_!mn9w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74bbd26c-5055-4674-b33d-e18156d02bea_4813x3950.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!4ove!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe27276b0-b814-4dd0-8330-adb762162b0b_865x401.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!MQj5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F582106f5-720b-4aa0-8ca8-4941b6bfe3cd_1241x433.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!M2Z1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe0e0bed-a699-429b-94ee-b29c768c5323_800x60.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!iggC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb37fb50-b8f5-4348-9675-1129f39e6926_800x60.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!Wk_Z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cd056dd-a3ed-4cd0-8a5c-88321c90bbc6_800x60.png"></p></div></section>]]></description><pubDate>Sun, 14 Sep 2025 13:24:39 +0530</pubDate></item><item><link>https://theaxolot.wordpress.com/2025/09/10/be-an-agnostic-programmer/</link><title>Be An Agnostic Programmer (theaxolot.wordpress.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1ng1yn4/be_an_agnostic_programmer/</guid><comments>https://www.reddit.com/r/programming/comments/1ng1yn4/be_an_agnostic_programmer/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 7 min | <a href='https://www.reddit.com/r/programming/comments/1ng1yn4/be_an_agnostic_programmer/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys! Back with another article on a topic that&#39;s been stewing in the back of my mind for a while. Please enjoy!</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://theaxolot.wordpress.com/wp-content/uploads/2024/05/cropped-1677184869904.jpg?w=200' /></section><section class='parsed-content'><div><p>If civil engineering is a mature field, then software development is a baby. An ugly baby, but perhaps a late bloomer. The so-called &ldquo;best practices&rdquo; of our industry are a chaotic nebula of differing, often incompatible, perspectives.</p><p><em>&ldquo;If you don&rsquo;t follow TDD, you&rsquo;re not a professional.&rdquo;</em></p><p><em>&ldquo;If you think OOP leads to overengineering, you&rsquo;re doing it wrong.&rdquo;</em></p><p>&ldquo;<em>100% code coverage</em> <em>is the gold standard.&rdquo;</em></p><p>But I&rsquo;m not here to discuss individual practices in detail. Instead, I want to zoom out and look at how we got here in the first place. Strap in, because this is gonna get philosophical.</p><p><strong>Margaret Hamilton </strong>coined the term &ldquo;software engineering&rdquo; to lend the discipline a legitimacy when the moon landing was in the works. It seems to have worked. Although nowadays, the term is so broad that a simple React web app also qualifies.</p><p>When you include the word &ldquo;engineering&rdquo; in your title, people think your discipline is this scientific, rigorous, methodological process that yields the best answer based on collective historical experience. And to the layman, it might seem so. After all, software runs on machines, and machines fall under &ldquo;engineering.&rdquo;</p><p>But let&rsquo;s not kid ourselves.</p><p>Software development isn&rsquo;t a science.</p><p>It&rsquo;s not an art, either. It&rsquo;s a mix of both, and that&rsquo;s why I love it.</p><p>The science is in the logic of your program and the architecture of your system. It&rsquo;s in how well you can prove the correctness of your code (invariants, assertions, tests, etc). And it&rsquo;s in how you investigate and deduce your way to the root causes of issues.</p><p>But there&rsquo;s a human element, too.</p><p>With every sizable new system or feature, you explore while your design isn&rsquo;t yet crystallized, and that gives room to creativity. You don&rsquo;t always know how your interfaces (APIs, classes, namespaces, modules, etc) will end up until you have a good amount implementation in front of you (part of my dislike of by-the-book TDD).</p><p>And just like a UI/UX designer, you need empathy and foresight to ensure a good experience for future maintainers, even if you have to break conventions sometimes. If that&rsquo;s not art, then what is?</p><blockquote><p>Know the rules well, so you can break them effectively</p></blockquote><p>But you can&rsquo;t break the rules without a diverse set of tools in your toolbox. Whenever I see someone proselytize a specific programming paradigm as a one-size fits all &ldquo;best practice&rdquo;, it&rsquo;s because they don&rsquo;t get this.</p><p>Programming attracts people who are enamored with logic in action. Unfortunately, it also attracts people who can be obsessive about it, to the point where they fallaciously believe that programming is a discipline with hard rules like other kinds of engineering. And when that happens, you end up exclusively subscribing to a paradigm your were taught as &ldquo;the correct way,&rdquo; and it becomes difficult to think outside of that box.</p><p>The classic example is Object-Oriented Programming. Many developers had their first exposure to programming through this paradigm, and were taught little else. So when they went into the workforce, they constantly ran into problems that didn&rsquo;t map neatly onto OOP principles. So what did they do? They forced them to, like a square peg when all you have are round holes.</p><p>I was lucky in that I was exposed to many programming styles during my early education, such that I don&rsquo;t feel constrained to adhering to a single one. Does that make me unprincipled? No, it makes me versatile. Git gud.</p><p>Some endure a hard journey of unlearning these rigid beliefs. But others double down and become dogmatists, unable to concede to the drawbacks of their favorite paradigm lest they invalidate their whole identity as developers. And some, though far fewer, become grifters who profit off the air of authority that being &ldquo;principled&rdquo; gives them.</p><p>Programming books are a major culprit, because they hold the ultimate air of authority. Honest authors will:</p><ul> <li>Acknowledge when something is more their opinion than fact</li> <li>Highlight the drawbacks of their suggested methodologies as well as their benefits</li> <li>Give leeway for breaking of their rules in service of the ultimate goal of code: to be maintained by humans (until AI takes over, I mean)</li> </ul><p>But dishonest authors will:</p><ul> <li>Intersperse personal opinions with facts to subtly convert readers</li> <li>Treat their preferred paradigm like a product to be sold (e.g downplaying the flaws)</li> <li>Assert objective superiority of their methodologies in all scenarios</li> <li>Cherry-pick or misrepresent studies that confirm what they&rsquo;re pushing and ignore those that contradict</li> </ul><p>(Note: Isn&rsquo;t it interesting how we rarely cite studies when debating the best programming guidelines? But rather we always appeal to principles and/or psychology?).</p><p>But the biggest culprit of all, in my opinion, is a lack of confidence.</p><p>Every discipline has beginners that ask questions like, &ldquo;What&rsquo;s the best way to do X?&rdquo; or &ldquo;Is it okay if I don&rsquo;t do X?&rdquo;. In fields involving creativity, the most common answer is, &ldquo;it depends.&rdquo;</p><p>People still give general recommendations, but the experts are confident enough in their skills to pick and choose which guidelines to follow, yet humble enough to acknowledge they can&rsquo;t push hard rules.</p><p>Expert software developers, in particular, discuss trade-offs instead of asserting best practices with snappy soundbites. Because every situation is unique. That&rsquo;s what makes the job so difficult, yet so rewarding.</p><p>But single-minded adherents are terrified of tapping into their creative side, unless they&rsquo;re heavily constrained. They&rsquo;re terrified of improvising and not knowing exactly what to do in every situation. But most of all, they&rsquo;re terrified that there isn&rsquo;t an objective way to get a correct answer to their problems.</p><p>&ldquo;Everything has to adhere to a principle, or else how do I know I&rsquo;m doing the right thing?&rdquo;</p><p>Experience, intuition, and being comfortable with uncertainty.</p><p>P.S</p><p>I&rsquo;m not saying there aren&rsquo;t any best practices at all. What I&rsquo;m saying is that the more high-level a practice is, the more leeway you need to exercise with how rigidly you adhere to it. Simply because that&rsquo;s the nature of creative fields. If I had to roughly rank the following kinds of software development guidelines from 1 to 5, where 1 is the most low-level, and 5 is most high-level, it would look like this:</p><ul> <li>(1) Code style</li> <li>(2) Module dependency structure</li> <li>(3) Development paradigm (TDD, OOP)</li> <li>(4) System architecture practices</li> <li>(5) Work-planning (Agile, Scrum, etc)</li> </ul><p>Notice how the higher you go, the less agreement you&rsquo;ll find on what best practices are. Here&rsquo;s another ranking with creative writing:</p><ul> <li>(1) Spelling &amp; grammar</li> <li>(2) Sentence variation and prose</li> <li>(3) Chapter/Scene structure</li> <li>(4) Plot beats</li> <li>(5) Overall story and theme</li> </ul><p>Obviously unlike stories, software is never finished, but you get my point. The higher-level you go, the more complicating factors and cases there are to consider. There&rsquo;s actually a similar kind of debate in the writing world, where you have &ldquo;plotters&rdquo; who plan their story before writing, and &ldquo;pantsers&rdquo; who go by the seat of their pants and explore their plot as they write, but that&rsquo;s a whole other topic.</p><p>Anyway, thanks for reading.</p></div></section>]]></description><pubDate>Sat, 13 Sep 2025 22:01:41 +0530</pubDate></item><item><link>https://open.substack.com/pub/allvpv/p/gits-hidden-simplicity?r=6ehrq6&amp;</link><title>Git’s hidden simplicity: what’s behind every commit (open.substack.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nfzfuo/gits_hidden_simplicity_whats_behind_every_commit/</guid><comments>https://www.reddit.com/r/programming/comments/1nfzfuo/gits_hidden_simplicity_whats_behind_every_commit/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 12 min | <a href='https://www.reddit.com/r/programming/comments/1nfzfuo/gits_hidden_simplicity_whats_behind_every_commit/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>It’s time to learn some Git internals.</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://substackcdn.com/image/fetch/$s_!mPYn!,w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16171831-6ba2-4abc-8d7d-ec29ea5a06d2_2678x1826.png' /></section><section class='parsed-content'><div><article><div><p>Many programmers would admit this: our knowledge of Git tends to be pretty&hellip; superficial. &ldquo;Oops, what happened? Screw that, I&rsquo;ll cherry pick my commits and start again on a fresh branch&rdquo;.</p><p>I&rsquo;ve been there. I knew the basic use cases. I even thought I was pretty experienced after a hundred or so resolved merge conflicts. But the confidence or fluency somehow wasn&rsquo;t coming. It was a hunch: learned scenarios, commands from Stack Overflow or ChatGPT, trivia-like knowledge without a solid base.</p><p><span>In software engineering, you don&rsquo;t need to have </span><em>all the knowledge</em><span>: you just need to</span><em> quickly identify and fetch the missing bits of knowledge</em><span>. My goal is to give you that low-level grounding to sharpen your intuition. Git isn&rsquo;t really complicated in its principles!</span></p><p>Disclaimer: I am not a Git expert either. Let&rsquo;s learn together.</p><p><span>Do you know how </span><em>commit</em><em>hashes</em><span> are generated? I have to admit, I thought for a while that those hashes were somehow randomized. After all, I can run </span><code>git commit --amend</code><span>, change nothing, and still get the same commit, but with a new hash, right? Likewise, </span><code>cherry-pick</code><span>ing the same commit onto another branch gives me yet another hash.</span></p><p>Boy, I couldn&rsquo;t be more wrong. The commit hash is literally just a SHA-1 checksum of the information that constitutes the commit. So two identical commits have identical hashes. Let&rsquo;s look what a commit consists of. Run the following command:</p><pre><code><code>$ git --no-replace-objects cat-file commit HEAD</code></code></pre><p><em>(In case you don&rsquo;t know: HEAD resolves to the commit you currently checked out).</em><span> Let&rsquo;s call the output of this command the </span><code>payload</code><span>. For example, the </span><code>payload</code><span> might be:</span></p><pre><code><code>tree a55ff598781e0c7870fa5c87154a7b731b1c3336 parent c1f4476718c232f4fd8d24cf6249e42995734abc author Przemys&#322;aw Kusiak <mail> 1757612521 +0200 committer Przemys&#322;aw Kusiak <mail> 1757612563 +0200 nushell: short `git status` (`-s`) by default, remove ambiguity, scale factor</mail></mail></code></code></pre><p><span>That&rsquo;s it. That&rsquo;s the full commit. Then prepend the following </span><strong>null-terminated</strong><span> string to the </span><code>payload</code><span>: &ldquo;</span><code>commit 298</code><span>&rdquo;, where </span><code>298</code><span> is the size of the payload in bytes. Compute a SHA-1 over the result and boom: you&rsquo;ve got a Git commit hash! Try it yourself:</span></p><pre><code>$ git --no-replace-objects cat-file commit HEAD &gt; payload $ printf "commit %s\0" $(wc -c &lt; payload) &gt; payload_with_header $ cat payload &gt;&gt; payload_with_header $ sha1sum payload_with_header</code></pre><p>Now compare the output to the actual commit hash:</p><pre><code>$ git rev-parse HEAD</code></pre><p><span>It works. So simple. Now, let&rsquo;s ponder what the </span><code>payload</code><span> contains:</span></p><ol><li><p><code>tree</code><span> &ndash; the hash of a tree object. More on trees later; for now, think of it as a snapshot of all files in the repo.</span></p></li><li><p><code>parent</code><span> &ndash; a hash of parent commit(s).</span></p></li><li><p><code>author</code><span>, </span><code>committer</code><span> &ndash; self-explanatory, but notice that they include date (seconds since the Unix epoch) and time zone; </span><a href="https://stackoverflow.com/a/6755848">in several scenarios it&rsquo;s possible that the author is not the committer</a><span>.</span></p></li><li><p>the commit message.</p></li></ol><p><span>We are not hashing the </span><strong>diff</strong><span> a commit introduces.</span><strong> </strong><span>Rather, the commit </span><strong>header</strong><span>, together with the referenced </span><strong>tree</strong><span> and </span><strong>parent</strong><span>, determines the hash.</span></p><p><span>And now it&rsquo;s easy to see what happens when you run </span><code>git commit --amend</code><span> and change &ldquo;nothing&rdquo;. Something still changes: the date in the </span><code>committer</code><span> field! (Note that </span><code>git show</code><span> doesn&rsquo;t display the </span><code>committer</code><span>; the date you see comes from the </span><code>author</code><span> field). But if you are fast enough to amend within the same second as the original commit, the commit hash remains unchanged!</span></p><p><span>And on a </span><code>cherry-pick</code><span>, the </span><code>parent</code><span> field changes, and usually, though not always, the </span><code>tree</code><span> field as well.</span></p><blockquote><p><span>If you&rsquo;re a careful reader, you might wonder what the </span><code>parent</code><span> field is for the </span><strong>first</strong><span> commit in a repo, and for a </span><strong>merge</strong><span> commit. What do you think? Grab a repo and verify.</span></p></blockquote><p>We saw that a commit references a tree. Let&rsquo;s check what it really is:</p><pre><code>$ tree_hash=a55ff598781e0c7870fa5c87154a7b731b1c3336 $ git cat-file tree $tree_hash</code></pre><p><span>Oops, the </span><code>payload</code><span> isn&rsquo;t human-readable text; it&rsquo;s binary data. But just like with commits, if you prepend &ldquo;</span><code>tree<payloadsize><nul></nul></payloadsize></code><span>&rdquo; to the </span><code>payload</code><span> bytes, you can compute the tree&rsquo;s hash from the result!</span></p><p>Fortunately, Git lets you pretty-print a tree&rsquo;s contents:</p><pre><code>$ git cat-file -p $tree_hash 100644 blob b9768f0236f3d932e680f1edfca69f2d8de776b8 .gitconfig 100644 blob d960f12b4f187ee82d7a1ac545e6452ebb9c2d5b .gitignore 100644 blob 2bb1c65b1090c881adc201d78ea2654d575146ea README.md 100644 blob a26fd7ac25e457c22af2f2436aac581b50b0558a bashrc 040000 tree 4572efa73b2d3d822ef76b6771a2dc4f9a22772a bin 100755 blob b9956764ddc570a78d5daa825c6b0ad4cafbc26e bootstrap.sh 040000 tree ad5b3107519883dad04997e0e1161ddbb392fc63 keyboard 040000 tree eea34f8abc6358d88ca654774dd00d8bca32fa58 lumber 040000 tree cfea0128ab25dfd83ec43f035adfe71ab1e18583 neovide 040000 tree b2504f0e9c082f5a04d07c3eb41116fecb821e7d nushell 040000 tree 6eb525af45a6f346c34a9add71600c6b8a5c9729 nvim 100644 blob 287ee75ab7c9fea8995c9219e8f90b08ba457134 screen.png</code></pre><p><span>A </span><code>tree</code><span> is just like a directory: it references other files (blobs) and directories (trees) nested inside it. It looks a bit like </span><code>ls</code><span> output. The first column records, of course, the Unix file permissions.</span></p><p><span>Nothing more, nothing less than the raw file content &ndash; no metadata. And yes, prepend null-terminated &ldquo;</span><code>blob <file_size></file_size></code><span>&rdquo; to the bytes, run </span><code>sha1sum</code><span>, and you&rsquo;ll get the blob&rsquo;s hash!</span></p><p><span>No extra metadata such as file modification time: that can be inferred from commit history. </span><strong>A simple and immutable structure:</strong><span> you can&rsquo;t change a commit without changing its hash.</span></p><p>And if you think about it, you will notice that it is a&hellip;</p><p><span>There are three types of nodes in this graph: </span><strong>commits</strong><span>, </span><strong>trees</strong><span>, and </span><strong>blobs</strong><span>. And four types of edges:</span></p><ul><li><p><strong>commit &rarr; commit</strong><span> &ndash; parent relationship; a commit has zero or more parents (usually one).</span></p></li><li><p><strong>commit &rarr; tree</strong><span> &ndash; each commit points to exactly one tree (a snapshot of files and folders).</span></p></li><li><p><strong>tree &rarr; tree</strong><span> &ndash; subdirectory relationship.</span></p></li><li><p><strong>tree &rarr; blob</strong><span> &ndash; files contained in a directory.</span></p></li></ul><p><span>Interestingly, the graph fragment reachable from a </span><strong>tree</strong><span> node doesn&rsquo;t have to form a strict tree. For example, a single blob can be referenced by multiple parents.</span></p><p><span>As you probably know, a branch is just a </span><code>ref</code><span> pointing to a commit hash. If you run this in your repo root,</span></p><pre><code>$ ls .git/refs/heads/</code></pre><p><span>you&rsquo;ll see all local branches as file names, each file just a few bytes, with the referenced commit&rsquo;s hash inside. Likewise, </span><code>.git/refs/remotes/origin/</code><span> directory contains pointers to the remote-tracking branches.</span></p><p><span>So you can think of branches as labels for commit histories. If you commit on </span><code>main</code><span>:</span></p><ul><li><p><span>the new commit will have the hash pointed to by </span><code>main</code><span> as its </span><code>parent</code><span> field;</span></p></li><li><p><span>then the </span><code>main</code><span> branch label will be updated to point to the new commit&rsquo;s hash.</span></p></li></ul><p><span>And the </span><code>.git/HEAD</code><span> file contains the name of the current branch &ndash; or commit hash, if you&rsquo;re in a detached state. This special pointer tells Git what is currently checked out.</span></p><p>I hope this clarifies your mental model and clears some of the mystery around Git. The building blocks are simple. Now you shouldn&rsquo;t have a problem answering questions such as:</p><ol><li><p>How are Git commit hashes generated? Why does rebasing produce different commit hashes?</p></li><li><p>Can a remote-tracking branch update without your local branch updating?</p></li><li><p>Which data structure represents the repository? What are the node and edge types in this DAG, and how do they relate?</p></li></ol><p>In the next articles, I plan to cover more advanced concepts, such as Git object storage, garbage collection, and how the default merge strategy works.</p><p>If you have a little more time and want to keep going, I recommend a few resources:</p><ul><li><p><a href="https://git-scm.com/book/en/v2">Pro Git Book</a><span>: very practical, but it doesn&rsquo;t lack depth; look at the </span><strong><a href="https://git-scm.com/book/en/v2/Git-Internals-Plumbing-and-Porcelain">Git Internals</a></strong><span> section</span><strong>.</strong></p></li><li><p><a href="https://eagain.net/articles/git-for-computer-scientists/">Git for Computer Scientists</a><span> by Tommi Virtanen; short and sweet: this is where I got the DAG analogy.</span></p></li></ul></div></article></div><div><h4>Discussion about this post</h4></div><div class="gallery"><p><img src="https://substackcdn.com/image/fetch/$s_!mPYn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16171831-6ba2-4abc-8d7d-ec29ea5a06d2_2678x1826.png"></p></div></section>]]></description><pubDate>Sat, 13 Sep 2025 20:21:31 +0530</pubDate></item><item><link>https://open.substack.com/pub/verbosemode/p/on-staying-sane-as-a-developer?r=31x3tz&amp;&amp;</link><title>On Staying Sane as a Developer (open.substack.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nfy4xg/on_staying_sane_as_a_developer/</guid><comments>https://www.reddit.com/r/programming/comments/1nfy4xg/on_staying_sane_as_a_developer/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 6 min | <a href='https://www.reddit.com/r/programming/comments/1nfy4xg/on_staying_sane_as_a_developer/'>Post permalink</a></p></section><section class='preview-image'><img src='https://images.unsplash.com/photo-1603880920705-3fcc96d6e602?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxOHx8Y2hhb3N8ZW58MHx8fHwxNzU3Njg3NTE0fDA&ixlib=rb-4.1.0&q=80&w=1080' /></section><section class='parsed-content'><div><p><span>Chaos is part of a developer&rsquo;s life. Priorities shift, tasks multiply, and the sense of being &ldquo;done&rdquo; is rare. </span><a href="https://verbosemode.dev/p/im-changing-my-writing-schedule-for">I had planned to publish one post per month in 2025</a><span>, yet here we are&mdash;</span><a href="https://verbosemode.dev/p/why-kotlins-result-type-falls-short">only one post in February so far</a><span>.</span></p><p><span>Not because I ran out of ideas. I wanted to write only </span><em>quality</em><span> posts, things that felt worth sharing, like my earlier one on Kotlin&rsquo;s </span><code>Result</code><span> that came out of a big refactor. Then I started a new job, and life quickly filled with busyness.</span></p><p>That small story is just another example of how unpredictable and messy this work can be. Which is why I want to share the habits that help me keep a bit of sanity in the middle of engineering chaos.</p><p><span>I </span><strong>try</strong><span> to do this every morning: sit down, look at my calendar, and write down the three most important tasks of the day. The goal is to bring my brain into &ldquo;work mode.&rdquo; Combine this with a coffee, and voil&agrave;&mdash;you&rsquo;ve got a simple morning routine.</span></p><p><span>If I recall correctly, I got this idea from the book </span><strong><a href="https://chrisbailey.com/hyperfocus/">Hyperfocus</a></strong><span>.</span></p><p>Morning routines are trendy right now. You&rsquo;ll find thousands of videos of people&mdash;famous or not&mdash;explaining why their morning routine is the best one ever. To be honest, sometimes I wish I had one of those strict one-hour routines with sports, meditation, and everything else. But reality bites: I don&rsquo;t want to wake up before everyone else just to cram all that in, and I simply don&rsquo;t have the time every morning.</p><p>Still, these 10 minutes of focus help me immensely to start the day right.</p><p>If you&rsquo;re familiar with Cal Newport, you might know his concept of a &ldquo;shutdown routine&rdquo;: closing open loops by moving tasks into a trusted system so they don&rsquo;t linger in your mind after work.</p><p>Open loops are everywhere in software engineering. Some tasks&mdash;like reproducing a bug&mdash;can take minutes, hours, or even days, and during that time they stick in your head. I often found myself thinking about these things well into the evening.</p><p><span>My solution is simple: an end-of-day </span><strong>brain dump</strong><span>. I jot down, often unstructured, everything that comes to mind in my notebook:</span></p><ul><li><p>What I accomplished today</p></li><li><p>What went well and what didn&rsquo;t</p></li><li><p>Any disagreements or frustrations</p></li><li><p>Thoughts on tricky tasks or ongoing bugs</p></li></ul><p><span>Some days, it&rsquo;s just a couple of bullet points. Other days, it can fill several pages. The goal is always the same: </span><strong>get everything work-related out of my head.</strong></p><p>If something pops into my mind later, I write it down immediately. This has significantly reduced the sleepless nights spent replaying problem X over and over.</p><p>It also helps me transition from &ldquo;work&rdquo; to &ldquo;private life&rdquo;&mdash;a crucial boundary when working from home.</p><p>We&rsquo;ve all been in retros, trying to recall what we worked on over the past few weeks. Sometimes, we can barely remember what we did the day before.</p><p>To make things easier, I&rsquo;ve adopted a small trick: I don&rsquo;t stop work with everything perfectly neat. Instead, I deliberately leave a test failing or a piece of code unfinished.</p><p>That way, when I sit down the next day, I know exactly where to pick things up. It jump-starts my brain and makes it easier to re-engage with the task.</p><p>You&rsquo;ve probably heard of &ldquo;Definition of Done&rdquo; (DoD)&mdash;a Scrum buzzword meant to ensure a user story is truly complete. In my experience, these team-wide DoDs often fade into obscurity.</p><p><span>But having a </span><strong>personal</strong><span> DoD can be a game changer. Mine applies to pull requests, and before opening one, I mentally check off a short list, for example:</span></p><ul><li><p>Are all tests green?</p></li><li><p>Have I removed debug logs or quick hacks?</p></li><li><p>Is this the simplest yet most effective solution?</p></li><li><p>Did I test this on Dev?</p></li><li><p>(If applicable) Did I check performance for new queries?</p></li></ul><p><span>This feels less like Scrum jargon and more like the </span><a href="https://en.wikipedia.org/wiki/Pointing_and_calling">Pointing and Calling</a><span> method&mdash;simple but effective.</span></p><p>For me, early mornings are the quietest&mdash;and most productive&mdash;parts of the day. That&rsquo;s when I tackle the hardest problems, aka &ldquo;eat the frog.&rdquo;</p><p>To protect this time, I&rsquo;ve blocked two slots in my calendar:</p><ul><li><p><strong>08:30&ndash;10:00</strong></p></li><li><p><strong>10:30&ndash;12:15</strong></p></li></ul><p>These are reserved for deep work. I try to protect them as much as possible, but I&rsquo;m not dogmatic about it&mdash;if a meeting really needs to happen then, I&rsquo;ll allow it.</p><p>The first block begins right after my short morning routine. It&rsquo;s consistently my most productive window of the day.</p><p>One of the most important lessons I&rsquo;ve learned is that there will always be open tasks. You are never truly &ldquo;done.&rdquo; That&rsquo;s why it&rsquo;s essential to have tools and habits that help you stay sane amidst the chaos.</p><p>In this post, I shared some of mine. Hopefully, they inspire you&mdash;or at least give you a starting point for developing your own.</p><p>For further reading:</p><ul><li><p><em><a href="https://calnewport.com/writing/#books">Deep Work</a></em><span> by Cal Newport (Shutdown, Time Blocking)</span></p></li><li><p><em><a href="https://chrisbailey.com/hyperfocus/">Hyperfocus</a></em><span> by Chris Bailey (Three Most Important Things)</span></p></li><li><p>Unfortunately, I can&rsquo;t find the original post where I first read about &ldquo;Leave things broken for tomorrow.&rdquo;</p></li></ul><p>If this post sparked a thought, I&rsquo;d love to hear from you&mdash;drop me a line or leave a comment &#128172;. Always curious about how others tackle the chaos!</p></div></section>]]></description><pubDate>Sat, 13 Sep 2025 19:26:19 +0530</pubDate></item><item><link>https://blog.rust-lang.org/2025/09/12/crates-io-phishing-campaign/</link><title>crates.io phishing campaign | Rust Blog (blog.rust-lang.org)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nfsfml/cratesio_phishing_campaign_rust_blog/</guid><comments>https://www.reddit.com/r/programming/comments/1nfsfml/cratesio_phishing_campaign_rust_blog/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/programming/comments/1nfsfml/cratesio_phishing_campaign_rust_blog/'>Post permalink</a></p></section><section class='preview-image'><img src='https://www.rust-lang.org/static/images/rust-social-wide.jpg' /></section><section class='parsed-content'><p>Sept. 12, 2025 &middot; Rust Security Response WG, crates.io team </p><div><p>We received multiple reports of a phishing campaign targeting crates.io users (from the <code>rustfoundation.dev</code> domain name), mentioning a compromise of our infrastructure and asking users to authenticate to limit damage to their crates.</p><p>These emails are malicious and come from a domain name not controlled by the Rust Foundation (nor the Rust Project), seemingly with the purpose of stealing your GitHub credentials. We have no evidence of a compromise of the crates.io infrastructure.</p><p>We are taking steps to get the domain name taken down and to monitor for suspicious activity on crates.io. Do not follow any links in these emails if you receive them, and mark them as phishing with your email provider.</p><p>If you have any further questions please reach out to <a href="https://blog.rust-lang.orgmailto:security@rust-lang.org">security@rust-lang.org</a> and <a href="https://blog.rust-lang.orgmailto:help@crates.io">help@crates.io</a>.</p></div></section>]]></description><pubDate>Sat, 13 Sep 2025 14:10:48 +0530</pubDate></item><item><link>https://www.cerbos.dev/blog/productivity-paradox-of-ai-coding-assistants</link><title>The productivity paradox of AI coding assistants (cerbos.dev)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nf9buo/the_productivity_paradox_of_ai_coding_assistants/</guid><comments>https://www.reddit.com/r/programming/comments/1nf9buo/the_productivity_paradox_of_ai_coding_assistants/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 13 min | <a href='https://www.reddit.com/r/programming/comments/1nf9buo/the_productivity_paradox_of_ai_coding_assistants/'>Post permalink</a></p></section><section class='preview-image'><img src='https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/The_productivity_paradox_of_AI_coding_assistants_1fb36a40a2.png' /></section><section class='parsed-content'><div><p>Our development team at Cerbos is split into two camps. One side uses AI coding help like Cursor or Claude Code and sees it as the fastest way to ship. The other side has the typical &ldquo;meh&rdquo; reaction you often see on Reddit, arguing that AI assistance is mostly a racket.</p><p>Some of us lean on AI coding to push side projects faster into the delivery pipeline. These are not core product features but experiments and MVP-style initiatives. For bringing that kind of work to its first version, the speed-up is real.</p><p>AI coding assistants promise less boilerplate, fewer doc lookups, and quicker iteration. From my perspective, they deliver on that promise when building MVPs, automations, and hobby projects. Outside of those use cases, the picture changes. You may feel like you are moving quickly, but getting code production ready often takes longer.</p><p>Let&rsquo;s dig into the data.</p><ol> <li><a href="https://www.cerbos.dev#dopamine-vs.-reality">Dopamine vs. reality</a></li> <li><a href="https://www.cerbos.dev#the-quality-problem">The quality problem</a></li> <li><a href="https://www.cerbos.dev#so-where-is-the-magical-10x-productivity-boost?">So where is the magical 10x productivity boost?</a></li> <li><a href="https://www.cerbos.dev#security-is-where-the-gap-shows-most-clearly">Security is where the gap shows most clearly</a></li> <li><a href="https://www.cerbos.dev#how-ai-assistants-create-new-attack-surfaces">How AI assistants create new attack surfaces</a></li> <li><a href="https://www.cerbos.dev#the-70%25-problem">The 70% problem</a></li> <li><a href="https://www.cerbos.dev#business-opinion-vs.-developer-opinion">Business opinion vs. developer opinion</a></li> </ol> <h2><strong>Dopamine vs. reality</strong> <a></a></h2><p>AI coding assistants feel productive because they give instant feedback. You type a prompt and code drops in right away. That loop feels like progress, the same reward you get from closing a ticket or fixing a failing test. The problem is that dopamine rewards activity in the editor, not working code in production.</p><p>The METR randomized <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/">trial</a> in July 2025 with experienced open source developers showed how strong this illusion is. Half the group had AI tools, the other half coded without them. Participants mainly used Cursor Pro with Claude 3.5 and 3.7 Sonnet, which we use internally as well. Developers using AI were on average 19% slower. Yet they were convinced they had been faster.</p><ul> <li>Before starting, they predicted AI would make them 24% faster.</li> <li>After finishing, even with slower results, they still believed AI had sped them up by ~20%<strong>.</strong></li> </ul><p>The chart below from the study makes the point clearly. The green dots show developer expectations and self-reports, while the red dots show actual performance. AI coding &ldquo;felt faster&rdquo;, but in reality, it slowed experienced developers down.</p><p>Marcus Hutchins described this perfectly in his <a href="https://malwaretech.com/2025/08/every-reason-why-i-hate-ai.html">essay</a>:</p><p><em>&ldquo;LLMs inherently hijack the human brain&rsquo;s reward system&hellip; LLMs give the same feeling of achievement one would get from doing the work themselves, but without any of the heavy lifting.&rdquo;</em></p><p>That gap between perception and reality is the productivity placebo. It also shows up in the 2025 Stack Overflow Developer <a href="https://survey.stackoverflow.co/2025/ai#ai-agents">Survey</a>. Only 16.3% of developers said AI made them more productive to a great extent. The largest group, 41.4%, said it had little or no effect. Most developers were in the middle, reporting &ldquo;somewhat&rdquo; better output. That lines up with the METR findings. AI feels faster, but the measurable gains are marginal or even negative.</p><h2><strong>The quality problem</strong> <a></a></h2><p>Speed is one thing, but quality is another. Developers who have used AI assistants in long sessions see the same pattern: output quality gets worse the more context you add. The model starts pulling in irrelevant details from earlier prompts, and accuracy drops. This effect is often called context rot.</p><p>More context is not always better. In theory, a bigger context window should help, but in practice, it often distracts the model. The result is bloated or off-target code that looks right but does not solve the problem you are working on.</p><p>In the Stack Overflow Developer Survey of more than 90,000 developers, 66% said the most common frustration with AI assistants is that the code is &ldquo;almost right, but not quite.&rdquo; Another 45.2% pointed to time spent debugging AI-generated code.</p><p>That lines up with what most of us see in practice. As one of my teammates from the dev team mentioned in our discussion: &ldquo;A lot of the positive sentiments you&rsquo;d read about online are likely because the problems these people are solving are repetitive, boilerplate-y problems solved a gazillion times before. I don&rsquo;t trust the code output from an agent... It&rsquo;s dangerous and (in the rare case that the code is effective) an instant tech-debt factory. That said, I do sometimes find it a useful way of asking questions about a codebase, a powerful grokking tool if you will&rdquo;. I can&rsquo;t agree more.</p><h2><strong>So where is the magical 10x productivity boost?</strong> <a></a></h2><p>The claim that AI makes developers 10x more productive gets repeated pretty often. But the math does not hold up. A 10x boost means what used to take three months now takes a week and a half. Anyone who has actually shipped complex software knows that it is impossible.</p><p>The bottlenecks are not typing speed. They are design reviews, PR queues, test failures, context switching, and waiting on deployments.</p><p>Let&rsquo;s look at some interesting research. In 2023, GitHub and Microsoft ran <a href="https://arxiv.org/abs/2302.06590">a controlled experiment</a> where developers were asked to implement a small HTTP server in JavaScript. Developers using Copilot finished the task <strong>55.8% faster</strong> than the control group. The setup was closer to a benchmark exercise than day-to-day work, and most of the gains came from less experienced devs who leaned on the AI for scaffolding. And, obviously, those were vendor-run experiments.</p><p>METR tested the opposite scenario. Senior engineers worked in large OSS repositories that they already knew well. In that environment, the minutes saved on boilerplate were wiped out by time spent reviewing, fixing, or discarding AI output. As one of my teammates put it, you&rsquo;re not actually saving time with AI coding; you&rsquo;re just trading less typing for more time reading and untangling code.</p><p>Even when AI enables parallelism, one more research shows the cost is more juggling, more reviews, not less time to ship. In July 2025, Faros AI <a href="https://go.faros.ai/ai-engineering">analyzed</a> telemetry from over 10,000 developers across 1,255 teams. They found that teams with high AI adoption interacted with 9% more tasks and 47% more pull requests per day. Developers were juggling more parallel workstreams because AI could scaffold multiple tasks at once.</p><p>Historically, context switching is a negative indicator, correlated with cognitive overload and reduced focus. Faros points out that developers spend more time orchestrating and validating AI contributions across streams. That extra juggling cancels out much of the speed-up you get in typing.</p><p>Not all findings are negative. In 2024, <a href="https://economics.mit.edu/sites/default/files/inline-files/draft_copilot_experiments.pdf">researchers</a> from MIT, Harvard, and Microsoft ran large-scale field experiments across three companies: Microsoft, Accenture, and a Fortune 100 firm. The sample covered 4,867 professional developers working on production code.</p><p>With access to AI coding tools, developers completed 26.08% more tasks on average compared to the control group. Junior and newer hires adopted the tools more readily and showed the largest productivity boost:</p><ul> <li>Senior developers, especially those already familiar with the codebase and stack, saw little or no measurable speed-up.</li> <li>The boost was strongest in situations where devs lacked prior context and used the AI to scaffold, fill in boilerplate, or cut down on docs lookups.</li> </ul><p>These gains &#128070; were good but not near 10x.</p><h2><strong>Security is where the gap shows most clearly</strong> <a></a></h2><p>With the core focus of our company on <a href="https://www.cerbos.dev/product-cerbos-hub">permission management</a> for humans and machines, we naturally look at AI coding assistance through a security lens.</p><p>Older data from 2023 found that developers using assistants shipped more vulnerabilities because they trusted the output too much (<a href="https://arxiv.org/pdf/2211.03622">Stanford research</a>). Obviously, in 2025, fewer developers would trust AI-generated code.</p><p>However, Apiiro&rsquo;s 2024 research is actually very alarming. It showed AI-generated code introduced <strong>322% more privilege escalation paths</strong> and <strong>153% more design flaws</strong> compared to human-written code.</p><p>Apiiro&rsquo;s 2024 research also found:</p><ul> <li><p>AI-assisted commits were merged into production 4x faster than regular commits, which meant insecure code bypassed normal review cycles.</p></li> <li><p>Projects using assistants showed a 40% increase in secrets exposure, mostly hard-coded credentials and API keys generated in scaffolding code. Accidentally pasting API keys, tokens, or configs into an AI assistant is one of the top risks. Even if rotated later, those secrets are now in someone else&rsquo;s logs.</p></li> <li><p>AI-generated changes were linked to a 2.5x higher rate of critical vulnerabilities (CVSS 7.0+) flagged later in scans.</p></li> <li><p>Review complexity went up significantly: PRs with AI code required 60% more reviewer comments on security issues.</p></li> </ul><p>And it is not just security; it is <a href="https://www.cerbos.dev/blog/staying-compliant">compliance</a> too. If code, credentials, or production data leave your environment through an AI assistant, you cannot guarantee deletion or control over where that data ends up. For organizations under SOC2, ISO, GDPR, or HIPAA, that can mean stepping outside policy or outright violations. This is exactly the kind of blind spot CISOs worry about.</p><h2><strong>How AI assistants create new attack surfaces</strong> <a></a></h2><p>AI coding assistants don&rsquo;t just generate code. They also bring new runtimes, plugins, and extensions into the developer workflow. That extra surface means more places where things can go wrong and attackers have already started exploiting them:</p><ul> <li><p>In July 2025, Google&rsquo;s Gemini CLI shipped with a bug that let attackers trigger arbitrary code execution on a dev machine. The tool that was supposed to speed up coding workflows basically turned into a local RCE vector.</p></li> <li><p>A year earlier, the Amazon Q extension in VS Code (August 2024) carried a poisoned update. Hidden prompts in the release told the assistant to delete local files and even shut down AWS EC2 instances. Because the extension shipped with broad local and cloud permissions, the malicious instructions executed without barriers.</p></li> </ul><p>These incidents highlight specific failures, but the bigger issue is structural. Coding assistants expand the software supply chain and increase the number of privileged connections that can be abused.</p><p>The diagram below, from Jim Gumbley and Lilly Ryan&rsquo;s piece on <a href="https://martinfowler.com/articles/exploring-gen-ai/software-supply-chain-attack-surface.html">Martin Fowler</a>, maps this new attack surface. It shows how agents, MCP servers, file systems, CI/CD, and LLM backends are all interconnected, each link a potential entry point for context poisoning or privilege escalation.</p><h2><strong>The 70% problem</strong> <a></a></h2><p>I loved Addy Osmani&rsquo;s piece in <a href="https://newsletter.pragmaticengineer.com/p/how-ai-will-change-software-engineering">Pragmatic Engineer</a> because it nailed what many developers see. AI can get you 70% of the way, but the last 30% is the hard part. The assistant scaffolds a feature, but production readiness means edge cases, architecture fixes, tests, and cleanup:</p><p>For juniors, 70% feel magical. For seniors, the last 30% is often slower than writing it clean from the start. That is why METR&rsquo;s experienced developers were slower with AI; they already knew the solution, and the assistant just added friction.</p><p>This is the difference between a demo and production. AI closes the demo gap quickly, but shipping to production still belongs to humans. A demo only has to run once. Production code has to run a million times without breaking. Humans at least know what they want, even if they misunderstand requirements. An LLM has no intent, which is why the final 30% always falls apart.</p><p>Our team also flagged another issue. Patterns you learn while vibe coding with these tools often break with every model update. There is no stable base to build on. Engineering needs determinism, not shifting patterns that collapse the moment the model retrain.</p><h2><strong>Business opinion vs. developer opinion</strong> <a></a></h2><p>The story of the &ldquo;10x engineer&rdquo; has always been more appealing in boardrooms than in code reviews. Under pressure to do more with less, it is tempting for leadership to see AI as a multiplier that could let one team ship the work of ten. The current AI hype plays straight into that narrative.</p><p>Developers, though, know where the real bottlenecks are. No AI collapses design discussions, sprint planning, meetings, or QA cycles. It does not erase tech debt or magically handle system dependencies. The reality is incremental speed-ups in boilerplate, not 10x multipliers across the delivery pipeline.</p><p>Engineers and EMs both need the same thing &mdash; software that is secure, reliable, and production-ready. AI can play a role in getting there, but only when expectations are grounded in how development actually works.</p><hr><p>Shameless plug: If you are working on IAM and permission management, our product <strong><a href="https://www.cerbos.dev/product-cerbos-hub">Cerbos Hub</a></strong> handles fine-grained authorization for humans and machines. Enforce contextual and continuous access control across apps, APIs, services, workloads, MCP servers, and AI agents &mdash; all from one place.</p></div><div class="gallery"><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_coding_research_1_930ab08028.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_agents_productivity_90254da6a3.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_tools_frustraton_a93d1fb96a.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/Context_switching_with_AI_0b5971d327.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_coding_security_risks_fda1644620.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_tools_supply_chain_risks_95a478012a.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_productivity_issues_d2047e21b0.png"></p></div></section>]]></description><pubDate>Fri, 12 Sep 2025 22:57:32 +0530</pubDate></item><item><link>https://labs.iximiuz.com/tutorials/container-filesystem-from-scratch</link><title>How Containers Work: Building a Docker-like Container From Scratch (labs.iximiuz.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nf96ir/how_containers_work_building_a_dockerlike/</guid><comments>https://www.reddit.com/r/programming/comments/1nf96ir/how_containers_work_building_a_dockerlike/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 67 min | <a href='https://www.reddit.com/r/programming/comments/1nf96ir/how_containers_work_building_a_dockerlike/'>Post permalink</a></p></section><section class='preview-image'><img src='https://labs.iximiuz.com/content/files/tutorials/container-filesystem-from-scratch/__static__/container-rootfs-full-rev2.png' /></section><section class='parsed-content'><div><p>One of the superpowers of containers is their isolated <strong>filesystem view</strong> - from inside a container it can look like a full Linux distro, often different from the host. Run <code>docker run nginx</code>, and Nginx lands in its familiar Debian userspace no matter what Linux flavor your host runs. But how is that illusion built?</p><p>In this post, we'll assemble a tiny but realistic, Docker-like container using only stock Linux tools: <code>unshare</code>, <code>mount</code>, and <code>pivot_root</code>. No runtime magic and (almost) no cut corners. Along the way, you'll see why the <strong>mount namespace</strong> is the bedrock of container isolation, while other namespaces, such as <strong>PID</strong>, <strong>cgroup</strong>, <strong>UTS</strong>, and even <strong>network</strong>, play rather complementary roles.</p><p>By the end - especially if you pair this with the <a href="https://labs.iximiuz.com/tutorials/container-networking-from-scratch">container networking tutorial</a> - you'll be able to spin up fully featured, Docker-style containers using nothing but standard Linux commands. The ultimate goal of every aspiring container guru.</p><h2>Prerequisites</h2><ul><li>Some prior familiarity with Docker (or Podman, or the like) containers</li><li>Basic Linux knowledge (shell scripting, general namespace awareness)</li><li>Filesystem fundamentals (single directory hierarchy, mount table, bind mount, etc.)</li></ul><h2>Visualizing the end result</h2><p>The diagram below shows what filesystem isolation looks like when Docker creates a new container. It's all right if the drawing feels overwhelming. With the help of the hands-on exercises in this tutorial, we'll build a comprehensive mental model of how containers work, so when we revisit the diagram in the closing section, it'll look much more digestible.</p><div><p><i>Click to enlarge</i></p></div><h2>What exactly does Mount Namespace isolate?</h2><p>Let's do a quick experiment. In <span>Terminal 1</span>, start a new shell session in its own mount namespace:</p><div><pre><code><span><span>sudo</span><span> unshare</span><span> --mount</span><span> bash </span></span></code></pre></div><p>Now in <span>Terminal 2</span>, create a file somewhere on the host's filesystem:</p><div><pre><code><span><span>echo</span><span> "Hello from host's mount namespace"</span><span> |</span><span> sudo</span><span> tee</span><span> /opt/marker.txt </span></span></code></pre></div><p>Surprisingly or not, when you try locating this file in the newly created mount namespace using the <span>Terminal 1</span> tab, it'll be there:</p><p>So what exactly did we just isolate with <code>unshare --mount</code>? &#129300;</p><p>The answer is - a <strong>mount table</strong>. Here is how to verify it. From <span>Terminal 1</span>, mount something:</p><div><pre><code><span><span>sudo</span><span> mount</span><span> --bind</span><span> /tmp</span><span> /mnt </span></span></code></pre></div><p>&#128161; The above command uses a <a href="https://labs.iximiuz.com/challenges/storage-bind-mount">bind mount</a> for simplicity, but a <a href="https://labs.iximiuz.com/challenges/storage-simple-mount">regular mount</a> (of a block device) would do, too.</p><p>Now if you list the contents of the <code>/mnt</code> folder in <span>Terminal 1</span>, you should see the files of the <code>/tmp</code> folder:</p><div><pre><code>total 12 drwx------ 3 root root 4096 Sep 11 14:16 file1 drwx------ 3 root root 4096 Sep 11 14:16 file2 ... </code></pre></div><p>But at the same time, the <code>/mnt</code> folder remained empty in the host mount namespace. If you run the same <code>ls</code> command from <span>Terminal 2</span>, you'll see no files:</p><p>Finally, the filesystem "views" started diverging between namespaces. However, we could only achieve it by creating a new mount point.</p><div><p><i>Mount namespaces, visualized</i></p></div><p>From <a href="https://man7.org/linux/man-pages/man7/mount_namespaces.7.html">the mount namespace man page</a>:</p><blockquote><p>Mount namespaces provide isolation of the list of mounts seen by the processes in each namespace instance. Thus, the processes in each of the mount namespace instances will see distinct single directory hierarchies.</p></blockquote><p>Compare the mount tables by running <code>findmnt</code> from <span>Terminal 1</span> and <span>Terminal 2</span>:</p><div><div><p>Host namespace</p><p>New namespace</p></div><div><pre><code><span><span>TARGET SOURCE FSTYPE OPTIONS </span></span><span><span>/ /dev/vda ext4 rw,... </span></span><span><span>&#9500;&#9472;/dev devtmpfs devtmpfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/dev/shm tmpfs tmpfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/dev/pts devpts devpts rw,... </span></span><span><span>&#9474; &#9492;&#9472;/dev/mqueue mqueue mqueue rw,... </span></span><span><span>&#9500;&#9472;/proc proc proc rw,... </span></span><span><span>&#9500;&#9472;/sys sysfs sysfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/sys/kernel/security securityfs securityfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/sys/fs/cgroup cgroup2 cgroup2 rw,... </span></span><span><span>&#9474; ... </span></span><span><span>&#9492;&#9472;/run tmpfs tmpfs rw,... </span></span><span><span> &#9500;&#9472;/run/lock tmpfs tmpfs rw,... </span></span><span><span> &#9492;&#9472;/run/user/1001 tmpfs tmpfs rw,... </span></span></code></pre></div></div><p>In hindsight, it should probably make sense - after all, we are playing with a <em>mount</em> namespace (and there is no such thing as <em>filesystem</em> namespaces, for better or worse).</p><p>&#128161; <strong>Interesting fact:</strong> Mount namespaces were the first namespace type added to Linux, appearing in Linux 2.4, ca. 2002.</p><div><p>&#128161; <strong>Pro Tip:</strong> You can quickly check the current mount namespace of a process using the following command:</p><div><pre><code><span><span>readlink</span><span> /proc/</span><span>$PID</span><span>/ns/mnt </span></span></code></pre></div><p>Different inode numbers in the output will indicate different namespaces. Try running <code>readlink /proc/self/ns/mnt</code> from <span>Terminal 1</span> and <span>Terminal 2</span>.</p></div><h2>What the heck is Mount Propagation?</h2><p>Before we jump to how exactly mount namespaces are applied by <del>Docker</del> an OCI runtime (e.g., <a href="https://labs.iximiuz.com/challenges/start-container-with-runc">runc</a>) to create containers, we need to learn about one more important (and related) concept - <strong>mount propagation</strong>.</p><p>&#9888;&#65039; Make sure to exit the namespaced shell in <span>Terminal 1</span> before proceeding with the commands in this section.</p><p>If you tried to re-do the experiment from the previous section using the <code>unshare()</code> system call instead of the <code>unshare</code> CLI command, the results might look different.</p><div><p><span>unshare_lite.go</span></p><pre><code><span><span>package</span><span> main </span></span><span><span>import</span><span> "os" </span></span><span><span>import</span><span> "os/exec" </span></span><span><span>import</span><span> "syscall" </span></span><span><span>func</span><span> main</span><span>() { </span></span><span><span> if</span><span> err</span><span> :=</span><span> syscall</span><span>.</span><span>Unshare</span><span>(</span><span>syscall</span><span>.</span><span>CLONE_NEWNS</span><span>); </span><span>err</span><span> !=</span><span> nil</span><span> { </span></span><span><span> panic</span><span>(</span><span>err</span><span>) </span></span><span><span> } </span></span><span><span> cmd</span><span> :=</span><span> exec</span><span>.</span><span>Command</span><span>(</span><span>"bash"</span><span>) </span></span><span><span> cmd</span><span>.</span><span>Stdin</span><span> =</span><span> os</span><span>.</span><span>Stdin </span></span><span><span> cmd</span><span>.</span><span>Stdout</span><span> =</span><span> os</span><span>.</span><span>Stdout </span></span><span><span> cmd</span><span>.</span><span>Stderr</span><span> =</span><span> os</span><span>.</span><span>Stderr </span></span><span><span> cmd</span><span>.</span><span>Env</span><span> =</span><span> os</span><span>.</span><span>Environ</span><span>() </span></span><span><span> cmd</span><span>.</span><span>Run</span><span>() </span></span><span><span>} </span></span></code></pre></div><p>Build the above improvised <code>unshare_lite</code> program with:</p><div><pre><code><span><span>go</span><span> build</span><span> -o</span><span> unshare_lite</span><span> unshare_lite.go </span></span></code></pre></div><p>And run it from <span>Terminal 1</span>:</p><p>Then mount something:</p><p>This time, the results of the <code>ls -l /mnt</code> will look identical in <span>Terminal 1</span> and <span>Terminal 2</span>. <strong>Thus, the mount namespace alone may not be enough to provide the mount table isolation.</strong></p><p>If you compare the mount tables by running <code>findmnt</code> from <span>Terminal 1</span> and <span>Terminal 2</span>, they will look the same:</p><div><div><p>Host namespace</p><p>New namespace</p></div><div><pre><code><span><span>TARGET SOURCE FSTYPE OPTIONS </span></span><span><span>/ /dev/vda ext4 rw,... </span></span><span><span>&#9500;&#9472;/dev devtmpfs devtmpfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/dev/shm tmpfs tmpfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/dev/pts devpts devpts rw,... </span></span><span><span>&#9474; &#9492;&#9472;/dev/mqueue mqueue mqueue rw,... </span></span><span><span>&#9500;&#9472;/proc proc proc rw,... </span></span><span><span>&#9500;&#9472;/sys sysfs sysfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/sys/kernel/security securityfs securityfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/sys/fs/cgroup cgroup2 cgroup2 rw,... </span></span><span><span>&#9474; ... </span></span><span><span>&#9500;&#9472;/run tmpfs tmpfs rw,... </span></span><span><span>&#9474; &#9500;&#9472;/run/lock tmpfs tmpfs rw,... </span></span><span><span>&#9474; &#9492;&#9472;/run/user/1001 tmpfs tmpfs rw,... </span></span><span><span>&#9492;&#9472;/mnt /dev/vda[/tmp] ext4 rw,... </span></span></code></pre></div></div><p>When you <em>unshare</em> a new mount namespace, it gets a full copy of the mount table of the caller process. However, changes to the caller's mount table <em>may be propagated</em> to the new mount table and vice versa.</p><p>But why? &#129300;</p><p>Today, containers can easily be the main "consumer" of mount namespaces. However, the applicability of mount namespaces is not limited to containerization use cases. For example, they can be used to provide per-user views of the filesystem.</p><p>The original implementation of mount namespaces came out too strict, and <a href="https://lwn.net/Articles/689856/">it led to tedious repetitive work for system administrators</a>. To alleviate the problem, the kernel was extended with the mechanism of <a href="https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt">shared subtrees</a>, which in particular introduced <strong>mount event propagation</strong> between <em>peer groups</em> (of mount points).</p><div><p><i>Mount event propagation, visualized</i></p></div><p>For instance, if multiple users on the system were using separate mount namespaces to isolate their root filesystems, <strong>without mount event propagation</strong>, mounting a new shared volume would require N <code>mount</code> operations, where N is equal to the number of users. While <strong>with mount event propagation</strong>, system administrators need to mount the volume only once, and the change will be replicated in all <em>peer groups</em>, even across different mount namespaces.</p><p>&#129299; Neither kernel documentation nor the mount namespace man page use the term <strong>mount propagation</strong> - instead, they refer to it as <strong>propagation type</strong> (of a mount point). However, the term <strong>mount propagation</strong> seems to be <a href="https://lwn.net/Articles/690679/">commonly used in the industry</a>, including in the Docker (<a href="https://docs.docker.com/engine/storage/bind-mounts/#configure-bind-propagation">example</a>) and Kubernetes (<a href="https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation">example</a>) documentation.</p><p>Mount event propagation is exactly what we've just observed when we tried using the <code>unshare</code> system call directly from a Go program: when the <code>/tmp</code> folder was bind-mounted to the <code>/mnt</code> folder in the new mount namespace, the original namespace received a <em>mount event</em> and replicated the change creating a similar <code>/tmp:/mnt</code> mount.</p><p>Hmm... Why didn't it happen when we used the standard <code>unshare</code> command-line tool? &#129300;</p><p>The <code>unshare</code> CLI tool does slightly more than just the <code>unshare()</code> system call. You can sneak a peek under the hood of the <code>unshare</code> CLI with the following <code>strace</code> trick (from a <span>fresh terminal</span>):</p><div><pre><code><span><span>sudo</span><span> strace</span><span> unshare</span><span> --mount</span><span> bash </span></span></code></pre></div><p>When you cut through the noise of the trace, you'll spot these three important system calls done in a sequence:</p><div><pre><code><span><span>... </span></span><span><span>unshare(CLONE_NEWNS) = 0 </span></span><span><span>mount("none", "/", NULL, MS_REC|MS_PRIVATE, NULL) = 0 </span></span><span><span>execve("/usr/bin/bash", ["bash"], 0x7fff03d0e038 /* 19 vars */) = 0 </span></span><span><span>... </span></span></code></pre></div><p>Right <em>after</em> creating a new mount namespace and <em>before</em> executing the <code>bash</code> binary, the <code>unshare</code> command also changed the mount propagation type of the root mount point. The above <code>mount()</code> call is equivalent to the following <code>mount</code> command:</p><p>...which means that in the new mount namespace, the root mount and <em>all its sub-mounts</em> (<code>MS_REC</code> and <code>r</code> in <code>rprivate</code> stand for <em>recursive</em>) become completely isolated from the outside world - mounting new filesystems inside the mount namespace won't be noticeable in the caller's (i.e., the host's, in our case) mount namespace and vice versa.</p><p>&#128161; Mount propagation type is a property of a mount point. Since each mount point belongs to the corresponding mount namespace, the mount propagation type is also a namespace-specific property. For instance, the root mount <code>/</code> can have a <code>shared</code> mount propagation type in one namespace and <code>private</code> in another.</p><div><p>No mount event propagation between namespaces:</p><div><pre><code><span><span>sudo</span><span> unshare</span><span> --mount</span><span> --propagation</span><span> private </span></span><span><span>findmnt</span><span> -o</span><span> TARGET,SOURCE,FSTYPE,PROPAGATION </span></span></code></pre></div><div><pre><code>TARGET SOURCE FSTYPE PROPAGATION / /dev/vda ext4 private &#9500;&#9472;/dev devtmpfs devtmpfs private &#9474; &#9500;&#9472;/dev/shm tmpfs tmpfs private &#9474; &#9500;&#9472;/dev/pts devpts devpts private &#9474; &#9492;&#9472;/dev/mqueue mqueue mqueue private &#9500;&#9472;/proc proc proc private &#9474; &#9492;&#9472;/proc/sys/fs/binfmt_misc systemd-1 autofs private &#9474; &#9492;&#9472;/proc/sys/fs/binfmt_misc binfmt_misc binfmt_misc private &#9500;&#9472;/sys sysfs sysfs private &#9474; &#9500;&#9472;/sys/kernel/security securityfs securityfs private &#9474; &#9500;&#9472;/sys/fs/selinux selinuxfs selinuxfs private &#9474; ... &#9492;&#9472;/run tmpfs tmpfs private &#9500;&#9472;/run/lock tmpfs tmpfs private &#9492;&#9472;/run/user/1001 tmpfs tmpfs private </code></pre></div></div><p>Why does mount propagation matter for us? Two reasons:</p><ul><li><code>pivot_root</code>, the modern <code>chroot</code> alternative most container runtimes rely on, comes with its own requirements for the mount propagation type of the involved mount points (we'll see it in the next section).</li><li>Some applications may want to mount filesystems on the host while running in a container and some others may need to spot the host (or peer containers) mounting filesystems in runtime (e.g., <a href="https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation"><code>HostToContainer</code> and <code>Bidirectional</code> mount propagations in Kubernetes</a>). More on it later.</li></ul><h2>A naive attempt to isolate container filesystem</h2><p>Mount namespaces and propagation are great, but how is all this stuff used in containers? Let's try creating a simple container to see this machinery in action.</p><p>&#9888;&#65039; Make sure to exit the namespaced shell in <span>Terminal 1</span> before proceeding with the commands in this section.</p><h3>Preparing container rootfs</h3><p>First off, we'll need to prepare the future root filesystem. From the host's standpoint, each container's rootfs is just a regular folder with some files inside:</p><div><pre><code><span><span>sudo</span><span> mkdir</span><span> -p</span><span> /opt/container-1/rootfs </span></span></code></pre></div><p>For this experiment, we can "borrow" the Alpine filesystem <a href="https://labs.iximiuz.com/tutorials/extracting-container-image-filesystem">by extracting the <code>alpine:3</code> image</a> into the directory we just created:</p><div><pre><code><span><span>crane</span><span> export</span><span> alpine:3</span><span> |</span><span> sudo</span><span> tar</span><span> -xvC</span><span> /opt/container-1/rootfs </span></span></code></pre></div><p>If you compare the contents of the <code>/opt/container-1/rootfs</code> and the host's <code>/</code> folders, they will look surprisingly similar:</p><div><div><pre><code><span><span>tree</span><span> -L</span><span> 1</span><span> /opt/container-1/rootfs </span></span></code></pre></div><div><pre><code>/opt/container-1/rootfs/ &#9500;&#9472;&#9472; bin &#9500;&#9472;&#9472; dev &#9500;&#9472;&#9472; etc &#9500;&#9472;&#9472; home &#9500;&#9472;&#9472; lib ... &#9500;&#9472;&#9472; tmp &#9500;&#9472;&#9472; usr &#9492;&#9472;&#9472; var 18 directories, 0 files </code></pre></div></div><p>However, upon closer inspection, you'll see that it's two different Linux distributions:</p><div><div><pre><code><span><span>cat</span><span> /opt/container-1/rootfs/etc/os-release </span></span></code></pre></div><div><pre><code>NAME="Alpine Linux" ID=alpine VERSION_ID=3.22.1 PRETTY_NAME="Alpine Linux v3.22" HOME_URL="https://alpinelinux.org/" BUG_REPORT_URL="https://gitlab.alpinelinux.org/alpine/aports/-/issues" </code></pre></div></div><h3>Switching to new rootfs (pivot_root)</h3><p>The <a href="https://man7.org/linux/man-pages/man2/pivot_root.2.html"><code>pivot_root(new_root, put_old)</code> syscall</a> changes the root mount <strong>in the mount namespace of the calling process</strong>. More precisely, it moves the current root mount of the caller to the directory <code>put_old</code> and makes <code>new_root</code> the new root mount.</p><p>What it practically means is that by calling <code>pivot_root("/opt/container-1/rootfs")</code> in a new mount namespace, we'll switch to the new root filesystem.</p><p>&#128161; From a layman's standpoint, <code>pivot_root</code> is a safer version of <code>chroot</code> - similar effect but no risk of breakouts via forgotten symlinks to the old root filesystem or the double-chroot trick.</p><p>The <code>pivot_root()</code> call comes with a number of restrictions, in particular:</p><ul><li>The <code>new_root</code> path must be a mount point, but can't be <code>/</code> (well, attempting <code>pivot_root("/")</code> wouldn't make much sense anyway).</li><li>The <strong>propagation type</strong> of the parent mount of <code>new_root</code> and the parent mount of the current root directory must not be <code>shared</code>.</li><li>If <code>put_old</code> is an existing mount point, its <strong>propagation type</strong> must not be <code>shared</code>.</li></ul><p>Expectedly, we only want to perform such a disruptive operation from a separate mount namespace (otherwise, we'd damage the host), and the last two restrictions ensure that <code>pivot_root</code> never propagates any mount table changes to another mount namespace:</p><div><pre><code><span><span>sudo</span><span> unshare</span><span> --mount</span><span> bash </span></span></code></pre></div><p>Now let's try satisfying the <code>pivot_root</code>'s requirements. The propagation type of the <code>/</code> mount (the parent mount of the current root directory) should not be <code>shared</code>. The above <code>unshare</code> command has likely already set it to <code>private</code> but being explicit won't hurt:</p><p>In our case, the <code>/opt/container-1/rootfs</code> folder is not a mount point (it's a regular folder somewhere in the host's filesystem), but we can easily make it a mount point by bind mounting the path onto itself (using a recursive bind mount because hypothetically the container rootfs folder itself can contain sub-mounts):</p><div><pre><code><span><span>mount</span><span> --rbind</span><span> /opt/container-1/rootfs</span><span> /opt/container-1/rootfs </span></span></code></pre></div><p>Lastly, ensuring that the propagation type of the <code>new_root</code> itself isn't <code>shared</code>:</p><div><pre><code><span><span>mount</span><span> --make-rprivate</span><span> /opt/container-1/rootfs </span></span></code></pre></div><p>Now we're ready to <del>choort</del> pivot the root filesystem:</p><div><pre><code><span><span>cd</span><span> /opt/container-1/rootfs </span></span><span><span>mkdir</span><span> .oldroot </span></span></code></pre></div><p>...and immediately after that, switch to a shell from the new rootfs because the current <code>bash</code> process may get broken in subtle ways after a <code>pivot_root</code> into a completely different Linux distro (this part is only needed for our demo example - real-world container runtimes usually don't have this issue because they communicate with the kernel directly, using syscalls instead of shell commands):</p><p>Interestingly, after the <code>pivot_root</code> operation, <a href="https://github.com/opencontainers/runtime-spec/blob/main/config-linux.md#rootfs-mount-propagation">container runtimes are free to set the propagation type of the new root filesystem to pretty much any value</a> (<code>shared</code>, <code>slave</code>, <code>private</code>, and even <code>unbindable</code>):</p><p>&#128161; Propagation type of the container root filesystem should not be confused with the propagation type of bind mounts and volumes in Docker and Kubernetes respectively (see below). This is an advanced setting that is often not even exposed through the user-facing APIs of the higher-level container runtimes, and the most typical use case for it is nested containers (e.g., <a href="https://hub.docker.com/_/docker">DinD</a>).</p><p>Finally, since you probably don't want the original root filesystem to be accessible in the container, the <code>.oldroot</code> can (and should) be removed right after the <code>pivot_root</code> call:</p><div><pre><code><span><span>umount</span><span> -l</span><span> .oldroot</span><span> # -l stands for "lazy" because the fs can be busy </span></span><span><span>rm</span><span> -rf</span><span> .oldroot </span></span></code></pre></div><p>Yay! We've just pivoted into a new container. Let's look around:</p><div><pre><code>ls -l / total 68 drwxr-xr-x 2 root root 4096 Jul 15 10:42 bin drwxr-xr-x 2 root root 4096 Sep 7 12:40 dev drwxr-xr-x 17 root root 4096 Jul 15 10:42 etc drwxr-xr-x 2 root root 4096 Jul 15 10:42 home ... drwxr-xr-x 11 root root 4096 Jul 15 10:42 var </code></pre></div><div><pre><code>NAME="Alpine Linux" ID=alpine VERSION_ID=3.22.1 PRETTY_NAME="Alpine Linux v3.22" HOME_URL="https://alpinelinux.org/" BUG_REPORT_URL="https://gitlab.alpinelinux.org/alpine/aports/-/issues" </code></pre></div><p>So far so good! But if you try listing processes, the output will be empty (which of course can't be true):</p><p>And the <code>df</code> command also seems broken:</p><div><pre><code>Filesystem Size Used Available Use% Mounted on df: /proc/mounts: No such file or directory </code></pre></div><h2>Preparing a complete container filesystem</h2><p>The <code>df</code>'s error message contained a hint - the <code>/proc</code> folder is empty in the new mount namespace:</p><p>Hmm... How come?</p><p>Well, apparently, not every part of the container root filesystem comes from its image!</p><p>Similarly to the host, where <a href="https://docs.kernel.org/filesystems/proc.html"><code>/proc</code> is populated by the corresponding kernel pseudo filesystem</a>, container's <code>/proc</code> needs to be set up separately. And the same goes for <code>/dev</code> and <code>/sys</code> virtual filesystems.</p><p>On top of that, some special files like <code>/etc/hosts</code>, <code>/etc/hostname</code>, or <code>/etc/resolv.conf</code> should be crafted for each container individually because the corresponding files in the image (if present) can only contain generic values (e.g., <code>localhost</code>) while Docker typically sets the hostname of a container to a prefix of its random ID and derives the <code>resolv.conf</code> from the eponymous file on the host.</p><h3>Populating /proc pseudo filesystem</h3><p>Populating the <code>/proc</code> pseudo filesystem is as simple as:</p><p>&#128161; In reality, container runtimes usually populate the <code>/proc</code> filesystem <em>before</em> the <code>pivot_root</code> call, so the command would look like <code>mount -t proc proc $ROOTFS/proc</code>.</p><p>However, if you run the above command right away, the <code>/proc</code> filesystem in the container will look exactly the same as the one on the host. In particular, it means that the <code>ps</code> command will start showing the full list of processes on the server, which is usually undesirable in a container.</p><p>This is where the <strong>PID namespace</strong> comes into play. We need to go a few steps back and adjust the <code>unshare</code> command to create not just the mount but also a new PID namespace, so that the container's topmost process would become PID 1 and the process hierarchy in the container would start from it:</p><div><pre><code><span><span># DO NOT RUN ME </span></span><span><span>sudo</span><span> unshare</span><span> --mount</span><span> --pid</span><span> --fork</span><span> bash </span></span></code></pre></div><p>But let's not do it just yet...</p><p>&#128161; The extra <code>--fork</code> flag above doesn't create any new namespaces, but rather makes <code>unshare</code> create a new process instead of exec'ing the <code>bash</code> command directly. This is a requirement to make the <code>--pid</code> flag actually have the effect on the unshared command because it's the first child that gets placed into the new PID namespace, not the process that called <code>unshare(CLONE_NEWPID)</code> itself.</p><h3>Populating /dev pseudo filesystem</h3><p>Another special folder is <code>/dev</code>. On the host, it's typically provided by the <code>devtmpfs</code> and a number of subordinate virtual filesystems (from a <span>fresh terminal</span>):</p><div><pre><code>TARGET SOURCE FSTYPE OPTIONS / /dev/vda ext4 rw,relatime,stripe=4 &#9500;&#9472;/dev devtmpfs devtmpfs rw,relatime,size=4068368k,nr_inodes=1017092,mode=755 &#9474; &#9500;&#9472;/dev/shm tmpfs tmpfs rw,nosuid,nodev &#9474; &#9500;&#9472;/dev/pts devpts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 &#9474; &#9500;&#9472;/dev/mqueue mqueue mqueue rw,nosuid,nodev,noexec,relatime ... </code></pre></div><p>However, containers usually get a more limited version of the <code>/dev</code> folder, backed by a regular <code>tmpfs</code>. Here is how it can be populated from inside the new mount namespace (back from <span>Terminal 1</span>):</p><div><pre><code><span><span>mkdir</span><span> -p</span><span> /dev </span></span><span><span>mount</span><span> -t</span><span> tmpfs</span><span> -o</span><span> nosuid,strictatime,mode=0755,size=65536k</span><span> tmpfs</span><span> /dev </span></span></code></pre></div><p>&#128161; In reality, container runtimes usually populate the <code>/dev</code> filesystem <em>before</em> the <code>pivot_root</code> call, so the command would look like <code>mount -t tmpfs ... $ROOTFS/dev</code>.</p><p>After mounting the <code>/dev</code> tmpfs, you'd need to create special character devices such as <code>/dev/null</code>, <code>/dev/zero</code>, <code>/dev/random</code>, etc. Here is how you can do it using the <code>mknod</code> command:</p><div><pre><code><span><span>mknod</span><span> -m</span><span> 666</span><span> /dev/null</span><span> c</span><span> 1</span><span> 3 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> /dev/zero</span><span> c</span><span> 1</span><span> 5 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> /dev/full</span><span> c</span><span> 1</span><span> 7 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> /dev/random</span><span> c</span><span> 1</span><span> 8 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> /dev/urandom</span><span> c</span><span> 1</span><span> 9 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> /dev/tty</span><span> c</span><span> 5</span></span><span></span><span><span>chown</span><span> root:root</span><span> "/dev/{null,zero,full,random,urandom,tty} </span></span></code></pre></div><p>Then, mount the subordinate filesystems (<code>/dev/shm</code>, <code>/dev/pts</code>, and <code>/dev/mqueue</code>):</p><div><pre><code><span><span>mkdir</span><span> -p</span><span> /dev/{shm,pts,mqueue} </span></span><span><span>mount</span><span> -t</span><span> tmpfs</span><span> -o</span><span> nosuid,nodev,noexec,mode=1777,size=</span><span>67108864</span><span> tmpfs</span><span> /dev/shm </span></span><span><span>mount</span><span> -t</span><span> devpts</span><span> -o</span><span> newinstance,ptmxmode=0666,mode=</span><span>0620</span><span> devpts</span><span> /dev/pts </span></span><span><span>mount</span><span> -t</span><span> mqueue</span><span> -o</span><span> nosuid,nodev,noexec</span><span> mqueue</span><span> /dev/mqueue </span></span></code></pre></div><p>And lastly, set up some well-known symlinks:</p><div><pre><code><span><span>ln</span><span> -sf</span><span> /proc/self/fd</span><span> /dev/fd </span></span><span><span>ln</span><span> -sf</span><span> /proc/self/fd/0</span><span> /dev/stdin </span></span><span><span>ln</span><span> -sf</span><span> /proc/self/fd/1</span><span> /dev/stdout </span></span><span><span>ln</span><span> -sf</span><span> /proc/self/fd/2</span><span> /dev/stderr </span></span><span><span>ln</span><span> -sf</span><span> /proc/kcore</span><span> /dev/core </span></span></code></pre></div><h3>Populating /sys pseudo filesystem</h3><p>The most limited of the containers' pseudo filesystems is probably <a href="https://www.kernel.org/doc/Documentation/filesystems/sysfs.txt"><code>/sys</code></a>. It's usually mounted read-only and contains only a few nodes:</p><div><pre><code><span><span>mount</span><span> -t</span><span> sysfs</span><span> -o</span><span> ro,nosuid,nodev,noexec</span><span> sysfs</span><span> /sys </span></span></code></pre></div><p>&#128161; In reality, container runtimes usually populate the <code>/sys</code> filesystem <em>before</em> the <code>pivot_root</code> call, so the command would look like <code>mount -t sysfs ... $ROOTFS/sys</code>.</p><p>A prominent part of the <code>/sys</code> filesystem is the <a href="https://labs.iximiuz.com/tutorials/controlling-process-resources-with-cgroups/">virtual cgroup filesystem</a>. Since a few years ago, Docker and other popular container runtimes started fully isolating the container's cgroup hierarchy by default. Similarly to the <code>/proc</code> filesystem that works best in combination with a new PID namespace, a new <strong>cgroup namespace</strong> can be used to make the <code>cgroup2</code> mount rooted at the host's cgroupfs node that corresponds to the container's topmost process. Thus, the <code>unshare</code> command <strong>would need</strong> one more flag, <code>--cgroup</code>:</p><div><pre><code><span><span># DO NOT RUN ME </span></span><span><span>sudo</span><span> unshare</span><span> --mount</span><span> --pid</span><span> --fork</span><span> --cgroup</span><span> bash </span></span></code></pre></div><p>To mount the <code>cgroup2</code> filesystem, you can use the following command:</p><div><pre><code><span><span>mkdir</span><span> -p</span><span> /sys/fs/cgroup </span></span><span><span>mount</span><span> -t</span><span> cgroup2</span><span> -o</span><span> ro,nosuid,nodev,noexec</span><span> cgroup2</span><span> /sys/fs/cgroup </span></span></code></pre></div><h3>Hardening pseudo filesystems</h3><p>While it is not strictly necessary for a demo, real-world container root filesystems usually go through an extra round of hardening. For instance, Docker typically marks a few parts of the <code>/proc</code> filesystem as <em>read-only</em> and <em>masks</em> others, making them completely inaccessible to the containerized app.</p><p>Here is how you can get a list of sensitive locations that are made read-only by Docker (from a <span>fresh terminal</span>):</p><div><pre><code><span><span>docker</span><span> container</span><span> inspect</span><span> \ </span></span><span><span> $(</span><span>docker</span><span> run</span><span> --rm</span><span> -d</span><span> alpine:3</span><span> sleep</span><span> 5</span><span>) </span><span>\ </span></span><span><span> --format</span><span> '{{join .HostConfig.ReadonlyPaths "\n"}}' </span></span></code></pre></div><div><pre><code>/proc/bus /proc/fs /proc/irq /proc/sys /proc/sysrq-trigger </code></pre></div><p>You can make any file or folder read-only by binding it to itself and remounting it using the <code>ro</code> option:</p><div><pre><code><span><span>RO_PATH</span><span>=</span><span>/proc/bus</span><span> # or /proc/fs, /proc/irq, etc. </span></span><span><span>if</span><span> [[ </span><span>-e</span><span> "</span><span>$RO_PATH</span><span>"</span><span> ]]; </span><span>then </span></span><span><span> mount</span><span> --bind</span><span> "</span><span>$RO_PATH</span><span>"</span><span> "</span><span>$RO_PATH</span><span>" </span></span><span><span> mount</span><span> -o</span><span> remount,bind,ro</span><span> "</span><span>$RO_PATH</span><span>" </span></span><span><span>fi </span></span></code></pre></div><p>Similarly, here is how you can get a list of locations that are typically made completely inaccessible (through masking) to the containerized app:</p><div><pre><code><span><span>docker</span><span> container</span><span> inspect</span><span> \ </span></span><span><span> $(</span><span>docker</span><span> run</span><span> --rm</span><span> -d</span><span> alpine:3</span><span> sleep</span><span> 5</span><span>) </span><span>\ </span></span><span><span> --format</span><span> '{{join .HostConfig.MaskedPaths "\n"}}' </span></span></code></pre></div><div><pre><code>/proc/asound /proc/acpi /proc/interrupts /proc/kcore /proc/keys /proc/latency_stats /proc/timer_list /proc/timer_stats /proc/sched_debug /proc/scsi /sys/firmware /sys/devices/virtual/powercap </code></pre></div><p>Masking of folders and regular files differs. To mask a folder, a read-only <code>tmpfs</code> filesystem can be mounted over it, and to mask a regular file, the <code>/dev/null</code> device can be bound to its path.</p><div><pre><code><span><span>MASKED_FILE</span><span>=</span><span>/proc/asound</span><span> # or /proc/interrupts, /proc/kcore, etc. </span></span><span><span>mount</span><span> --bind</span><span> /dev/null</span><span> $MASKED_FILE </span></span><span><span>MASKED_DIR</span><span>=</span><span>/proc/acpi</span><span> # or /proc/scsi, etc. </span></span><span><span>mount</span><span> -t</span><span> tmpfs</span><span> -o</span><span> ro</span><span> tmpfs</span><span> $MASKED_DIR </span></span></code></pre></div><p>&#128161; The above read-only and masked paths are Docker's <strong>defaults for non-privileged containers</strong>, while the OCI Runtime Spec defines only the hardening mechanism and not the exact locations (see <a href="https://github.com/opencontainers/runtime-spec/blob/e3c8d12d94cdd269a145a263ace7457f56c74eff/config-linux.md#masked-paths">Masked Paths</a> and <a href="https://github.com/opencontainers/runtime-spec/blob/e3c8d12d94cdd269a145a263ace7457f56c74eff/config-linux.md#readonly-paths">Readonly Paths</a>).</p><h3>Preparing special /etc files</h3><p>Some of the regular files in the container rootfs also require special treatment:</p><ul><li><code>/etc/hosts</code></li><li><code>/etc/hostname</code></li><li><code>/etc/resolv.conf</code></li></ul><p>Inspecting these files in the <code>/opt/container-1/rootfs</code> folder right after extracting the Alpine rootfs into it would reveal why:</p><div><pre><code><span><span>cat</span><span> /opt/container-1/rootfs/etc/{hosts,hostname,resolv.conf} </span></span></code></pre></div><div><pre><code># -- /opt/container-1/rootfs/etc/hosts 127.0.0.1 localhost localhost.localdomain ::1 localhost localhost.localdomain # -- /opt/container-1/rootfs/etc/hostname localhost # -- /opt/container-1/rootfs/etc/resolv.conf cat: /opt/container-1/rootfs/resolv.conf: No such file or directory </code></pre></div><p>The above are some generic values that come directly from the <code>alpine:3</code> image, which wouldn't make much sense in any particular container. At the same time, these files would look very different when inspected from a running <code>alpine:3</code> container:</p><div><pre><code><span><span>docker</span><span> run</span><span> --rm</span><span> alpine:3</span><span> cat</span><span> /etc/{hosts,hostname,resolv.conf} </span></span></code></pre></div><div><pre><code># -- /etc/hosts 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback 172.17.0.2 2f26e97ae70c # -- /etc/hostname 2f26e97ae70c # -- /etc/resolv.conf # Generated by Docker Engine. # This file can be edited; Docker Engine will not make further changes once it # has been modified. nameserver 168.119.149.157 nameserver 8.8.8.8 nameserver 1.1.1.1 # Based on host file: '/etc/resolv.conf' (legacy) # Overrides: [] </code></pre></div><p>Thus, Docker (or one of its underlying runtimes) replaces the generic <code>/etc/hosts</code>, <code>/etc/hostname</code>, and <code>/etc/resolv.conf</code> files from the image with container-specific variants.</p><p>We can do it, too! Our container has no network interfaces (modulo <code>loopback</code>), but it can still have a proper hostname set (from the host's <span>terminal</span>):</p><div><pre><code><span><span>cat</span><span> &lt;&lt;</span><span>EOF</span><span> |</span><span> sudo</span><span> tee</span><span> /opt/container-1/hosts </span></span><span><span>127.0.0.1 localhost container-1 </span></span><span><span>::1 localhost ip6-localhost ip6-loopback </span></span><span><span>EOF </span></span></code></pre></div><div><pre><code><span><span>cat</span><span> |</span><span> sudo</span><span> tee</span><span> /opt/container-1/hostname</span><span> &lt;&lt;</span><span>EOF </span></span><span><span>container-1 </span></span><span><span>EOF </span></span></code></pre></div><div><pre><code><span><span>sudo</span><span> cp</span><span> /etc/resolv.conf</span><span> /opt/container-1/resolv.conf </span></span></code></pre></div><p>&#128161; The <code>/etc/resolv.conf</code> file is usually based on the host's <code>/etc/resolv.conf</code> file, and then potentially adjusted to the container's needs.</p><p>The most interesting part is how these files are placed into the container's rootfs. Instead of just overwriting the files from the image, container runtimes usually mount the container-specific variants of these files on top of the original ones, effectively masking them:</p><div><pre><code><span><span>sudo</span><span> mount</span><span> --bind</span><span> /opt/container-1/hosts</span><span> /opt/container-1/rootfs/etc/hosts </span></span><span><span>sudo</span><span> mount</span><span> --bind</span><span> /opt/container-1/hostname</span><span> /opt/container-1/rootfs/etc/hostname </span></span><span><span>sudo</span><span> mount</span><span> --bind</span><span> /opt/container-1/resolv.conf</span><span> /opt/container-1/rootfs/etc/resolv.conf </span></span></code></pre></div><p>Last but not least, for the container to have its own hostname, the container needs to use a new <strong>network</strong> and <strong>UTS namespaces</strong>, so the <code>unshare</code> command would need to have two more flags (<code>--uts</code> and <code>--net</code>):</p><div><pre><code><span><span># DO NOT RUN ME </span></span><span><span>sudo</span><span> unshare</span><span> --mount</span><span> --pid</span><span> --fork</span><span> --cgroup</span><span> --uts</span><span> --net</span><span> bash </span></span></code></pre></div><p>&#128161; If we forget to use a new <strong>UTS namespace</strong>, setting the hostname in the new container will overwrite the host's hostname, which is something we definitely don't want. And without a new <strong>network namespace</strong>, the container simply cannot have its own hostname, because then it technically <a href="https://labs.iximiuz.com/tutorials/container-networking-from-scratch">has the same network stack as the host (which in particular includes the hostname)</a>.</p><p>Finally, we're ready to prepare a fully isolated container filesystem!</p><h2>Creating a container from scratch (end-to-end example)</h2><p>With all the above lessons learned, let's try creating our second container, this time applying all the necessary namespaces and rootfs adjustments.</p><div><p>&#128161; The below commands are based on the <a href="https://github.com/opencontainers/runc/blob/b27d6f3f1af9a56f2770c8ec6e1a1ff986ca9c09/libcontainer/rootfs_linux.go">real container preparation steps taken by the runc runtime</a> obtained with the following <code>strace</code> trick:</p><div><pre><code><span><span># Terminal 1 </span></span><span><span>sudo</span><span> strace</span><span> -f</span><span> -qqq</span><span> -e</span><span> \ </span></span><span><span> trace=/clone,/exec,/unshare,/mount,/mknod,/mkdir,/link,/chdir,/root</span><span> \ </span></span><span><span> -p</span><span> $(</span><span>pgrep</span><span> containerd</span><span>) </span></span></code></pre></div><div><pre><code><span><span># Terminal 2 </span></span><span><span>docker</span><span> run</span><span> alpine:3</span><span> sleep</span><span> 9999 </span></span></code></pre></div></div><h3>Step 1: Prepare rootfs files</h3><p>&#9888;&#65039; Make sure to exit the namespaced shell in <span>Terminal 1</span> before proceeding with the commands in this section.</p><p>The second container will be stored in the <code>/opt/container-2</code> directory:</p><div><pre><code><span><span>CONTAINER_DIR</span><span>=</span><span>/opt/container-2 </span></span><span><span>ROOTFS_DIR</span><span>=</span><span>${</span><span>CONTAINER_DIR</span><span>}</span><span>/rootfs </span></span></code></pre></div><p>Similar to the first container, we'll use the <code>alpine:3</code> image to "borrow" the rootfs files:</p><div><pre><code><span><span>sudo</span><span> mkdir</span><span> -p</span><span> $ROOTFS_DIR </span></span><span><span>crane</span><span> export</span><span> alpine:3</span><span> |</span><span> sudo</span><span> tar</span><span> -xvC</span><span> $ROOTFS_DIR </span></span></code></pre></div><p>This time, we'll create the <code>/etc/hosts</code>, <code>/etc/hostname</code>, and <code>/etc/resolv.conf</code> files beforehand (but store them outside of the rootfs dir for now):</p><div><pre><code><span><span>cat</span><span> &lt;&lt;</span><span>EOF</span><span> |</span><span> sudo</span><span> tee</span><span> $CONTAINER_DIR</span><span>/hosts </span></span><span><span>127.0.0.1 localhost container-2 </span></span><span><span>::1 localhost ip6-localhost ip6-loopback </span></span><span><span>EOF </span></span></code></pre></div><div><pre><code><span><span>cat</span><span> &lt;&lt;</span><span>EOF</span><span> |</span><span> sudo</span><span> tee</span><span> $CONTAINER_DIR</span><span>/hostname </span></span><span><span>container-2 </span></span><span><span>EOF </span></span></code></pre></div><div><pre><code><span><span>sudo</span><span> cp</span><span> /etc/resolv.conf</span><span> $CONTAINER_DIR</span><span>/resolv.conf </span></span></code></pre></div><h3>Step 2: Create namespaces</h3><p>Create all the required namespaces with the <code>unshare</code> command (<strong>mount</strong>, <strong>PID</strong>, <strong>cgroup</strong>, <strong>UTS</strong>, and <strong>network</strong>):</p><div><pre><code><span><span>sudo</span><span> unshare</span><span> --mount</span><span> --pid</span><span> --fork</span><span> --cgroup</span><span> --uts</span><span> --net</span><span> bash </span></span></code></pre></div><div><p>&#128161; <a href="https://man7.org/linux/man-pages/man7/namespaces.7.html">Other possible namespaces are</a>:</p><ul><li><code>ipc</code> - this namespace has no impact on the rootfs creation, so we're skipping it for brevity</li><li><code>time</code> - (optional) not used by Docker or other mainstream container runtimes yet</li><li><code>user</code> - (optional) rootless containers is an advanced topic that deserves its own tutorial</li></ul></div><h3>Step 3: Isolate new mount namespace</h3><div><p>From now on, all commands are executed as <code>root</code> and in the new namespaces, so we're skipping the <code>sudo</code> prefix, and the <code>CONTAINER_DIR</code> and <code>ROOTFS_DIR</code> variables may need to be re-set:</p><div><pre><code><span><span>CONTAINER_DIR</span><span>=</span><span>/opt/container-2 </span></span><span><span>ROOTFS_DIR</span><span>=</span><span>${</span><span>CONTAINER_DIR</span><span>}</span><span>/rootfs </span></span></code></pre></div></div><p>First, we need to make sure that no mount events are propagated back to the host's mount namespace:</p><p>Then, we need to make sure that the root filesystem itself is a mount point:</p><div><pre><code><span><span>mount</span><span> --rbind</span><span> $ROOTFS_DIR</span><span> $ROOTFS_DIR </span></span></code></pre></div><p>...and that the propagation type of the root filesystem isn't <code>shared</code>:</p><div><pre><code><span><span>mount</span><span> --make-private</span><span> $ROOTFS_DIR </span></span></code></pre></div><h3>Step 4: Prepare /proc pseudo filesystem</h3><p>Mount <code>/proc</code> pseudo filesystem:</p><div><pre><code><span><span>mkdir</span><span> -p</span><span> $ROOTFS_DIR</span><span>/proc </span></span><span><span>mount</span><span> -t</span><span> proc</span><span> proc</span><span> $ROOTFS_DIR</span><span>/proc </span></span></code></pre></div><div><p>&#9888;&#65039; <strong>Security Caveat:</strong> In untrusted rootfs, <code>$ROOTFS_DIR/<path></path></code> can be a symlink pointing outside of <code>$ROOTFS_DIR</code>. This can make the above and many of the below operations corrupt the host system.</p><p>Real-world container runtimes typically use the <a href="https://man7.org/linux/man-pages/man2/openat2.2.html"><code>openat2()</code> syscall with the <code>RESOLVE_NO_SYMLINKS</code> flag</a> to first open the target file or directory ensuring it's not a symlink, and then use <code>mount</code> (or other filesystem operations) on an open file descriptor instead of a textual filename. The latter helps to avoid <a href="https://en.wikipedia.org/wiki/Time-of-check_to_time-of-use">TOCTTOU vulnerabilities</a> when the <code>$ROOTFS_DIR</code> contents are changed <em>while</em> the container is being created.</p><p>However, in a demo context it should be relatively safe to operate with regular filenames. So, we'll do it the simpler way for brevity.</p></div><h3>Step 5: Prepare /dev pseudo filesystem</h3><p>Mount <code>/dev</code> pseudo filesystem as a regular <code>tmpfs</code>:</p><div><pre><code><span><span>mount</span><span> -t</span><span> tmpfs</span><span> \ </span></span><span><span> -o</span><span> nosuid,strictatime,mode=0755,size=65536k</span><span> tmpfs</span><span> \ </span></span><span><span> $ROOTFS_DIR</span><span>/dev </span></span></code></pre></div><p>Create the standard character devices (<code>/dev/null</code>, <code>/dev/zero</code>, <code>/dev/random</code>, etc.):</p><div><pre><code><span><span>mknod</span><span> -m</span><span> 666</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/null"</span><span> c</span><span> 1</span><span> 3 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/zero"</span><span> c</span><span> 1</span><span> 5 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/full"</span><span> c</span><span> 1</span><span> 7 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/random"</span><span> c</span><span> 1</span><span> 8 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/urandom"</span><span> c</span><span> 1</span><span> 9 </span></span><span><span>mknod</span><span> -m</span><span> 666</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/tty"</span><span> c</span><span> 5</span></span><span></span><span><span>chown</span><span> root:root</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/"{null,zero,full,random,urandom,tty} </span></span></code></pre></div><p>Create typical symlinks:</p><div><pre><code><span><span>ln</span><span> -sf</span><span> /proc/self/fd</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/fd" </span></span><span><span>ln</span><span> -sf</span><span> /proc/self/fd/0</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/stdin" </span></span><span><span>ln</span><span> -sf</span><span> /proc/self/fd/1</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/stdout" </span></span><span><span>ln</span><span> -sf</span><span> /proc/self/fd/2</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/stderr" </span></span><span><span>ln</span><span> -sf</span><span> /proc/kcore</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/core" </span></span></code></pre></div><p>Create subordinate filesystems (<code>/dev/pts</code>, <code>/dev/shm</code>, <code>/dev/mqueue</code>):</p><div><pre><code><span><span>mkdir</span><span> -p</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/pts" </span></span><span><span>mount</span><span> -t</span><span> devpts</span><span> \ </span></span><span><span> -o</span><span> newinstance,ptmxmode=0666,mode=</span><span>0620</span><span> devpts</span><span> \ </span></span><span><span> $ROOTFS_DIR</span><span>/dev/pts </span></span><span><span>ln</span><span> -sf</span><span> /dev/pts/ptmx</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/ptmx" </span></span></code></pre></div><div><pre><code><span><span>mkdir</span><span> -p</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/mqueue" </span></span><span><span>mount</span><span> -t</span><span> mqueue</span><span> \ </span></span><span><span> -o</span><span> nosuid,nodev,noexec</span><span> mqueue</span><span> \ </span></span><span><span> $ROOTFS_DIR</span><span>/dev/mqueue </span></span></code></pre></div><div><pre><code><span><span>mkdir</span><span> -p</span><span> "</span><span>$ROOTFS_DIR</span><span>/dev/shm" </span></span><span><span>mount</span><span> -t</span><span> tmpfs</span><span> \ </span></span><span><span> -o</span><span> nosuid,nodev,noexec,mode=1777,size=</span><span>67108864</span><span> tmpfs</span><span> \ </span></span><span><span> $ROOTFS_DIR</span><span>/dev/shm </span></span></code></pre></div><h3>Step 6: Prepare /sys pseudo filesystem</h3><p>Mount a read-only <code>/sys</code> pseudo filesystem:</p><div><pre><code><span><span>mkdir</span><span> -p</span><span> "</span><span>$ROOTFS_DIR</span><span>/sys" </span></span><span><span>mount</span><span> -t</span><span> sysfs</span><span> \ </span></span><span><span> -o</span><span> ro,nosuid,nodev,noexec</span><span> sysfs</span><span> \ </span></span><span><span> $ROOTFS_DIR</span><span>/sys </span></span></code></pre></div><p>Mount the subordinate <code>cgroup2</code> filesystem as <code>/sys/fs/cgroup</code>:</p><div><pre><code><span><span>mkdir</span><span> -p</span><span> "</span><span>$ROOTFS_DIR</span><span>/sys/fs/cgroup" </span></span><span><span>mount</span><span> -t</span><span> cgroup2</span><span> \ </span></span><span><span> -o</span><span> ro,nosuid,nodev,noexec</span><span> cgroup2</span><span> \ </span></span><span><span> $ROOTFS_DIR</span><span>/sys/fs/cgroup </span></span></code></pre></div><h3>Step 7: Bind hostname, hosts, and resolv.conf files</h3><p>Bind the container-specific <code>hostname</code>, <code>hosts</code>, and <code>resolv.conf</code> files from <code>/opt/container-2</code>, masking the original files in the rootfs' <code>/etc</code> directory:</p><div><pre><code><span><span>for</span><span> p</span><span> in</span><span> hostname</span><span> hosts</span><span> resolv.conf </span></span><span><span>do </span></span><span><span> touch</span><span> $ROOTFS_DIR</span><span>/etc/</span><span>$p </span></span><span><span> mount</span><span> --bind</span><span> "</span><span>$CONTAINER_DIR</span><span>/</span><span>$p</span><span>"</span><span> $ROOTFS_DIR</span><span>/etc/</span><span>$p </span></span><span><span>done </span></span></code></pre></div><h3>Step 8: Pivot into the new rootfs</h3><p>Finally, pivot into the fully prepared root filesystem:</p><div><pre><code><span><span>cd</span><span> $ROOTFS_DIR </span></span><span><span>mkdir</span><span> -p</span><span> .oldroot </span></span><span><span>pivot_root</span><span> .</span><span> .oldroot </span></span></code></pre></div><div><p>This is not something a real runtime would do, but since we use a shell, it's better to exec into the target container's shell as soon as possible after the <code>pivot_root</code> call:</p></div><p>Configure the propagation type of the container's root filesystem (setting it arbitrarily to <code>slave</code>, <a href="https://github.com/opencontainers/runtime-spec/blob/383cadbf08c0be925b62a4532e99a538249797e6/config-linux.md#rootfs-mount-propagation">but the OCI Runtime Specification supports <code>private</code> and even <code>shared</code></a>):</p><p>And lastly, getting rid of the link to the old root filesystem:</p><div><pre><code><span><span>umount</span><span> -l</span><span> .oldroot </span></span><span><span>rmdir</span><span> .oldroot </span></span></code></pre></div><p>Set the hostname of the container using the value from the container's <code>/etc/hostname</code> file:</p><div><pre><code><span><span>hostname</span><span> $(</span><span>cat</span><span> /etc/hostname</span><span>) </span></span></code></pre></div><h3>Step 9: Harden container filesystem</h3><p>Making a good part of the <code>/proc</code> filesystem read-only:</p><div><pre><code><span><span>for</span><span> d</span><span> in</span><span> bus</span><span> fs</span><span> irq</span><span> sys</span><span> sysrq-trigger </span></span><span><span>do </span></span><span><span> if</span><span> [ </span><span>-e</span><span> "/proc/</span><span>$d</span><span>"</span><span> ]; </span><span>then </span></span><span><span> mount</span><span> --bind</span><span> "/proc/</span><span>$d</span><span>"</span><span> "/proc/</span><span>$d</span><span>" </span></span><span><span> mount</span><span> -o</span><span> remount,bind,ro</span><span> "/proc/</span><span>$d</span><span>" </span></span><span><span> fi </span></span><span><span>done </span></span></code></pre></div><p>Masking sensitive paths in the <code>/proc</code> and <code>/sys</code> filesystems:</p><div><pre><code><span><span>for</span><span> p</span><span> in</span><span> \ </span></span><span><span> /proc/asound</span><span> \ </span></span><span><span> /proc/interrupts</span><span> \ </span></span><span><span> /proc/kcore</span><span> \ </span></span><span><span> /proc/keys</span><span> \ </span></span><span><span> /proc/latency_stats</span><span> \ </span></span><span><span> /proc/timer_list</span><span> \ </span></span><span><span> /proc/timer_stats</span><span> \ </span></span><span><span> /proc/sched_debug</span><span> \ </span></span><span><span> /proc/acpi</span><span> \ </span></span><span><span> /proc/scsi</span><span> \ </span></span><span><span> /sys/firmware </span></span><span><span>do </span></span><span><span> if</span><span> [ </span><span>-d</span><span> "</span><span>$p</span><span>"</span><span> ]; </span><span>then </span></span><span><span> # Masking a folder </span></span><span><span> mount</span><span> -t</span><span> tmpfs</span><span> -o</span><span> ro</span><span> tmpfs</span><span> $p </span></span><span><span> elif</span><span> [ </span><span>-f</span><span> "</span><span>$p</span><span>"</span><span> ]; </span><span>then </span></span><span><span> # Masking a regular file </span></span><span><span> mount</span><span> --bind</span><span> /dev/null</span><span> $p </span></span><span><span> fi </span></span><span><span>done </span></span></code></pre></div><h3>Step 10: Execute target application</h3><p>At this point, the containerized environment is ready to be used. Feel free to look around using the <code>ps</code>, <code>ls</code>, <code>mount</code>, <code>df</code>, <code>hostname</code>, and any other commands you can think of, and then <code>exec</code> the containerized application:</p><div><pre><code><span><span>APP</span><span>=</span><span>${</span><span>APP</span><span>:-/</span><span>bin</span><span>/</span><span>sh</span><span>} </span></span><span><span>exec</span><span> $APP </span></span></code></pre></div><h2>Bonus: Sharing host files and folders with containers</h2><p>One of the very common Docker use cases, especially during local development, is sharing files and folders from the host into the container via <a href="https://docs.docker.com/engine/storage/bind-mounts/">bind mounts</a> like this:</p><div><pre><code><span><span># Traditional -v|--volume flag </span></span><span><span>docker</span><span> run</span><span> -v</span><span> ./data:/data</span><span> redis </span></span><span><span># More modern but equivalent --mount form </span></span><span><span>docker</span><span> run</span><span> --mount</span><span> type='bind,src=./data,dst=/data'</span><span> redis </span></span></code></pre></div><p>In the previous section(s), we saw that regular files located on the host can be bind mounted into the future container's root filesystem. This is exactly how <del>Docker</del> runc and similar container runtimes inject the customized <code>/etc/hosts</code>, <code>/etc/hostname</code>, and <code>/etc/resolv.conf</code> files.</p><p>But the exact same technique can be used to inject any other files or folders from the host into the container.</p><p>The <code>strace -p $(pgrep containerd)</code> command that we used to reverse engineer the rootfs preparation steps will reveal that the bind mounts of the <code>-v|--volume</code> flag happen right after the pseudo filesystems preparation and just before the mounts of the <code>/etc/hosts</code>, <code>/etc/hostname</code>, and <code>/etc/resolv.conf</code> files.</p><p>And it's a good thing we invested some time in learning about the <em>mount event propagation</em> mechanism - Docker allows <a href="https://docs.docker.com/engine/storage/bind-mounts/#configure-bind-propagation">configuring the propagation type for bind mounts</a>, so the following command should not look like a magic spell anymore:</p><div><pre><code><span><span>docker</span><span> run</span><span> -v</span><span> .:/project:ro,rshared</span><span> ... </span></span></code></pre></div><p>In the above example, if the containerized application would mount a sub-folder under <code>/project</code>, it would be visible on the host as well (and vice versa). However, the default propagation type of a Docker bind mount is <code>rprivate</code>, so don't be surprised if you don't see sub-mounts showing up.</p><h2>Bonus: Adding support for data volumes</h2><p>While Docker docs position <a href="https://docs.docker.com/engine/storage/volumes/">volumes</a> as a distinct concept, under the hood, they are just bind mounts, but with a few extra features like naming, lifecycle management, and various data source drivers support:</p><div><pre><code><span><span># Traditional -v|--volume flag </span></span><span><span>docker</span><span> run</span><span> --volume</span><span> redis-data:/data</span><span> redis </span></span><span><span># More modern but equivalent --mount form </span></span><span><span>docker</span><span> run</span><span> --mount</span><span> type='volume,src=redis-data,dst=/data'</span><span> redis </span></span></code></pre></div><p>Instead of arbitrary folders on the host, volume data is always stored in <code>/var/lib/docker/volumes/CONTAINER_ID/_data</code>, and you can list all existing volumes with the <code>docker volume ls</code> command, or create new ones with <code>docker volume create</code>, or even purge them with <code>docker volume rm</code>. But at the end of the day, you're just listing, creating, or removing <code>_data</code> folders in the <code>/var/lib/docker/volumes</code> directory.</p><p>Interesting that Docker always sets mount propagation for volumes to <code>rprivate</code> (for bind mounts you could tweak it), while Kubernetes, despite relying on the exact same runc (or the like) runtime under the hood, allows more flexible mount propagation configuration (<a href="https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation"><code>HostToContainer</code>, <code>Bidirectional</code>, etc.</a>).</p><p>So, in Docker, <strong>bind mounts vs. volumes</strong> is more of a semantic difference (and induced artificial constraints on the data location and propagation type) than an actual technical difference.</p><h2>Where do union filesystems come into play?</h2><p>One of the things we didn't talk about in this article is <em>union filesystems</em> like <code>overlayfs</code> - simply because despite popular belief, <strong>they're not mandatory for containers.</strong></p><p>As we just proved with the above demo, it's possible to create a fully-fledged container without relying on a union filesystem at all. Docker uses <a href="https://docs.docker.com/engine/storage/drivers/overlayfs-driver/"><code>overlay2</code></a> (or <a href="https://docs.docker.com/engine/storage/drivers/">an alternative</a>) storage driver to unpack layered container images into "flat" local folders. However, this is only an optimization, mainly focused on the disk space efficiency - as we just saw, it's possible to extract a container image filesystem into a regular folder with <code>crane export</code> (or a similar command), and the container runtime (e.g., runc) will happily use it as a root filesystem.</p><h2>Summarizing</h2><p>At the heart of containers lies the <strong>mount namespace</strong>. That's not an accident - Linux has long treated the filesystem as the central interface for managing processes, devices, and resources. Once you start assembling a root filesystem for a container, it quickly becomes clear that other namespaces - <strong>PID</strong>, <strong>cgroup</strong>, <strong>UTS</strong>, and <strong>network</strong> - are interconnected and much needed to complete the task.</p><p>This is why walking through the rootfs exercise isn't just an impressive low-level demo you could give at a conference. It's a way to build a comprehensive mental model of how containers work. And with that model in place, higher-level topics like bind mounts, volumes, mount propagation, and persistence in Docker or Kubernetes stop feeling like special cases - they become natural extensions of the same foundation.</p><p>Ah, and if you made it this far, take another look at the diagram from the opening part - it should make much more sense now!</p><div><p><i>Click to enlarge</i></p></div><h2>Resources</h2><ul><li><a href="https://man7.org/linux/man-pages/man7/namespaces.7.html">namespaces(7) &mdash; Linux manual page</a></li><li><a href="https://man7.org/linux/man-pages/man7/mount_namespaces.7.html">mount_namespaces(7) &mdash; Linux manual page</a></li><li><a href="https://man7.org/linux/man-pages/man2/mount.2.html">mount(2) &mdash; Linux manual page</a></li><li><a href="https://man7.org/linux/man-pages/man2/pivot_root.2.html">pivot_root(2) &mdash; Linux manual page</a></li><li><a href="https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt">Shared Subtrees - Linux kernel documentation</a></li><li><a href="https://lwn.net/Articles/689856/">Mount namespaces and shared subtrees - LWN.net</a></li><li><a href="https://lwn.net/Articles/690679/">Mount namespaces, mount propagation, and unbindable mounts - LWN.net</a></li><li><a href="https://github.com/opencontainers/runtime-spec/blob/383cadbf08c0be925b62a4532e99a538249797e6/config.md#mounts">Mounts - OCI Runtime Specification</a></li><li><a href="https://github.com/opencontainers/runc/blob/b27d6f3f1af9a56f2770c8ec6e1a1ff986ca9c09/libcontainer/rootfs_linux.go">rootfs_linux.go - runc source code</a></li><li><a href="https://docs.docker.com/engine/storage/bind-mounts/">Bind mounts - Docker documentation</a></li><li><a href="https://docs.docker.com/engine/storage/volumes/">Volumes - Docker documentation</a></li><li><a href="https://docs.docker.com/engine/storage/drivers/overlayfs-driver/">OverlayFS storage driver - Docker documentation</a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation">Mount propagation - Kubernetes documentation</a></li><li><a href="https://brauner.io/2023/02/28/mounting-into-mount-namespaces.html">Mounting into mount namespaces - Christian Brauner's blog</a></li><li><a href="https://jpetazzo.github.io/2015/01/13/docker-mount-dynamic-volumes/">Attach a volume to a container while it is running - J&eacute;r&ocirc;me Petazzoni's blog</a></li><li><a href="https://sid-agrawal.ca/linux,/docker,/mount,/namespaces,/mount_namespaces/2024/11/26/docker-mounts.html">Understanding the various mounts setup by a Docker container - Sid Agrawal's blog</a></li></ul><h2>Practice</h2></div></section>]]></description><pubDate>Fri, 12 Sep 2025 22:51:50 +0530</pubDate></item><item><link>https://www.crunchydata.com/blog/get-excited-about-postgres-18</link><title>Get Excited About Postgres 18 (crunchydata.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nf90qr/get_excited_about_postgres_18/</guid><comments>https://www.reddit.com/r/programming/comments/1nf90qr/get_excited_about_postgres_18/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 9 min | <a href='https://www.reddit.com/r/programming/comments/1nf90qr/get_excited_about_postgres_18/'>Post permalink</a></p></section><section class='preview-image'><img src='https://imagedelivery.net/lPM0ntuwQfh8VQgJRu0mFg/24623c31-81a1-4004-3cbf-cb8511db4500/public' /></section><section class='parsed-content'><div><p>Postgres 18 will be released in just a couple weeks! Here&rsquo;s some details on the most important and exciting features.</p><h2><a href="https://www.crunchydata.com#asynchronous-io">Asynchronous i/o</a></h2><p>Postgres 18 is adding asynchronous i/o. This means faster reads for many use cases. This is also part of a bigger series of performance improvements planned for future Postgres, part of which may be multi-threading. Expect to see more on this in coming versions.</p><p><strong>What is async I/O?</strong></p><p>When <a href="https://www.crunchydata.com/blog/postgres-data-flow">data</a> isn&rsquo;t in the shared memory buffers already, Postgres reads from disk, and <a href="https://www.crunchydata.com/blog/understanding-postgres-iops">I/O is needed to retrieve data</a>. Synchronous I/O means that each individual request to the disk is waited on for completion before moving on to something else. For busy databases with a lot of activity, this can be a bottleneck.</p><p>Postgres 18 will introduce asynchronous I/O, allowing workers to optimize idle time and improve system throughput by batching reads. Currently, Postgres relies on the operating system for intelligent I/O handling, expecting OS or storage read-ahead for sequential scans and using features like Linux's posix_fadvise for other read types like Bitmap Index Scans. Moving this work into the database with asynchronous I/O will provide a more predictable and better-performing method for batching operations at the database level. Additionally, a new system view, pg_aios, will be available to provide data about the asynchronous I/O system.</p><p>Postgres writes will continue to be synchronous - since this is needed for ACID compliance.</p><p>If async i/o seems confusing, think of it like ordering food at a restaurant. In a synchronous model, you would place your order and stand at the counter, waiting, until your food is ready before you can do anything else. In an asynchronous model, you place your order, receive a buzzer, and are free to go back to your table and chat with friends until the buzzer goes off, signaling that your food is ready to be picked up.</p><p>Async I/O will affect:</p><ul><li>sequential scans</li><li>bitmap heap scans (following the bitmap index scan)</li><li>some maintenance operations like VACUUM.</li></ul><p>By default Postgres will turn on <strong>io_method = worker</strong>. By default there are 3 workers and this can be adjusted up for systems with larger CPU workers. I haven&rsquo;t seen any reliable recommendations on this, so stay tuned for more on that from our team soon.</p><p>For Postgres running on Linux 5.1+ you can utilize the io_uring system calls and have the invocations made via the actual backends rather than having separate processes with the optional <strong>io_method = io_uring</strong>.</p><h2><a href="https://www.crunchydata.com#uuid-v7">UUID v7</a></h2><p>UUIDs are getting a bit of an overhaul in this version by moving to v7.</p><p>UUIDs are randomly generated strings which are globally unique and often used for primary keys. UUIDs are popular in modern applications for a couple reasons:</p><ul><li>They&rsquo;re unique: You can use keys generated from more than one place.</li><li>Decoupled:Your application can generate a primary key <em>before</em> sending the data to the database.</li><li>URL obscurity: If your URLs use primary keys (e.g., .../users/5), other URLs are easy to guess (.../users/6, .../users/7). With a UUID (.../users/f47ac10b-58cc-4372-a567-0e02b2c3d479), it's impossible to guess other IDs.</li></ul><p>A new standard for UUID v7 came out in mid-2024 via a series of standards updates. UUIDv4 was the prior version of uuid with native Postgres support. But sorting and indexing in large tables had performance issues due to the relative randomness, leading to fragmented indexes and bad locality.&nbsp; UUIDv7 helps with the sort and indexing issues. It is still random but that first 48 bits (12 characters) are a timestamp, and the remaining bits are random; this gives better locality for data inserted around the same time and thus better indexability.</p><p>The timestamp part is a hexadecimal value (i.e. compressed decimal). So for example a uuid that begins with <code>01896d6e4a5d6</code> (hex) would represent the <code>2707238289622</code> (decimal) and that is the number of milliseconds since 1970.</p><p>This is how the DDL will look for uuid v7:</p><pre><code>CREATE TABLE user_actions ( action_id UUID PRIMARY KEY DEFAULT uuidv7(), user_id BIGINT NOT NULL, action_description TEXT, action_time TIMESTAMPTZ NOT NULL DEFAULT NOW() ); CREATE INDEX idx_action_id ON user_actions (action_id); </code></pre><h2><a href="https://www.crunchydata.com#b-tree-skip-scans">B-tree skip scans</a></h2><p>There&rsquo;s a nice performance bump coming in Postgres 18 for some multi-column B-tree indexes.</p><p>In Postgres, if you have an index on columns (<code>status</code>, <code>date</code>) in a table, this index can be used to match queries which query both <code>status</code> and <code>date</code> fields, or just <code>status</code>.</p><p>In Postgres 17 and below, this same index cannot be used to answer queries against just the <code>date</code> field; you would have to have that column indexed separately or the database would resort to a sequence scan + filter approach if there were no appropriate indexes for that table.</p><p>In Postgres 18, in many cases it can automatically use this multi-column index for queries touching only the <code>date</code> field.&nbsp; Known as a skip scan, this lets the system "skip" over portions of the index.</p><p>This works when queries don&rsquo;t use the leading columns in the conditions and the omitted column has a low cardinality, like a small number of distinct values. The optimization works by:</p><ol><li>Identifying all the distinct values in the omitted leading column(s).</li><li>Effectively transform the query to add the conditions to match the leading values.</li><li>The resulting query is able to use existing infrastructure to optimize lookups across multiple leading columns, effectively skipping any pages in the index scan which do not match both conditions.</li></ol><p>For example, if we had a sales table with columns <code>status</code> and <code>date</code>, we might have a multi-column index:</p><pre><code>CREATE INDEX idx_status_date ON sales (status, date); </code></pre><p>An example query could have a where clause that doesn&rsquo;t include status.</p><pre><code>SELECT * FROM sales WHERE date = '2025-01-01'; </code></pre><p>Nothing in the query plan tells you this is a skip scan, so you&rsquo;ll end up with a normal Index scan like this, showing you the index conditions.</p><pre><code> QUERY PLAN ------------------------------------------------------------- Index Only Scan using idx_status_date on sales (cost=0.29..21.54 rows=4 width=8) Index Cond: (date = '2025-01-01'::date) (2 rows) </code></pre><p>Before 18, a full table scan would be done, since the leading column of the index is not included, but with skip scan Postgres can use the same index for this index scan.</p><p>In Postgres 18, because status has a low cardinality and just a few values, a compound index scan can be done. Note that this optimization only works for queries which use the <code>=</code> operator, so it will not work with inequalities or ranges.</p><p>This all happens behind-the-scenes in the Postgres planner so you don&rsquo;t need to turn it on. The idea is that it will benefit analytics use cases where filters and conditions often change and aren&rsquo;t necessarily related to existing indexes.</p><p>The query planner will decide if using a skip scan is worthwhile, based on the table's statistics and the number of distinct values in the columns being skipped.</p><h2><a href="https://www.crunchydata.com#generated-columns-on-the-fly">Generated columns on-the-fly</a></h2><p>PostgreSQL 18 introduces virtual generated columns. Previously, generated columns were always stored on disk. This meant for generated columns, values were computed at the time of an insert or update and adding a bit of write overhead.</p><p>In PostgreSQL 18, virtual generated columns are now the default type for generated columns. if you define a generated column without explicitly specifying STORED, it will be created as a virtual generated column.</p><pre><code>CREATE TABLE user_profiles ( user_id SERIAL PRIMARY KEY, settings JSONB, username VARCHAR(100) GENERATED ALWAYS AS (settings -&gt;&gt; 'username') VIRTUAL ); </code></pre><p>This is a great update for folks using JSON data, queries can be simplified and data changes or normalization can be done on the fly as needed.</p><p>Note that virtual generated columns are not indexable - since they&rsquo;re not stored on disk. For <a href="https://www.crunchydata.com/blog/indexing-jsonb-in-postgres">indexing of JSONB</a>, use the stored version or expression index.</p><h2><a href="https://www.crunchydata.com#oauth-20">OAUTH 2.0</a></h2><p>Good news for folks that use Okta, Keycloak, and other managed authentication services, Postgres is now compatible with OAUTH 2.0. This is specified in the main host based authentication configuration (pg_hba.conf) file.</p><p>The Oauth system uses bearer tokens where the client application presents a token instead of a password to prove identity. The token is an opaque string and its format is determined by the authorization server. This feature removes the need to store passwords in the database. It also allows for more robust security measures like multi-factor authentication (MFA) and single sign-on (SSO) to be managed by external identity providers.</p><h2><a href="https://www.crunchydata.com#postgres-versions-are-packed-with-other-improvements">Postgres versions are packed with other improvements</a></h2><p>Postgres 18 comes with a staggering 3,000 commits from more than 200 authors. While many of these are features, there are numerous additions and optimizations under the hood to the Postgres query planner and other parts of the system that are behind the scenes. Even if you don&rsquo;t utilize optional features, there&rsquo;s still performance benefits (uh ... asyc i/o is a biggie), bug fixes, and security patches that make upgrading on a regular cadence a good idea.</p></div><div class="gallery"><p><img src="https://imagedelivery.net/lPM0ntuwQfh8VQgJRu0mFg/2bf43dd0-9a3a-4535-55c0-5f18a9a9a200/public"></p><p><img src="https://imagedelivery.net/lPM0ntuwQfh8VQgJRu0mFg/6d5ed16d-2a24-4ff4-4a6c-fd42773e4b00/public"></p></div></section>]]></description><pubDate>Fri, 12 Sep 2025 22:45:30 +0530</pubDate></item><item><link>https://buttondown.com/hillelwayne/archive/many-hard-leetcode-problems-are-easy-constraint/</link><title>Many Hard Leetcode Problems are Easy Constraint Problems (buttondown.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nf8hyo/many_hard_leetcode_problems_are_easy_constraint/</guid><comments>https://www.reddit.com/r/programming/comments/1nf8hyo/many_hard_leetcode_problems_are_easy_constraint/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 7 min | <a href='https://www.reddit.com/r/programming/comments/1nf8hyo/many_hard_leetcode_problems_are_easy_constraint/'>Post permalink</a></p></section><section class='preview-image'><img src='https://assets.buttondown.email/images/63337f78-7138-4b21-87a0-917c0c5b1706.jpg?w=960&fit=max' /></section><section class='parsed-content'><div><date> September 10, 2025 </date> <h2> Use the right tool for the job. </h2><p>In my first interview out of college I was asked the change counter problem:</p><blockquote><p>Given a set of coin denominations, find the minimum number of coins required to make change for a given number. IE for USA coinage and 37 cents, the minimum number is four (quarter, dime, 2 pennies).</p></blockquote><p>I implemented the simple greedy algorithm and immediately fell into the trap of the question: the greedy algorithm only works for "well-behaved" denominations. If the coin values were <code>[10, 9, 1]</code>, then making 37 cents would take 10 coins in the greedy algorithm but only 4 coins optimally (<code>10+9+9+9</code>). The "smart" answer is to use a dynamic programming algorithm, which I didn't know how to do. So I failed the interview.</p><p>But you only need dynamic programming if you're writing your own algorithm. It's really easy if you throw it into a constraint solver like <a href="https://www.minizinc.org/">MiniZinc</a> and call it a day. </p><div><pre><code>int: total; array[int] of int: values = [10, 9, 1]; array[index_set(values)] of var 0..: coins; constraint sum (c in index_set(coins)) (coins[c] * values[c]) == total; solve minimize sum(coins); </code></pre></div><p>You can try this online <a href="https://play.minizinc.dev/">here</a>. It'll give you a prompt to put in <code>total</code> and then give you successively-better solutions:</p><div><pre><span></span><code>coins = [0, 0, 37]; ---------- coins = [0, 1, 28]; ---------- coins = [0, 2, 19]; ---------- coins = [0, 3, 10]; ---------- coins = [0, 4, 1]; ---------- coins = [1, 3, 0]; ---------- </code></pre></div><p>Lots of similar interview questions are this kind of mathematical optimization problem, where we have to find the maximum or minimum of a function corresponding to constraints. They're hard in programming languages because programming languages are too low-level. They are also exactly the problems that constraint solvers were designed to solve. Hard leetcode problems are easy constraint problems.<sup><a href="https://buttondown.com#fn:leetcode">1</a></sup>Here I'm using MiniZinc, but you could just as easily use Z3 or OR-Tools or whatever your favorite generalized solver is.</p><h3>More examples</h3><p>This was a question in a different interview (which I thankfully passed):</p><blockquote><p>Given a list of stock prices through the day, find maximum profit you can get by buying one stock and selling one stock later.</p></blockquote><p>It's easy to do in O(n^2) time, or if you are clever, you can do it in O(n). Or you could be not clever at all and just write it as a constraint problem:</p><div><pre><code>array[int] of int: prices = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8]; var int: buy; var int: sell; var int: profit = prices[sell] - prices[buy]; constraint sell &gt; buy; constraint profit &gt; 0; solve maximize profit; </code></pre></div><p>Reminder, link to trying it online <a href="https://play.minizinc.dev/">here</a>. While working at that job, one interview question we tested out was:</p><blockquote><p>Given a list, determine if three numbers in that list can be added or subtracted to give 0? </p></blockquote><p>This is a satisfaction problem, not a constraint problem: we don't need the "best answer", any answer will do. We eventually decided against it for being too tricky for the engineers we were targeting. But it's not tricky in a solver; </p><div><pre><span></span><code>include "globals.mzn"; array[int] of int: numbers = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8]; array[index_set(numbers)] of var {0, -1, 1}: choices; constraint sum(n in index_set(numbers)) (numbers[n] * choices[n]) = 0; constraint count(choices, -1) + count(choices, 1) = 3; solve satisfy; </code></pre></div><p>Okay, one last one, a problem I saw last year at <a href="https://chicagopython.github.io/algosig/">Chipy AlgoSIG</a>. Basically they pick some leetcode problems and we all do them. I failed to solve <a href="https://leetcode.com/problems/largest-rectangle-in-histogram/description/">this one</a>:</p><blockquote><p>Given an array of integers heights representing the histogram's bar height where the width of each bar is 1, return the area of the largest rectangle in the histogram.</p></blockquote><p>The "proper" solution is a tricky thing involving tracking lots of bookkeeping states, which you can completely bypass by expressing it as constraints:</p><div><pre><code>array[int] of int: numbers = [2,1,5,6,2,3]; var 1..length(numbers): x; var 1..length(numbers): dx; var 1..: y; constraint x + dx &lt;= length(numbers); constraint forall (i in x..(x+dx)) (y &lt;= numbers[i]); var int: area = (dx+1)*y; solve maximize area; output ["(\(x)-&gt;\(x+dx))*\(y) = \(area)"] </code></pre></div><p>There's even a way to <a href="https://docs.minizinc.dev/en/2.9.3/visualisation.html">automatically visualize the solution</a> (using <code>vis_geost_2d</code>), but I didn't feel like figuring it out in time for the newsletter.</p><h3>Is this better?</h3><p>Now if I actually brought these questions to an interview the interviewee could ruin my day by asking "what's the runtime complexity?" Constraint solvers runtimes are unpredictable and almost always than an ideal bespoke algorithm because they are more expressive, in what I refer to as the <a href="https://buttondown.com/hillelwayne/archive/the-capability-tractability-tradeoff/">capability/tractability tradeoff</a>. But even so, they'll do way better than a <em>bad</em> bespoke algorithm, and I'm not experienced enough in handwriting algorithms to consistently beat a solver.</p><p>The real advantage of solvers, though, is how well they handle new constraints. Take the stock picking problem above. I can write an O(n&sup2;) algorithm in a few minutes and the O(n) algorithm if you give me some time to think. Now change the problem to</p><blockquote><p>Maximize the profit by buying and selling up to <code>max_sales</code> stocks, but you can only buy or sell one stock at a given time and you can only hold up to <code>max_hold</code> stocks at a time?</p></blockquote><p>That's a way harder problem to write even an inefficient algorithm for! While the constraint problem is only a tiny bit more complicated:</p><div><pre><span></span><code>include "globals.mzn"; int: max_sales = 3; int: max_hold = 2; array[int] of int: prices = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8]; array [1..max_sales] of var int: buy; array [1..max_sales] of var int: sell; array [index_set(prices)] of var 0..max_hold: stocks_held; var int: profit = sum(s in 1..max_sales) (prices[sell[s]] - prices[buy[s]]); constraint forall (s in 1..max_sales) (sell[s] &gt; buy[s]); constraint profit &gt; 0; constraint forall(i in index_set(prices)) (stocks_held[i] = (count(s in 1..max_sales) (buy[s] &lt;= i) - count(s in 1..max_sales) (sell[s] &lt;= i))); constraint alldifferent(buy ++ sell); solve maximize profit; output ["buy at \(buy)\n", "sell at \(sell)\n", "for \(profit)"]; </code></pre></div><p>Most constraint solving examples online are puzzles, like <a href="https://docs.minizinc.dev/en/stable/modelling2.html#ex-sudoku">Sudoku</a> or "<a href="https://docs.minizinc.dev/en/stable/modelling2.html#ex-smm">SEND + MORE = MONEY</a>". Solving leetcode problems would be a more interesting demonstration. And you get more interesting opportunities to teach optimizations, like symmetry breaking.</p><p><em>If you're reading this on the web, you can subscribe <a href="https://buttondown.com/hillelwayne">here</a>. Updates are once a week. My main website is <a href="https://www.hillelwayne.com">here</a>.</em></p><p><em>My new book, </em>Logic for Programmers<em>, is now in early access! Get it <a href="https://leanpub.com/logic/">here</a>.</em></p></div></section>]]></description><pubDate>Fri, 12 Sep 2025 22:25:19 +0530</pubDate></item><item><link>https://www.youtube.com/watch?v=fdUKJ-4y2zo</link><title>“I Got Pwned”: npm maintainer of Chalk &amp;amp; Debug speaks on the massive supply-chain attack (youtube.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nf6df3/i_got_pwned_npm_maintainer_of_chalk_debug_speaks/</guid><comments>https://www.reddit.com/r/programming/comments/1nf6df3/i_got_pwned_npm_maintainer_of_chalk_debug_speaks/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/programming/comments/1nf6df3/i_got_pwned_npm_maintainer_of_chalk_debug_speaks/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey Everyone,<br/>This week I posted our discovery of finding that a popular open-source projects, including debug and chalk had been breached. I&#39;m happy to say the Josh (Qix) the maintainer that was compromised agreed to sit down with me and discuss his experience, it was a very candid conversation but one I think was important to have.</p><p>Below are some of the highlight and takeaways from the conversation, since the “how could this happen?” question is still circulating.</p><p><strong>Was MFA on the account?</strong></p><p>“There was definitely MFA… but timed one-time passwords are not phishing resistant. They can be man in the middle. There’s no cryptographic checks, no domain association, nothing like U2F would have.”</p><p>The attackers used a fake NPM login flow and captured his TOTP, allowing them to fully impersonate him. Josh called out not enabling phishing-resistant MFA (FIDO2/U2F) as his biggest technical mistake.</p><p><strong>The scale of the blast radius</strong></p><p>Charlie (our researcher) spotted the issue while triaging suspicious packages:</p><p>“First I saw the debug package… then I saw chalk and error-ex… and I knew a significant portion of the JS ecosystem would be impacted.”</p><p>Wiz later reported that <strong>99% of cloud environments used at least one affected package</strong>.</p><p>“The fact it didn’t do anything was the bullet we dodged. It ran in CI/CD, on laptops, servers, enterprise machines. It could have done anything.”</p><p>Wiz also reported that 10% of cloud environments they analyzed had the malware inside them. There were some &#39;hot takes&#39; on the internet that, in fact this was not a big deal and some said it was a win for security. Josh shared that this was not a win and the only reason we got away with it was because how ineffective the attackers were. The malicious packages were downloaded 2.5 million times in the 2 hour window they were live.</p><p><strong>Ecosystem-level shortcomings</strong></p><p>Josh was frank about registry response times and missing safeguards:</p><p>“There was a huge process breakdown during this attack with NPM. Extremely slow to respond. No preemptive ‘switch to U2F’ push despite billions of downloads. I had no recourse except filing a ticket through their public form.&quot;</p><p>Josh also gave some advice for anyone going through this in the future which is to be open and transparent, the internet largely agreed Josh handled this in the best way possible (short of not getting phished in the first place )</p><p>“If you screw up, own it. In open source, being transparent and immediate saves a lot of people’s time and money. Vulnerability (the human kind) goes a long way.”</p></div><!-- SC_ON --></section><section class='embedded-media'><iframe width="267" height="200" src="https://www.youtube.com/embed/fdUKJ-4y2zo?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Yep, I Got Pwned: A Candid Chat With The Chalk &amp; Debug Maintainer"></iframe></section>]]></description><pubDate>Fri, 12 Sep 2025 21:02:55 +0530</pubDate></item><item><link>https://lwn.net/Articles/1034966/</link><title>The Challenge of Maintaining Curl (lwn.net)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nesqco/the_challenge_of_maintaining_curl/</guid><comments>https://www.reddit.com/r/programming/comments/1nesqco/the_challenge_of_maintaining_curl/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 4 min | <a href='https://www.reddit.com/r/programming/comments/1nesqco/the_challenge_of_maintaining_curl/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><div><p>By <b>Jonathan Corbet</b><br>August 29, 2025</p><hr><p><a href="https://lwn.net/Archives/ConferenceByYear/#2025-Open_Source_Summit_Europe">OSS EU</a> </p></div><p>Keynote sessions at Open Source Summit events tend not to allow much time for detailed talks, and the 2025 <a href="https://events.linuxfoundation.org/open-source-summit-europe/">Open Source Summit Europe</a> did not diverge from that pattern. Even so, Daniel Stenberg, the maintainer of the <a href="https://curl.se/">curl</a> project, managed to cram a lot into the 15&nbsp;minutes given to him. Like the maintainers of many other projects, Stenberg is feeling some stress, and the problems appear to be getting worse over time. </p><p><a href="https://lwn.net/Articles/1034968/"><img src="https://static.lwn.net/images/conf/2025/osseu/DanielStenberg-sm.png" alt="[Daniel Stenberg]" title="Daniel Stenberg"></a> Curl, he began, is "<q>a small project with a big impact</q>". It began in 1996 with all of 100&nbsp;lines of code; it has since grown to 180,000 lines that have been contributed by 1,400 authors. In any given month, there are 20-25 developers who are actively contributing to curl. The project has exactly one full-time employee &mdash; that being Stenberg himself. </p><p>The program is widely used, having been deployed in at least one-billion devices. Just about anything that occasionally connects to the net, he said, uses curl to do it. But using curl is different from supporting its development. As an example, he put up a slide listing the 47 car brands that use curl in their products; he followed it with a slide listing the brands that contribute to curl. The second slide, needless to say, was empty. (A version of both slides can be seen on <a href="https://mastodon.social/@bagder/115025727082593712">this page</a>). </p><p>Companies tend to assume that somebody else is paying for the development of open-source software, so they do not have to contribute. He emphasized that he has released curl under a free license, so there is no <i>legal</i> problem with what these companies are doing. But, he suggested, these companies might want to think a bit more about the future of the software they depend on. </p><p>Open-source software is the best choice, he said, but maintaining it is a tough job. Most projects out there have a single maintainer, and that person is often doing the work in their spare time, without funding. Maintenance involves a lot of tasks, including taking care of security, reviewing patches, writing documentation, keeping the web site going, administering the mailing list, and a long list of other tasks. Occasionally, if a little time is left over, it might also be possible to do a bit of feature development. That is a lot for one person to keep up with. </p><p>Companies have a certain tendency to make things worse. He put up an excerpt of a message from Apple support, referring a customer to the curl project for help with their (Apple) device. He has received demands from companies for information on the project's development and security practices, often with tight deadlines for a response. He typically replies by sending back a support contract; that usually results in never hearing from the company again, he said. More recently, he has been getting demands from European companies seeking information on the curl project's Cyber Resilience Act compliance practices. </p><p>Some communications are rather less humorous than that; one email came with a subject reading "<q>I will slaughter you</q>". He gets emails from people who found his address in the license notices shipped with their automobiles asking for support. But he also gets nice thank-you emails at times. </p><p>Problematic email takes other forms as well. There is an increasing crowd of people who ask a large language model to "<q>find a problem in curl, make it sound terrible</q>", then send the result, which is never correct, to the project, thinking that they are somehow helping. Dealing with these useless problem reports takes an increasing amount of time. </p><p>Recently, the curl project, like many operators of web sites, has been contending with distributed denial-of-service attacks by scrapers run by AI companies. He put up a link to <a href="https://lwn.net/Articles/1008897/">LWN's article on this problem</a> for those who are unfamiliar with it. The curl site consumes a massive amount of bandwidth every month, but only 0.01% of that is source downloads. Most of the rest is bot traffic. That, too, adds to the difficulty of maintaining the project. </p><p>He concluded the brief talk with one last email; it was from an 11-year-old child who had found curl useful in some project they were working on. It included an expression of gratitude that, Stenberg said, was truly heartwarming. </p><p>[Thanks to the Linux Foundation, LWN's travel sponsor, for supporting our travel to this event.]<br></p><table> <tr><th>Index entries for this article</th></tr> <tr><td><a href="https://lwn.net/Archives/ConferenceIndex/">Conference</a></td><td><a href="https://lwn.net/Archives/ConferenceIndex/#Open_Source_Summit_Europe-2025">Open Source Summit Europe/2025</a></td></tr> </table><br> <hr> </div></section>]]></description><pubDate>Fri, 12 Sep 2025 08:56:40 +0530</pubDate></item><item><link>https://fabiensanglard.net/floating_point_visually_explained/</link><title>Floating Point Visually Explained (fabiensanglard.net)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nesp2g/floating_point_visually_explained/</guid><comments>https://www.reddit.com/r/programming/comments/1nesp2g/floating_point_visually_explained/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 4 min | <a href='https://www.reddit.com/r/programming/comments/1nesp2g/floating_point_visually_explained/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><br><center> </center><p>August 29, 2017</p><p>Floating Point Visually Explained</p><hr><div><p>While I was writing the Wolfenstein 3D book<a href="https://fabiensanglard.net#footnote_1"><sup>[1]</sup></a>, I wanted to demonstrate how much of a handicap it was to work without floating points. My attempts at understanding floating points using canonical<a href="https://fabiensanglard.net#footnote_2"><sup>[2]</sup></a> articles<a href="https://fabiensanglard.net#footnote_3"><sup>[3]</sup></a> were met with resistance from my brain. </p><p>I tried to find a different way. Something far from $(-1)^S * 1.M * 2^{(E-127)}$ and </p><p>its mysterious exponent/mantissa. Possibly a drawing since they seem to flow through my brain better.</p><p>I ended up with what follows and I decided to include it in the book. I am not claiming this is my invention but I have never seen floating points explained this way so far. I hope it will helps a few people like me who are a bit allergic to mathematic notations. </p></div><p>How Floating Point are usually explained</p><hr><p>In the C language, floats are 32-bit container following the IEEE 754 standard. Their purpose is to store and allow operations on approximation of real numbers. The way I have seen them explained so far is as follow. The 32 bits are divided in three sections: </p><ul> <li>1 bit S for the sign </li><li>8 bits E for the exponent </li><li>23 bits for the mantissa </li></ul><div><p><img src="https://fabiensanglard.netfloating_point_layout.svg" width="1135.6868" height="63.758217"><span><i><small>Floating Point internals.</small></i></span> <img src="https://fabiensanglard.netfloating_point_math.svg" width="1136.25" height="38.75"><span><i><small>The three sections of a floating point number.</small></i></span> So far, so good. Now, how numbers are interpreted is usually explained with the formula:</p></div><p>$$ (-1)^S * 1.M * 2^{(E-127)} $$ </p><span><i><small>How everybody hates floating point to be explained to them.</small></i></span><p>This is usually where I flip the table. Maybe I am allergic to mathematic notation but something just doesn't click when I read it. It feels like learning to <a href="https://fabiensanglard.net01.jpg">draw a owl</a>.<br> </p><blockquote> Floating-point arithmetic is considered an esoteric subject by many people.<p>- David Goldberg</p></blockquote><p>A different way to explain...</p><hr><div><p>Although correct, this way of explaining floating point will leaves some of us completely clueless. Fortunately, there is a different way to explain it. Instead of Exponent, think of a Window between two consecutive power of two integers. Instead of a Mantissa, think of an Offset within that window.</p><p><img src="https://fabiensanglard.netfloating_point_intuitive.svg"><br> <span><i><small>The three sections of a floating Point number.</small></i></span> The window tells within which two consecutive power-of-two the number will be: [0.5,1], [1,2], [2,4], [4,8] and so on (up to [$2^{127}$,$2^{128}$]). The offset divides the window in $ 2^{23} = 8388608 $ buckets. With the window and the offset you can approximate a number. The window is an excellent mechanism to protect from overflowing. Once you have reached the maximum in a window (e.g [2,4]), you can "float" it right and represent the number within the next window (e.g [4,8]). It only costs a little bit of precision since the window becomes twice as large.</p><p>The next figure illustrates how the number 6.1 would be encoded. The window must start at 4 and span to next power of two, 8. The offset is about half way down the window.</p><p><span><i><small>Value 6.1 approximated with floating point.</small></i></span></p></div><p>Precision</p><hr><p>How much precision is lost when the window covers a wider range? Let's take an example with window [1,2] where the 8388608 offsets cover a range of 1 which gives a precision of $\frac{(2-1)}{8388608}=0.00000011920929$. In the window [2048,4096] the 8388608 offsets cover a range of $4096 - 2048 = 2048$ which gives a precision of $ \frac{4096-2048}{8388608}=0.0002 $.<br> </p><p>An other example</p><hr><p>Let's take an other example with the detailed calculations of the floating point representation of a number we all know well: 3.14.<br> </p><ul> <li>The number 3.14 is positive $\rightarrow S=0$. </li><li>The number 3.14 is between the power of two 2 and 4 so the floating window must start at $2^1$ $\rightarrow E=128$ (see formula where window is $2^{(E-127)}$). </li><li>Finally there are $2^{23}$ offsets available to express where 3.14 falls within the interval [2-4]. It is at $\frac{3.14 -2 }{4 - 2} = 0.57$ within the interval which makes the offset $ M = 2^{23}*0.57 = 4781507$ </li></ul><p>Which in binary translates to: </p><ul> <li> S = 0 = 0b </li><li> E = 128 = 10000000b </li><li> M = 4781507 = 10010001111010111000011b </li></ul><p><img src="https://fabiensanglard.netfloating_point_layout_pi.svg"><br> <span><i><small>3.14 floating point binary representation.</small></i></span> The value 3.14 is therefore approximated to 3.1400001049041748046875. The corresponding value with the ugly formula:<br> </p><p>$$ 3.14 = (-1)^0 * 1.57 * 2^{(128-127)} $$ </p><br><div><p>And finally the graphic representation with window and offset:</p><p><img src="https://fabiensanglard.netfloating_point_window_pi.svg"><br> <span><i><small>3.14 window and offset.</small></i></span></p></div><p><br> I hope that helped :) ! </p><p>References</p><hr> <hr> <center>*</center></div></section>]]></description><pubDate>Fri, 12 Sep 2025 08:54:47 +0530</pubDate></item><item><link>https://43081j.com/2025/09/bloat-of-edge-case-libraries</link><title>The bloat of edge-case first libraries (43081j.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1neezti/the_bloat_of_edgecase_first_libraries/</guid><comments>https://www.reddit.com/r/programming/comments/1neezti/the_bloat_of_edgecase_first_libraries/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 15 min | <a href='https://www.reddit.com/r/programming/comments/1neezti/the_bloat_of_edgecase_first_libraries/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><article><p>This is just some of what I&rsquo;ve been pondering recently - particularly in terms of how we ended up with such overly-granular dependency trees.</p><p>I think we&rsquo;ve ended up with many libraries in our ecosystem which are edge-case-first, the opposite to what I&rsquo;d expect. I&rsquo;ll give a few examples and some thoughts around this, mostly in the hope we can start to trim some of it away.</p><h2>The problem</h2><p>I believe a lot of the questionably small libraries hiding in our deep dependency trees are a result of over-engineering for inputs and edge cases we&rsquo;ve probably never seen.</p><p>For example, say we&rsquo;re building a <code>clamp</code> function:</p><div><pre><code><span>export</span> <span>function</span> <span>clamp</span><span>(</span><span>value</span><span>:</span> <span>number</span><span>,</span> <span>min</span><span>:</span> <span>number</span><span>,</span> <span>max</span><span>:</span> <span>number</span><span>):</span> <span>number</span> <span>{</span> <span>return</span> <span>Math</span><span>.</span><span>min</span><span>(</span><span>Math</span><span>.</span><span>max</span><span>(</span><span>value</span><span>,</span> <span>min</span><span>),</span> <span>max</span><span>);</span> <span>}</span> </code></pre></div><p>Pretty simple!</p><p>What if someone passes nonsensical ranges? Let&rsquo;s handle that.</p><div><pre><code><span>export</span> <span>function</span> <span>clamp</span><span>(</span><span>value</span><span>:</span> <span>number</span><span>,</span> <span>min</span><span>:</span> <span>number</span><span>,</span> <span>max</span><span>:</span> <span>number</span><span>):</span> <span>number</span> <span>{</span> <span>if</span> <span>(</span><span>min</span> <span>&gt;</span> <span>max</span><span>)</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>min must be less than or equal to max</span><span>'</span><span>);</span> <span>}</span> <span>return</span> <span>Math</span><span>.</span><span>min</span><span>(</span><span>Math</span><span>.</span><span>max</span><span>(</span><span>value</span><span>,</span> <span>min</span><span>),</span> <span>max</span><span>);</span> <span>}</span> </code></pre></div><p><strong>This is probably as far as I&rsquo;d go.</strong> But let&rsquo;s over-engineer - what if someone passes a number-like string?</p><div><pre><code><span>export</span> <span>function</span> <span>clamp</span><span>(</span><span>value</span><span>:</span> <span>number</span> <span>|</span> <span>string</span><span>,</span> <span>min</span><span>:</span> <span>number</span> <span>|</span> <span>string</span><span>,</span> <span>max</span><span>:</span> <span>number</span> <span>|</span> <span>string</span><span>):</span> <span>number</span> <span>{</span> <span>if</span> <span>(</span><span>typeof</span> <span>value</span> <span>===</span> <span>'</span><span>string</span><span>'</span> <span>&amp;&amp;</span> <span>Number</span><span>.</span><span>isNaN</span><span>(</span><span>Number</span><span>(</span><span>value</span><span>)))</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>value must be a number or a number-like string</span><span>'</span><span>);</span> <span>}</span> <span>if</span> <span>(</span><span>typeof</span> <span>min</span> <span>===</span> <span>'</span><span>string</span><span>'</span> <span>&amp;&amp;</span> <span>Number</span><span>.</span><span>isNaN</span><span>(</span><span>Number</span><span>(</span><span>min</span><span>)))</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>min must be a number or a number-like string</span><span>'</span><span>);</span> <span>}</span> <span>if</span> <span>(</span><span>typeof</span> <span>max</span> <span>===</span> <span>'</span><span>string</span><span>'</span> <span>&amp;&amp;</span> <span>Number</span><span>.</span><span>isNaN</span><span>(</span><span>Number</span><span>(</span><span>max</span><span>)))</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>max must be a number or a number-like string</span><span>'</span><span>);</span> <span>}</span> <span>if</span> <span>(</span><span>Number</span><span>(</span><span>min</span><span>)</span> <span>&gt;</span> <span>Number</span><span>(</span><span>max</span><span>))</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>min must be less than or equal to max</span><span>'</span><span>);</span> <span>}</span> <span>return</span> <span>Math</span><span>.</span><span>min</span><span>(</span><span>Math</span><span>.</span><span>max</span><span>(</span><span>value</span><span>,</span> <span>min</span><span>),</span> <span>max</span><span>);</span> <span>}</span> </code></pre></div><p>At this point, it seems clear to me we&rsquo;ve just poorly designed our function. It solely exists to clamp numbers, so why would we accept strings?</p><p>But hey, let&rsquo;s go further! What if other libraries also want to accept such loose inputs? Let&rsquo;s extract this into a separate library:</p><div><pre><code><span>import</span> <span>isNumber</span> <span>from</span> <span>'</span><span>is-number</span><span>'</span><span>;</span> <span>export</span> <span>function</span> <span>clamp</span><span>(</span><span>value</span><span>:</span> <span>number</span> <span>|</span> <span>string</span><span>,</span> <span>min</span><span>:</span> <span>number</span> <span>|</span> <span>string</span><span>,</span> <span>max</span><span>:</span> <span>number</span> <span>|</span> <span>string</span><span>):</span> <span>number</span> <span>{</span> <span>if</span> <span>(</span><span>!</span><span>isNumber</span><span>(</span><span>value</span><span>))</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>value must be a number or a number-like string</span><span>'</span><span>);</span> <span>}</span> <span>if</span> <span>(</span><span>!</span><span>isNumber</span><span>(</span><span>min</span><span>))</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>min must be a number or a number-like string</span><span>'</span><span>);</span> <span>}</span> <span>if</span> <span>(</span><span>!</span><span>isNumber</span><span>(</span><span>max</span><span>))</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>max must be a number or a number-like string</span><span>'</span><span>);</span> <span>}</span> <span>if</span> <span>(</span><span>Number</span><span>(</span><span>min</span><span>)</span> <span>&gt;</span> <span>Number</span><span>(</span><span>max</span><span>))</span> <span>{</span> <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>'</span><span>min must be less than or equal to max</span><span>'</span><span>);</span> <span>}</span> <span>return</span> <span>Math</span><span>.</span><span>min</span><span>(</span><span>Math</span><span>.</span><span>max</span><span>(</span><span>value</span><span>,</span> <span>min</span><span>),</span> <span>max</span><span>);</span> <span>}</span> </code></pre></div><p><strong>Whoops!</strong> We&rsquo;ve just created the infamous <a href="https://www.npmjs.com/package/is-number"><code>is-number</code></a> library!</p><h2>How it should be</h2><p>This, in my opinion, is poor technical design we&rsquo;ve all ended up dealing with over the years. Carrying the baggage of these overly-granular libraries that exist to handle edge cases we&rsquo;ve probably never encountered.</p><p>I think it should have been:</p><div><pre><code><span>export</span> <span>function</span> <span>clamp</span><span>(</span><span>value</span><span>:</span> <span>number</span><span>,</span> <span>min</span><span>:</span> <span>number</span><span>,</span> <span>max</span><span>:</span> <span>number</span><span>):</span> <span>number</span> <span>{</span> <span>return</span> <span>Math</span><span>.</span><span>min</span><span>(</span><span>Math</span><span>.</span><span>max</span><span>(</span><span>value</span><span>,</span> <span>min</span><span>),</span> <span>max</span><span>);</span> <span>}</span> </code></pre></div><p><em>Maybe</em> with some <code>min &lt;= max</code> validation, but even that is debatable. At this point, you may as well inline the <code>Math.min(Math.max(...))</code> expression instead of using a dependency.</p><p><strong>We should be able to define our functions to accept the inputs they are designed for, and not try to handle every possible edge case.</strong></p><p>There are two things at play here:</p><ul> <li>Data types</li> <li>Values</li> </ul><p>A well designed library would assume the right <strong>data types</strong> have been passed in, but may validate that the <strong>values</strong> make sense (e.g. <code>min</code> is less than or equal to <code>max</code>).</p><p>These over-engineered libraries have decided to implement <em>both</em> at runtime - essentially run-time type checking and value validation. One could argue that this is just a result of building in the pre-TypeScript era, but that still doesn&rsquo;t justify the overly specific <em>value validation</em> (e.g. the real <code>is-number</code> also checks that it is finite).</p><h2>What we shouldn&rsquo;t do</h2><p>We shouldn&rsquo;t build edge-case-first libraries, i.e. those which solve for edge cases we have yet to encounter or are unlikely to ever encounter.</p><h2>Example: <code>is-arrayish</code> (76M downloads/week)</h2><p>The <code>is-arrayish</code> library determines if a value is an <code>Array</code> or behaves like one.</p><p>There will be some edge cases where this matters a lot, where we want to accept something we can index into but don&rsquo;t care if it is a real <code>Array</code> or not.</p><p>However, the common use case clearly will not be that and we could&rsquo;ve just used <code>Array.isArray()</code> all along.</p><h2>Example: <code>is-number</code> (90M downloads/week)</h2><p>The <code>is-number</code> library determines if a value is a positive, finite number or number-like string (maybe we should name it <code>is-positive-finite-number</code> to be more accurate).</p><p>Again, there will be edge cases where we want to deal with number-like strings or we want to validate that a number is within a range (e.g. finite).</p><p>The common use case will not be this. The common use case will be that we want to check <code>typeof n === 'number'</code> and be done with it.</p><p>For those edge cases where we want to <em>additionally</em> validate what kind of number it is, we could use a library (but one which exists for the validation, not for the type check).</p><h2>Example: <code>pascalcase</code> (9.7M downloads/week)</h2><p>The <code>pascalcase</code> library transforms text to PascalCase.</p><p>It has 1 dependency (<code>camelcase</code>) and accepts a variety of input types:</p><ul> <li>strings</li> <li>null</li> <li>undefined</li> <li>arrays of strings</li> <li>functions</li> <li>arbitrary objects with <code>toString</code> methods</li> </ul><p>In reality, almost every user will be passing a <code>string</code>.</p><h2>Example: <code>is-regexp</code> (10M downloads/week)</h2><p>The <code>is-regexp</code> library checks if a value is a <code>RegExp</code> object, and supports cross-realm values.</p><p>In reality, almost every user will be passing a <code>RegExp</code> object, and not one from another realm.</p><p>For context, cross-realm values can happen when you retrieve a value from an <code>iframe</code> or VM for example:</p><div><pre><code><span>const</span> <span>iframe</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'</span><span>iframe</span><span>'</span><span>);</span> <span>iframe</span><span>.</span><span>contentWindow</span><span>.</span><span>RegExp</span> <span>===</span> <span>RegExp</span><span>;</span> <span>// false</span> <span>const</span> <span>iframeRegex</span> <span>=</span> <span>iframe</span><span>.</span><span>contentWindow</span><span>.</span><span>someRegexp</span><span>;</span> <span>iframeRegex</span> <span>instanceof</span> <span>RegExp</span><span>;</span> <span>// false</span> <span>isRegex</span><span>(</span><span>iframeRegex</span><span>);</span> <span>// true</span> </code></pre></div><p>This is indeed useful, and I do support this myself in chai (which I maintain). However, this is an edge case most libraries don&rsquo;t need to care about.</p><h2>What we should do</h2><p>We should build libraries which solve the common use case and make assumptions about the input types they will be given.</p><h2>Example: scule (1.8M downloads/week)</h2><p><a href="https://www.npmjs.com/package/scule">scule</a> is a library for transforming casing of text (e.g. camel case, etc).</p><p>It only accepts inputs it is designed for (strings and arrays of strings) and has zero dependencies.</p><p>In most of the functions it exports, it assumes valid input data types.</p><h2>Example: dlv (14.9M downloads/week)</h2><p><a href="https://www.npmjs.com/package/dlv">dlv</a> is a library for deep property access.</p><p>It only accepts strings and arrays of strings as the path to access, and assumes this (i.e. does no validation).</p><h2>Validation is important</h2><p>Validation is important, and I want to be clear that I&rsquo;m not saying we should stop validating our data.</p><p>However, we should usually be validating the data in the project that owns it (e.g. at the app level), and not in every library that later consumes it as input.</p><p>Deep dependencies applying validation like this actually shift the burden from where it belongs (at data boundaries) to deep in the dependency tree.</p><p>Often at this point, it is invisible to the consumer of the library.</p><p>How many people are passing values into <code>is-number</code> (via other libraries), not realising it will prevent them from using negative numbers and <code>Infinity</code>?</p><h2>A note on overly-granular libraries</h2><p>This post isn&rsquo;t about overly-granular libraries in general, but I&rsquo;d like to briefly mention them for visibility.</p><p>An overly-granular library is one where someone took a useful library and split it up into an almost atomic-level of granularity.</p><p>Some examples:</p><ul> <li><code>shebang-regex</code> - 2LOC, does the same as <code>startsWith('#!')</code>, <strong>86M downloads/week</strong></li> <li><code>is-whitespace</code> - 7LOC, checks if a string is only whitespace, <strong>1M downloads/week</strong></li> <li><code>is-npm</code> - 8LOC, checks <code>npm_config_user_agent</code> or <code>npm_package_json</code> are set, <strong>7M downloads/week</strong></li> </ul><p>This is a personal preference some maintainers clearly prefer. The thought seems to be that by having atomic libraries, you can easily build your next library mostly from the existing building blocks you have.</p><p>I don&rsquo;t really agree with this and think downloading a package for <code>#!</code> 86 million times a week is a bit much.</p><h2>What can be done about this?</h2><p>The <a href="https://e18e.dev">e18e</a> community is already tackling a lot of this by contributing performance improvements across the ecosystem, including removing and replacing dependencies with more modern, performant ones.</p><p>Through these efforts, there&rsquo;s already a useful <a href="https://e18e.dev/guide/replacements.html">list of replacements</a> and an <a href="https://github.com/es-tooling/eslint-plugin-depend/">ESLint plugin</a>.</p><h2>As a maintainer</h2><p>If you&rsquo;re maintaining a library, it would be worth reviewing your dependencies to see if:</p><ul> <li>Any are replaceable by native functionality these days (e.g. <code>Array.isArray</code>)</li> <li>Any are replaceable by smaller, less granular and/or more performant alternatives (e.g. <code>scule</code> instead of <code>pascalcase</code>)</li> <li>Any are redundant if you make more assumptions about input types</li> </ul><p>Tools like <a href="https://npmgraph.js.org/">npmgraph</a> can help you visualise your dependency tree to make this task easier.</p><p>Also, being stricter around input types will allow you to reduce a lot of code and dependencies.</p><p>If you can assume the data being passed in is the correct type, you can leave validation up to the consumer.</p><h2>As a user</h2><p>Keep a close eye on your dependencies (both deep and direct), and what alternatives are available to your direct dependencies.</p><p>Often, it is easy to stick with a dependency from long ago and forget to re-visit it one day in case there is a better way. Many of these packages are possible natively, or have more modern alternatives.</p><p>Useful tools:</p><ul> <li><a href="https://npmgraph.js.org/">npmgraph</a> for visualising your dependency tree</li> <li><a href="https://node-modules.dev/">node-modules.dev</a> for visualising your dependencies and lots of useful meta data</li> <li><a href="https://docs.github.com/en/code-security/getting-started/dependabot-quickstart-guide">Dependabot</a> for keeping your dependencies up to date</li> </ul><p>On the topic of data, it is also worth ensuring validation happens at data boundaries rather than being delegated to various dependencies. Try to validate the type and value up front, before passing into dependencies.</p><h2>Conclusion</h2><p>Most of these libraries exist to handle edge cases that do certainly exist. However, <strong>we are all paying the cost of that rather than only those who need to support those edge cases</strong>.</p><p>This is the wrong way around. Libraries should implement the main use case, and alternatives (or plugins) can exist to provide the edge cases the minority needs.</p><p>We should all be more aware of what is in our dependency tree, and should push for more concise, lighter libraries.</p></article> </div></section>]]></description><pubDate>Thu, 11 Sep 2025 22:59:35 +0530</pubDate></item><item><link>https://reiner.org/hashed-sorting</link><title>Hashed sorting is typically faster than hash tables1 (reiner.org)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1ne6jtd/hashed_sorting_is_typically_faster_than_hash/</guid><comments>https://www.reddit.com/r/programming/comments/1ne6jtd/hashed_sorting_is_typically_faster_than_hash/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 23 min | <a href='https://www.reddit.com/r/programming/comments/1ne6jtd/hashed_sorting_is_typically_faster_than_hash/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><p>Problem statement: count the unique values in a large array of mostly-unique uint64s. Two standard approaches are:</p><ul> <li>Insert into a hash table and return the number of entries.</li> <li>Sort the array, then count positions that differ from their predecessor.</li> </ul><p>Hash tables win the interview (<span><span><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>)</mo></mrow><annotation>O(n)</annotation></semantics></math></span></span> vs <span><span><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mi>log</mi><mo>&#8289;</mo><mi>n</mi><mo>)</mo></mrow><annotation>O(n \log n)</annotation></semantics></math></span></span>), but sorting is typically faster in a well-tuned implementation. This problem and its variants are the inner loop of <a href="https://reiner.org#applications">some of the world&rsquo;s biggest CPU workloads</a>.</p><h2>Benchmark highlights</h2><p>Here is performance on M2 Pro<a href="https://reiner.org#fn2"><sup>2</sup></a>, comparing our best-tuned hash table and our best-tuned sorting implementation. We also include Rust&rsquo;s default implementations (<code>sort_unstable()</code>, and <code>HashSet</code> with <code>foldhash::fast</code>) as a reference point for high-quality general-purpose implementations:</p><table> <colgroup><col> <col> <col> <col> <col> </colgroup><thead> <tr> <th>Data size</th> <th>Baseline hash table</th> <th>Baseline sorting</th> <th>Tuned hash table</th> <th>Tuned sorting</th> </tr> </thead> <tbody> <tr> <td>8 KiB</td> <td>3.8&#8239;&micro;s</td> <td>5.1&#8239;&micro;s</td> <td><strong>1.6&#8239;&micro;s</strong></td> <td>6.5&#8239;&micro;s</td> </tr> <tr> <td>256 KiB</td> <td>219&#8239;&micro;s</td> <td>264&#8239;&micro;s</td> <td>193&#8239;&micro;s</td> <td><strong>92&#8239;&micro;s</strong></td> </tr> <tr> <td>8 MiB</td> <td>8.1&#8239;ms</td> <td>12.0&#8239;ms</td> <td>7.1&#8239;ms</td> <td><strong>3.9&#8239;ms</strong></td> </tr> <tr> <td>256 MiB</td> <td>875&#8239;ms</td> <td>464&#8239;ms</td> <td>269&#8239;ms</td> <td><strong>185&#8239;ms</strong></td> </tr> <tr> <td>2 GiB</td> <td>7.6&#8239;s</td> <td>4.3&#8239;s</td> <td>2.6&#8239;s</td> <td><strong>1.9&#8239;s</strong></td> </tr> </tbody> </table><p>Tuned sorting beats our best hash table by ~1.5&times; on non-tiny sizes, and is up to 4&times; faster than the excellent <a href="https://www.youtube.com/watch?v=ncHmEUmJZf4">&ldquo;Swiss Table&rdquo;</a> hash tables that ship with Rust&rsquo;s standard library.</p><p><a href="https://github.com/reinerp/hashed-sorting-benchmark">Benchmark code is available</a>.</p><h2>Why does sorting win?</h2><p>Memory bandwidth: <em>even though sorting makes multiple passes through memory, each pass uses bandwidth far more efficiently than a hash table&rsquo;s single pass.</em></p><p>Once the dataset outgrows CPU caches (often around ~1 MiB on a single core, CPU-dependent), both hashing and sorting become limited by cache-line fetch bandwidth to main memory. Cache lines are typically 64 bytes, and the CPU fetches the entire line if you touch even a single byte.</p><p>Hash tables waste bandwidth: each 8-byte key access pulls a full 64-byte cache line. So a hash table must incur at least 128 bytes of traffic per uint64 processed: 64 bytes read and 64 bytes written.</p><p>For sorting we use a radix sort that splits into 1024 buckets per pass, needing just 3 passes for <span><span><math><semantics><mrow><msup><mn>2</mn><mn>30</mn></msup></mrow><annotation>2^{30}</annotation></semantics></math></span></span> elements<a href="https://reiner.org#fn3"><sup>3</sup></a>. Each pass reads and writes the entire array once, and the accesses have enough spatial locality that the <em>whole</em> cache line is used productively, unlike in hash tables. So processing a single uint64 takes only 48 bytes of memory traffic: 8 bytes &times; 3 passes of reads, 8 bytes &times; 3 passes of writes.</p><p>This analysis would suggests a 2.7&times; speedup vs.&nbsp;hash tables: 128 bytes vs.&nbsp;48 bytes of memory traffic per uint64. The measured speedup is only ~1.5&times;, primarily (as far as I can tell) because it&rsquo;s harder to make the CPU cache system do exactly what you want for radix sort than for hash tables<a href="https://reiner.org#fn4"><sup>4</sup></a>. Hash tables get closer to their ideal than radix sort; radix sort&rsquo;s ideal is better enough that it still wins.</p><h2>Making radix sort robust</h2><p>Although radix sort is often the fastest sorting algorithm on random data, its performance degrades on data that isn&rsquo;t uniformly spread out over the key space. One pass sorts one byte of the key into 256 buckets; if only a small subset of the 256 buckets are used by keys in practice&mdash;for example, if the top byte of a uint64 is always zero&mdash;then a lot of the work on that pass of radix sort may be wasted.</p><p>To show this, I benchmarked sorting on (a) random uint64s and (b) uint64s where the random bits are &ldquo;spread out&rdquo;: even bits random, odd bits zero, like <span><span><math><semantics><mrow><mn>0</mn><mi>a</mi><mn>0</mn><mi>b</mi><mn>0</mn><mi>c</mi><mn>0</mn><mi>d</mi><mo>&hellip;</mo></mrow><annotation>0a0b0c0d\ldots</annotation></semantics></math></span></span>. For the spread-out numbers, each radix pass uses only 16 of 256 buckets, hurting efficiency. Radix sort is ~2&times; faster than quicksort on random numbers, but ~1.5&times; slower on spread-out numbers:</p><table> <thead> <tr> <th>Data size</th> <th>Quicksort</th> <th>Radix sort</th> </tr> </thead> <tbody> <tr> <td>8 KiB (random)</td> <td>5.1&#8239;&micro;s</td> <td><strong>4.8&#8239;&micro;s</strong></td> </tr> <tr> <td>256 KiB (random)</td> <td>262&#8239;&micro;s</td> <td><strong>111&#8239;&micro;s</strong></td> </tr> <tr> <td>8 MiB (random)</td> <td>11.9&#8239;ms</td> <td><strong>5.4&#8239;ms</strong></td> </tr> <tr> <td>256 MiB (random)</td> <td>466&#8239;ms</td> <td><strong>267&#8239;ms</strong></td> </tr> <tr> <td>2 GiB (random)</td> <td>4.2&#8239;s</td> <td><strong>2.8&#8239;s</strong></td> </tr> <tr> <td>8 KiB (spread-out)</td> <td><strong>5.2&#8239;&micro;s</strong></td> <td>6.3&#8239;&micro;s</td> </tr> <tr> <td>256 KiB (spread-out)</td> <td><strong>262&#8239;&micro;s</strong></td> <td>459&#8239;&micro;s</td> </tr> <tr> <td>8 MiB (spread-out)</td> <td><strong>12.2&#8239;ms</strong></td> <td>17.4&#8239;ms</td> </tr> <tr> <td>256 MiB (spread-out)</td> <td><strong>462&#8239;ms</strong></td> <td>628&#8239;ms</td> </tr> <tr> <td>2 GiB (spread-out)</td> <td>4.2&#8239;s</td> <td><strong>3.2&#8239;s</strong></td> </tr> </tbody> </table><p>To avoid these slowdowns, we borrow an idea from hash tables: sort by <code>hash(key)</code> rather than <code>key</code>. For counting uniques we don&rsquo;t care about final order, only grouping. Of course, this will change the sort order, but given that we&rsquo;re only interested in counting uniques rather than the sort order per se, this doesn&rsquo;t matter.</p><p>Better yet, use an invertible (<a href="https://en.wikipedia.org/wiki/Bijection">bijective</a>) hash to transform keys in place. That avoids storing both key <em>and</em> hash, or recomputing the hash each pass. Many widely used hash functions are invertible on <code>uint64</code>; I used <a href="https://github.com/reinerp/hashed-sorting-benchmark/blob/174a64335e879ad367a5393892ebd8461529af58/src/hashers.rs#L19">Murmur3</a> and a cheaper variant, <a href="https://github.com/reinerp/hashed-sorting-benchmark/blob/174a64335e879ad367a5393892ebd8461529af58/src/hashers.rs#L43">MulSwapMul</a>.</p><p>With a reasonably fast hasher, the cost of hashing is cheap and it fixes bad distributions. Using MulSwapMul, performance looks like this mostly regardless of data distribution:</p><table> <thead> <tr> <th>Data size</th> <th>Hashed radix sort</th> </tr> </thead> <tbody> <tr> <td>8 KiB</td> <td>6.5&#8239;&micro;s</td> </tr> <tr> <td>256 KiB</td> <td>92&#8239;&micro;s</td> </tr> <tr> <td>8 MiB</td> <td>3.9&#8239;ms</td> </tr> <tr> <td>256 MiB</td> <td>185&#8239;ms</td> </tr> <tr> <td>2 GiB</td> <td>1.9&#8239;s</td> </tr> </tbody> </table><p>This is the &ldquo;best algorithm&rdquo; shown at the top of this article: hash with MulSwapMul, then radix sort using <a href="https://axelle.me/2022/04/19/diverting-lsd-sort/">diverting LSD</a> <a href="https://en.wikipedia.org/wiki/Radix_sort">radix sort</a>. I fuse hashing with the first pass of radix sort, and counting with the last pass of radix sort, for a small additional speedup.</p><h2>When should I choose hash tables versus hashed radix sort?</h2><p>Some hash-table uses cannot be reformulated as hashed radix sort; the latter is more restrictive. Converting hash-table lookups/inserts to hashed radix sort fundamentally requires <em>batching</em>: you must issue many lookups without needing results until much later. Sometimes you can turn a one-pass algorithm (traverse a data structure and look up as you go) into two passes of the data structure: first gather keys from the data structure; do the radix sort; then traverse the data structure again to write results back. In some scenarios, such as <a href="https://en.wikipedia.org/wiki/Hash_consing">hash consing</a>, <a href="https://en.wikipedia.org/wiki/Common_subexpression_elimination">common subexpression elimination</a>, or parser lookup tables, the requirement for batching is a dealbreaker: sorting isn&rsquo;t viable.</p><p>Where batching <em>is</em> viable, hashed radix sorts are typically viable:</p><ul> <li>Any key types usable with hash tables work with hashed radix sorts: apply the same hash.</li> <li>If you are <em>constructing</em> a hash table, the analog is building a sorted array by radix sort. If you are <em>querying</em> an existing table, the analog is: sort the queries, then do a linear-time merge with the existing sorted array of keys.</li> <li>Radix sort <a href="https://reiner.org#appendix-4-parallelism">parallelizes at least as well</a> as hash tables, if not better.</li> </ul><p>If hashed radix sorts are viable for your problem, will they be faster? The main determining factor seems to be the number of <em>repeat accesses per unique key</em>. If you perform many more inserts/lookups than there are unique keys, hash tables tend to win; if accesses are on the same order as unique keys&mdash;most keys touched only a few times&mdash;radix sort tends to win. This is because hash tables use O(unique keys) memory, while radix sort uses O(accesses); fitting into a smaller footprint often yields better memory-system performance.</p><p>On my benchmarks, hash tables start to pull ahead when keys are accessed ~30 times on average. The threshold rises with problem size:</p><table> <colgroup><col> <col> <col> <col> <col> <col> <col> </colgroup><thead> <tr> <th>Size</th> <th>8 accesses per key<br>HashSet</th> <th>8 accesses per key<br>Hashed sorting</th> <th>32 accesses per key<br>HashSet</th> <th>32 accesses per key<br>Hashed sorting</th> <th>128 accesses per key<br>HashSet</th> <th>128 accesses per key<br>Hashed sorting</th> </tr> </thead> <tbody> <tr> <td>8 KiB</td> <td><strong>1.9&#8239;&micro;s</strong></td> <td>6.1&#8239;&micro;s</td> <td><strong>1.9&#8239;&micro;s</strong></td> <td>6.6&#8239;&micro;s</td> <td><strong>1.7&#8239;&micro;s</strong></td> <td>7.2&#8239;&micro;s</td> </tr> <tr> <td>256 KiB</td> <td>125&#8239;&micro;s</td> <td><strong>119&#8239;&micro;s</strong></td> <td><strong>120 us</strong></td> <td>141&#8239;&micro;s</td> <td><strong>149&#8239;&micro;s</strong></td> <td>157&#8239;&micro;s</td> </tr> <tr> <td>8 MiB</td> <td>6.7&#8239;ms</td> <td><strong>6.1&#8239;ms</strong></td> <td><strong>5.7&#8239;ms</strong></td> <td>7.0&#8239;ms</td> <td><strong>5.1&#8239;ms</strong></td> <td>7.1&#8239;ms</td> </tr> <tr> <td>256 MiB</td> <td>414&#8239;ms</td> <td><strong>246&#8239;ms</strong></td> <td><strong>240&#8239;ms</strong></td> <td><strong>242&#8239;ms</strong></td> <td><strong>200&#8239;ms</strong></td> <td>267&#8239;ms</td> </tr> <tr> <td>2 GiB</td> <td>3.9&#8239;s</td> <td><strong>2.6&#8239;s</strong></td> <td>3.2&#8239;s</td> <td><strong>2.7&#8239;s</strong></td> <td><strong>2.5&#8239;s</strong></td> <td>2.7&#8239;s</td> </tr> </tbody> </table> <h2>Why does it matter?</h2><p>Several high-performance systems have an inner loop equivalent to this; I&rsquo;ve personally worked on two.</p><p>First, extremely sparse unstructured matrix multiplication with e.g.&nbsp;sparsity of one in a billion and one scalar per nonzero. This applies to <a href="https://www.bigdatawire.com/2014/07/17/inside-sibyl-googles-massively-parallel-machine-learning-platform/">Google&rsquo;s Sibyl</a>, which in 2015 was one of Google&rsquo;s largest workloads and consumed several percent of fleetwide CPU. I and many others collectively spent years optimizing these inner loops.</p><p>Second, the <a href="https://www-labs.iro.umontreal.ca/~simul/testu01/tu01.html">BigCrush test suite for random number generators</a> counts duplicates among n-grams of generated numbers to find anomalies.</p><p>In my experience the default for problems like these is hash tables. The surprise to me, once I ran these benchmarks, is that radix sort can beat them by a substantial margin.</p><hr> <h2>Appendix 1: tuning radix sort</h2><p>There&rsquo;s a rich literature and set of libraries for fast radix sort. I primarily relied on and recommend <a href="https://axelle.me/2022/04/19/diverting-lsd-sort/">diverting LSD sort</a>, <a href="https://arxiv.org/pdf/2207.14334">diverting fast radix</a>, and the Rust library <a href="https://docs.rs/voracious_radix_sort/latest/voracious_radix_sort/">voracious_radix_sort</a>. The main characteristics of high-performance implementations like these are listed below:</p><p>Use a <em>diverting</em> radix sort, which only performs a few passes of radix sort before falling back to insertion sort. If <span><span><math><semantics><mrow><mtext>radix</mtext></mrow><annotation>\text{radix}</annotation></semantics></math></span></span> is the number of buckets per pass (e.g., 1024), then after <span><span><math><semantics><mrow><mi>p</mi></mrow><annotation>p</annotation></semantics></math></span></span> passes the array has been sorted into <span><span><math><semantics><mrow><msup><mtext>radix</mtext><mi>p</mi></msup></mrow><annotation>\text{radix}^p</annotation></semantics></math></span></span> buckets, with each bucket still needing sorting. After enough passes the buckets are tiny (average size &lt;1), so you stop doing radix passes and fix up locally with insertion sort. This makes diverting radix sort <span><span><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><msub><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mtext>radix</mtext></msub><mi>n</mi><mo>)</mo></mrow><annotation>O(n \log_{\text{radix}} n)</annotation></semantics></math></span></span>, versus non-diverting radix sort&rsquo;s <span><span><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>&sdot;</mo><mi>w</mi><mo>)</mo></mrow><annotation>O(n \cdot w)</annotation></semantics></math></span></span> where <span><span><math><semantics><mrow><mi>w</mi></mrow><annotation>w</annotation></semantics></math></span></span> is word length. For large keys like uint64 you can typically save more than half the passes.</p><p>Form the histograms for all <span><span><math><semantics><mrow><mi>p</mi></mrow><annotation>p</annotation></semantics></math></span></span> passes of radix sort in a single histogramming sweep, rather than one by one before each pass. This saves bandwidth: you only do <span><span><math><semantics><mrow><mi>p</mi><mo>+</mo><mn>1</mn></mrow><annotation>p+1</annotation></semantics></math></span></span> instead of <span><span><math><semantics><mrow><mn>2</mn><mi>p</mi></mrow><annotation>2p</annotation></semantics></math></span></span> sweeps through memory.</p><p>Beyond the above techniques standard in the literature, I did two other optimizations.</p><p>First, I fused the hashing pass (before the sort) into the histogramming pass (first step of the sort) to save memory bandwidth.</p><p>I fall back to inlined insertion sort rather than a function call into the standard library. Calling into standard library sort, which needs to perform many branches on size to figure out that this is a very small array, adds a lot of overhead for buckets whose average size is 1. Inlined insertion sort finishes very quickly for small arrays.</p><h2>Appendix 2: other memory-bandwidth-efficient sorts</h2><p>We prefer radix sort because it makes <span><span><math><semantics><mrow><msub><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mn>1024</mn></msub><mo>(</mo><mi>n</mi><mo>)</mo></mrow><annotation>\log_{1024}(n)</annotation></semantics></math></span></span> passes through memory rather than quicksort/mergesort&rsquo;s <span><span><math><semantics><mrow><msub><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mn>2</mn></msub><mo>(</mo><mi>n</mi><mo>)</mo></mrow><annotation>\log_{2}(n)</annotation></semantics></math></span></span> passes. Another way to reduce passes is <em>multi-way mergesort</em>: merge <span><span><math><semantics><mrow><mi>k</mi></mrow><annotation>k</annotation></semantics></math></span></span> arrays at once instead of 2, best implemented with a <a href="https://en.wikipedia.org/wiki/K-way_merge_algorithm#Tournament_Tree">tournament tree</a>.</p><p>I implemented and benchmarked this. In my implementation it substantially underperforms quicksort and radix sort. Even though memory bandwidth usage is better, the instruction usage seems to be worse. However, especially here, where the literature appears sparser, it&rsquo;s possible that my implementation was poorly tuned.</p><h2>Appendix 3: tuning hash tables</h2><p>Our tuned hash tables outperform baseline &ldquo;Swiss Table&rdquo; hash tables by up to 3&times; on this workload, largely because we can optimize for our specific case (huge tables, uint64 keys, prioritizing runtime time over memory footprint) whereas Swiss Tables must balance many use cases (strings, small tables, memory footprint).</p><p>Our tuned hash table is different than Swiss Tables in the following ways.</p><p><em>Single table vs.&nbsp;metadata+data.</em> Swiss Tables keep a &ldquo;metadata&rdquo; table (1-byte tags derived from 7-bit hash codes) in addition to the real data table. They probe metadata first (using SIMD instructions to probe 8&ndash;16 keys at once), then they probe the data table on a match. This allows efficient probing even when probe sequences are long, it has the disadvantage of requiring <em>two</em> cache misses per lookup: one for the metadata table, one for the data table. By contrast, we only use a data table: probing uint64 keys is already plenty fast. This allows us to pay just one cache miss per lookup<a href="https://reiner.org#fn5"><sup>5</sup></a>.</p><p>Swiss Table probing is not cacheline-aligned. A probe sequence may start in the middle of a cache line and continue to the next before finishing the current line, wasting bandwidth. Our tuned table also starts probing mid-line, but when we hit the end of the cache line we wrap to the beginning and probe the rest of the cache line before advancing to the next.<a href="https://reiner.org#fn6"><sup>6</sup></a></p><p>Swiss Tables pack their data more densely than our table does: they target a max load factor of 78.5%, whereas we target 50%. This is a pure time-vs-memory tradeoff: Swiss Tables value memory more, we value time more. By using a lower load factor, we keep probe distances shorter, and increase the probability that a lookup can succeed with just one cache miss.</p><p>We use <a href="https://doc.rust-lang.org/std/intrinsics/fn.prefetch_read_data.html">prefetching instructions</a> when accessing the table; Rust&rsquo;s Swiss Tables implementation does not offer such functionality. Prefetching for hash tables is advanced user functionality, because users need to restructure their loops (and typically split into a loop prologue, loop body, loop epilogue) to support prefetching. Compared to any other API offered by Rust&rsquo;s HashSet, this functionality is hugely more niche, and perhaps appropriately is not included in the standard library. That said, if you are willing to put in the effort to restructure your loops and tune them, prefetching can offer a huge speedup.</p><p>Unlike the above differences, the hash function itself is the same between our tuned hash table and Swiss Tables: we use MulSwapMul in both. This is similar speed to <code>foldhash::fast</code> and hugely faster than Rust&rsquo;s default <code>SipHash</code>.</p><p>All of these differences together add up to a ~3&times; speedup over Swiss Tables on this workload:</p><table> <thead> <tr> <th>Data size</th> <th>Swiss table</th> <th>Tuned hash table</th> </tr> </thead> <tbody> <tr> <td>8 KiB</td> <td>3.8&#8239;&micro;s</td> <td>1.6&#8239;&micro;s</td> </tr> <tr> <td>256 KiB</td> <td>219&#8239;&micro;s</td> <td>193&#8239;&micro;s</td> </tr> <tr> <td>8 MiB</td> <td>8.1&#8239;ms</td> <td>7.1&#8239;ms</td> </tr> <tr> <td>256 MiB</td> <td>875&#8239;ms</td> <td>269&#8239;ms</td> </tr> <tr> <td>2 GiB</td> <td>7.6&#8239;s</td> <td>2.6&#8239;s</td> </tr> </tbody> </table> <h2>Appendix 4: parallelism</h2><p>In many high-performance applications we want parallelism. Both radix sort and hash tables parallelize efficiently: for radix sort, parallelize within each pass and synchronize at pass boundaries; for hash tables, use fine-grained locking such as a lock per cacheline inside the table itself.</p><p>From a theoretical analysis I expect both radix sort and hash tables to parallelize very well, and the advantage of radix sort to be sustained even in the parallel context. I benchmarked a few of the top Rust libraries for parallel radix sort and parallel hash tables, but I didn&rsquo;t build any custom tuned versions myself.</p><p>For radix sort, I found <a href="https://docs.rs/voracious_radix_sort/latest/voracious_radix_sort/">voracious_radix_sort</a> to have excellent parallel performance; it starts beating the best single-threaded implementation once the data exceeded 8MiB, and I suspect it could be improved further by porting over some of the single-threaded tuning work I did.</p><table> <thead> <tr> <th>Data size</th> <th>Best sequential sorting</th> <th>Parallel sorting (8 cores)</th> </tr> </thead> <tbody> <tr> <td>8 KiB</td> <td>6.5&#8239;&micro;s</td> <td>109&#8239;&micro;s</td> </tr> <tr> <td>256 KiB</td> <td><strong>92&#8239;&micro;s</strong></td> <td>386&#8239;&micro;s</td> </tr> <tr> <td>8 MiB</td> <td><strong>3.9&#8239;ms</strong></td> <td>4.5&#8239;ms</td> </tr> <tr> <td>256 MiB</td> <td>185&#8239;ms</td> <td><strong>61&#8239;ms</strong></td> </tr> <tr> <td>2 GiB</td> <td>1.9&#8239;s</td> <td><strong>440&#8239;ms</strong></td> </tr> </tbody> </table><p>For hash tables, I didn&rsquo;t find an implementation that outperforms my best single-threaded implementation. This surprises me: in other contexts I&rsquo;ve seen near-linear parallel speedups with a lock-per&ndash;cache-line design, and I suspect it&rsquo;s possible here too. I didn&rsquo;t spend long tuning this, and it&rsquo;s possible I was using the existing libraries wrong or outside of their target workload profile.</p><section> <hr> <ol> <li><p>For batch algorithms with balanced lookups and inserts.<a href="https://reiner.org#fnref1">&#8617;&#65038;</a></p></li> <li><p>I also ran on a large AMD Zen 4 machine (AMD EPYC 9B14), and got similar results:</p><table> <colgroup><col> <col> <col> <col> <col> </colgroup><thead> <tr> <th>Data size</th> <th>Baseline hash table</th> <th>Baseline sorting</th> <th>Tuned hash table</th> <th>Tuned sorting</th> </tr> </thead> <tbody> <tr> <td>8 KiB</td> <td>3.6&#8239;&micro;s</td> <td>6.1&#8239;&micro;s</td> <td><strong>2.7&#8239;&micro;s</strong></td> <td>5.4&#8239;&micro;s</td> </tr> <tr> <td>256 KiB</td> <td>172&#8239;&micro;s</td> <td>302&#8239;&micro;s</td> <td><strong>98.2&#8239;&micro;s</strong></td> <td>123&#8239;&micro;s</td> </tr> <tr> <td>8 MiB</td> <td>12.5&#8239;ms</td> <td>14.6&#8239;ms</td> <td>9.0&#8239;ms</td> <td><strong>4.2&#8239;ms</strong></td> </tr> <tr> <td>256 MiB</td> <td>1.2&#8239;s</td> <td>585&#8239;ms</td> <td>378&#8239;ms</td> <td><strong>186&#8239;ms</strong></td> </tr> <tr> <td>2 GiB</td> <td>10.3&#8239;s</td> <td>5.2&#8239;s</td> <td>3.1&#8239;s</td> <td><strong>1.5&#8239;s</strong></td> </tr> </tbody> </table> <a href="https://reiner.org#fnref2">&#8617;&#65038;</a></li> <li><p>Traditional radix sorts wouldn&rsquo;t take 3 passes, they&rsquo;d take 8: one per byte of an 8-byte key. That&rsquo;s wasteful: if you&rsquo;re processing only <span><span><math><semantics><mrow><msup><mn>2</mn><mn>32</mn></msup></mrow><annotation>2^{32}</annotation></semantics></math></span></span> elements, then after 4 passes the array is almost fully sorted (each bucket has, on average, one element), so that&rsquo;s a good time to fall back to a simpler, cache-local algorithm such as insertion sort. The best form of this approach seems to be <a href="https://axelle.me/2022/04/19/diverting-lsd-sort/">Diverting LSD radix sort</a>.<a href="https://reiner.org#fnref3">&#8617;&#65038;</a></p></li> <li><p>In one pass, radix sort maintains 1024 write pointers (one per bucket). We write a uint64 to one of the write pointers, then advance the write pointer by 8 bytes. To hit ideal cache performance, you want the current cache line for each pointer to be kept in L1 cache (&ldquo;hot&rdquo;). Then, when the write pointer advances to the next cache line, we want the previous cache line to be flushed to main memory and then keep the next cache line hot. This requires the CPU to keep 1024 specific cache lines hot, and then quickly evicted and replaced with different cachelines when the pointers advance beyond the current line. Current CPUs typically have capacity for around 2048 cache lines in L1 cache, so in principle this is possible. Unfortunately, CPU caches are hardware-managed rather than software-managed, so we can&rsquo;t just <em>tell</em> the CPU to do this: instead, we rely on its cache eviction policy heuristics to do the job. Sometimes they will make mistakes, causing us to waste memory traffic; the fact that our cacheline demand is a large fraction of the total CPU cache size makes mistakes more likely.</p><p>For comparison, on hash tables there&rsquo;s much less guesswork required from the cache system. To hit the ideal cache system performance, the primary goal is to ensure that many cache misses can run in parallel so that we&rsquo;re cache-miss-throughput-bound (memory bandwidth) rather than cache-miss-latency-bound (memory latency). We achieve this by issuing cacheline prefetch instructions 64 iterations in advance of performing the lookup, so that we can have 64 cacheline fetches outstanding in parallel. This comes much closer to ideal than for radix sort because (a) there&rsquo;s CPU prefetch instructions that let us <em>tell</em> the cache system what we want to happen and (b) we only need 64 cache lines hot (out of an available 2048) rather than 1024, so mistakes are much less likely.<a href="https://reiner.org#fnref4">&#8617;&#65038;</a></p></li> <li><p>Meta&rsquo;s <a href="https://engineering.fb.com/2019/04/25/developer-tools/f14/">F14 table</a> places 14 metadata slots and 14 data slots side by side on the same pair of cache lines, achieving both locality <em>and</em> SIMD probing. At that sizing it still needs two (adjacent) misses; a 7-slot variant would be ideal for uint64 keys.<a href="https://reiner.org#fnref5">&#8617;&#65038;</a></p></li> <li><p>A simpler way to make probing cacheline-aligned would be to <em>start</em> all probe sequences at the beginning of a cache line. However, this increases average probe length: two different keys are more likely to start at the same probe location, since there are fewer probe sequence starting locations available.<a href="https://reiner.org#fnref6">&#8617;&#65038;</a></p></li> </ol> </section> </div></section>]]></description><pubDate>Thu, 11 Sep 2025 17:12:47 +0530</pubDate></item><item><link>https://eclipse.dev/eclipse/markdown/?f=news/4.37/index.md</link><title>Eclipse 4.37 Released (eclipse.dev)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1ne4hb2/eclipse_437_released/</guid><comments>https://www.reddit.com/r/programming/comments/1ne4hb2/eclipse_437_released/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/programming/comments/1ne4hb2/eclipse_437_released/'>Post permalink</a></p></section>]]></description><pubDate>Thu, 11 Sep 2025 15:13:53 +0530</pubDate></item><item><link>https://gizmodo.com/microsoft-goes-back-to-basic-open-sources-bill-gates-code-2000654010</link><title>Microsoft Goes Back to BASIC, Open-Sources Bill Gates' Code (gizmodo.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1ndpzfz/microsoft_goes_back_to_basic_opensources_bill/</guid><comments>https://www.reddit.com/r/programming/comments/1ndpzfz/microsoft_goes_back_to_basic_opensources_bill/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/programming/comments/1ndpzfz/microsoft_goes_back_to_basic_opensources_bill/'>Post permalink</a></p></section><section class='preview-image'><img src='https://gizmodo.com/app/uploads/2020/08/z1olh9qw8p81mdbspql3.jpg' /></section><section class='parsed-content'><div><p>In the era of vibe coding, when even professionals are pawning off their programming work on AI tools, Microsoft is throwing it all the way back to the language that launched a billion devices. On Wednesday, the company <a href="https://opensource.microsoft.com/blog/2025/09/03/microsoft-open-source-historic-6502-basic/">announced</a> that it would make the source code for Microsoft BASIC for the 6502 Version 1.1 publicly available and open-source. The code is now <a href="https://github.com/microsoft/BASIC-M6502">uploaded to GitHub</a> under an MIT license (with a cheeky commit time stamp of &ldquo;48 years ago&rdquo;).</p><p>Microsoft called the code&mdash;written by the company&rsquo;s founder, Bill Gates, and its second-ever employee, Ric Weiland&mdash;&rdquo;one of the most historically significant pieces of software from the early personal computer era.&rdquo; It&rsquo;s pretty simple, clocking in at just 6,955 lines of assembly language, but that simplicity was key to its becoming so foundational to just about everything.</p><p>The <a href="https://spectrum.ieee.org/chip-hall-of-fame-mos-technology-6502-microprocessor">MOS 6502 processor</a>, which ran the code, was inexpensive and accessible compared to contemporary alternatives, and variations of the chip would eventually find their way into the Atari 2600, Nintendo Entertainment System, and Commodore computers. In fact, the story goes that Microsoft licensed its 6502 BASIC to Commodore for a flat fee of $25,000, which turned out to be a great deal for Commodore, which shipped millions of computers running the code.</p><p>Per Microsoft, the company&rsquo;s first product was a BASIC interpreter for the Intel 8080, which was written by Gates and co-founder Paul Allen. The version the company dropped on GitHub is actually an updated version of BASIC, which contains bug fixes implemented by Gates and Commodore engineer John Feagans. While it&rsquo;s called 1.1 on GitHub, Microsoft said it initially shipped as BASIC V2.</p><p>It&rsquo;s kind of a big deal for Microsoft to finally open-source the entirety of the code, which was previously only available in bits and pieces. Without Microsoft&rsquo;s official blessing to make this code public, it was possible that the original documentation, as well as the legal permission needed to use the code, would have been lost to history. Now it&rsquo;s possible for the code to be preserved, played with, and better understood.</p><p>As <a href="https://arstechnica.com/gadgets/2025/09/microsoft-open-sources-bill-gates-6502-basic-from-1978/">Ars Technica points out</a>, the assembly code can&rsquo;t be run on modern devices directly, but is still functional in emulators and field-programmable gate array (FPGA) implementations that allow researchers and programmers to explore old code and mine it for everything from just understanding how it works to understanding how programmers of the past approached efficient design practices.</p><p>BASIC 5502 joins <a href="https://github.com/microsoft/GW-BASIC">GW-BASIC</a>, <a href="https://github.com/microsoft/MS-DOS">MS-DOS</a>, and the <a href="https://github.com/option8/Altair-BASIC">Altair BASIC</a> on the list of code that Microsoft has open-sourced in recent years.</p></div></section>]]></description><pubDate>Thu, 11 Sep 2025 02:23:02 +0530</pubDate></item><item><link>https://buttondown.com/hillelwayne/archive/many-hard-leetcode-problems-are-easy-constraint/</link><title>Many Hard Leetcode Problems are Easy Constraint Problems (buttondown.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1ndjw6y/many_hard_leetcode_problems_are_easy_constraint/</guid><comments>https://www.reddit.com/r/programming/comments/1ndjw6y/many_hard_leetcode_problems_are_easy_constraint/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 7 min | <a href='https://www.reddit.com/r/programming/comments/1ndjw6y/many_hard_leetcode_problems_are_easy_constraint/'>Post permalink</a></p></section><section class='preview-image'><img src='https://assets.buttondown.email/images/63337f78-7138-4b21-87a0-917c0c5b1706.jpg?w=960&fit=max' /></section><section class='parsed-content'><div><date> September 10, 2025 </date> <h2> Use the right tool for the job. </h2><p>In my first interview out of college I was asked the change counter problem:</p><blockquote><p>Given a set of coin denominations, find the minimum number of coins required to make change for a given number. IE for USA coinage and 37 cents, the minimum number is four (quarter, dime, 2 pennies).</p></blockquote><p>I implemented the simple greedy algorithm and immediately fell into the trap of the question: the greedy algorithm only works for "well-behaved" denominations. If the coin values were <code>[10, 9, 1]</code>, then making 37 cents would take 10 coins in the greedy algorithm but only 4 coins optimally (<code>10+9+9+9</code>). The "smart" answer is to use a dynamic programming algorithm, which I didn't know how to do. So I failed the interview.</p><p>But you only need dynamic programming if you're writing your own algorithm. It's really easy if you throw it into a constraint solver like <a href="https://www.minizinc.org/">MiniZinc</a> and call it a day. </p><div><pre><code>int: total; array[int] of int: values = [10, 9, 1]; array[index_set(values)] of var 0..: coins; constraint sum (c in index_set(coins)) (coins[c] * values[c]) == total; solve minimize sum(coins); </code></pre></div><p>You can try this online <a href="https://play.minizinc.dev/">here</a>. It'll give you a prompt to put in <code>total</code> and then give you successively-better solutions:</p><div><pre><span></span><code>coins = [0, 0, 37]; ---------- coins = [0, 1, 28]; ---------- coins = [0, 2, 19]; ---------- coins = [0, 3, 10]; ---------- coins = [0, 4, 1]; ---------- coins = [1, 3, 0]; ---------- </code></pre></div><p>Lots of similar interview questions are this kind of mathematical optimization problem, where we have to find the maximum or minimum of a function corresponding to constraints. They're hard in programming languages because programming languages are too low-level. They are also exactly the problems that constraint solvers were designed to solve. Hard leetcode problems are easy constraint problems.<sup><a href="https://buttondown.com#fn:leetcode">1</a></sup>Here I'm using MiniZinc, but you could just as easily use Z3 or OR-Tools or whatever your favorite generalized solver is.</p><h3>More examples</h3><p>This was a question in a different interview (which I thankfully passed):</p><blockquote><p>Given a list of stock prices through the day, find maximum profit you can get by buying one stock and selling one stock later.</p></blockquote><p>It's easy to do in O(n^2) time, or if you are clever, you can do it in O(n). Or you could be not clever at all and just write it as a constraint problem:</p><div><pre><code>array[int] of int: prices = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8]; var int: buy; var int: sell; var int: profit = prices[sell] - prices[buy]; constraint sell &gt; buy; constraint profit &gt; 0; solve maximize profit; </code></pre></div><p>Reminder, link to trying it online <a href="https://play.minizinc.dev/">here</a>. While working at that job, one interview question we tested out was:</p><blockquote><p>Given a list, determine if three numbers in that list can be added or subtracted to give 0? </p></blockquote><p>This is a satisfaction problem, not a constraint problem: we don't need the "best answer", any answer will do. We eventually decided against it for being too tricky for the engineers we were targeting. But it's not tricky in a solver; </p><div><pre><span></span><code>include "globals.mzn"; array[int] of int: numbers = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8]; array[index_set(numbers)] of var {0, -1, 1}: choices; constraint sum(n in index_set(numbers)) (numbers[n] * choices[n]) = 0; constraint count(choices, -1) + count(choices, 1) = 3; solve satisfy; </code></pre></div><p>Okay, one last one, a problem I saw last year at <a href="https://chicagopython.github.io/algosig/">Chipy AlgoSIG</a>. Basically they pick some leetcode problems and we all do them. I failed to solve <a href="https://leetcode.com/problems/largest-rectangle-in-histogram/description/">this one</a>:</p><blockquote><p>Given an array of integers heights representing the histogram's bar height where the width of each bar is 1, return the area of the largest rectangle in the histogram.</p></blockquote><p>The "proper" solution is a tricky thing involving tracking lots of bookkeeping states, which you can completely bypass by expressing it as constraints:</p><div><pre><code>array[int] of int: numbers = [2,1,5,6,2,3]; var 1..length(numbers): x; var 1..length(numbers): dx; var 1..: y; constraint x + dx &lt;= length(numbers); constraint forall (i in x..(x+dx)) (y &lt;= numbers[i]); var int: area = (dx+1)*y; solve maximize area; output ["(\(x)-&gt;\(x+dx))*\(y) = \(area)"] </code></pre></div><p>There's even a way to <a href="https://docs.minizinc.dev/en/2.9.3/visualisation.html">automatically visualize the solution</a> (using <code>vis_geost_2d</code>), but I didn't feel like figuring it out in time for the newsletter.</p><h3>Is this better?</h3><p>Now if I actually brought these questions to an interview the interviewee could ruin my day by asking "what's the runtime complexity?" Constraint solvers runtimes are unpredictable and almost always than an ideal bespoke algorithm because they are more expressive, in what I refer to as the <a href="https://buttondown.com/hillelwayne/archive/the-capability-tractability-tradeoff/">capability/tractability tradeoff</a>. But even so, they'll do way better than a <em>bad</em> bespoke algorithm, and I'm not experienced enough in handwriting algorithms to consistently beat a solver.</p><p>The real advantage of solvers, though, is how well they handle new constraints. Take the stock picking problem above. I can write an O(n&sup2;) algorithm in a few minutes and the O(n) algorithm if you give me some time to think. Now change the problem to</p><blockquote><p>Maximize the profit by buying and selling up to <code>max_sales</code> stocks, but you can only buy or sell one stock at a given time and you can only hold up to <code>max_hold</code> stocks at a time?</p></blockquote><p>That's a way harder problem to write even an inefficient algorithm for! While the constraint problem is only a tiny bit more complicated:</p><div><pre><span></span><code>include "globals.mzn"; int: max_sales = 3; int: max_hold = 2; array[int] of int: prices = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8]; array [1..max_sales] of var int: buy; array [1..max_sales] of var int: sell; array [index_set(prices)] of var 0..max_hold: stocks_held; var int: profit = sum(s in 1..max_sales) (prices[sell[s]] - prices[buy[s]]); constraint forall (s in 1..max_sales) (sell[s] &gt; buy[s]); constraint profit &gt; 0; constraint forall(i in index_set(prices)) (stocks_held[i] = (count(s in 1..max_sales) (buy[s] &lt;= i) - count(s in 1..max_sales) (sell[s] &lt;= i))); constraint alldifferent(buy ++ sell); solve maximize profit; output ["buy at \(buy)\n", "sell at \(sell)\n", "for \(profit)"]; </code></pre></div><p>Most constraint solving examples online are puzzles, like <a href="https://docs.minizinc.dev/en/stable/modelling2.html#ex-sudoku">Sudoku</a> or "<a href="https://docs.minizinc.dev/en/stable/modelling2.html#ex-smm">SEND + MORE = MONEY</a>". Solving leetcode problems would be a more interesting demonstration. And you get more interesting opportunities to teach optimizations, like symmetry breaking.</p><p><em>If you're reading this on the web, you can subscribe <a href="https://buttondown.com/hillelwayne">here</a>. Updates are once a week. My main website is <a href="https://www.hillelwayne.com">here</a>.</em></p><p><em>My new book, </em>Logic for Programmers<em>, is now in early access! Get it <a href="https://leanpub.com/logic/">here</a>.</em></p></div></section>]]></description><pubDate>Wed, 10 Sep 2025 22:31:10 +0530</pubDate></item><item><link>https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-10/</link><title>Performance Improvements in .NET 10 (devblogs.microsoft.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1ndemk4/performance_improvements_in_net_10/</guid><comments>https://www.reddit.com/r/programming/comments/1ndemk4/performance_improvements_in_net_10/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 354 min | <a href='https://www.reddit.com/r/programming/comments/1ndemk4/performance_improvements_in_net_10/'>Post permalink</a></p></section><section class='preview-image'><img src='https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2025/09/net10perf.jpg' /></section><section class='parsed-content'><article><div><p>My kids <em>love</em> &ldquo;Frozen&rdquo;. They can sing every word, re-enact every scene, and provide detailed notes on the proper sparkle of Elsa&rsquo;s ice dress. I&rsquo;ve seen the movie more times than I can recount, to the point where, if you&rsquo;ve seen me do any live coding, you&rsquo;ve probably seen my subconscious incorporate an Arendelle reference or two. After so many viewings, I began paying closer attention to the details, like how at the very beginning of the film the ice harvesters are singing a song that subtly foreshadows the story&rsquo;s central conflicts, the characters&rsquo; journeys, and even the key to resolving the climax. I&rsquo;m slightly ashamed to admit I didn&rsquo;t comprehend this connection until viewing number ten or so, at which point I also realized I had no idea if this ice harvesting was actually &ldquo;a thing&rdquo; or if it was just a clever vehicle for Disney to spin a yarn. Turns out, as I subsequently researched, it&rsquo;s quite real.</p><p>In the 19th century, before refrigeration, ice was an incredibly valuable commodity. Winters in the northern United States turned ponds and lakes into seasonal gold mines. The most successful operations ran with precision: workers cleared snow from the surface so the ice would grow thicker and stronger, and they scored the surface into perfect rectangles using horse-drawn plows, turning the lake into a frozen checkerboard. Once the grid was cut, teams with long saws worked to free uniform blocks weighing several hundred pounds each. These blocks were floated along channels of open water toward the shore, at which point men with poles levered the blocks up ramps and hauled them into storage. Basically, what the movie shows.</p><p>The storage itself was an art. Massive wooden ice houses, sometimes holding tens of thousands of tons, were lined with insulation, typically straw. Done well, this insulation could keep the ice solid for months, even through summer heat. Done poorly, you would open the doors to slush. And for those moving ice over long distances, typically by ship, every degree, every crack in the insulation, every extra day in transit meant more melting and more loss.</p><p>Enter Frederic Tudor, the &ldquo;Ice King&rdquo; of Boston. He was obsessed with systemic efficiency. Where competitors saw unavoidable loss, Tudor saw a solvable problem. After experimenting with different insulators, he leaned on cheap sawdust, a lumber mill byproduct that outperformed straw, packing it densely around the ice to cut melt losses significantly. For harvesting efficiency, his operations adopted Nathaniel Jarvis Wyeth&rsquo;s grid-scoring system, which produced uniform blocks that could be packed tightly, minimizing air gaps that would otherwise increase exposure in a ship&rsquo;s hold. And to shorten the critical time between shore and ship, Tudor built out port infrastructure and depots near docks, allowing ships to load and unload much faster. Each change, from tools to ice house design to logistics, amplified the last, turning a risky local harvest into a reliable global trade. With Tudor&rsquo;s enhancements, he had solid ice arriving in places like Havana, Rio de Janeiro, and even Calcutta (a voyage of four months in the 1830s). His performance gains allowed the product to survive journeys that were previously unthinkable.</p><p>What made Tudor&rsquo;s ice last halfway around the world wasn&rsquo;t one big idea. It was a plethora of small improvements, each multiplying the effect of the last. In software development, the same principle holds: big leaps forward in performance rarely come from a single sweeping change, rather from hundreds or thousands of targeted optimizations that compound into something transformative. .NET 10&rsquo;s performance story isn&rsquo;t about one Disney-esque magical idea; it&rsquo;s about carefully shaving off nanoseconds here and tens of bytes there, streamlining operations that are executed trillions of times.</p><p>In the rest of this post, just as we did in Performance Improvements in <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-9/">.NET 9</a>, <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-8/">.NET 8</a>, <a href="https://devblogs.microsoft.com/dotnet/performance_improvements_in_net_7/">.NET 7</a>, <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-6">.NET 6</a>, <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-5">.NET 5</a>, <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-core-3-0">.NET Core 3.0</a>, <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-core-2-1">.NET Core 2.1</a>, and <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-core">.NET Core 2.0</a>, we&rsquo;ll dig into hundreds of the small but meaningful and compounding performance improvements since .NET 9 that make up .NET 10&rsquo;s story (if you instead stay on LTS releases and thus are upgrading from .NET 8 instead of from .NET 9, you&rsquo;ll see even more improvements based on the aggregation from all the <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-9/">improvements in .NET 9</a> as well). So, without further ado, go grab a cup of your favorite hot beverage (or, given my intro, maybe something a bit more frosty), sit back, relax, and &ldquo;Let It Go&rdquo;!</p><p>Or, hmm, maybe, let&rsquo;s push performance &ldquo;Into the Unknown&rdquo;?</p><p>Let .NET 10 performance &ldquo;Show Yourself&rdquo;?</p><p>&ldquo;Do You Want To Build a <del>Snowman</del> Fast Service?&rdquo;</p><p>I&rsquo;ll see myself out.</p><h2>Benchmarking Setup</h2><p>As in previous posts, this tour is chock full of micro-benchmarks intended to showcase various performance improvements. Most of these benchmarks are implemented using <a href="https://www.nuget.org/packages/BenchmarkDotNet/0.15.2">BenchmarkDotNet 0.15.2</a>, with a simple setup for each.</p><p>To follow along, make sure you have <a href="https://dotnet.microsoft.com/download/dotnet/9.0">.NET 9</a> and <a href="https://dotnet.microsoft.com/download/dotnet/10.0">.NET 10</a> installed, as most of the benchmarks compare the same test running on each. Then, create a new C# project in a new <code>benchmarks</code> directory:</p><pre><code>dotnet new console -o benchmarks cd benchmarks</code></pre><p>That will produce two files in the <code>benchmarks</code> directory: <code>benchmarks.csproj</code>, which is the project file with information about how the application should be compiled, and <code>Program.cs</code>, which contains the code for the application. Finally, replace everything in <code>benchmarks.csproj</code> with this:</p><pre><code><project><propertygroup><outputtype>Exe</outputtype> <targetframeworks>net10.0;net9.0</targetframeworks> <langversion>Preview</langversion> <implicitusings>enable</implicitusings> <nullable>enable</nullable> <servergarbagecollection>true</servergarbagecollection> </propertygroup><itemgroup><packagereference></packagereference> </itemgroup></project></code></pre><p>With that, we&rsquo;re good to go. Unless otherwise noted, I&rsquo;ve tried to make each benchmark standalone; just copy/paste its whole contents into the Program.cs file, overwriting everything that&rsquo;s there, and then run the benchmarks. Each test includes at its top a comment for the <code>dotnet</code> command to use to run the benchmark. It&rsquo;s typically something like this:</p><pre><code>dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0</code></pre><p>which will run the benchmark in release on both .NET 9 and .NET 10 and show the compared results. The other common variation, used when the benchmark should only be run on .NET 10 (typically because it&rsquo;s comparing two approaches rather than comparing one thing on two versions), is the following:</p><pre><code>dotnet run -c Release -f net10.0 --filter "*"</code></pre><p>Throughout the post, I&rsquo;ve shown many benchmarks and the results I received from running them. Unless otherwise stated (e.g. because I&rsquo;m demonstrating an OS-specific improvement), the results shown are from running them on Linux (Ubuntu 24.04.1) on an x64 processor.</p><pre><code>BenchmarkDotNet v0.15.2, Linux Ubuntu 24.04.1 LTS (Noble Numbat) 11th Gen Intel Core i9-11950H 2.60GHz, 1 CPU, 16 logical and 8 physical cores .NET SDK 10.0.100-rc.1.25451.107 [Host] : .NET 9.0.9 (9.0.925.41916), X64 RyuJIT AVX-512F+CD+BW+DQ+VL+VBMI</code></pre><p>As always, a quick disclaimer: these are micro-benchmarks, timing operations so short you&rsquo;d miss them by blinking (but when such operations run millions of times, the savings really add up). The exact numbers you get will depend on your hardware, your operating system, what else your machine is juggling at the moment, how much coffee you&rsquo;ve had since breakfast, and perhaps whether Mercury is in retrograde. In other words, don&rsquo;t expect your results to match mine exactly, but I&rsquo;ve picked tests that should still be reasonably reproducible in the real world.</p><p>Now, let&rsquo;s start at the bottom of the stack. Code generation.</p><h2>JIT</h2><p>Among all areas of .NET, the Just-In-Time (JIT) compiler stands out as one of the most impactful. Every .NET application, whether a small console tool or a large-scale enterprise service, ultimately relies on the JIT to turn intermediate language (IL) code into optimized machine code. Any enhancement to the JIT&rsquo;s generated code quality has a ripple effect, improving performance across the entire ecosystem without requiring developers to change any of their own code or even recompile their C#. And with .NET 10, there&rsquo;s no shortage of these improvements.</p><h3>Deabstraction</h3><p>As with many languages, .NET historically has had an &ldquo;abstraction penalty,&rdquo; those extra allocations and indirections that can occur when using high-level language features like interfaces, iterators, and delegates. Each year, the JIT gets better and better at optimizing away layers of abstraction, so that developers get to write simple code and still get great performance. .NET 10 continues this tradition. The result is that idiomatic C# (using interfaces, <code>foreach</code> loops, lambdas, etc.) runs even closer to the raw speed of meticulously crafted and hand-tuned code.</p><h4>Object Stack Allocation</h4><p>One of the most exciting areas of deabstraction progress in .NET 10 is the expanded use of escape analysis to enable stack allocation of objects. Escape analysis is a compiler technique to determine whether an object allocated in a method escapes that method, meaning determining whether that object is reachable after the method returns (for example, by being stored in a field or returned to the caller) or used in some way that the runtime can&rsquo;t track within the method (like passed to an unknown callee). If the compiler can prove an object doesn&rsquo;t escape, then that object&rsquo;s lifetime is bounded by the method, and it can be allocated on the stack instead of on the heap. Stack allocation is much cheaper (just pointer bumping for allocation and automatic freeing when the method exits) and reduces GC pressure because, well, the object doesn&rsquo;t need to be tracked by the GC. .NET 9 had already introduced some limited escape analysis and stack allocation support; .NET 10 takes this significantly further.</p><p><a href="https://github.com/dotnet/runtime/pull/115172">dotnet/runtime#115172</a> teaches the JIT how to perform escape analysis related to delegates, and in particular that a delegate&rsquo;s <code>Invoke</code> method (which is implemented by the runtime) does not stash away the <code>this</code> reference. Then if escape analysis can prove that the delegate&rsquo;s object reference is something that otherwise hasn&rsquo;t escaped, the delegate can effectively evaporate. Consider this benchmark:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "y")] public partial class Tests { [Benchmark] [Arguments(42)] public int Sum(int y) { Func<int> addY = x =&gt; x + y; return DoubleResult(addY, y); } private int DoubleResult(Func<int> func, int arg) { int result = func(arg); return result + result; } }</int></int></code></pre><p>If we just run this benchmark and compare .NET 9 and .NET 10, we can immediately tell something interesting is happening.</p><table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Code Size</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>Sum</td> <td>.NET 9.0</td> <td>19.530 ns</td> <td>1.00</td> <td>118 B</td> <td>88 B</td> <td>1.00</td> </tr> <tr> <td>Sum</td> <td>.NET 10.0</td> <td>6.685 ns</td> <td>0.34</td> <td>32 B</td> <td>24 B</td> <td>0.27</td> </tr> </tbody> </table><p>The C# code for <code>Sum</code> belies complicated code generation by the C# compiler. It needs to create a <code>Func<int></int></code>, which is &ldquo;closing over&rdquo; the <code>y</code> &ldquo;local&rdquo;. That means the compiler needs to &ldquo;lift&rdquo; <code>y</code> to no longer be an actual local, and instead live as a field on an object; the delegate can then point to a method on that object, giving it access to <code>y</code>. This is approximately what the IL generated by the C# compiler looks like when decompiled to C#:</p><pre><code>public int Sum(int y) { &lt;&gt;c__DisplayClass0_0 c = new(); c.y = y; Func<int> func = new(c.<sum>b__0); return DoubleResult(func, c.y); } private sealed class &lt;&gt;c__DisplayClass0_0 { public int y; internal int <sum>b__0(int x) =&gt; x + y; }</sum></sum></int></code></pre><p>From that, we can see the closure is resulting in two allocations: an allocation for the &ldquo;display class&rdquo; (what the C# compiler calls these closure types) and an allocation for the delegate that points to the <code><sum>b__0</sum></code> method on that display class instance. That&rsquo;s what&rsquo;s accounting for the <code>88</code> bytes of allocation in the .NET 9 results: the display class is 24 bytes, and the delegate is 64 bytes. In the .NET 10 version, though, we only see a 24 byte allocation; that&rsquo;s because the JIT has successfully elided the delegate allocation. Here is the resulting assembly code:</p><pre><code>; .NET 9 ; Tests.Sum(Int32) push rbp push r15 push rbx lea rbp,[rsp+10] mov ebx,esi mov rdi,offset MT_Tests+&lt;&gt;c__DisplayClass0_0 call CORINFO_HELP_NEWSFAST mov r15,rax mov [r15+8],ebx mov rdi,offset MT_System.Func<system.int32> call CORINFO_HELP_NEWSFAST mov rbx,rax lea rdi,[rbx+8] mov rsi,r15 call CORINFO_HELP_ASSIGN_REF mov rax,offset Tests+&lt;&gt;c__DisplayClass0_0.<sum>b__0(Int32) mov [rbx+18],rax mov esi,[r15+8] cmp [rbx+18],rax jne short M00_L01 mov rax,[rbx+8] add esi,[rax+8] mov eax,esi M00_L00: add eax,eax pop rbx pop r15 pop rbp ret M00_L01: mov rdi,[rbx+8] call qword ptr [rbx+18] jmp short M00_L00 ; Total bytes of code 112 ; .NET 10 ; Tests.Sum(Int32) push rbx mov ebx,esi mov rdi,offset MT_Tests+&lt;&gt;c__DisplayClass0_0 call CORINFO_HELP_NEWSFAST mov [rax+8],ebx mov eax,[rax+8] mov ecx,eax add eax,ecx add eax,eax pop rbx ret ; Total bytes of code 32</sum></system.int32></code></pre><p>In both .NET 9 and .NET 10, the JIT successfully inlined <code>DoubleResult</code>, such that the delegate doesn&rsquo;t escape, but then in .NET 10, it&rsquo;s able to stack allocate it. Woo hoo! There&rsquo;s obviously still future opportunity here, as the JIT doesn&rsquo;t elide the allocation of the closure object, but that should be addressable with some more effort, hopefully in the near future.</p><p><a href="https://github.com/dotnet/runtime/pull/104906">dotnet/runtime#104906</a> from <a href="https://github.com/hez2010">@hez2010</a> and <a href="https://github.com/dotnet/runtime/pull/112250">dotnet/runtime#112250</a> extend this kind of analysis and stack allocation to arrays. How many times have you written code like this?</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Runtime.CompilerServices; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { [Benchmark] public void Test() { Process(new string[] { "a", "b", "c" }); static void Process(string[] inputs) { foreach (string input in inputs) { Use(input); } [MethodImpl(MethodImplOptions.NoInlining)] static void Use(string input) { } } } }</code></pre><p>Some method I want to call accepts an array of inputs and does something for each input. I need to allocate an array to pass my inputs in, either explicitly, or maybe implicitly due to using <code>params</code> or a collection expression. Ideally moving forward there would be an overload of such a <code>Process</code> method that accepted a <code>ReadOnlySpan<string></string></code> instead of a <code>string[]</code>, and I could then avoid the allocation by construction. But for all of these cases where I&rsquo;m forced to create an array, .NET 10 comes to the rescue.</p><table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>Test</td> <td>.NET 9.0</td> <td>11.580 ns</td> <td>1.00</td> <td>48 B</td> <td>1.00</td> </tr> <tr> <td>Test</td> <td>.NET 10.0</td> <td>3.960 ns</td> <td>0.34</td> <td>&ndash;</td> <td>0.00</td> </tr> </tbody> </table><p>The JIT was able to inline <code>Process</code>, see that the array never leaves the frame, and stack allocate it.</p><p>Of course, now that we&rsquo;re able to stack allocate arrays, we also want to be able to deal with a common way those arrays are used: via spans. <a href="https://github.com/dotnet/runtime/pull/113977">dotnet/runtime#113977</a> and <a href="https://github.com/dotnet/runtime/pull/116124">dotnet/runtime#116124</a> teach escape analysis to be able to reason about the fields in structs, which includes <code>Span<t></t></code>, as it&rsquo;s &ldquo;just&rdquo; a struct that stores a <code>ref T</code> field and an <code>int</code> length field.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Runtime.CompilerServices; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private byte[] _buffer = new byte[3]; [Benchmark] public void Test() =&gt; Copy3Bytes(0x12345678, _buffer); [MethodImpl(MethodImplOptions.NoInlining)] private static void Copy3Bytes(int value, Span<byte> dest) =&gt; BitConverter.GetBytes(value).AsSpan(0, 3).CopyTo(dest); }</byte></code></pre><p>Here, we&rsquo;re using <code>BitConverter.GetBytes</code>, which allocates a <code>byte[]</code> containing the bytes from the input (in this case, it&rsquo;ll be a four-byte array for the <code>int</code>), then we slice off three of the four bytes, and we copy them to the destination span.</p><table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>Test</td> <td>.NET 9.0</td> <td>9.7717 ns</td> <td>1.04</td> <td>32 B</td> <td>1.00</td> </tr> <tr> <td>Test</td> <td>.NET 10.0</td> <td>0.8718 ns</td> <td>0.09</td> <td>&ndash;</td> <td>0.00</td> </tr> </tbody> </table><p>In .NET 9, we get the 32-byte allocation we&rsquo;d expect for the <code>byte[]</code> in <code>GetBytes</code> (every object on 64-bit is at least 24 bytes, which will include the four bytes for the array&rsquo;s length, and then the four bytes for the data will be in slots 24-27, and the size will be padded up to the next word boundary, for 32). In .NET 10, with <code>GetBytes</code> and <code>AsSpan</code> inlined, the JIT can see that the array doesn&rsquo;t escape, and a stack allocated version of it can be used to seed the span, just as if it were created from any other stack allocation (like <code>stackalloc</code>). (This case also needed a little help from <a href="https://github.com/dotnet/runtime/pull/113093">dotnet/runtime#113093</a>, which taught the JIT that certain span operations, like the <code>Memmove</code> used internally by <code>CopyTo</code>, are non-escaping.)</p><h4>Devirtualization</h4><p>Interfaces and virtual methods are a critical aspect of .NET and the abstractions it enables. Being able to unwind these abstractions and &ldquo;devirtualize&rdquo; is then an important job for the JIT, which has taken notable leaps in capabilities here in .NET 10.</p><p>While arrays are one of the most central features provided by C# and .NET, and while the JIT exerts a lot of energy and does a great job optimizing many aspects of arrays, one area in particular has caused it pain: an array&rsquo;s interface implementations. The runtime manufactures a bunch of interface implementations for <code>T[]</code>, and because they&rsquo;re implemented differently from literally every other interface implementation in .NET, the JIT hasn&rsquo;t been able to apply the same devirtualization capabilities it&rsquo;s applied elsewhere. And, for anyone who&rsquo;s dived deep into micro-benchmarks, this can lead to some odd observations. Here&rsquo;s a performance comparison between iterating over a <code>ReadOnlyCollection<t></t></code> using a <code>foreach</code> loop (going through its enumerator) and using a <code>for</code> loop (indexing on each element).</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" // dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Collections.ObjectModel; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private ReadOnlyCollection<int> _list = new(Enumerable.Range(1, 1000).ToArray()); [Benchmark] public int SumEnumerable() { int sum = 0; foreach (var item in _list) { sum += item; } return sum; } [Benchmark] public int SumForLoop() { ReadOnlyCollection<int> list = _list; int sum = 0; int count = list.Count; for (int i = 0; i &lt; count; i++) { sum += _list[i]; } return sum; } }</int></int></code></pre><p>When asked &ldquo;which of these will be faster&rdquo;, the obvious answer is &ldquo;<code>SumForLoop</code>&ldquo;. After all, <code>SumEnumerable</code> is going to allocate an enumerator and has to make twice the number of interface calls (<code>MoveNext</code>+<code>Current</code> per iteration vs <code>this[int]</code> per iteration). As it turns out, the obvious answer is also wrong. Here are the timings on my machine for .NET 9:</p><table> <thead> <tr> <th>Method</th> <th>Mean</th> </tr> </thead> <tbody> <tr> <td>SumEnumerable</td> <td>949.5 ns</td> </tr> <tr> <td>SumForLoop</td> <td>1,932.7 ns</td> </tr> </tbody> </table><p>What the what?? If I change the <code>ToArray</code> to instead be <code>ToList</code>, however, the numbers are much more in line with our expectations.</p><table> <thead> <tr> <th>Method</th> <th>Mean</th> </tr> </thead> <tbody> <tr> <td>SumEnumerable</td> <td>1,542.0 ns</td> </tr> <tr> <td>SumForLoop</td> <td>894.1 ns</td> </tr> </tbody> </table><p>So what&rsquo;s going on here? It&rsquo;s super subtle. First, it&rsquo;s necessary to know that <code>ReadOnlyCollection<t></t></code> just wraps an arbitrary <code>IList<t></t></code>, the <code>ReadOnlyCollection<t></t></code>&lsquo;s <code>GetEnumerator()</code> returns <code>_list.GetEnumerator()</code> (I&rsquo;m ignoring for this discussion the special-case where the list is empty), and <code>ReadOnlyCollection<t></t></code>&lsquo;s indexer just indexes into the <code>IList<t></t></code>&lsquo;s indexer. So far presumably this all sounds like what you&rsquo;d expect. But where things gets interesting is around what the JIT is able to devirtualize. In .NET 9, it struggles to devirtualize calls to the interface implementations specifically on <code>T[]</code>, so it won&rsquo;t devirtualize either the <code>_list.GetEnumerator()</code> call nor the <code>_list[index]</code> call. However, the enumerator that&rsquo;s returned is just a normal type that implements <code>IEnumerator<t></t></code>, and the JIT has no problem devirtualizing its <code>MoveNext</code> and <code>Current</code> members. Which means that we&rsquo;re actually paying a lot more going through the indexer, because for <code>N</code> elements, we&rsquo;re having to make <code>N</code> interface calls, whereas with the enumerator, we only need the one with <code>GetEnumerator</code> interface call and then no more after that.</p><p>Thankfully, this is now addressed in .NET 10. <a href="https://github.com/dotnet/runtime/pull/108153">dotnet/runtime#108153</a>, <a href="https://github.com/dotnet/runtime/pull/109209">dotnet/runtime#109209</a>, <a href="https://github.com/dotnet/runtime/pull/109237">dotnet/runtime#109237</a>, and <a href="https://github.com/dotnet/runtime/pull/116771">dotnet/runtime#116771</a> all make it possible for the JIT to devirtualize array&rsquo;s interface method implementations. Now when we run the same benchmark (reverted back to using <code>ToArray</code>), we get results much more in line with our expectations, with both benchmarks improving from .NET 9 to .NET 10, and with <code>SumForLoop</code> on .NET 10 being the fastest.</p><table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>SumEnumerable</td> <td>.NET 9.0</td> <td>968.5 ns</td> <td>1.00</td> </tr> <tr> <td>SumEnumerable</td> <td>.NET 10.0</td> <td>775.5 ns</td> <td>0.80</td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>SumForLoop</td> <td>.NET 9.0</td> <td>1,960.5 ns</td> <td>1.00</td> </tr> <tr> <td>SumForLoop</td> <td>.NET 10.0</td> <td>624.6 ns</td> <td>0.32</td> </tr> </tbody> </table><p>One of the really interesting things about this is how many libraries are implemented on the premise that it&rsquo;s faster to use an <code>IList<t></t></code>&lsquo;s indexer for iteration than it is to use its <code>IEnumerable<t></t></code> for iteration, and that includes <code>System.Linq</code>. All these years, where LINQ has had specialized code paths for working with <code>IList<t></t></code> when possible, while in many cases it&rsquo;s been a welcome optimization, in <em>some</em> cases (such as when the concrete type is a <code>ReadOnlyCollection<t></t></code>), it&rsquo;s actually been a deoptimization.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Collections.ObjectModel; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private ReadOnlyCollection<int> _list = new(Enumerable.Range(1, 1000).ToArray()); [Benchmark] public int SkipTakeSum() =&gt; _list.Skip(100).Take(800).Sum(); }</int></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>SkipTakeSum</td> <td>.NET 9.0</td> <td>3.525 us</td> <td>1.00</td> </tr> <tr> <td>SkipTakeSum</td> <td>.NET 10.0</td> <td>1.773 us</td> <td>0.50</td> </tr> </tbody> </table><p>Fixing devirtualization for array&rsquo;s interface implementation then also has this transitive effect on LINQ.</p><p>Guarded Devirtualization (GDV) is also improved in .NET 10, such as from <a href="https://github.com/dotnet/runtime/pull/116453">dotnet/runtime#116453</a> and <a href="https://github.com/dotnet/runtime/pull/109256">dotnet/runtime#109256</a>. With <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-8/#tiering-and-dynamic-pgo">dynamic PGO</a>, the JIT is able to instrument a method&rsquo;s compilation and then use the resulting profiling data as part of emitting an optimized version of the method. One of the things it can profile are which types are used in a virtual dispatch. If one type dominates, it can special-case that type in the code gen and emit a customized implementation specific to that type. That then enables devirtualization in that dedicated path, which is &ldquo;guarded&rdquo; by the relevant type check, hence &ldquo;GDV&rdquo;. In some cases, however, such as if a virtual call was being made in a shared generic context, GDV would not kick in. Now it will.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Runtime.CompilerServices; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { [Benchmark] public bool Test() =&gt; GenericEquals("abc", "abc"); [MethodImpl(MethodImplOptions.NoInlining)] private static bool GenericEquals<t>(T a, T b) =&gt; EqualityComparer<t>.Default.Equals(a, b); }</t></t></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Test</td> <td>.NET 9.0</td> <td>2.816 ns</td> <td>1.00</td> </tr> <tr> <td>Test</td> <td>.NET 10.0</td> <td>1.511 ns</td> <td>0.54</td> </tr> </tbody> </table><p><a href="https://github.com/dotnet/runtime/pull/110827">dotnet/runtime#110827</a> from <a href="https://github.com/hez2010">@hez2010</a> also helps more methods to be inlined by doing another pass looking for opportunities after later phases of devirtualization. The JIT&rsquo;s optimizations are split up into multiple phases; each phase can make improvements, and those improvements can expose additional opportunities. If those opportunities would only be capitalized on by a phase that already ran, they can be missed. But for phases that are relatively cheap to perform, such as doing a pass looking for additional inlining opportunities, those phases can be repeated once enough other optimization has happened that it&rsquo;s likely productive to do so again.</p><h3>Bounds Checking</h3><p>C# is a memory-safe language, an important aspect of modern programming languages. A key component of this is the inability to walk off the beginning or end of an array, string, or span. The runtime ensures that any such invalid attempt produces an exception, rather than being allowed to perform the invalid memory access. We can see what this looks like with a small benchmark:</p><pre><code>// dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private int[] _array = new int[3]; [Benchmark] public int Read() =&gt; _array[2]; }</code></pre><p>This is a valid access: the <code>_array</code> contains three elements, and the <code>Read</code> method is reading its last element. However, the JIT can&rsquo;t be 100% certain that this access is in bounds (something could have changed what&rsquo;s in the <code>_array</code> field to be a shorter array), and thus it needs to emit a check to ensure we&rsquo;re not walking off the end of the array. Here&rsquo;s what the generated assembly code for <code>Read</code> looks like:</p><pre><code>; .NET 10 ; Tests.Read() push rax mov rax,[rdi+8] cmp dword ptr [rax+8],2 jbe short M00_L00 mov eax,[rax+18] add rsp,8 ret M00_L00: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 25</code></pre><p>The <code>this</code> reference is passed into the <code>Read</code> instance method in the <code>rdi</code> register, and the <code>_array</code> field is at offset 8, so the <code>mov rax,[rdi+8]</code> instruction is loading the address of the array into the <code>rax</code> register. Then the <code>cmp</code> is loading the value at offset 8 from that address; it so happens that&rsquo;s where the length of the array is stored in the array object. So, this <code>cmp</code> instruction is the bounds check; it&rsquo;s comparing <code>2</code> against that length to ensure it&rsquo;s in bounds. If the array were too short for this access, the next <code>jbe</code> instruction would branch to the <code>M00_L00</code> label, which calls the <code>CORINFO_HELP_RNGCHKFAIL</code> helper function that throws an <code>IndexOutOfRangeException</code>. Any time you see this pair of <code>call CORINFO_HELP_RNGCHKFAIL</code>/<code>int 3</code> at the end of a method, there was at least one bounds check emitted by the JIT in that method.</p><p>Of course, we not only want safety, we also want great performance, and it&rsquo;d be terrible for performance if every single read from an array (or string or span) incurred such an additional check. As such, the JIT strives to avoid emitting these checks when they&rsquo;d be redundant, when it can prove by construction that the accesses are safe. For example, let me tweak my benchmark slightly, moving the array from an instance field into a <code>static readonly</code> field:</p><pre><code>// dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private static readonly int[] s_array = new int[3]; [Benchmark] public int Read() =&gt; s_array[2]; }</code></pre><p>We now get this assembly:</p><pre><code>; .NET 10 ; Tests.Read() mov rax,705D5419FA20 mov eax,[rax+18] ret ; Total bytes of code 14</code></pre><p>The <code>static readonly</code> field is immutable, arrays can&rsquo;t be resized, and the JIT can guarantee that the field is initialized prior to generating the code for <code>Read</code>. Therefore, when generating the code for <code>Read</code>, it can know with certainty that the array is of length three, and we&rsquo;re accessing the element at index two. Therefore, the specified array index is guaranteed to be within bounds, and there&rsquo;s no need for a bounds check. We simply get two <code>mov</code>s, the first <code>mov</code> to load the address of the array (which, thanks to improvements in previous releases, is allocated on a heap that doesn&rsquo;t need to be compacted such that the array lives at a fixed address), and the second <code>mov</code> to read the <code>int</code> value at the location of index two (these are <code>int</code>s, so index two lives <code>2 * sizeof(int) = 8</code> bytes from the start of the array&rsquo;s data, which itself on 64-bit is offset 16 bytes from the start of the array reference, for a total offset of 24 bytes, or in hex 0x18, hence the <code>rax+18</code> in the disassembly).</p><p>Every release of .NET, more and more opportunities are found and implemented to eschew bounds checks that were previously being generated. .NET 10 continues this trend.</p><p>Our first example comes from <a href="https://github.com/dotnet/runtime/pull/109900">dotnet/runtime#109900</a>, which was inspired by the implementation of <code>BitOperations.Log2</code>. The operation has intrinsic hardware support on many architectures, and generally <code>BitOperations.Log2</code> will use one of the hardware intrinsics available to it for a very efficient implementation (e.g. <code>Lscnt.LeadingZeroCount</code>, <code>ArmBase.LeadingZeroCount</code>, or <code>X86Base.BitScanReverse</code>), however as a fallback implementation it uses a lookup table. The lookup table has 32 elements, and the operation involves computing a <code>uint</code> value and then shifting it down by 27 in order to get the top 5 bits. Any possible result is guaranteed to be a non-negative number less than 32, but indexing into the span with that result still produced a bounds check, and, as this is a critical path, &ldquo;unsafe&rdquo; code (meaning code that eschews the guardrails the runtime supplies by default) was then used to avoid the bounds check.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "value")] public partial class Tests { [Benchmark] [Arguments(42)] public int Log2SoftwareFallback2(uint value) { ReadOnlySpan<byte> Log2DeBruijn = [ 00, 09, 01, 10, 13, 21, 02, 29, 11, 14, 16, 18, 22, 25, 03, 30, 08, 12, 20, 28, 15, 17, 24, 07, 19, 27, 23, 06, 26, 05, 04, 31 ]; value |= value &gt;&gt; 01; value |= value &gt;&gt; 02; value |= value &gt;&gt; 04; value |= value &gt;&gt; 08; value |= value &gt;&gt; 16; return Log2DeBruijn[(int)((value * 0x07C4ACDDu) &gt;&gt; 27)]; } }</byte></code></pre><p>Now in .NET 10, the bounds check is gone (note the presence of the <code>call CORINFO_HELP_RNGCHKFAIL</code> in the .NET 9 assembly and the lack of it in the .NET 10 assembly).</p><pre><code>; .NET 9 ; Tests.Log2SoftwareFallback2(UInt32) push rax mov eax,esi shr eax,1 or esi,eax mov eax,esi shr eax,2 or esi,eax mov eax,esi shr eax,4 or esi,eax mov eax,esi shr eax,8 or esi,eax mov eax,esi shr eax,10 or eax,esi imul eax,7C4ACDD shr eax,1B cmp eax,20 jae short M00_L00 mov rcx,7913CA812E10 movzx eax,byte ptr [rax+rcx] add rsp,8 ret M00_L00: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 74 ; .NET 10 ; Tests.Log2SoftwareFallback2(UInt32) mov eax,esi shr eax,1 or esi,eax mov eax,esi shr eax,2 or esi,eax mov eax,esi shr eax,4 or esi,eax mov eax,esi shr eax,8 or esi,eax mov eax,esi shr eax,10 or eax,esi imul eax,7C4ACDD shr eax,1B mov rcx,7CA298325E10 movzx eax,byte ptr [rcx+rax] ret ; Total bytes of code 58</code></pre><p>This improvement then enabled <a href="https://github.com/dotnet/runtime/pull/118560">dotnet/runtime#118560</a> to simplify the code in the real <code>Log2SoftwareFallback</code>, avoiding manual use of unsafe constructs.</p><p><a href="https://github.com/dotnet/runtime/pull/113790">dotnet/runtime#113790</a> implements a similar case, where the result of a mathematical operation is guaranteed to be in bounds. In this case, it&rsquo;s the result of <code>Log2</code>. The change teaches the JIT to understand the maximum possible value that <code>Log2</code> could produce, and if that maximum is in bounds, then any result is guaranteed to be in bounds as well.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "value")] public partial class Tests { [Benchmark] [Arguments(12345)] public nint CountDigits(ulong value) { ReadOnlySpan<byte> log2ToPow10 = [ 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 10, 10, 10, 11, 11, 11, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 15, 15, 15, 16, 16, 16, 16, 17, 17, 17, 18, 18, 18, 19, 19, 19, 19, 20 ]; return log2ToPow10[(int)ulong.Log2(value)]; } }</byte></code></pre><p>We can see the bounds check present in the .NET 9 output and absent in the .NET 10 output:</p><pre><code>; .NET 9 ; Tests.CountDigits(UInt64) push rax or rsi,1 xor eax,eax lzcnt rax,rsi xor eax,3F cmp eax,40 jae short M00_L00 mov rcx,7C2D0A213DF8 movzx eax,byte ptr [rax+rcx] add rsp,8 ret M00_L00: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 45 ; .NET 10 ; Tests.CountDigits(UInt64) or rsi,1 xor eax,eax lzcnt rax,rsi xor eax,3F mov rcx,71EFA9400DF8 movzx eax,byte ptr [rcx+rax] ret ; Total bytes of code 29</code></pre><p>My choice of benchmark in this case was not coincidental. This pattern shows up in the <code>FormattingHelpers.CountDigits</code> internal method that&rsquo;s used by the core primitive types in their <code>ToString</code> and <code>TryFormat</code> implementations, in order to determine how much space will be needed to store rendered digits for a number. As with the previous example, this routine is considered core enough that it was using unsafe code to avoid the bounds check. With this fix, the code was able to be changed back to using a simple span access, and even with the simpler code, it&rsquo;s now also faster.</p><p>Now, consider this code:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "ids")] public partial class Tests { public IEnumerable<int> Ids { get; } = [[1, 2, 3, 4, 5, 1]]; [Benchmark] [ArgumentsSource(nameof(Ids))] public bool StartAndEndAreSame(int[] ids) =&gt; ids[0] == ids[^1]; }</int></code></pre><p>I have a method that&rsquo;s accepting an <code>int[]</code> and checking to see whether it starts and ends with the same value. The JIT has no way of knowing whether the <code>int[]</code> is empty or not, so it <em>does</em> need a bounds check; otherwise, accessing <code>ids[0]</code> could walk off the end of the array. However, this is what we see on .NET 9:</p><pre><code>; .NET 9 ; Tests.StartAndEndAreSame(Int32[]) push rax mov eax,[rsi+8] test eax,eax je short M00_L00 mov ecx,[rsi+10] lea edx,[rax-1] cmp edx,eax jae short M00_L00 mov eax,edx cmp ecx,[rsi+rax*4+10] sete al movzx eax,al add rsp,8 ret M00_L00: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 41</code></pre><p>Note there are two jumps to the <code>M00_L00</code> label that handles failed bounds checks&hellip; that&rsquo;s because there are two bounds checks here, one for the start access and one for the end access. But that shouldn&rsquo;t be necessary. <code>ids[^1]</code> is the same as <code>ids[ids.Length - 1]</code>. If the code has successfully accessed <code>ids[0]</code>, that means the array is at least one element in length, and if it&rsquo;s at least one element in length, <code>ids[ids.Length - 1]</code> will always be in bounds. Thus, the second bounds check shouldn&rsquo;t be needed. Indeed, thanks to <a href="https://github.com/dotnet/runtime/pull/116105">dotnet/runtime#116105</a>, this is what we now get on .NET 10 (one branch to <code>M00_L00</code> instead of two):</p><pre><code>; .NET 10 ; Tests.StartAndEndAreSame(Int32[]) push rax mov eax,[rsi+8] test eax,eax je short M00_L00 mov ecx,[rsi+10] dec eax cmp ecx,[rsi+rax*4+10] sete al movzx eax,al add rsp,8 ret M00_L00: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 34</code></pre><p>What&rsquo;s really interesting to me here is the knock-on effect of having removed the bounds check. It didn&rsquo;t just eliminate the <code>cmp/jae</code> pair of instructions that&rsquo;s typical of a bounds check. The .NET 9 version of the code had this:</p><pre><code>lea edx,[rax-1] cmp edx,eax jae short M00_L00 mov eax,edx</code></pre><p>At this point in the assembly, the <code>rax</code> register is storing the length of the array. It&rsquo;s calculating <code>ids.Length - 1</code> and storing the result into <code>edx</code>, and then checking to see whether <code>ids.Length-1</code> is in bounds of <code>ids.Length</code> (the only way it wouldn&rsquo;t be is if the array were empty such that <code>ids.Length-1</code> wrapped around to <code>uint.MaxValue</code>); if it&rsquo;s not, it jumps to the fail handler, and if it is, it stores the already computed <code>ids.Length - 1</code> into <code>eax</code>. By removing the bounds check, we get rid of those two intervening instructions, leaving these:</p><pre><code>lea edx,[rax-1] mov eax,edx</code></pre><p>which is a little silly, as this sequence is just computing a decrement, and as long as it&rsquo;s ok that flags get modified, it could instead just be:</p><pre><code>dec eax</code></pre><p>which, as you can see in the .NET 10 output, is exactly what .NET 10 now does.</p><p><a href="https://github.com/dotnet/runtime/pull/115980">dotnet/runtime#115980</a> addresses another case. Let&rsquo;s say I have this method:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "start", "text")] public partial class Tests { [Benchmark] [Arguments("abc", "abc.")] public bool IsFollowedByPeriod(string start, string text) =&gt; start.Length &lt; text.Length &amp;&amp; text[start.Length] == '.'; }</code></pre><p>We&rsquo;re validating that one input&rsquo;s length is less than the other, and then checking to see what comes immediately after it in the other. We know that <code>string.Length</code> is immutable, so a bounds check here is redundant, but until .NET 10, the JIT couldn&rsquo;t see that.</p><pre><code>; .NET 9 ; Tests.IsFollowedByPeriod(System.String, System.String) push rbp mov rbp,rsp mov eax,[rsi+8] mov ecx,[rdx+8] cmp eax,ecx jge short M00_L00 cmp eax,ecx jae short M00_L01 cmp word ptr [rdx+rax*2+0C],2E sete al movzx eax,al pop rbp ret M00_L00: xor eax,eax pop rbp ret M00_L01: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 42 ; .NET 10 ; Tests.IsFollowedByPeriod(System.String, System.String) mov eax,[rsi+8] mov ecx,[rdx+8] cmp eax,ecx jge short M00_L00 cmp word ptr [rdx+rax*2+0C],2E sete al movzx eax,al ret M00_L00: xor eax,eax ret ; Total bytes of code 26</code></pre><p>The removal of the bounds check almost halves the size of the function. If we don&rsquo;t need to do a bounds check, we get to elide the <code>cmp/jae</code>. Without that branch, nothing is targeting <code>M00_L01</code>, and we can remove the <code>call/int</code> pair that were only necessary to support a bounds check. Then without the <code>call</code> in <code>M00_L01</code>, which was the only <code>call</code> in the whole method, the prologue and epilogue can be elided, meaning we also don&rsquo;t need the opening and closing <code>push</code> and <code>pop</code> instructions.</p><p><a href="https://github.com/dotnet/runtime/pull/113233">dotnet/runtime#113233</a> improved handling &ldquo;assertions&rdquo; (facts the JIT claims and based on which the JIT makes optimizations) to be less order dependent. In .NET 9, this code:</p><pre><code>static bool Test(ReadOnlySpan<char> span, int pos) =&gt; pos &gt; 0 &amp;&amp; pos &lt;= span.Length - 42 &amp;&amp; span[pos - 1] != '\n';</char></code></pre><p>was successfully removing the bounds check on the span access, but the following variant, which just switches the order of the first two conditions, was still incurring the bounds check.</p><pre><code>static bool Test(ReadOnlySpan<char> span, int pos) =&gt; pos &lt;= span.Length - 42 &amp;&amp; pos &gt; 0 &amp;&amp; span[pos - 1] != '\n';</char></code></pre><p>Note that both conditions contribute an assertion (fact) that need to be merged in order to know the bounds check can be avoided. Now in .NET 10, the bounds check is elided, regardless of the order.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private string _s = new string('s', 100); private int _pos = 10; [Benchmark] public bool Test() { string s = _s; int pos = _pos; return pos &lt;= s.Length - 42 &amp;&amp; pos &gt; 0 &amp;&amp; s[pos - 1] != '\n'; } }</code></pre><pre><code>; .NET 9 ; Tests.Test() push rbp mov rbp,rsp mov rax,[rdi+8] mov ecx,[rdi+10] mov edx,[rax+8] lea edi,[rdx-2A] cmp edi,ecx jl short M00_L00 test ecx,ecx jle short M00_L00 dec ecx cmp ecx,edx jae short M00_L01 cmp word ptr [rax+rcx*2+0C],0A setne al movzx eax,al pop rbp ret M00_L00: xor eax,eax pop rbp ret M00_L01: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 55 ; .NET 10 ; Tests.Test() push rbp mov rbp,rsp mov rax,[rdi+8] mov ecx,[rdi+10] mov edx,[rax+8] add edx,0FFFFFFD6 cmp edx,ecx jl short M00_L00 test ecx,ecx jle short M00_L00 dec ecx cmp word ptr [rax+rcx*2+0C],0A setne al movzx eax,al pop rbp ret M00_L00: xor eax,eax pop rbp ret ; Total bytes of code 45</code></pre><p><a href="https://github.com/dotnet/runtime/pull/113862">dotnet/runtime#113862</a> addresses a similar case where assertions weren&rsquo;t being handled as precisely as they could have been. Consider this code:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private int[] _arr = Enumerable.Range(0, 10).ToArray(); [Benchmark] public int Sum() { int[] arr = _arr; int sum = 0; int i; for (i = 0; i &lt; arr.Length - 3; i += 4) { sum += arr[i + 0]; sum += arr[i + 1]; sum += arr[i + 2]; sum += arr[i + 3]; } for (; i &lt; arr.Length; i++) { sum += arr[i]; } return sum; } }</code></pre><p>The <code>Sum</code> method is trying to do manual loop unrolling. Rather than incurring a branch on each element, it&rsquo;s handling four elements per iteration. Then, for the case where the length of the input isn&rsquo;t evenly divisible by four, it&rsquo;s handling the remaining elements in a separate loop. In .NET 9, the JIT successfully elides the bounds checks in the main unrolled loop:</p><pre><code>; .NET 9 ; Tests.Sum() push rbp mov rbp,rsp mov rax,[rdi+8] xor ecx,ecx xor edx,edx mov edi,[rax+8] lea esi,[rdi-3] test esi,esi jle short M00_L02 M00_L00: mov r8d,edx add ecx,[rax+r8*4+10] lea r8d,[rdx+1] add ecx,[rax+r8*4+10] lea r8d,[rdx+2] add ecx,[rax+r8*4+10] lea r8d,[rdx+3] add ecx,[rax+r8*4+10] add edx,4 cmp esi,edx jg short M00_L00 jmp short M00_L02 M00_L01: cmp edx,edi jae short M00_L03 mov esi,edx add ecx,[rax+rsi*4+10] inc edx M00_L02: cmp edi,edx jg short M00_L01 mov eax,ecx pop rbp ret M00_L03: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 92</code></pre><p>You can see this in the <code>M00_L00</code> section, which has the five <code>add</code> instructions (four for the summed elements, and one for the index). However, we still see the <code>CORINFO_HELP_RNGCHKFAIL</code> at the end, indicating this method has a bounds check. That&rsquo;s coming from the final loop, due to the JIT losing track of the fact that <code>i</code> is guaranteed to be non-negative. Now in .NET 10, that bounds check is removed as well (again, just look for the lack of the <code>CORINFO_HELP_RNGCHKFAIL</code> call).</p><pre><code>; .NET 10 ; Tests.Sum() push rbp mov rbp,rsp mov rax,[rdi+8] xor ecx,ecx xor edx,edx mov edi,[rax+8] lea esi,[rdi-3] test esi,esi jle short M00_L01 M00_L00: mov r8d,edx add ecx,[rax+r8*4+10] lea r8d,[rdx+1] add ecx,[rax+r8*4+10] lea r8d,[rdx+2] add ecx,[rax+r8*4+10] lea r8d,[rdx+3] add ecx,[rax+r8*4+10] add edx,4 cmp esi,edx jg short M00_L00 M00_L01: cmp edi,edx jle short M00_L03 test edx,edx jl short M00_L04 M00_L02: mov esi,edx add ecx,[rax+rsi*4+10] inc edx cmp edi,edx jg short M00_L02 M00_L03: mov eax,ecx pop rbp ret M00_L04: mov esi,edx add ecx,[rax+rsi*4+10] inc edx cmp edi,edx jg short M00_L04 jmp short M00_L03 ; Total bytes of code 102</code></pre><p>Another nice improvement comes from <a href="https://github.com/dotnet/runtime/pull/112824">dotnet/runtime#112824</a>, which teaches the JIT to turn facts it already learned from earlier checks into concrete numeric ranges, and then use those ranges to fold away later relational tests and bounds checks. Consider this example:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Runtime.CompilerServices; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private int[] _array = new int[10]; [Benchmark] public void Test() =&gt; SetAndSlice(_array); [MethodImpl(MethodImplOptions.NoInlining)] private static Span<int> SetAndSlice(Span<int> src) { src[5] = 42; return src.Slice(4); } }</int></int></code></pre><p>We have to incur a bounds check for the <code>src[5]</code>, as the JIT has no evidence that <code>src</code> is at least six elements long. However, by the time we get to the <code>Slice</code> call, we know the span has a length of at least six, or else writing into <code>src[5]</code> would have failed. We can use that knowledge to remove the length check from within the <code>Slice</code> call (note the removal of the <code>call qword ptr [7F8DDB3A7810]</code>/<code>int 3</code> sequence, which is the manual length check and call to a throw helper method in <code>Slice</code>).</p><pre><code>; .NET 9 ; Tests.SetAndSlice(System.Span`1<int32>) push rbp mov rbp,rsp cmp esi,5 jbe short M01_L01 mov dword ptr [rdi+14],2A cmp esi,4 jb short M01_L00 add rdi,10 mov rax,rdi add esi,0FFFFFFFC mov edx,esi pop rbp ret M01_L00: call qword ptr [7F8DDB3A7810] int 3 M01_L01: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 48 ; .NET 10 ; Tests.SetAndSlice(System.Span`1<int32>) push rax cmp esi,5 jbe short M01_L00 mov dword ptr [rdi+14],2A lea rax,[rdi+10] lea edx,[rsi-4] add rsp,8 ret M01_L00: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 31</int32></int32></code></pre><p>Let&rsquo;s look at one more, which has a very nice impact on bounds checking, even though technically the optimization is broader than just that. <a href="https://github.com/dotnet/runtime/pull/113998">dotnet/runtime#113998</a> creates assertions from <code>switch</code> targets. This means that the body of a <code>switch</code> case statement inherits facts about what was switched over based on what the <code>case</code> was, e.g. in a <code>case 3</code> for <code>switch (x)</code>, the body of that case will now &ldquo;know&rdquo; that <code>x</code> is three. This is great for very popular patterns with arrays, strings, and spans, where developers switch over the length and then index into available indices in the appropriate branches. Consider this:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Runtime.CompilerServices; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private int[] _array = [1, 2]; [Benchmark] public int SumArray() =&gt; Sum(_array); [MethodImpl(MethodImplOptions.NoInlining)] public int Sum(ReadOnlySpan<int> span) { switch (span.Length) { case 0: return 0; case 1: return span[0]; case 2: return span[0] + span[1]; case 3: return span[0] + span[1] + span[2]; default: return -1; } } }</int></code></pre><p>On .NET 9, each of those six <code>span</code> dereferences ends up with a bounds check:</p><pre><code>; .NET 9 ; Tests.Sum(System.ReadOnlySpan`1<int32>) push rbp mov rbp,rsp M01_L00: cmp edx,2 jne short M01_L02 test edx,edx je short M01_L04 mov eax,[rsi] cmp edx,1 jbe short M01_L04 add eax,[rsi+4] M01_L01: pop rbp ret M01_L02: cmp edx,3 ja short M01_L03 mov eax,edx lea rcx,[783DA42091B8] mov ecx,[rcx+rax*4] lea rdi,[M01_L00] add rcx,rdi jmp rcx M01_L03: mov eax,0FFFFFFFF pop rbp ret test edx,edx je short M01_L04 mov eax,[rsi] cmp edx,1 jbe short M01_L04 add eax,[rsi+4] cmp edx,2 jbe short M01_L04 add eax,[rsi+8] jmp short M01_L01 test edx,edx je short M01_L04 mov eax,[rsi] jmp short M01_L01 xor eax,eax pop rbp ret M01_L04: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 103</int32></code></pre><p>You can see the tell-tale bounds check sign (<code>CORINFO_HELP_RNGCHKFAIL</code>) under <code>M01_L04</code>, and no fewer than six jumps targeting that label, one for each <code>span[...]</code> access. But on .NET 10, we get this:</p><pre><code>; .NET 10 ; Tests.Sum(System.ReadOnlySpan`1<int32>) push rbp mov rbp,rsp M01_L00: cmp edx,2 jne short M01_L02 mov eax,[rsi] add eax,[rsi+4] M01_L01: pop rbp ret M01_L02: cmp edx,3 ja short M01_L03 mov eax,edx lea rcx,[72C15C0F8FD8] mov ecx,[rcx+rax*4] lea rdx,[M01_L00] add rcx,rdx jmp rcx M01_L03: mov eax,0FFFFFFFF pop rbp ret xor eax,eax pop rbp ret mov eax,[rsi] jmp short M01_L01 mov eax,[rsi] add eax,[rsi+4] add eax,[rsi+8] jmp short M01_L01 ; Total bytes of code 70</int32></code></pre><p>The <code>CORINFO_HELP_RNGCHKFAIL</code> and all the jumps to it have evaporated.</p><h3>Cloning</h3><p>There are other ways the JIT can remove bounds checking even when it can&rsquo;t prove statically that every individual access is safe. Consider this method:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private int[] _arr = new int[16]; [Benchmark] public void Test() { int[] arr = _arr; arr[0] = 2; arr[1] = 3; arr[2] = 5; arr[3] = 8; arr[4] = 13; arr[5] = 21; arr[6] = 34; arr[7] = 55; } }</code></pre><p>Here&rsquo;s the assembly code generated on .NET 9:</p><pre><code>; .NET 9 ; Tests.Test() push rax mov rax,[rdi+8] mov ecx,[rax+8] test ecx,ecx je short M00_L00 mov dword ptr [rax+10],2 cmp ecx,1 jbe short M00_L00 mov dword ptr [rax+14],3 cmp ecx,2 jbe short M00_L00 mov dword ptr [rax+18],5 cmp ecx,3 jbe short M00_L00 mov dword ptr [rax+1C],8 cmp ecx,4 jbe short M00_L00 mov dword ptr [rax+20],0D cmp ecx,5 jbe short M00_L00 mov dword ptr [rax+24],15 cmp ecx,6 jbe short M00_L00 mov dword ptr [rax+28],22 cmp ecx,7 jbe short M00_L00 mov dword ptr [rax+2C],37 add rsp,8 ret M00_L00: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 114</code></pre><p>Even if you&rsquo;re not proficient at reading assembly, the pattern should still be obvious. In the C# code, we have eight writes into the array, and in the assembly code, we have eight repetitions of the same pattern: <code>cmp ecx,LENGTH</code> to compare the length of the array against the required <code>LENGTH</code>, <code>jbe short M00_L00</code> to jump to the <code>CORINFO_HELP_RNGCHKFAIL</code> helper if the bounds check fails, and <code>mov dword ptr [rax+OFFSET],VALUE</code> to store <code>VALUE</code> into the array at byte offset <code>OFFSET</code>. Inside the <code>Test</code> method, the JIT can&rsquo;t know how long <code>_arr</code> is, so it must include bounds checks. Moreover, it must include all of the bounds checks, rather than coalescing them, because it is forbidden from introducing behavioral changes as part of optimizations. Imagine instead if it chose to coalesce all of the bounds checks into a single check, and emitted this method as if it were the equivalent of the following:</p><pre><code>if (arr.Length &gt;= 8) { arr[0] = 2; arr[1] = 3; arr[2] = 5; arr[3] = 8; arr[4] = 13; arr[5] = 21; arr[6] = 34; arr[7] = 55; } else { throw new IndexOutOfRangeException(); }</code></pre><p>Now, let&rsquo;s say the array was actually of length four. The original program would have filled the array with values <code>[2, 3, 5, 8]</code> before throwing an exception, but this transformed code wouldn&rsquo;t (there wouldn&rsquo;t be any writes to the array). That&rsquo;s an observable behavioral change. An enterprising developer could of course <em>choose</em> to rewrite their code to avoid some of these checks, e.g.</p><pre><code>arr[7] = 55; arr[0] = 2; arr[1] = 3; arr[2] = 5; arr[3] = 8; arr[4] = 13; arr[5] = 21; arr[6] = 34;</code></pre><p>By moving the last store to the beginning, the developer has given the JIT extra knowledge. The JIT can now see that <em>if</em> the first store succeeds, the rest are guaranteed to succeed as well, and the JIT will emit a single bounds check. But, again, that&rsquo;s the developer choosing to change their program in a way the JIT must not. However, there are other things the JIT <em>can</em> do. Imagine the JIT chose to rewrite the method like this instead:</p><pre><code>if (arr.Length &gt;= 8) { arr[0] = 2; arr[1] = 3; arr[2] = 5; arr[3] = 8; arr[4] = 13; arr[5] = 21; arr[6] = 34; arr[7] = 55; } else { arr[0] = 2; arr[1] = 3; arr[2] = 5; arr[3] = 8; arr[4] = 13; arr[5] = 21; arr[6] = 34; arr[7] = 55; }</code></pre><p>To our C# sensibilities, that looks unnecessarily complicated; the <code>if</code> and the <code>else</code> block contain <em>exactly</em> the same C# code. But, knowing what we now know about how the JIT can use known length information to elide bounds checks, it starts to make a bit more sense. Here&rsquo;s what the JIT emits for this variant on .NET 9:</p><pre><code>; .NET 9 ; Tests.Test() push rbp mov rbp,rsp mov rax,[rdi+8] mov ecx,[rax+8] cmp ecx,8 jl short M00_L00 mov rcx,300000002 mov [rax+10],rcx mov rcx,800000005 mov [rax+18],rcx mov rcx,150000000D mov [rax+20],rcx mov rcx,3700000022 mov [rax+28],rcx pop rbp ret M00_L00: test ecx,ecx je short M00_L01 mov dword ptr [rax+10],2 cmp ecx,1 jbe short M00_L01 mov dword ptr [rax+14],3 cmp ecx,2 jbe short M00_L01 mov dword ptr [rax+18],5 cmp ecx,3 jbe short M00_L01 mov dword ptr [rax+1C],8 cmp ecx,4 jbe short M00_L01 mov dword ptr [rax+20],0D cmp ecx,5 jbe short M00_L01 mov dword ptr [rax+24],15 cmp ecx,6 jbe short M00_L01 mov dword ptr [rax+28],22 cmp ecx,7 jbe short M00_L01 mov dword ptr [rax+2C],37 pop rbp ret M00_L01: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 177</code></pre><p>The <code>else</code> block is compiled to the <code>M00_L00</code> label, which contains those same eight repeated blocks we saw earlier. But the <code>if</code> block (above the <code>M00_L00</code> label) is interesting. The only branch there is the initial <code>array.Length &gt;= 8</code> check I wrote in the C# code, emitted as the <code>cmp ecx,8</code>/<code>jl short M00_L00</code> pair of instructions. The rest of the block is just <code>mov</code> instructions (and you can see there are only four writes into the array rather than eight&hellip; the JIT has optimized the eight four-byte writes into four eight-byte writes). In our rewrite, we&rsquo;ve manually cloned the code, so that in what we expect to be the vast, vast, vast majority case (presumably we wouldn&rsquo;t have written the array writes in the first place if we thought they&rsquo;d fail), we only incur the single length check, and then we have our &ldquo;hopefully this is never needed&rdquo; fallback case for the rare situation where it is. Of course, you shouldn&rsquo;t (and shouldn&rsquo;t need to) do such manual cloning. But, the JIT can do such cloning for you, and does.</p><p>&ldquo;Cloning&rdquo; is an optimization long employed by the JIT, where it will do this kind of code duplication, typically of loops, when it believes that in doing so, it can heavily optimize a common case. Now in .NET 10, thanks to <a href="https://github.com/dotnet/runtime/pull/112595">dotnet/runtime#112595</a>, it can employ this same technique for these kinds of sequences of writes. Going back to our original benchmark, here&rsquo;s what we now get on .NET 10:</p><pre><code>; .NET 10 ; Tests.Test() push rbp mov rbp,rsp mov rax,[rdi+8] mov ecx,[rax+8] mov edx,ecx cmp edx,7 jle short M00_L01 mov rdx,300000002 mov [rax+10],rdx mov rcx,800000005 mov [rax+18],rcx mov rcx,150000000D mov [rax+20],rcx mov rcx,3700000022 mov [rax+28],rcx M00_L00: pop rbp ret M00_L01: test edx,edx je short M00_L02 mov dword ptr [rax+10],2 cmp ecx,1 jbe short M00_L02 mov dword ptr [rax+14],3 cmp ecx,2 jbe short M00_L02 mov dword ptr [rax+18],5 cmp ecx,3 jbe short M00_L02 mov dword ptr [rax+1C],8 cmp ecx,4 jbe short M00_L02 mov dword ptr [rax+20],0D cmp ecx,5 jbe short M00_L02 mov dword ptr [rax+24],15 cmp ecx,6 jbe short M00_L02 mov dword ptr [rax+28],22 cmp ecx,7 jbe short M00_L02 mov dword ptr [rax+2C],37 jmp short M00_L00 M00_L02: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 179</code></pre><p>This structure looks almost identical to what we got when we manually cloned: the JIT has emitted the same code twice, except in one case, there are no bounds checks, and in the other case, there are all the bounds checks, and a single length check determines which path to follow. Pretty neat.</p><p>As noted, the JIT has been doing cloning for years, in particular for loops over arrays. However, more and more code is being written against spans instead of arrays, and unfortunately this valuable optimization didn&rsquo;t apply to spans. Now with <a href="https://github.com/dotnet/runtime/pull/113575">dotnet/runtime#113575</a>, it does! We can see this with a basic looping example:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private int[] _arr = new int[16]; private int _count = 8; [Benchmark] public void WithSpan() { Span<int> span = _arr; int count = _count; for (int i = 0; i &lt; count; i++) { span[i] = i; } } [Benchmark] public void WithArray() { int[] arr = _arr; int count = _count; for (int i = 0; i &lt; count; i++) { arr[i] = i; } } }</int></code></pre><p>In both <code>WithArray</code> and <code>WithSpan</code>, we have the same loop, iterating from 0 to a <code>_count</code> with an unknown relationship to the length of <code>_arr</code>, so there has to be some kind of bounds checking emitted. Here&rsquo;s what we get on .NET 9 for <code>WithSpan</code>:</p><pre><code>; .NET 9 ; Tests.WithSpan() push rbp mov rbp,rsp mov rax,[rdi+8] test rax,rax je short M00_L03 lea rcx,[rax+10] mov eax,[rax+8] M00_L00: mov edi,[rdi+10] xor edx,edx test edi,edi jle short M00_L02 nop dword ptr [rax] M00_L01: cmp edx,eax jae short M00_L04 mov [rcx+rdx*4],edx inc edx cmp edx,edi jl short M00_L01 M00_L02: pop rbp ret M00_L03: xor ecx,ecx xor eax,eax jmp short M00_L00 M00_L04: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 59</code></pre><p>There&rsquo;s some upfront assembly here associated with loading <code>_array</code> into a span, loading <code>_count</code>, and checking to see whether the count is 0 (in which case the whole loop can be skipped). Then the core of the loop is at <code>M00_L01</code>, which is repeatedly checking <code>edx</code> (which contains <code>i</code>) against the length of the span (in <code>eax</code>), jumping to <code>CORINFO_HELP_RNGCHKFAIL</code> if it&rsquo;s an out-of-bounds access, writing <code>edx</code> (<code>i</code>) into the span at the next position, bumping up <code>i</code>, and then jumping back to <code>M00_L01</code> to keep iterating if <code>i</code> is still less than <code>count</code> (stored in <code>edi</code>). In other words, we have two checks per iteration: is <code>i</code> still within the bounds of the span, and is <code>i</code> still less than <code>count</code>. Now here&rsquo;s what we get on .NET 9 for <code>WithArray</code>:</p><pre><code>; .NET 9 ; Tests.WithArray() push rbp mov rbp,rsp mov rax,[rdi+8] mov ecx,[rdi+10] xor edx,edx test ecx,ecx jle short M00_L01 test rax,rax je short M00_L02 cmp [rax+8],ecx jl short M00_L02 nop dword ptr [rax+rax] M00_L00: mov edi,edx mov [rax+rdi*4+10],edx inc edx cmp edx,ecx jl short M00_L00 M00_L01: pop rbp ret M00_L02: cmp edx,[rax+8] jae short M00_L03 mov edi,edx mov [rax+rdi*4+10],edx inc edx cmp edx,ecx jl short M00_L02 jmp short M00_L01 M00_L03: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 71</code></pre><p>Here, label <code>M00_L02</code> looks very similar to the loop we just saw in <code>WithSpan</code>, incurring both the check against <code>count</code> and the bounds check on every iteration. But note section <code>M00_L00</code>: it&rsquo;s a clone of the same loop, still with the <code>cmp edx,ecx</code> that checks <code>i</code> against <code>count</code> on each iteration, but no additional bounds checking in sight. The JIT has cloned the loop, specializing one to not have bounds checks, and then in the upfront section, it determines which path to follow based on a single check against the array&rsquo;s length (<code>cmp [rax+8],ecx</code>/<code>jl short M00_L02</code>). Now in .NET 10, here&rsquo;s what we get for <code>WithSpan</code>:</p><pre><code>; .NET 10 ; Tests.WithSpan() push rbp mov rbp,rsp mov rax,[rdi+8] test rax,rax je short M00_L04 lea rcx,[rax+10] mov eax,[rax+8] M00_L00: mov edx,[rdi+10] xor edi,edi test edx,edx jle short M00_L02 cmp edx,eax jg short M00_L03 M00_L01: mov eax,edi mov [rcx+rax*4],edi inc edi cmp edi,edx jl short M00_L01 M00_L02: pop rbp ret M00_L03: cmp edi,eax jae short M00_L05 mov esi,edi mov [rcx+rsi*4],edi inc edi cmp edi,edx jl short M00_L03 jmp short M00_L02 M00_L04: xor ecx,ecx xor eax,eax jmp short M00_L00 M00_L05: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 75</code></pre><p>As with <code>WithArray</code> in .NET 9, <code>WithSpan</code> for .NET 10 has the loop cloned, with the <code>M00_L03</code> block containing the bounds check on each iteration, and the <code>M00_L01</code> block eliding the bounds check on each iteration.</p><p>The JIT gains more cloning abilities in .NET 10, as well. <a href="https://github.com/dotnet/runtime/pull/110020">dotnet/runtime#110020</a>, <a href="https://github.com/dotnet/runtime/pull/108604">dotnet/runtime#108604</a>, and <a href="https://github.com/dotnet/runtime/pull/110483">dotnet/runtime#110483</a> make it possible for the JIT to clone <code>try/finally</code> blocks, whereas previously it would immediately bail out of cloning any regions containing such constructs. This might seem niche, but it&rsquo;s actually quite valuable when you consider that <code>foreach</code>&lsquo;ing over an enumerable typically involves a hidden <code>try</code>/<code>finally</code> for the <code>finally</code> to call the enumerator&rsquo;s <code>Dispose</code>.</p><p>Many of these different optimizations interact with each other. Dynamic PGO triggers a form of cloning, as part of the guarded devirtualization (GDV) mentioned earlier: if the instrumentation data reveals that a particular virtual call is generally performed on an instance of a specific type, the JIT can clone the resulting code into one path specific to that type and another path that handles any type. That then enables the specific-type code path to devirtualize the call and possibly inline it. And if it inlines it, that then provides more opportunities for the JIT to see that an object doesn&rsquo;t escape, and potentially stack allocate it. <a href="https://github.com/dotnet/runtime/pull/111473">dotnet/runtime#111473</a>, <a href="https://github.com/dotnet/runtime/pull/116978">dotnet/runtime#116978</a>, <a href="https://github.com/dotnet/runtime/pull/116992">dotnet/runtime#116992</a>, <a href="https://github.com/dotnet/runtime/pull/117222">dotnet/runtime#117222</a>, and <a href="https://github.com/dotnet/runtime/pull/117295">dotnet/runtime#117295</a> enable that, enhancing escape analysis to determine if an object only escapes when such a generated type test fails (when the target object isn&rsquo;t of the expected common type).</p><p>I want to pause for a moment, because my words thus far aren&rsquo;t nearly enthusiastic enough to highlight the magnitude of what this enables. The <code>dotnet/runtime</code> repo uses an automated performance analysis system which flags when benchmarks significantly improve or regress and ties those changes back to the responsible PR. This is what it looked like for this PR: <img src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2025/09/ConditionalEscapeAnalysisImprovements.png" alt="Conditional Escape Analysis Triggering Many Benchmark Improvements"> We can see why this is so good from a simple example:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Runtime.CompilerServices; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private int[] _values = Enumerable.Range(1, 100).ToArray(); [Benchmark] public int Sum() =&gt; Sum(_values); [MethodImpl(MethodImplOptions.NoInlining)] private static int Sum(IEnumerable<int> values) { int sum = 0; foreach (int value in values) { sum += value; } return sum; } }</int></code></pre><p>With dynamic PGO, the instrumented code for <code>Sum</code> will see that <code>values</code> is generally an <code>int[]</code>, and it&rsquo;ll be able to emit a specialized code path in the optimized <code>Sum</code> implementation for when it is. And then with this ability to do conditional escape analysis, for the common path the JIT can see that the resulting <code>GetEnumerator</code> produces an <code>IEnumerator<int></int></code> that never escapes, such that along with all of the relevant methods being devirtualized and inlined, the enumerator can be stack allocated.</p><table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>Sum</td> <td>.NET 9.0</td> <td>109.86 ns</td> <td>1.00</td> <td>32 B</td> <td>1.00</td> </tr> <tr> <td>Sum</td> <td>.NET 10.0</td> <td>35.45 ns</td> <td>0.32</td> <td>&ndash;</td> <td>0.00</td> </tr> </tbody> </table><p>Just think about how many places in your apps and services you enumerate collections like this, and you can see why it&rsquo;s such an exciting improvement. Note that these cases don&rsquo;t always even require PGO. Consider a case like this:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private static readonly IEnumerable<int> s_values = new int[] { 1, 2, 3, 4, 5 }; [Benchmark] public int Sum() { int sum = 0; foreach (int value in s_values) { sum += value; } return sum; } }</int></code></pre><p>Here, the JIT can see that even though the <code>s_values</code> is typed as <code>IEnumerable<int></int></code>, it&rsquo;s always actually an <code>int[]</code>. In that case, <a href="https://github.com/dotnet/runtime/pull/111948">dotnet/runtime#111948</a> enables the return type to be retyped in the JIT as <code>int[]</code> and the enumerator can be stack allocated.</p><table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>Sum</td> <td>.NET 9.0</td> <td>16.341 ns</td> <td>1.00</td> <td>32 B</td> <td>1.00</td> </tr> <tr> <td>Sum</td> <td>.NET 10.0</td> <td>2.059 ns</td> <td>0.13</td> <td>&ndash;</td> <td>0.00</td> </tr> </tbody> </table><p>Of course, too much cloning can be a bad thing, in particular as it increases code size. <a href="https://github.com/dotnet/runtime/pull/108771">dotnet/runtime#108771</a> employs a heuristic to determine whether loops that <em>can</em> be cloned <em>should</em> be cloned; the larger the loop, the less likely it&rsquo;ll be to be cloned.</p><h3>Inlining</h3><p>&ldquo;Inlining&rdquo;, which replaces a call to a function with a copy of that function&rsquo;s implementation, has always been a critically important optimization. It&rsquo;s easy to think about the benefits of inlining as just being about avoiding the overhead of a call, and while that can be meaningful (especially when considering security mechanisms like Intel&rsquo;s Control-Flow Enforcement Technology, which slightly increases the cost of calls), generally the most benefit from inlining comes from knock-on benefits. Just as a simple example, if you have code like:</p><pre><code>int i = Divide(10, 5); static int Divide(int n, int d) =&gt; n / d;</code></pre><p>if <code>Divide</code> doesn&rsquo;t get inlined, then when <code>Divide</code> is called, it&rsquo;ll need to perform the actual <code>idiv</code>, which is a relatively expensive operation. In contrast, if <code>Divide</code> is inlined, then the call site becomes:</p><pre><code>int i = 10 / 5;</code></pre><p>which can be evaluated at compile time and becomes just:</p><pre><code>int i = 2;</code></pre><p>More compelling examples were already seen throughout the discussion of escape analysis and stack allocation, which depend heavily on the ability to inline methods. Given the increased importance of inlining, it&rsquo;s gotten even more focus in .NET 10.</p><p>Some of the .NET work related to inlining is about enabling more kinds of things to be inlined. Historically, a variety of constructs present in a method would prevent that method from even being considered for inlining. Arguably the most well known of these is exception handling: methods with exception handling clauses, e.g. <code>try/catch</code> or <code>try/finally</code>, would not be inlined. Even a simple method like <code>M</code> in this example:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private readonly object _o = new(); [Benchmark] public int Test() { M(_o); return 42; } private static void M(object o) { Monitor.Enter(o); try { } finally { Monitor.Exit(o); } } }</code></pre><p>does not get inlined on .NET 9:</p><pre><code>; .NET 9 ; Tests.Test() push rax mov rdi,[rdi+8] call qword ptr [78F199864EE8]; Tests.M(System.Object) mov eax,2A add rsp,8 ret ; Total bytes of code 21</code></pre><p>But with a plethora of PRs, in particular <a href="https://github.com/dotnet/runtime/pull/112968">dotnet/runtime#112968</a>, <a href="https://github.com/dotnet/runtime/pull/113023">dotnet/runtime#113023</a>, <a href="https://github.com/dotnet/runtime/pull/113497">dotnet/runtime#113497</a>, and <a href="https://github.com/dotnet/runtime/pull/112998">dotnet/runtime#112998</a>, methods containing <code>try/finally</code> are no longer blocked from inlining (<code>try/catch</code> regions are still a challenge). For the same benchmark on .NET 10, we now get this assembly:</p><pre><code>; .NET 10 ; Tests.Test() push rbp push rbx push rax lea rbp,[rsp+10] mov rbx,[rdi+8] test rbx,rbx je short M00_L03 mov rdi,rbx call 00007920A0EE65E0 test eax,eax je short M00_L02 M00_L00: mov rdi,rbx call 00007920A0EE6D50 test eax,eax jne short M00_L04 M00_L01: mov eax,2A add rsp,8 pop rbx pop rbp ret M00_L02: mov rdi,rbx call qword ptr [79202393C1F8] jmp short M00_L00 M00_L03: xor edi,edi call qword ptr [79202393C1C8] int 3 M00_L04: mov edi,eax mov rsi,rbx call qword ptr [79202393C1E0] jmp short M00_L01 ; Total bytes of code 86</code></pre><p>The details of the assembly don&rsquo;t matter, other than it&rsquo;s a whole lot more than was there before, because we&rsquo;re now looking in large part at the implementation of <code>M</code>. In addition to methods with <code>try/finally</code> now being inlineable, other improvements have also been made around exception handling. For example, <a href="https://github.com/dotnet/runtime/pull/110273">dotnet/runtime#110273</a> and <a href="https://github.com/dotnet/runtime/pull/110464">dotnet/runtime#110464</a> enable the removal of <code>try/catch</code> and <code>try/fault</code> blocks if it can prove the <code>try</code> block can&rsquo;t possibly throw. Consider this:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "i")] public partial class Tests { [Benchmark] [Arguments(42)] public int Test(int i) { try { i++; } catch { Console.WriteLine("Exception caught"); } return i; } }</code></pre><p>There&rsquo;s nothing the <code>try</code> block here can do that will result in an exception being thrown (assuming the developer hasn&rsquo;t enabled checked arithmetic, in which case it could possibly throw an <code>OverflowException</code>), yet on .NET 9 we get this assembly:</p><pre><code>; .NET 9 ; Tests.Test(Int32) push rbp sub rsp,10 lea rbp,[rsp+10] mov [rbp-10],rsp mov [rbp-4],esi mov eax,[rbp-4] inc eax mov [rbp-4],eax M00_L00: mov eax,[rbp-4] add rsp,10 pop rbp ret push rbp sub rsp,10 mov rbp,[rdi] mov [rsp],rbp lea rbp,[rbp+10] mov rdi,784B08950018 call qword ptr [784B0DE44EE8] lea rax,[M00_L00] add rsp,10 pop rbp ret ; Total bytes of code 79</code></pre><p>Now on .NET 10, the JIT is able to elide the <code>catch</code> and remove all ceremony related to the <code>try</code> because it can see that ceremony is pointless overhead.</p><pre><code>; .NET 10 ; Tests.Test(Int32) lea eax,[rsi+1] ret ; Total bytes of code 4</code></pre><p>That&rsquo;s true even when the contents of the <code>try</code> calls into other methods that are then inlined, exposing their contents to the JIT&rsquo;s analysis.</p><p>(As an aside, the JIT was already able to remove <code>try/finally</code> when the <code>finally</code> was empty, but <a href="https://github.com/dotnet/runtime/pull/108003">dotnet/runtime#108003</a> catches even more cases of checking for empty <code>finally</code>s again after most other optimizations have been run, in case they revealed additional empty blocks.)</p><p>Another example is &ldquo;GVM&rdquo;. Previously, any method that called a GVM, or generic virtual method (a virtual method with a generic type parameter), would be blocked from being inlined.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private Base _base = new(); [Benchmark] public int Test() { M(); return 42; } private void M() =&gt; _base.M<object>(); } class Base { public virtual void M<t>() { } }</t></object></code></pre><p>On .NET 9, the above results in this assembly:</p><pre><code>; .NET 9 ; Tests.Test() push rax call qword ptr [728ED5664FD8]; Tests.M() mov eax,2A add rsp,8 ret ; Total bytes of code 17</code></pre><p>Now on .NET 10, with <a href="https://github.com/dotnet/runtime/pull/116773">dotnet/runtime#116773</a>, <code>M</code> can now be inlined.</p><pre><code>; .NET 10 ; Tests.Test() push rbp push rbx push rax lea rbp,[rsp+10] mov rbx,[rdi+8] mov rdi,rbx mov rsi,offset MT_Base mov rdx,78034C95D2A0 call System.Runtime.CompilerServices.VirtualDispatchHelpers.VirtualFunctionPointer(System.Object, IntPtr, IntPtr) mov rdi,rbx call rax mov eax,2A add rsp,8 pop rbx pop rbp ret ; Total bytes of code 57</code></pre><p>Another area of investment with inlining is to do with the heuristics around when methods should be inlined. Just inlining everything would be bad; inlining copies code, which results in more code, which can have significant negative repercussions. For example, inlining&rsquo;s increased code size puts more pressure on caches. Processors have an instruction cache, a small amount of super fast memory in a CPU that stores recently used instructions, making them really fast to access again the next time they&rsquo;re needed (such as the next iteration through a loop, or the next time that same function is called). Consider a method <code>M</code>, and 100 call sites to <code>M</code> that are all being accessed. If all of those share the same instructions for <code>M</code>, because the 100 call sites are all actually calling <code>M</code>, the instruction cache will only need to load <code>M</code>&lsquo;s instructions once. If all of those 100 call sites each have their own copy of <code>M</code>&lsquo;s instructions, then all 100 copies will separately be loaded through the cache, fighting with each other and other instructions for residence. The less likely it is that instructions are in the cache, the more likely it is that the CPU will stall waiting for the instructions to be loaded from memory.</p><p>For this reason, the JIT needs to be careful what it inlines. It tries hard to avoid inlining anything that won&rsquo;t benefit (e.g. a larger method whose instructions won&rsquo;t be materially influenced by the caller&rsquo;s context) while also trying hard to inline anything that will materially benefit (e.g. small functions where the code required to call the function is similar in size to the contents of the function, functions with instructions that could be materially impacted by information from the call site, etc.) As part of these heuristics, the JIT has the notion of &ldquo;boosts,&rdquo; where observations it makes about things methods do boost the chances of that method being inlined. <a href="https://github.com/dotnet/runtime/pull/114806">dotnet/runtime#114806</a> gives a boost to methods that appear to be returning new arrays of a small, fixed length; if those arrays can instead be allocated in the caller&rsquo;s frame, the JIT might then be able to discover they don&rsquo;t escape and enable them to be stack allocated. <a href="https://github.com/dotnet/runtime/pull/110596">dotnet/runtime#110596</a> similarly looks for boxing, as the caller could possibly instead avoid the box entirely.</p><p>For the same purpose (and also just to minimize time spent performing compilation), the JIT also maintains a budget for how much it allows to be inlined into a method compilation&hellip; once it hits that budget, it might stop inlining anything. The budgeting scheme overall works <em>ok</em>, however in certain circumstances it can run out of budget at very inopportune times, for example doing a lot of inlining at top-level call sites but then running out of budget by the time it gets to small methods that are critically-important to inline for good performance. To help mitigate these scenarios, <a href="https://github.com/dotnet/runtime/pull/114191">dotnet/runtime#114191</a> and <a href="https://github.com/dotnet/runtime/pull/118641">dotnet/runtime#118641</a> more than double the JIT&rsquo;s default inlining budget.</p><p>The JIT also pays a lot of attention to the number of local variables (e.g. parameters/locals explicitly in the IL, JIT-created temporary locals, promoted struct fields, etc.) it tracks. To avoid creating too many, the JIT would stop inlining once it was already tracking 512. But as other changes have made inlining more aggressive, this (strangely hardcoded) limit gets hit more often, leaving very valuable inlinees out in the cold. <a href="https://github.com/dotnet/runtime/pull/118515">dotnet/runtime#118515</a> removed this fixed limit and instead ties it to a large percentage of the number of locals the JIT is allowed to track (by default, this ends up almost doubling the limit used by the inliner).</p><h3>Constant Folding</h3><p>Constant folding is a compiler&rsquo;s ability to perform operations, typically math, at compile-time rather than at run-time: given multiple constants and an expressed relationship between them, the compiler can &ldquo;fold&rdquo; those constants together into a new constant. So, if you have the C# code <code>int M(int i) =&gt; i + 2 * 3;</code>, the C# compiler does constant folding and emits that into your compilation as if you&rsquo;d written <code>int M(int i) =&gt; i + 6;</code>. The JIT can and does also do constant folding, which is valuable especially when it&rsquo;s based on information not available to the C# compiler. For example, the JIT can treat <code>static readonly</code> fields or <code>IntPtr.Size</code> or <code>Vector128<t>.Count</t></code> as constants. And the JIT can do folding across inlines. For example, if you have:</p><pre><code>int M1(int i) =&gt; i + M2(2 * 3); int M2(int j) =&gt; j * Environment.ProcessorCount;</code></pre><p>the C# compiler will only be able to fold the <code>2 * 3</code>, and will emit the equivalent of:</p><pre><code>int M1(int i) =&gt; i + M2(6); int M2(int j) =&gt; j * Environment.ProcessorCount;</code></pre><p>but when compiling <code>M1</code>, the JIT can inline <code>M2</code> and treat <code>ProcessorCount</code> as a constant (on my machine it&rsquo;s 16), and produce the following assembly code for <code>M1</code>:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "i")] public partial class Tests { [Benchmark] [Arguments(42)] public int M1(int i) =&gt; i + M2(6); private int M2(int j) =&gt; j * Environment.ProcessorCount; }</code></pre><pre><code>; .NET 9 ; Tests.M1(Int32) lea eax,[rsi+60] ret ; Total bytes of code 4</code></pre><p>That&rsquo;s as if the code for <code>M1</code> had been <code>public int M1(int i) =&gt; i + 96;</code> (the displayed assembly renders hexadecimal, so the <code>60</code> is hexadecimal <code>0x60</code> and thus decimal <code>96</code>).</p><p>Or consider:</p><pre><code>string M() =&gt; GetString() ?? throw new Exception(); static string GetString() =&gt; "test";</code></pre><p>The JIT will be able to inline <code>GetString</code>, at which point it can see that the result is non-<code>null</code> and can fold away the check for the <code>null</code> constant, at which point it can also dead-code eliminate the <code>throw</code>. Constant folding is useful on its own in avoiding unnecessary work, but it also often unlocks other optimizations, like dead-code elimination and bounds-check elimination. The JIT is already quite good at finding constant folding opportunities, and gets better in .NET 10. Consider this benchmark:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "s")] public partial class Tests { [Benchmark] [Arguments("test")] public ReadOnlySpan<char> Test(string s) { s ??= ""; return s.AsSpan(); } }</char></code></pre><p>Here&rsquo;s the assembly that gets produced for .NET 9:</p><pre><code>; .NET 9 ; Tests.Test(System.String) push rbp mov rbp,rsp mov rax,75B5D6200008 test rsi,rsi cmove rsi,rax test rsi,rsi jne short M00_L01 xor eax,eax xor edx,edx M00_L00: pop rbp ret M00_L01: lea rax,[rsi+0C] mov edx,[rsi+8] jmp short M00_L00 ; Total bytes of code 41</code></pre><p>Of particular note are those two <code>test rsi,rsi</code> instructions, which are <code>null</code> checks. The assembly starts by loading a value into <code>rax</code>; that value is the address of the <code>""</code> string literal. It then uses <code>test rsi,rsi</code> to check whether the <code>s</code> parameter, which was passed into this instance method in the <code>rsi</code> register, is <code>null</code>. If it is <code>null</code>, the <code>cmove rsi,rax</code> instruction sets it to the address of the <code>""</code> literal. And then&hellip; it does <code>test rsi,rsi</code> again? That second test is the <code>null</code> check at the beginning of <code>AsSpan</code>, which looks like this:</p><pre><code>public static ReadOnlySpan<char> AsSpan(this string? text) { if (text is null) return default; return new ReadOnlySpan<char>(ref text.GetRawStringData(), text.Length); }</char></char></code></pre><p>Now with <a href="https://github.com/dotnet/runtime/pull/111985">dotnet/runtime#111985</a>, that second <code>null</code> check, along with others, can be folded, resulting in this:</p><pre><code>; .NET 10 ; Tests.Test(System.String) mov rax,7C01C4600008 test rsi,rsi cmove rsi,rax lea rax,[rsi+0C] mov edx,[rsi+8] ret ; Total bytes of code 25</code></pre><p>Similar impact comes from <a href="https://github.com/dotnet/runtime/pull/108420">dotnet/runtime#108420</a>, which is also able to fold a different class of <code>null</code> checks.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "condition")] public partial class Tests { [Benchmark] [Arguments(true)] public bool Test(bool condition) { string tmp = condition ? GetString1() : GetString2(); return tmp is not null; } private static string GetString1() =&gt; "Hello"; private static string GetString2() =&gt; "World"; }</code></pre><p>In this benchmark, <em>we</em> can see that neither <code>GetString1</code> nor <code>GetString2</code> return <code>null</code>, and thus the <code>is not null</code> check shouldn&rsquo;t be necessary. The JIT in .NET 9 couldn&rsquo;t see that, but its improved .NET 10 self can.</p><pre><code>; .NET 9 ; Tests.Test(Boolean) mov rax,7407F000A018 mov rcx,7407F000A050 test sil,sil cmove rax,rcx test rax,rax setne al movzx eax,al ret ; Total bytes of code 37 ; .NET 10 ; Tests.Test(Boolean) mov eax,1 ret ; Total bytes of code 6</code></pre><p>Constant folding also applies to SIMD (Single Instruction Multiple Data), instructions that enable processing multiple pieces of data at once rather than only one element at a time. <a href="https://github.com/dotnet/runtime/pull/117099">dotnet/runtime#117099</a> and <a href="https://github.com/dotnet/runtime/pull/117572">dotnet/runtime#117572</a> both enable more SIMD comparison operations to participate in folding.</p><h3>Code Layout</h3><p>When the JIT compiler generates assembly from the IL emitted by the C# compiler, it organizes that code into &ldquo;basic blocks,&rdquo; a sequence of instructions with one entry point and one exit point, no jumps inside, no branches out except at the end. These blocks can then be moved around as a unit, and the order in which these blocks are placed in memory is referred to as &ldquo;code layout&rdquo; or &ldquo;basic block layout.&rdquo; This ordering can have a significant performance impact because modern CPUs rely heavily on an instruction cache and on branch prediction to keep things moving fast. If frequently executed (&ldquo;hot&rdquo;) blocks are close together and follow a common execution path, the CPU can execute them with fewer cache misses and fewer mispredicted jumps. If the layout is poor, where the hot code is split into pieces far apart from each other, or where rarely executed (&ldquo;cold&rdquo;) code sits in between, the CPU can spend more time jumping around and refilling caches than doing actual work. Consider a tight loop executed millions of times. A good layout keeps the loop entry, body, and backward edge (the jump back to the beginning of the body to do the next iteration) right next to each other, letting the CPU fetch them straight from the cache. In a bad layout, that loop might be interwoven with unrelated cold blocks (say, a <code>catch</code> block for a <code>try</code> in the loop), forcing the CPU to load instructions from different places and disrupting the flow. Similarly, for an <code>if</code> block, the likely path should generally be the next block so no jump is required, with the unlikely branch behind a short jump away, as that better aligns with the sensibilities of branch predictors. Code layout heuristics control how that happens, and as a result, how efficient the resulting code is able to execute.</p><p>When determining the starting layout of the blocks (before additional optimizations are done for the layout), <a href="https://github.com/dotnet/runtime/pull/108903">dotnet/runtime#108903</a> employs a &ldquo;loop-aware reverse post-order&rdquo; traversal. A reverse post-order traversal is an algorithm for visiting the nodes in a control flow graph such that each block appears after its predecessors. The &ldquo;loop aware&rdquo; part means the traversal recognizes loops as units, effectively creating a block around the whole loop, and tries to keep the whole loop together as the layout algorithm moves things around. The intent here is to start the larger layout optimizations from a more sensible place, reducing the amount of later reshuffling and situations where loop bodies get broken up.</p><p>In the extreme, layout is essentially the <a href="https://en.wikipedia.org/wiki/Travelling_salesman_problem">traveling salesman problem</a>. The JIT must decide the order of basic blocks so that control transfers follow short, predictable paths and make efficient use of instruction cache and branch prediction. Just like the salesman visiting cities with minimal total travel distance, the compiler is trying to arrange blocks so that the &ldquo;distance&rdquo; between blocks, which might be measured in bytes or instruction fetch cost or something similar, is minimized. For any meaningfully-sized set of blocks, this is prohibitively expensive to compute optimally, as the number of possible orderings grows factorially with the number of blocks. Thus, the JIT has to rely on approximations rather than attempting an exact solution. One such approximation it employs now as of <a href="https://github.com/dotnet/runtime/pull/103450">dotnet/runtime#103450</a> (and then tweaked further in <a href="https://github.com/dotnet/runtime/pull/109741">dotnet/runtime#109741</a> and <a href="https://github.com/dotnet/runtime/pull/109835">dotnet/runtime#109835</a>) is a &ldquo;3-opt,&rdquo; which really just means that rather than considering all blocks together, it looks at only three and tries to produce an optimal ordering amongst those (there are only eight possible orderings to be checked). The JIT can choose to iterate through sets of three blocks until either it doesn&rsquo;t see any more improvements or hits a self-imposed limit. Specifically when handling backward jumps, with <a href="https://github.com/dotnet/runtime/pull/110277">dotnet/runtime#110277</a>, it expands this &ldquo;3-opt&rdquo; to &ldquo;4-opt&rdquo; (four blocks).</p><p>.NET 10 also does a better job of factoring PGO data into layout. With dynamic PGO, the JIT is able to gather instrumentation data from an initial compilation and then use the results of that profiling to impact an optimized re-compilation. That data can lead to conclusions about what blocks are hot or cold, and which direction branches take, all information that&rsquo;s valuable for layout optimization. However, data can sometimes be missing from these profiles, so the JIT has a &ldquo;profile synthesis&rdquo; algorithm that makes realistic guesses for these gaps in order to fill them in (if you&rsquo;ve read or seen &ldquo;Jurassic Park,&rdquo; this is the JIT-equivalent to filling in gaps in the dinosaur DNA sequences with that from present-day frogs.) With <a href="https://github.com/dotnet/runtime/pull/111915">dotnet/runtime#111915</a>, that repairing of the profile data is now performed just before layout, so that layout has a more complete picture.</p><p>Let&rsquo;s take a concrete example of all this. Here I&rsquo;ve extracted the core function from <code>MemoryExtensions.BinarySearch</code>:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Runtime.CompilerServices; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private int[] _values = Enumerable.Range(0, 512).ToArray(); [Benchmark] public int BinarySearch() { int[] values = _values; return BinarySearch(ref values[0], values.Length, 256); } [MethodImpl(MethodImplOptions.NoInlining)] private static int BinarySearch<t>( ref T spanStart, int length, TComparable comparable) where TComparable : IComparable<t>, allows ref struct { int lo = 0; int hi = length - 1; while (lo &lt;= hi) { int i = (int)(((uint)hi + (uint)lo) &gt;&gt; 1); int c = comparable.CompareTo(Unsafe.Add(ref spanStart, i)); if (c == 0) { return i; } else if (c &gt; 0) { lo = i + 1; } else { hi = i - 1; } } return ~lo; } }</t></t></code></pre><p>And here&rsquo;s the assembly we get for .NET 9 and .NET 10, diff&rsquo;d from the former to the latter:</p><pre><code>; Tests.BinarySearch[[System.Int32, System.Private.CoreLib],[System.Int32, System.Private.CoreLib]](Int32 ByRef, Int32, Int32) push rbp mov rbp,rsp xor ecx,ecx dec esi js short M01_L07 + jmp short M01_L03 M01_L00: - lea eax,[rsi+rcx] - shr eax,1 - movsxd r8,eax - mov r8d,[rdi+r8*4] - cmp edx,r8d - jge short M01_L03 mov r9d,0FFFFFFFF M01_L01: test r9d,r9d je short M01_L06 test r9d,r9d jg short M01_L05 lea esi,[rax-1] M01_L02: cmp ecx,esi - jle short M01_L00 - jmp short M01_L07 + jg short M01_L07 M01_L03: + lea eax,[rsi+rcx] + shr eax,1 + movsxd r8,eax + mov r8d,[rdi+r8*4] cmp edx,r8d - jg short M01_L04 - xor r9d,r9d + jl short M01_L00 + cmp edx,r8d + jle short M01_L04 + mov r9d,1 jmp short M01_L01 M01_L04: - mov r9d,1 + xor r9d,r9d jmp short M01_L01 M01_L05: lea ecx,[rax+1] jmp short M01_L02 M01_L06: pop rbp ret M01_L07: mov eax,ecx not eax pop rbp ret ; Total bytes of code 83</code></pre><p>We can see that the main change here is a block that&rsquo;s moved (the bulk of <code>M01_L00</code> moving down to <code>M01_L03</code>). In .NET 9, the <code>lo &lt;= hi</code> &ldquo;stay in the loop check&rdquo; (<code>cmp ecx,esi</code>) is a backward conditional branch (<code>jle short M01_L00</code>), where every iteration of the loop except for the last jumps back to the top (<code>M01_L00</code>). In .NET 10, it instead does a forward branch to exit the loop only in the rarer case, otherwise falling through to the body of the loop in the common case, and then unconditionally branching back.</p><h3>GC Write Barriers</h3><p>The .NET garbage collector (GC) works on a generational model, organizing the managed heap according to how long objects have been alive. The newest allocations land in &ldquo;generation 0&rdquo; (gen0), objects that have survived at least one collection are promoted to &ldquo;generation 1&rdquo; (gen1), and those that have been around for longer end up in &ldquo;generation 2&rdquo; (gen2). This is based on the premise that most objects are temporary, and that once an object has been around for a while, it&rsquo;s likely to stick around for a while longer. Splitting up the heap into generations enables for quickly collecting gen0 objects by only scanning the gen0 heap for remaining references to that object. The expectation is that all, or at least the vast majority, of references to a gen0 object are also in gen0. Of course, if a reference to a gen0 object snuck into gen1 or gen2, not scanning gen1 or gen2 during a gen0 collection could be, well, bad. To avoid that case, the JIT collaborates with the GC to track references from older to younger generations. Whenever there&rsquo;s a reference write that could cross a generation, the JIT emits a call to a helper that tracks the information in a &ldquo;card table,&rdquo; and when the GC runs, it consults this table to see if it needs to scan a portion of the higher generations. That helper is referred to as a &ldquo;GC write barrier.&rdquo; Since a write barrier is potentially employed on every reference write, it must be super fast, and in fact the runtime has several different variations of write barriers so that the JIT can pick one optimized for the given situation. Of course, the fastest write barrier is one that doesn&rsquo;t need to exist at all, so as with bounds checks, the JIT also exerts energy to try to prove when write barriers aren&rsquo;t needed, eliding them when it can. And it can even more in .NET 10.</p><p><code>ref structs</code>, referred to in runtime vernacular as &ldquo;byref-like types,&rdquo; can never live on the heap, which means any reference fields in them will similarly never live on the heap. As such, if the JIT can prove that a reference write is targeting a field of a <code>ref struct</code>, it can elide the write barrier. Consider this example:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private object _object = new(); [Benchmark] public MyRefStruct Test() =&gt; new MyRefStruct() { Obj1 = _object, Obj2 = _object, Obj3 = _object }; public ref struct MyRefStruct { public object Obj1; public object Obj2; public object Obj3; } }</code></pre><p>In the .NET 9 assembly, we can see three write barriers (<code>CORINFO_HELP_CHECKED_ASSIGN_REF</code>) corresponding to the three fields in <code>MyRefStruct</code> in the benchmark:</p><pre><code>; .NET 9 ; Tests.Test() push r15 push r14 push rbx mov rbx,rsi mov r15,[rdi+8] mov rsi,r15 mov r14,r15 mov rdi,rbx call CORINFO_HELP_CHECKED_ASSIGN_REF lea rdi,[rbx+8] mov rsi,r14 call CORINFO_HELP_CHECKED_ASSIGN_REF lea rdi,[rbx+10] mov rsi,r15 call CORINFO_HELP_CHECKED_ASSIGN_REF mov rax,rbx pop rbx pop r14 pop r15 ret ; Total bytes of code 59</code></pre><p>With <a href="https://github.com/dotnet/runtime/pull/111576">dotnet/runtime#111576</a> and <a href="https://github.com/dotnet/runtime/pull/111733">dotnet/runtime#111733</a> in .NET 10, all of those write barriers are elided:</p><pre><code>; .NET 10 ; Tests.Test() mov rax,[rdi+8] mov rcx,rax mov rdx,rax mov [rsi],rcx mov [rsi+8],rdx mov [rsi+10],rax mov rax,rsi ret ; Total bytes of code 25</code></pre><p>Much more impactful, however, are <a href="https://github.com/dotnet/runtime/pull/112060">dotnet/runtime#112060</a> and <a href="https://github.com/dotnet/runtime/pull/112227">dotnet/runtime#112227</a>, which have to do with &ldquo;return buffers.&rdquo; When a .NET method is typed to return a value, the runtime has to decide how that value gets from the callee back to the caller. For small types, like integers, floating-point numbers, pointers, or object references, the answer is simple: the value can be passed back via one or more CPU registers reserved for return values, making the operation essentially free. But not all values fit neatly into registers. Larger value types, such as structs with multiple fields, require a different strategy. In these cases, the caller allocates a &ldquo;return buffer,&rdquo; a block of memory, typically in the caller&rsquo;s stack frame, and the caller passes a pointer to that buffer as a hidden argument to the method. The method then writes the return value directly into that buffer in order to provide the caller with the data. When it comes to write barriers, the challenge here is that there historically hasn&rsquo;t been a requirement that the return buffer be on the stack; it&rsquo;s technically feasible it could have been allocated on the heap, even if it rarely or never is. And since the callee doesn&rsquo;t know where the buffer lives, any object reference writes needed to be tracked with GC write barriers. We can see that with a simple benchmark:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private string _firstName = "Jane", _lastName = "Smith", _address = "123 Main St", _city = "Anytown"; [Benchmark] public Person GetPerson() =&gt; new(_firstName, _lastName, _address, _city); public record struct Person(string FirstName, string LastName, string Address, string City); }</code></pre><p>On .NET 9, each field of the returned value type is incurring a <code>CORINFO_HELP_CHECKED_ASSIGN_REF</code> write barrier:</p><pre><code>; .NET 9 ; Tests.GetPerson() push r15 push r14 push r13 push rbx mov rbx,rsi mov rsi,[rdi+8] mov r15,[rdi+10] mov r14,[rdi+18] mov r13,[rdi+20] mov rdi,rbx call CORINFO_HELP_CHECKED_ASSIGN_REF lea rdi,[rbx+8] mov rsi,r15 call CORINFO_HELP_CHECKED_ASSIGN_REF lea rdi,[rbx+10] mov rsi,r14 call CORINFO_HELP_CHECKED_ASSIGN_REF lea rdi,[rbx+18] mov rsi,r13 call CORINFO_HELP_CHECKED_ASSIGN_REF mov rax,rbx pop rbx pop r13 pop r14 pop r15 ret ; Total bytes of code 81</code></pre><p>Now in .NET 10, the calling convention has been updated to require that the return buffer live on the stack (if the caller wants the data somewhere else, it&rsquo;s responsible for subsequently doing that copy). And because the return buffer is now guaranteed to be on the stack, the JIT can elide all GC write barriers as part of returning values.</p><pre><code>; .NET 10 ; Tests.GetPerson() mov rax,[rdi+8] mov rcx,[rdi+10] mov rdx,[rdi+18] mov rdi,[rdi+20] mov [rsi],rax mov [rsi+8],rcx mov [rsi+10],rdx mov [rsi+18],rdi mov rax,rsi ret ; Total bytes of code 35</code></pre><p><a href="https://github.com/dotnet/runtime/pull/111636">dotnet/runtime#111636</a> from <a href="https://github.com/a74nh">@a74nh</a> is also interesting from a performance perspective because, as is common in optimization, it trades off one thing for another. Prior to this change, Arm64 had one universal write barrier helper for all GC modes. This change brings Arm64 in line with x64 by routing through a <code>WriteBarrierManager</code> that selects among multiple <code>JIT_WriteBarrier</code> variants based on runtime configuration. In doing so, it makes each Arm64 write barrier a bit more expensive, by adding region checks and moving to a region-aware card marking scheme, but in exchange it lets the GC do less work: fewer cards in the card table get marked, and the GC can scan more precisely. <a href="https://github.com/dotnet/runtime/pull/106191">dotnet/runtime#106191</a> also helps reduce the cost of write barriers on Arm64 by tightening the hot-path comparisons and eliminating some avoidable saves and restores.</p><h3>Instruction Sets</h3><p>.NET continues to see meaningful optimizations and improvements across all supported architectures, along with various architecture-specific improvements. Here are a handful of examples.</p><h4>Arm SVE</h4><p>APIs for Arm SVE were introduced in .NET 9. As noted in the <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-9/#arm-sve">Arm SVE</a> section of last year&rsquo;s post, enabling SVE is a multi-year effort, and in .NET 10, support is still considered experimental. However, the support has continued to be improved and extended, with PRs like <a href="https://github.com/dotnet/runtime/pull/115775">dotnet/runtime#115775</a> from <a href="https://github.com/snickolls-arm">@snickolls-arm</a> adding <code>BitwiseSelect</code> methods, <a href="https://github.com/dotnet/runtime/pull/117711">dotnet/runtime#117711</a> from <a href="https://github.com/jacob-crawley">@jacob-crawley</a> adding <code>MaxPairwise</code> and <code>MinPairwise</code> methods, and <a href="https://github.com/dotnet/runtime/pull/117051">dotnet/runtime#117051</a> from <a href="https://github.com/jonathandavies-arm">@jonathandavies-arm</a> adding <code>VectorTableLookup</code> methods.</p><h4>Arm64</h4><p><a href="https://github.com/dotnet/runtime/pull/111893">dotnet/runtime#111893</a> from <a href="https://github.com/jonathandavies-arm">@jonathandavies-arm</a>, <a href="https://github.com/dotnet/runtime/pull/111904">dotnet/runtime#111904</a> from <a href="https://github.com/jonathandavies-arm">@jonathandavies-arm</a>, <a href="https://github.com/dotnet/runtime/pull/111452">dotnet/runtime#111452</a> from <a href="https://github.com/jonathandavies-arm">@jonathandavies-arm</a>, <a href="https://github.com/dotnet/runtime/pull/112235">dotnet/runtime#112235</a> from <a href="https://github.com/jonathandavies-arm">@jonathandavies-arm</a>, and <a href="https://github.com/dotnet/runtime/pull/111797">dotnet/runtime#111797</a> from <a href="https://github.com/snickolls-arm">@snickolls-arm</a> all improved .NET&rsquo;s support for utilizing Arm64&rsquo;s multi-operation compound instructions. For example, when implementing a compare and branch, rather than emitting a <code>cmp</code> against 0 followed by <code>beq</code> instruction, the JIT may now emit a <code>cbz</code> (&ldquo;Compare and Branch on Zero&rdquo;) instruction.</p><h4>APX</h4><p>Intel&rsquo;s Advanced Performance Extensions (APX) was announced in 2023 as an extension of the x86/x64 instruction set. It expands the number of general-purpose registers from 16 to 32 and adds new instructions such as conditional operations designed to reduce memory traffic, improve performance, and lower power consumption. <a href="https://github.com/dotnet/runtime/pull/106557">dotnet/runtime#106557</a> from <a href="https://github.com/Ruihan-Yin">@Ruihan-Yin</a>, <a href="https://github.com/dotnet/runtime/pull/108796">dotnet/runtime#108796</a> from <a href="https://github.com/Ruihan-Yin">@Ruihan-Yin</a>, and <a href="https://github.com/dotnet/runtime/pull/113237">dotnet/runtime#113237</a> from <a href="https://github.com/Ruihan-Yin">@Ruihan-Yin</a> essentially teach the JIT how to speak the new dialect of assembly code (the REX and expanded EVEX encodings), and <a href="https://github.com/dotnet/runtime/pull/108799">dotnet/runtime#108799</a> from <a href="https://github.com/Ruihan-Yin">@Ruihan-Yin</a> updates the JIT to be able to use the expanded set of registers. The most impactful new instructions in APX are around conditional compares (<code>ccmp</code>), a concept the JIT already supports from targeting other instruction sets, and <a href="https://github.com/dotnet/runtime/pull/111072">dotnet/runtime#111072</a> from <a href="https://github.com/anthonycanino">@anthonycanino</a>, <a href="https://github.com/dotnet/runtime/pull/112153">dotnet/runtime#112153</a> from <a href="https://github.com/anthonycanino">@anthonycanino</a>, and <a href="https://github.com/dotnet/runtime/pull/116445">dotnet/runtime#116445</a> from <a href="https://github.com/khushal1996">@khushal1996</a> all teach the JIT how to make good use of these new instructions with APX.</p><h4>AVX512</h4><p>.NET 8 added broad support for AVX512, and .NET 9 significantly improved its handling and adoption throughout the core libraries. .NET 10 includes a plethora of additional related optimizations:</p><ul> <li><a href="https://github.com/dotnet/runtime/pull/109258">dotnet/runtime#109258</a> from <a href="https://github.com/saucecontrol">@saucecontrol</a> and <a href="https://github.com/dotnet/runtime/pull/109267">dotnet/runtime#109267</a> from <a href="https://github.com/saucecontrol">@saucecontrol</a> expand the number of places the JIT is able to use EVEX embedded broadcasts, a feature that lets vector instructions read a single scalar element from memory and implicitly replicate it to all the lanes of the vector, without needing a separate broadcast or shuffle operation.</li> <li><a href="https://github.com/dotnet/runtime/pull/108824">dotnet/runtime#108824</a> removes a redundant sign extension from broadcasts.</li> <li><a href="https://github.com/dotnet/runtime/pull/116117">dotnet/runtime#116117</a> from <a href="https://github.com/alexcovington">@alexcovington</a> improves the code generated for <code>Vector.Max</code> and <code>Vector.Min</code> when AVX512 is supported.</li> <li><a href="https://github.com/dotnet/runtime/pull/109474">dotnet/runtime#109474</a> from <a href="https://github.com/saucecontrol">@saucecontrol</a> improves &ldquo;containment&rdquo; (where an instruction can be eliminated by having its behaviors fully encapsulated by another instruction) for AVX512 widening intrinsics (similar containment-related improvements were made in <a href="https://github.com/dotnet/runtime/pull/110736">dotnet/runtime#110736</a> from <a href="https://github.com/saucecontrol">@saucecontrol</a> and <a href="https://github.com/dotnet/runtime/pull/111778">dotnet/runtime#111778</a> from <a href="https://github.com/saucecontrol">@saucecontrol</a>).</li> <li><a href="https://github.com/dotnet/runtime/pull/111853">dotnet/runtime#111853</a> from <a href="https://github.com/saucecontrol">@saucecontrol</a> improves <code>Vector128/256/512.Dot</code> to be better accelerated with AVX512.</li> <li><a href="https://github.com/dotnet/runtime/pull/110195">dotnet/runtime#110195</a>, <a href="https://github.com/dotnet/runtime/pull/110307">dotnet/runtime#110307</a>, and <a href="https://github.com/dotnet/runtime/pull/117118">dotnet/runtime#117118</a> all improve how vector masks are handled. In AVX512, masks are special registers that can be included as part of various instructions to control which subset of vector elements should be utilized (each bit in a mask corresponds to one element in the vector). This enables operating on only part of a vector without needing extra branching or shuffling.</li> <li><a href="https://github.com/dotnet/runtime/pull/115981">dotnet/runtime#115981</a> improves zeroing (where the JIT emits instructions to zero out memory, often as part of initializing a stack frame) on AVX512. After zeroing as much as it can with 64-byte instructions, it was falling back to using 16-byte instructions, when it could have used 32-byte instructions.</li> <li><a href="https://github.com/dotnet/runtime/pull/110662">dotnet/runtime#110662</a> improves the code generated for <code>ExtractMostSignificantBits</code> (which is used by many of the searching algorithms in the core libraries) when working with <code>short</code> and <code>ushort</code> (and <code>char</code>, as most of those core library implementations reinterpret cast <code>char</code> as one of the others) by using EVEX mask support.</li> <li><a href="https://github.com/dotnet/runtime/pull/113864">dotnet/runtime#113864</a> from <a href="https://github.com/saucecontrol">@saucecontrol</a> improves the code generated for <code>ConditionalSelect</code> when not used with mask registers.</li> </ul> <h4>AVX10.2</h4><p>.NET 9 added support and intrinsics for the AVX10.1 instruction set. With <a href="https://github.com/dotnet/runtime/pull/111209">dotnet/runtime#111209</a> from <a href="https://github.com/khushal1996">@khushal1996</a>, .NET 10 adds support and intrinsics for the AVX10.2 instruction set. <a href="https://github.com/dotnet/runtime/pull/112535">dotnet/runtime#112535</a> from <a href="https://github.com/khushal1996">@khushal1996</a> optimizes floating-point min/max operations with AVX10.2 instructions, while <a href="https://github.com/dotnet/runtime/pull/111775">dotnet/runtime#111775</a> from <a href="https://github.com/khushal1996">@khushal1996</a> enables floating-point conversions to utilize AVX10.2.</p><h4>GFNI</h4><p><a href="https://github.com/dotnet/runtime/pull/109537">dotnet/runtime#109537</a> from <a href="https://github.com/saucecontrol">@saucecontrol</a> adds intrinsics for the GFNI (Galois Field New Instructions) instruction set, which can be used for accelerating operations over Galois fields GF(2^8). These are common in cryptography, error correction, and data encoding.</p><h4>VPCLMULQDQ</h4><p><code>VPCLMULQDQ</code> is an x86 instruction set extension that adds vector support to the older <code>PCLMULQDQ</code> instruction, which performs carry-less multiplication over 64-bit integers. <a href="https://github.com/dotnet/runtime/pull/109137">dotnet/runtime#109137</a> from <a href="https://github.com/saucecontrol">@saucecontrol</a> adds new intrinsic APIs for <code>VPCLMULQDQ</code>.</p><h3>Miscellaneous</h3><p>Many more PRs than the ones I&rsquo;ve already called out have gone into the JIT this release. Here are a few more:</p><ul> <li><strong>Eliminating some covariance checks</strong>. Writing into arrays of reference types can require &ldquo;covariance checks.&rdquo; Imagine you have a class <code>Base</code> and two derived types <code>Derived1 : Base</code> and <code>Derived2 : Base</code>. Since arrays in .NET are covariant, I can have a <code>Derived1[]</code> and cast it successfully to a <code>Base[]</code>, but under the covers that&rsquo;s still a <code>Derived1[]</code>. That means, for example, that any attempt to store a <code>Derived2</code> into that array should fail at runtime, even if it compiles. To achieve that, the JIT needs to insert such covariance checks when writing into arrays, but just like with bounds checking and write barriers, the JIT can elide those checks when it can prove statically that they&rsquo;re not necessary. Such an example is with sealed types. If the JIT sees an array of type <code>T[]</code> and <code>T</code> is known to be sealed, <code>T[]</code> must exactly be a <code>T[]</code> and not some <code>DerivedT[]</code>, because there can&rsquo;t be a <code>DerivedT</code>. So with a benchmark like this:<pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private List<string> _list = new() { "hello" }; [Benchmark] public void Set() =&gt; _list[0] = "world"; }</string></code></pre><p>as long as the JIT can see that the array underlying the <code>List<string></string></code> is a <code>string[]</code> (<code>string</code> is sealed), it shouldn&rsquo;t need a covariance check. In .NET 9, we get this:</p><pre><code>; .NET 9 ; Tests.Set() push rbx mov rbx,[rdi+8] cmp dword ptr [rbx+10],0 je short M00_L00 mov rdi,[rbx+8] xor esi,esi mov rdx,78914920A038 call System.Runtime.CompilerServices.CastHelpers.StelemRef(System.Object[], IntPtr, System.Object) inc dword ptr [rbx+14] pop rbx ret M00_L00: call qword ptr [78D1F80558A8] int 3 ; Total bytes of code 44</code></pre><p>Note that <code>CastHelpers.StelemRef</code> call&hellip; that&rsquo;s the helper that performs the write with the covariance check. But now in .NET 10, thanks to <a href="https://github.com/dotnet/runtime/pull/107116">dotnet/runtime#107116</a> (which teaches the JIT how to resolve the exact type for the field of the closed generic), we get this:</p><pre><code>; .NET 10 ; Tests.Set() push rbp mov rbp,rsp mov rax,[rdi+8] cmp dword ptr [rax+10],0 je short M00_L00 mov rcx,[rax+8] mov edx,[rcx+8] test rdx,rdx je short M00_L01 mov rdx,75E2B9009A40 mov [rcx+10],rdx inc dword ptr [rax+14] pop rbp ret M00_L00: call qword ptr [762368116760] int 3 M00_L01: call CORINFO_HELP_RNGCHKFAIL int 3 ; Total bytes of code 58</code></pre><p>No covariance check, thank you very much.</p></li> <li><strong>More strength reduction</strong>. &ldquo;Strength reduction&rdquo; is a classic compiler optimization that replaces more expensive operations, like multiplications, with cheaper ones, like additions. In .NET 9, this was used to transform indexed loops that used multiplied offsets (e.g. <code>index * elementSize</code>) into loops that simply incremented a pointer-like offset (e.g. <code>offset += elementSize</code>), cutting down on arithmetic overhead and improving performance. In .NET 10, strength reduction has been extended, in particular with <a href="https://github.com/dotnet/runtime/pull/110222">dotnet/runtime#110222</a>. This enables the JIT to detect multiple loop induction variables with different step sizes and strength-reduce them by leveraging their greatest common divisor (GCD). Essentially, it creates a single primary induction variable based on the GCD of the varying step sizes, and then recovers each original induction variable by appropriately scaling. Consider this example:<pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "numbers")] public partial class Tests { [Benchmark] [Arguments("128514801826028643102849196099776734920914944609068831724328541639470403818631040")] public int[] Parse(string numbers) { int[] results = new int[numbers.Length]; for (int i = 0; i &lt; numbers.Length; i++) { results[i] = numbers[i] - '0'; } return results; } }</code></pre><p>In this benchmark, we&rsquo;re iterating through an input <code>string</code>, which is a collection of 2-byte <code>char</code> elements, and we&rsquo;re storing the results into an array of 4-byte <code>int</code> elements. The core loop in the .NET 9 assembly looks like this:</p><pre><code>; .NET 9 M00_L00: mov edx,ecx movzx edi,word ptr [rbx+rdx*2+0C] add edi,0FFFFFFD0 mov [rax+rdx*4+10],edi inc ecx cmp r15d,ecx jg short M00_L00</code></pre><p>The <code>movzx edi,word ptr [rbx+rdx*2+0C]</code> is the read of <code>numbers[i]</code>, and the <code>mov [rax+rdx*4+10],edi</code> is the assignment to <code>results[i]</code>. <code>rdx</code> here is <code>i</code>, so each assignment is effectively having to do <code>i*2</code> to compute the byte offset of the <code>char</code> at index <code>i</code>, and similarly do <code>i*4</code> to compute the byte offset of the <code>int</code> at offset <code>i</code>. Now here&rsquo;s the .NET 10 assembly:</p><pre><code>; .NET 10 M00_L00: movzx edx,word ptr [rbx+rcx+0C] add edx,0FFFFFFD0 mov [rax+rcx*2+10],edx add rcx,2 dec r15d jne short M00_L00</code></pre><p>The multiplication in the <code>numbers[i]</code> read is gone. Instead, it can just increment <code>rcx</code> by 2 on each iteration, treating that as the offset to the <code>i</code>th <code>char</code>, and then instead of multiplying by 4 to compute the <code>int</code> offset, it just multiples by 2.</p></li> <li><strong>CSE integration with SSA</strong>. As with most compilers, the JIT employs common subexpression elimination (CSE) to find identical computations and avoid doing them repeatedly. <a href="https://github.com/dotnet/runtime/pull/106637">dotnet/runtime#106637</a> teaches the JIT how to do so in a more consistent manner by more fully integrating CSE with its Static Single Assignment (SSA) representation. This in turn allows for more optimizations to kick in, e.g. some of the strength reduction done around loop induction variables in .NET 9 wasn&rsquo;t applying as much as it should have, and now it will.</li> <li><strong><code>return someCondition ? true : false</code></strong>. There are often multiple ways to represent the same thing, but it often happens in compilers that certain patterns will be recognized during optimization while other equivalent ones won&rsquo;t, and it can therefore behoove the compiler to first normalize the representations to all use the better recognized one. There&rsquo;s a really common and interesting case of this with <code>return someCondition</code>, where, for reasons relating to the JIT&rsquo;s internal representation, the JIT is better able to optimize with the equivalent <code>return someCondition ? true : false</code>. <a href="https://github.com/dotnet/runtime/pull/107499">dotnet/runtime#107499</a> normalizes to the latter. As an example of this, consider this benchmark:<pre><code>// dotnet run -c Release -f net9.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "i")] public partial class Tests { [Benchmark] [Arguments(42)] public bool Test1(int i) { if (i &gt; 10 &amp;&amp; i &lt; 20) return true; return false; } [Benchmark] [Arguments(42)] public bool Test2(int i) =&gt; i &gt; 10 &amp;&amp; i &lt; 20; }</code></pre><p>On .NET 9, that results in this assembly code for <code>Test1</code>:</p><pre><code>; .NET 9 ; Tests.Test1(Int32) sub esi,0B cmp esi,8 setbe al movzx eax,al ret ; Total bytes of code 13</code></pre><p>The JIT has successfully recognized that it can change the two comparisons to instead be a subtraction and a single comparison, as if the <code>i &gt; 10 &amp;&amp; i &lt; 20</code> were instead written as <code>(uint)(i - 11) &lt;= 8</code>. But for <code>Test2</code>, .NET 9 produces this:</p><pre><code>; .NET 9 ; Tests.Test2(Int32) xor eax,eax cmp esi,14 setl cl movzx ecx,cl cmp esi,0A cmovg eax,ecx ret ; Total bytes of code 18</code></pre><p>Because of how the return condition is being represented internally by the JIT, it&rsquo;s missing this particular optimization, and the assembly code more directly reflects what was written in the C#. But now in .NET 10, because of this normalization, we now get this for <code>Test2</code>, exactly what we got for <code>Test1</code>:</p><pre><code>; .NET 10 ; Tests.Test2(Int32) sub esi,0B cmp esi,8 setbe al movzx eax,al ret ; Total bytes of code 13</code></pre> </li> <li><strong>Bit tests</strong>. The C# compiler has a lot of flexibility in how it emits <code>switch</code> and <code>is</code> expressions. Consider a case like this: <code>c is ' ' or '\t' or '\r' or '\n'</code>. It could emit that as the equivalent of a series of cascading <code>if</code>/<code>else</code> branches, as an IL <code>switch</code> instruction, as a bit test, or as combinations of those. The C# compiler, though, doesn&rsquo;t have all of the information the JIT has, such as whether the process is 32-bit or 64-bit, or knowledge of what instructions cost on given hardware. With <a href="https://github.com/dotnet/runtime/pull/107831">dotnet/runtime#107831</a>, the JIT will now recognize more such expressions that can be implemented as a bit test and generate the code accordingly.<pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Runtime.CompilerServices; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "c")] public partial class Tests { [Benchmark] [Arguments('s')] public void Test(char c) { if (c is ' ' or '\t' or '\r' or '\n' or '.') { Handle(c); } [MethodImpl(MethodImplOptions.NoInlining)] static void Handle(char c) { } } }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Code Size</th> </tr> </thead> <tbody> <tr> <td>Test</td> <td>.NET 9.0</td> <td>0.4537 ns</td> <td>1.02</td> <td>58 B</td> </tr> <tr> <td>Test</td> <td>.NET 10.0</td> <td>0.1304 ns</td> <td>0.29</td> <td>44 B</td> </tr> </tbody> </table><p>It&rsquo;s also common to see bit tests implemented in C# against shifted values; a constant mask is created with bits set at various indices, and then an incoming value to check is tested by shifting a bit to the corresponding index and seeing whether it aligns with one in the mask. For example, here is how <code>Regex</code> tests to see whether a provided <code>UnicodeCategory</code> is one of those that composes the &ldquo;word&rdquo; class (`\w`):</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Globalization; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "uc")] public partial class Tests { [Benchmark] [Arguments(UnicodeCategory.DashPunctuation)] public bool Test(UnicodeCategory uc) =&gt; (WordCategoriesMask &amp; (1 &lt;&lt; (int)uc)) != 0; private const int WordCategoriesMask = 1 &lt;&lt; (int)UnicodeCategory.UppercaseLetter | 1 &lt;&lt; (int)UnicodeCategory.LowercaseLetter | 1 &lt;&lt; (int)UnicodeCategory.TitlecaseLetter | 1 &lt;&lt; (int)UnicodeCategory.ModifierLetter | 1 &lt;&lt; (int)UnicodeCategory.OtherLetter | 1 &lt;&lt; (int)UnicodeCategory.NonSpacingMark | 1 &lt;&lt; (int)UnicodeCategory.DecimalDigitNumber | 1 &lt;&lt; (int)UnicodeCategory.ConnectorPunctuation; }</code></pre><p>Previously, the JIT would end up emitting that similar to how it&rsquo;s written: a shift followed by a test. Now with <a href="https://github.com/dotnet/runtime/pull/111979">dotnet/runtime#111979</a> from <a href="https://github.com/varelen">@varelen</a>, it can emit it as a bit test.</p><pre><code>; .NET 9 ; Tests.Test(System.Globalization.UnicodeCategory) mov eax,1 shlx eax,eax,esi test eax,4013F setne al movzx eax,al ret ; Total bytes of code 22 ; .NET 10 ; Tests.Test(System.Globalization.UnicodeCategory) mov eax,4013F bt eax,esi setb al movzx eax,al ret ; Total bytes of code 15</code></pre> </li> <li><strong>Redundant sign extensions</strong>. With <a href="https://github.com/dotnet/runtime/pull/111305">dotnet/runtime#111305</a>, the JIT can now remove more redundant sign extensions (when you take a smaller size type, e.g. <code>int</code>, and convert it to a larger size type, e.g. <code>long</code>, while preserving the value&rsquo;s sign). For example, with a test like this <code>public ulong Test(int x) =&gt; (uint)x &lt; 10 ? (ulong)x &lt;&lt; 60 : 0</code>, the JIT can now emit a <code>mov</code> (just copy the bits) instead of <code>movsxd</code> (move with sign extension), since it knows from the first comparison that the shift will only ever be performed with a non-negative <code>x</code>.</li> <li><strong>Better division with BMI2</strong>. If the BMI2 instruction set is available, with <a href="https://github.com/dotnet/runtime/pull/116198">dotnet/runtime#116198</a> from <a href="https://github.com/Daniel-Svensson">@Daniel-Svensson</a> the JIT can now use the <code>mulx</code> instruction (&ldquo;Unsigned Multiply Without Affecting Flags&rdquo;) to implement integer division, e.g.<pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "value")] public partial class Tests { [Benchmark] [Arguments(12345)] public ulong Div10(ulong value) =&gt; value / 10; }</code></pre><p>results in:</p><pre><code>; .NET 9 ; Tests.Div10(UInt64) mov rdx,0CCCCCCCCCCCCCCCD mov rax,rsi mul rdx mov rax,rdx shr rax,3 ret ; Total bytes of code 24 ; .NET 10 ; Tests.Div10(UInt64) mov rdx,0CCCCCCCCCCCCCCCD mulx rax,rax,rsi shr rax,3 ret ; Total bytes of code 20</code></pre> </li> <li><strong>Better range comparison</strong>. When comparing a <code>ulong</code> expression against <code>uint.MaxValue</code>, rather than being emitted as a comparison, with <a href="https://github.com/dotnet/runtime/pull/113037">dotnet/runtime#113037</a> from <a href="https://github.com/shunkino">@shunkino</a> it can be handled more efficiently as a shift.<pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "x")] public partial class Tests { [Benchmark] [Arguments(12345)] public bool Test(ulong x) =&gt; x &lt;= uint.MaxValue; }</code></pre><p>resulting in:</p><pre><code>; .NET 9 ; Tests.Test(UInt64) mov eax,0FFFFFFFF cmp rsi,rax setbe al movzx eax,al ret ; Total bytes of code 15 ; .NET 10 ; Tests.Test(UInt64) shr rsi,20 sete al movzx eax,al ret ; Total bytes of code 11</code></pre> </li> <li><strong>Better dead branch elimination</strong>. The JIT&rsquo;s branch optimizer is already able to use implications from comparisons to statically determine the outcome of other branches. For example, if I have this:<pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "x")] public partial class Tests { [Benchmark] [Arguments(42)] public void Test(int x) { if (x &gt; 100) { if (x &gt; 10) { Console.WriteLine(); } } } }</code></pre><p>the JIT generates this on .NET 9:</p><pre><code>; .NET 9 ; Tests.Test(Int32) cmp esi,64 jg short M00_L00 ret M00_L00: jmp qword ptr [7766D3E64FA8] ; Total bytes of code 12</code></pre><p>Note there&rsquo;s only a single comparison against 100 (0x64), with the comparison against 10 elided (as it&rsquo;s implied by the previous comparison). However, there are many variations to this, and not all of them were handled equally well. Consider this:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "x")] public partial class Tests { [Benchmark] [Arguments(42)] public void Test(int x) { if (x &lt; 16) return; if (x &lt; 8) Console.WriteLine(); } }</code></pre><p>Here, the <code>Console.WriteLine</code> ideally wouldn&rsquo;t appear in the emitted assembly at all, as it&rsquo;s never reachable. Alas, on .NET 9, we get this (the <code>jmp</code> instruction here is a tail call to <code>WriteLine</code>):</p><pre><code>; .NET 9 ; Tests.Test(Int32) push rbp mov rbp,rsp cmp esi,10 jl short M00_L00 cmp esi,8 jge short M00_L00 pop rbp jmp qword ptr [731ED8054FA8] M00_L00: pop rbp ret ; Total bytes of code 23</code></pre><p>With <a href="https://github.com/dotnet/runtime/pull/111766">dotnet/runtime#111766</a> on .NET 10, it successfully recognizes that by the time it gets to the <code>x &lt; 8</code>, that condition will always be <code>false</code>, and it can be eliminated. And once it&rsquo;s eliminated, the initial branch is also unnecessary. So the whole method reduces to this:</p><pre><code>; .NET 10 ; Tests.Test(Int32) ret ; Total bytes of code 1</code></pre> </li> <li><strong>Better floating-point conversion</strong>. <a href="https://github.com/dotnet/runtime/pull/114410">dotnet/runtime#114410</a> from <a href="https://github.com/saucecontrol">@saucecontrol</a>, <a href="https://github.com/dotnet/runtime/pull/114597">dotnet/runtime#114597</a> from <a href="https://github.com/saucecontrol">@saucecontrol</a>, and <a href="https://github.com/dotnet/runtime/pull/111595">dotnet/runtime#111595</a> from <a href="https://github.com/saucecontrol">@saucecontrol</a> all speed up conversions between floating-point and integers, such as by using <code>vcvtusi2s</code> when AVX512 is available, or when it isn&rsquo;t, avoiding the intermediate <code>double</code> conversion.<pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "i")] public partial class Tests { [Benchmark] [Arguments(42)] public float Compute(uint i) =&gt; i; }</code></pre><pre><code>; .NET 9 ; Tests.Compute(UInt32) mov eax,esi vxorps xmm0,xmm0,xmm0 vcvtsi2sd xmm0,xmm0,rax vcvtsd2ss xmm0,xmm0,xmm0 ret ; Total bytes of code 16 ; .NET 10 ; Tests.Compute(UInt32) vxorps xmm0,xmm0,xmm0 vcvtusi2ss xmm0,xmm0,esi ret ; Total bytes of code 11</code></pre> </li> <li><strong>Unrolling</strong>. When using <code>CopyTo</code> (or other &ldquo;memmove&rdquo;-based operations) with a constant source, <a href="https://github.com/dotnet/runtime/pull/108576">dotnet/runtime#108576</a> reduces costs by avoiding a redundant memory load. <a href="https://github.com/dotnet/runtime/pull/109036">dotnet/runtime#109036</a> unblocks more unrolling on Arm64 for <code>Equals</code>/<code>StartsWith</code>/<code>EndsWith</code>. And <a href="https://github.com/dotnet/runtime/pull/110893">dotnet/runtime#110893</a> enables unrolling non-zero fills (unrolling already happened for zero fills).<pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private char[] _chars = new char[100]; [Benchmark] public void Fill() =&gt; _chars.AsSpan(0, 16).Fill('x'); }</code></pre><pre><code>; .NET 9 ; Tests.Fill() push rbp mov rbp,rsp mov rdi,[rdi+8] test rdi,rdi je short M00_L00 cmp dword ptr [rdi+8],10 jb short M00_L00 add rdi,10 mov esi,10 mov edx,78 call qword ptr [7F3093FBF1F8]; System.SpanHelpers.Fill[[System.Char, System.Private.CoreLib]](Char ByRef, UIntPtr, Char) nop pop rbp ret M00_L00: call qword ptr [7F3093787810] int 3 ; Total bytes of code 49 ; .NET 10 ; Tests.Fill() push rbp mov rbp,rsp mov rax,[rdi+8] test rax,rax je short M00_L00 cmp dword ptr [rax+8],10 jl short M00_L00 add rax,10 vbroadcastss ymm0,dword ptr [78EFC70C9340] vmovups [rax],ymm0 vzeroupper pop rbp ret M00_L00: call qword ptr [78EFC7447B88] int 3 ; Total bytes of code 48</code></pre><p>Note the call to <code>SpanHelpers.Fill</code> in the .NET 9 assembly and the absence of it in the .NET 10 assembly.</p></li> </ul> <h2>Native AOT</h2><p>Native AOT is the ability for a .NET application to be compiled directly to assembly code at build-time. The JIT is still used for code generation, but only at build time; the JIT isn&rsquo;t part of the shipping app at all, and no code generation is performed at run-time. As such, most of the optimizations to the JIT already discussed, as well as optimizations throughput the rest of this post, apply to Native AOT equally. Native AOT presents some unique opportunities and challenges, however.</p><p>One super power of the Native AOT tool chain is the ability to interpret (some) code at build-time and use the results of that execution rather than performing the operation at run-time. This is particularly relevant for static constructors, where the constructor&rsquo;s code can be interpreted to initialize various <code>static readonly</code> fields, and then the contents of those fields can be persisted into the generated assembly; at run-time, the contents needs only be rehydrated from the assembly rather than being recomputed. This also potentially helps to make more code redundant and removable, if for example the static constructor and anything it (and only it) referenced were no longer needed. Of course, it would be dangerous and problematic if any arbitrary code could be run during build, so instead there&rsquo;s a very filtered allow list and specialized support for the most common and appropriate constructs. <a href="https://github.com/dotnet/runtime/pull/107575">dotnet/runtime#107575</a> augments this &ldquo;preinitialization&rdquo; capability to support spans sourced from arrays, such that using methods like <code>.AsSpan()</code> doesn&rsquo;t cause preinitialization to bail out. <a href="https://github.com/dotnet/runtime/pull/114374">dotnet/runtime#114374</a> also improved preinitialization, removing restrictions around accessing static fields of other types, calling methods on other types that have their own static constructors, and dereferencing pointers.</p><p>Conversely, Native AOT has its own challenges, specifically that size really matters and is harder to control. With a JIT available at run-time, code generation for only exactly what&rsquo;s needed can be deferred until run-time. With Native AOT, <em>all</em> assembly code generation needs to be done at build-time, which means the Native AOT tool chain needs to work hard to determine the least amount of code it needs to emit to support everything the app might need to do at run-time. Most of the effort on Native AOT in any given release ends up being about helping it to further decrease the size of generated code. For example:</p><ul> <li><a href="https://github.com/dotnet/runtime/pull/117411">dotnet/runtime#117411</a> enables folding bodies of generic instantations of the same method, essentially avoiding duplication by using the same code for copies of the same method where possible.</li> <li><a href="https://github.com/dotnet/runtime/pull/117080">dotnet/runtime#117080</a> similarly helps improve the existing method body deduplication logic.</li> <li><a href="https://github.com/dotnet/runtime/pull/117345">dotnet/runtime#117345</a> from <a href="https://github.com/huoyaoyuan">@huoyaoyuan</a> tweaks a bit of code in reflection that would previously artificially force the code to be preserved for all enumerators of all generic instantations of every collection type.</li> <li><a href="https://github.com/dotnet/runtime/pull/112782">dotnet/runtime#112782</a> adds the same distinction that already existed for <code>MethodTable</code>s for non-generic methods (&ldquo;is this method table visible to user code or not&rdquo;) to generic methods, allowing more metadata for the non-user visible ones to be optimized away.</li> <li><a href="https://github.com/dotnet/runtime/pull/118718">dotnet/runtime#118718</a> and <a href="https://github.com/dotnet/runtime/pull/118832">dotnet/runtime#118832</a> enable size reductions related to boxed enums. The former tweaks a few methods in <code>Thread</code>, <code>GC</code>, and <code>CultureInfo</code> to avoid boxing some enums, which means the code for those needn&rsquo;t be generated. The latter tweaks the implementation of <code>RuntimeHelpers.CreateSpan</code>, which is used by the C# compiler as part of creating spans with constructs like collection expressions. <code>CreateSpan</code> is a generic method, and the Native AOT toolchain&rsquo;s whole-program analysis would end up treating the generic type parameter as being &ldquo;reflected on,&rdquo; meaning the compiler had to assume any type parameter would be used with reflection and thus had to preserve relevant metadata. When used with enums, it would need to ensure support for boxed enums was kept around, and <code>System.Console</code> has such a use with an enum. That in turn meant that a simple &ldquo;hello, world&rdquo; console app couldn&rsquo;t trim away that boxed enum reflection support; now it can.</li> </ul> <h2>VM</h2><p>The .NET runtime offers a wide range of services to managed applications, most obviously the garbage collector and the JIT compiler, but it also encompasses a host of other capabilities: assembly and type loading, exception handling, virtual method dispatch, interoperability support, stub generation, and so on. Collectively, all of these features are referred to as being a part of the .NET Virtual Machine (VM).</p><p><a href="https://github.com/dotnet/runtime/pull/108167">dotnet/runtime#108167</a> and <a href="https://github.com/dotnet/runtime/pull/109135">dotnet/runtime#109135</a> rewrote various runtime helpers from C in the runtime to C# in <code>System.Private.CoreLib</code>, including the &ldquo;unboxing&rdquo; helpers, which are used to unbox <code>object</code>s to value types in niche scenarios. This rewrite avoids overheads associated with transitioning between native and managed and also enables the JIT an opportunity to optimize in the context of callers, such as with inlining. Note that these unboxing helpers are only used in obscure situations, so it requires a bit of a complicated benchmark to demonstrate the impact:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Runtime.CompilerServices; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [DisassemblyDiagnoser(0)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private object[] _objects = [new GenericStruct<mystruct>()]; [Benchmark] public void Unbox() =&gt; Unbox<genericstruct>&gt;(_objects[0]); private void Unbox<t>(object o) where T : struct, IStaticMethod<t> { T? local = (T?)o; if (local.HasValue) { T localCopy = local.Value; T.Method(ref localCopy); } } public interface IStaticMethod<t> { public static abstract void Method(ref T param); } struct MyStruct : IStaticMethod<mystruct> { public static void Method(ref MyStruct param) { } } struct GenericStruct<t> : IStaticMethod<genericstruct>&gt; where T : IStaticMethod<t> { public T Value; [MethodImpl(MethodImplOptions.NoInlining)] public static void Method(ref GenericStruct<t> value) =&gt; T.Method(ref value.Value); } }</t></t></genericstruct></t></mystruct></t></t></t></genericstruct></mystruct></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Code Size</th> </tr> </thead> <tbody> <tr> <td>Unbox</td> <td>.NET 9.0</td> <td>1.626 ns</td> <td>1.00</td> <td>148 B</td> </tr> <tr> <td>Unbox</td> <td>.NET 10.0</td> <td>1.379 ns</td> <td>0.85</td> <td>148 B</td> </tr> </tbody> </table><p>What it means to move the implementation from native to managed is most easily seen just by looking at the generated assembly. Other than uninteresting and non-impactful changes in which registers happen to get assigned, the only real difference between .NET 9 and .NET 10 is a single instruction:</p><pre><code>- call CORINFO_HELP_UNBOX_NULLABLE + call System.Runtime.CompilerServices.CastHelpers.Unbox_Nullable(Byte ByRef, System.Runtime.CompilerServices.MethodTable*, System.Object)</code></pre><p><a href="https://github.com/dotnet/runtime/pull/115284">dotnet/runtime#115284</a> streamlines how the runtime sets up and tears down the little code blocks (&ldquo;funclets&rdquo;) the runtime uses to implement <code>catch</code>/<code>finally</code> on x64. Historically, these funclets acted a lot like tiny functions, saving and restoring non-volatile CPU registers on entry and exit (a &ldquo;non-volatile&rdquo; register is effectively one where the caller can expect it to contain the same value after a function call as it did before the function call). This PR changes the contract so that funclets no longer need to preserve those registers themselves; instead, the runtime takes care of preserving them. That shrinks the prologs and epilogs the JIT emits for funclets, reduces instruction count and code size, and lowers the cost of entering and exiting exception handlers.</p><p>With <a href="https://github.com/dotnet/runtime/pull/114462">dotnet/runtime#114462</a>, the runtime now uses a single shared &ldquo;template&rdquo; for many of the small executable &ldquo;stubs&rdquo; it needs at runtime; stubs are tiny chunks of machine code that act as jump points, call counters, or patchable trampolines. Previously, each memory allocation for stubs would regenerate the same instructions over and over. The new approach builds one copy of the stub code in a read-only page and then maps that same physical page into every place it&rsquo;s needed, while giving each allocation its own writable page for the per-stub data that changes at runtime. This lets hundreds of virtual stub pages all point to one physical code page, cutting memory use, reducing startup work, and improving instruction cache locality.</p><p>Also interesting are <a href="https://github.com/dotnet/runtime/pull/117218">dotnet/runtime#117218</a> and <a href="https://github.com/dotnet/runtime/pull/116031">dotnet/runtime#116031</a>, which together help optimize the generation of stack traces in large, heavily multi-threaded applications when being profiled.</p><h2>Threading</h2><p>The <code>ThreadPool</code> underlies most work in most .NET apps and services. It&rsquo;s a critical-path component that has to be able to deal with all manners of workloads efficiently.</p><p><a href="https://github.com/dotnet/runtime/pull/109841">dotnet/runtime#109841</a> implemented an opt-in feature that <a href="https://github.com/dotnet/runtime/pull/112796">dotnet/runtime#112796</a> then enabled by default for .NET 10. The idea behind it is fairly straightforward, but to understand it, we first need to examine how the thread pool queues work items. The thread pool has multiple queues, typically one &ldquo;global&rdquo; queue and then one &ldquo;local&rdquo; queue per thread in the pool. When threads outside of the pool queue work, that work goes to the global queue, and when a thread pool thread queues work, especially a <code>Task</code> or work related to an <code>await</code>, that work item typically goes to that thread&rsquo;s local queue. Then when a thread pool thread finishes whatever it was doing and goes in search of more work, it first checks its own local queue (treating its local queue as highest priority), then if that&rsquo;s empty it checks the global queue, and then if that&rsquo;s empty it goes and helps out the other threads in the pool by searching their queues for work to be done. This is all in an attempt to a) minimize contention on the global queue (if threads are mainly queueing and dequeuing from their own local queue, they&rsquo;re not contending with each other), and b) prioritize work that&rsquo;s logically part of already started work (the only way for work to get into a local queue is if that thread was processing a work item that created it). Generally, this works out well, but sometimes we get into degenerate scenarios, typically when an app does something that goes against best practices&hellip; like blocking.</p><p>Blocking a thread pool thread means that thread can&rsquo;t service other work coming into the pool. If the blocking is brief, it&rsquo;s generally fine, and if it&rsquo;s longer, the thread pool tries to accommodate it by injecting more threads and finding a steady state at which things hum along. But a certain kind of blocking can be really problematic: <a href="https://devblogs.microsoft.com/dotnet/should-i-expose-synchronous-wrappers-for-asynchronous-methods/">&ldquo;sync over async&rdquo;</a>. With &ldquo;sync over async&rdquo;, one thread blocks while waiting for an asynchronous operation to complete, and if <em>that</em> operation needs to do something on the thread pool in order to complete, you now have one thread pool thread blocked waiting for another thread pool thread to pick up a particular work item and process it. This can quickly lead to the whole pool getting into a jam&hellip; especially with the thread local queues. If a thread is blocked on an operation that depends on work items in that thread&rsquo;s local queue getting processed, that work item being picked off now depends on the global queue being exhausted and another thread coming along and stealing the work item from this thread&rsquo;s queue. If there&rsquo;s a steady stream of incoming work into the global queue, though, that will never happen; essentially, the highest priority work item has become the lowest priority work item.</p><p>So, back to these PRs. The idea is fairly simple: when the thread is about to block, and in particular when it&rsquo;s about to block waiting on a <code>Task</code>, it first dumps its entire local queue into the global queue. That way, this work which was highest priority for the blocked thread has a fairer chance of being processed by other threads, rather than it being the lowest priority work for everyone. We can try to see the impact of this with a specifically-crafted workload:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" // dotnet run -c Release -f net10.0 --filter "*" using System.Diagnostics; int numThreads = Environment.ProcessorCount; ThreadPool.SetMaxThreads(numThreads, 1); ManualResetEventSlim start = new(); CountdownEvent allDone = new(numThreads); new Thread(() =&gt; { while (true) { for (int i = 0; i &lt; 10_000; i++) { ThreadPool.QueueUserWorkItem(_ =&gt; Thread.SpinWait(1)); } Thread.Yield(); } }) { IsBackground = true }.Start(); for (int i = 0; i &lt; numThreads; i++) { ThreadPool.QueueUserWorkItem(_ =&gt; { start.Wait(); TaskCompletionSource tcs = new(); const int LocalItemsPerThread = 4; var remaining = LocalItemsPerThread; for (int j = 0; j &lt; LocalItemsPerThread; j++) { Task.Run(() =&gt; { Thread.SpinWait(100); if (Interlocked.Decrement(ref remaining) == 0) { tcs.SetResult(); } }); } tcs.Task.Wait(); allDone.Signal(); }); } var sw = Stopwatch.StartNew(); start.Set(); Console.WriteLine(allDone.Wait(20_000) ? $"Completed: {sw.ElapsedMilliseconds}ms" : $"Timed out after {sw.ElapsedMilliseconds}ms");</code></pre><p>This is:</p><ul> <li>creating a noise thread that tries to keep the global queue inundated with new work</li> <li>queuing <code>Environment.ProcessorCount</code> work items, each of which queues four work items to the local queue that all do a little work and then blocks on a <code>Task</code> until they all complete</li> <li>waiting for those <code>Environment.ProcessorCount</code> work items to complete</li> </ul><p>When I run this on .NET 9, it hangs, because there&rsquo;s so much work in the global queue, no threads are able to process those sub-work items that are necessary to unblock the main work items:</p><pre><code>Timed out after 20002ms</code></pre><p>On .NET 10, it generally completes almost instantly:</p><pre><code>Completed: 4ms</code></pre><p>Some other tweaks were made to the pool as well:</p><ul> <li><a href="https://github.com/dotnet/runtime/pull/115402">dotnet/runtime#115402</a> reduced the amount of spin-waiting done on Arm processors, bringing it more in line with x64.</li> <li><a href="https://github.com/dotnet/runtime/pull/112789">dotnet/runtime#112789</a> reduced the frequency at which the thread pool checked CPU utilization, as in some circumstances it was adding noticeable overhead, and makes the frequency configurable.</li> <li><a href="https://github.com/dotnet/runtime/pull/108135">dotnet/runtime#108135</a> from <a href="https://github.com/AlanLiu90">@AlanLiu90</a> removed a bit of lock contention that could happen under load when starting new thread pool threads.</li> </ul><p>On the subject of locking, and only for developers that find themselves with a strong need to do really low-level low-lock development, <a href="https://github.com/dotnet/runtime/pull/107843">dotnet/runtime#107843</a> from <a href="https://github.com/hamarb123">@hamarb123</a> adds two new methods to the <code>Volatile</code> class: <code>ReadBarrier</code> and <code>WriteBarrier</code>. A read barrier has &ldquo;load acquire&rdquo; semantics, and is sometimes referred to as a &ldquo;downward fence&rdquo;: it prevents instructions from being reordered in such a way that memory accesses below/after the barrier move to above/before it. In contrast, a write barrier has &ldquo;store release&rdquo; semantics, and is sometimes referred to as an &ldquo;upwards fence&rdquo;: it prevents instructions from being reordered in such a way that memory accesses above/before the barrier move to below/after it. I find it helps to think about this with regards to a <code>lock</code>:</p><pre><code>A; lock (...) { B; } C;</code></pre><p>While in practice the implementation may provide stronger fences, by specification entering a <code>lock</code> has acquire semantics and exiting a <code>lock</code> has release semantics. Imagine if the instructions in the above code could be reordered like this:</p><pre><code>A; B; lock (...) { } C;</code></pre><p>or like this:</p><pre><code>A; lock (...) { } B; C;</code></pre><p>Both of those would be really bad. Thankfully, the barriers help us here. The acquire / read barrier semantics of entering the lock are a downwards fence: logically the brace that starts the lock puts downwards pressure on everything inside the lock to not move to before it, and the brace that ends the lock puts upwards pressure on everything inside the lock to not move to after it. Interestingly, nothing about the semantics of these barriers prevents this from happening:</p><pre><code>lock (...) { A; B; C; }</code></pre><p>These barriers are referred to as &ldquo;half fences&rdquo;; the read barrier prevents later things from moving earlier, but not the other way around, and the write barrier prevents earlier things from moving later, but not the other way around. (As it happens, though, while not required by specification, today the implementation of <code>lock</code> does use a full barrier on both enter and exit, so nothing before or after a <code>lock</code> will move into it.)</p><p>For <code>Task</code> in .NET 10, <code>Task.WhenAll</code> has a few changes to improve its performance. <a href="https://github.com/dotnet/runtime/pull/110536">dotnet/runtime#110536</a> avoids a temporary collection allocation when needing to buffer up tasks from an <code>IEnumerable<task></task></code>.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Runtime.CompilerServices; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { [Benchmark] public Task WhenAllAlloc() { AsyncTaskMethodBuilder t = default; Task whenAll = Task.WhenAll(from i in Enumerable.Range(0, 2) select t.Task); t.SetResult(); return whenAll; } }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>WhenAllAlloc</td> <td>.NET 9.0</td> <td>216.8 ns</td> <td>1.00</td> <td>496 B</td> <td>1.00</td> </tr> <tr> <td>WhenAllAlloc</td> <td>.NET 10.0</td> <td>181.9 ns</td> <td>0.84</td> <td>408 B</td> <td>0.82</td> </tr> </tbody> </table><p>And <a href="https://github.com/dotnet/runtime/pull/117715">dotnet/runtime#117715</a> from <a href="https://github.com/CuteLeon">@CuteLeon</a> avoids the overhead of the <code>Task.WhenAll</code> altogether when the input ends up just being a single task, in which case it simply returns that task instance.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Runtime.CompilerServices; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { [Benchmark] public Task WhenAllAlloc() { AsyncTaskMethodBuilder t = default; Task whenAll = Task.WhenAll([t.Task]); t.SetResult(); return whenAll; } }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>WhenAllAlloc</td> <td>.NET 9.0</td> <td>72.73 ns</td> <td>1.00</td> <td>144 B</td> <td>1.00</td> </tr> <tr> <td>WhenAllAlloc</td> <td>.NET 10.0</td> <td>33.06 ns</td> <td>0.45</td> <td>72 B</td> <td>0.50</td> </tr> </tbody> </table><p><a href="https://devblogs.microsoft.com/dotnet/an-introduction-to-system-threading-channels/"><code>System.Threading.Channels</code></a> is one of the lesser-known but quite useful areas of threading in .NET (you can watch <a href="https://www.youtube.com/watch?v=J3IQBI5HVOw">Yet Another &ldquo;Highly Technical Talk&rdquo; with Hanselman and Toub</a> from Build 2025 to learn more about it). If you find yourself needing a queue to hand off some data between a producer and a consumer, you should likely look into <code>Channel<t></t></code>. The library was introduced in .NET Core 3.0 as a small, robust, and fast producer/consumer queueing mechanism; it&rsquo;s evolved since, such as gaining a <code>ReadAllAsync</code> method for consuming the contents of a channel as an <code>IAsyncEnumerable<t></t></code> and a <code>PeekAsync</code> method for peeking at its contents without consuming. The original release supported <code>Channel.CreateUnbounded</code> and <code>Channel.CreateBounded</code> methods, and .NET 9 augmented those with a <code>Channel.CreateUnboundedPrioritized</code>. .NET 10 continues to expand on channels, both with functional improvements (such as with <a href="https://github.com/dotnet/runtime/pull/116097">dotnet/runtime#116097</a>, which adds an unbuffered channel implementation), and with performance improvements.</p><p>.NET 10 helps to to reduce overall memory consumption of an application using channels. One of the cross-cutting features channels supports is cancellation: you can cancel pretty much any interaction with a channel, which sports asynchronous methods for both producing and consuming data. When a reader or writer needs to pend, it creates (or reuses a pooled instance of) an <code>AsyncOperation</code> object that gets added to a queue; a later writer or reader that&rsquo;s then able to satisfy a pending reader or writer dequeues one and marks it as completed. These queues were implemented with arrays, which makes it challenging to remove an entry from the middle of the queue if the associated operation gets canceled. So, rather than trying, it simply left the canceled object in the queue, and then when it would eventually get dequeued, it&rsquo;s just thrown away and the dequeuer tries again. The theory was that, at steady state, you will quickly dequeue any canceled operations, and it&rsquo;d be better to not exert a lot of effort to try to remove them more quickly. As it turns out, that assumption was problematic for some scenarios, where the workload wasn&rsquo;t balanced, e.g. lots of readers would pend and timeout due to lack of writers, and each of those timed out readers would leave behind a canceled item in the queue. The next time a writer would come along, yes, all those canceled readers would get cleared out, but in the meantime, it would manifest as a notable increase in working set.</p><p><a href="https://github.com/dotnet/runtime/pull/116021">dotnet/runtime#116021</a> addresses that by switching from array-backed queues to linked-list-based queues. The waiter objects themselves double as the nodes in the linked lists, so the only additional memory overhead is a couple of fields for the previous and next nodes in the linked list. But even that modest increase is undesirable, so as part of the PR, it also tries to find compensating optimizations to balance things out. It&rsquo;s able to remove a field from <code>Channel<t></t></code>&lsquo;s custom implementation of <code>IValueTaskSource<t></t></code> by applying a similar optimization that was made to <code>ManualResetValueTaskSourceCore<t></t></code> in a previous release: it&rsquo;s incredibly rare for an awaiter to supply an <code>ExecutionContext</code> (via use of the awaiter&rsquo;s <code>OnCompleted</code> rather than <code>UnsafeOnCompleted</code> method), and even more so for that to happen when there&rsquo;s also a non-default <code>TaskScheduler</code> or <code>SynchronizationContext</code> that needs to be stored, so rather than using two fields for those concepts, they just get grouped into one (which means that in the super duper rare case where both are needed, it incurs an extra allocation). Another field is removed for storing a <code>CancellationToken</code> on the instance, which on .NET Core can be retrieved from other available state. These changes then actually result in the size of the <code>AsyncOperation</code> waiter instance decreasing rather than increasing. Win win. It&rsquo;s hard to see the impact of this change on throughput; it&rsquo;s easier to just see what the impact is on working set in the degenerate case where canceled operations are never removed. If I run this code:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" // dotnet run -c Release -f net10.0 --filter "*" using System.Threading.Channels; Channel<int> c = Channel.CreateUnbounded<int>(); for (int i = 0; ; i++) { CancellationTokenSource cts = new(); var vt = c.Reader.ReadAsync(cts.Token); cts.Cancel(); await ((Task)vt.AsTask()).ConfigureAwait(ConfigureAwaitOptions.SuppressThrowing); if (i % 100_000 == 0) { Console.WriteLine($"Working set: {Environment.WorkingSet:N0}b"); } }</int></int></code></pre><p>in .NET 9 I get output like this, with an ever increasing working set:</p><pre><code>Working set: 31,588,352b Working set: 164,884,480b Working set: 210,698,240b Working set: 293,711,872b Working set: 385,495,040b Working set: 478,158,848b Working set: 553,385,984b Working set: 608,206,848b Working set: 699,695,104b Working set: 793,034,752b Working set: 885,309,440b Working set: 986,103,808b Working set: 1,094,234,112b Working set: 1,156,239,360b Working set: 1,255,198,720b Working set: 1,347,604,480b Working set: 1,439,879,168b Working set: 1,532,284,928b</code></pre><p>and in .NET 10, I get output like this, with a nice level steady state working set:</p><pre><code>Working set: 33,030,144b Working set: 44,826,624b Working set: 45,481,984b Working set: 45,613,056b Working set: 45,875,200b Working set: 45,875,200b Working set: 46,006,272b Working set: 46,006,272b Working set: 46,006,272b Working set: 46,006,272b Working set: 46,006,272b Working set: 46,006,272b Working set: 46,006,272b Working set: 46,006,272b Working set: 46,006,272b Working set: 46,006,272b Working set: 46,006,272b Working set: 46,006,272b</code></pre> <h2>Reflection</h2><p>.NET 8 added the <code>[UnsafeAccessor]</code> attribute, which enables a developer to write an <code>extern</code> method that matches up with some non-visible member the developer wants to be able to use, and the runtime fixes up the accesses to be just as if the target member was being used directly. .NET 9 then extended it with generic support.</p><pre><code>// dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Reflection; using System.Runtime.CompilerServices; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private List<int> _list = new List<int>(16); private FieldInfo _itemsField = typeof(List<int>).GetField("_items", BindingFlags.NonPublic | BindingFlags.Instance)!; private static class Accessors<t> { [UnsafeAccessor(UnsafeAccessorKind.Field, Name = "_items")] public static extern ref T[] GetItems(List<t> list); } [Benchmark] public int[] WithReflection() =&gt; (int[])_itemsField.GetValue(_list)!; [Benchmark] public int[] WithUnsafeAccessor() =&gt; Accessors<int>.GetItems(_list); }</int></t></t></int></int></int></code></pre> <table> <thead> <tr> <th>Method</th> <th>Mean</th> </tr> </thead> <tbody> <tr> <td>WithReflection</td> <td>2.6397 ns</td> </tr> <tr> <td>WithUnsafeAccessor</td> <td>0.7300 ns</td> </tr> </tbody> </table><p>But there are still gaps in that story. The signature of the <code>UnsafeAccessor</code> member needs to align with the signature of the target member, but what if that target member has parameters that aren&rsquo;t visible to the code writing the <code>UnsafeAccessor</code>? Or, what if the target member is static? There&rsquo;s no way for the developer to express in the <code>UnsafeAccessor</code> on which type the target member exists.</p><p>For these scenarios, <a href="https://github.com/dotnet/runtime/pull/114881">dotnet/runtime#114881</a> augments the story with the <code>[UnsafeAccessorType]</code> attribute. The <code>UnsafeAccessor</code> method can type the relevant parameters as <code>object</code> but then adorn them with an <code>[UnsafeAccessorType("...")]</code> that provides a fully-qualified name of the target type. There are a bunch of examples then of this being used in <a href="https://github.com/dotnet/runtime/pull/115583">dotnet/runtime#115583</a>, which replaces most of the cross-library reflection done between libraries in .NET itself with use of <code>[UnsafeAccessor]</code>. An example of where this is handy is with a cyclic relationship between <code>System.Net.Http</code> and <code>System.Security.Cryptography</code>. <code>System.Net.Http</code> sits above <code>System.Security.Cryptography</code>, referencing it for critical features like <code>X509Certificate</code>. But <code>System.Security.Cryptography</code> needs to be able to make HTTP requests in order to download OCSP information, and with <code>System.Net.Http</code> referencing <code>System.Security.Cryptography</code>, <code>System.Security.Cryptography</code> can&rsquo;t in turn explicitly reference <code>System.Net.Http</code>. It can, however, use reflection or <code>[UnsafeAccessor]</code> and <code>[UnsafeAccessorType]</code> to do so, and it does. It used to use reflection, now in .NET 10 it uses <code>[UnsafeAccessor]</code>.</p><p>There are a few other nice improvements in and around reflection. <a href="https://github.com/dotnet/runtime/pull/105814">dotnet/runtime#105814</a> from <a href="https://github.com/huoyaoyuan">@huoyaoyuan</a> updates <code>ActivatorUtilities.CreateFactory</code> to remove a layer of delegates. <code>CreateFactory</code> returns an <code>ObjectFactory</code> delegate, but under the covers the implementation was creating a <code>Func&lt;...&gt;</code> and then creating an <code>ObjectFactory</code> delegate for that <code>Func&lt;...&gt;</code>&lsquo;s <code>Invoke</code> method. The PR changes it to just create the <code>ObjectFactory</code> initially, which means every invocation avoids one layer of delegate invocation.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Configs; using BenchmarkDotNet.Environments; using BenchmarkDotNet.Jobs; using BenchmarkDotNet.Running; using Microsoft.Extensions.DependencyInjection; var config = DefaultConfig.Instance .AddJob(Job.Default.WithRuntime(CoreRuntime.Core90).WithNuGet("Microsoft.Extensions.DependencyInjection.Abstractions", "9.0.9").AsBaseline()) .AddJob(Job.Default.WithRuntime(CoreRuntime.Core10_0).WithNuGet("Microsoft.Extensions.DependencyInjection.Abstractions", "10.0.0-rc.1.25451.107")); BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args, config); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "NuGetReferences")] public partial class Tests { private IServiceProvider _sp = new ServiceCollection().BuildServiceProvider(); private ObjectFactory _factory = ActivatorUtilities.CreateFactory(typeof(object), Type.EmptyTypes); [Benchmark] public object CreateInstance() =&gt; _factory(_sp, null); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>CreateInstance</td> <td>.NET 9.0</td> <td>8.136 ns</td> <td>1.00</td> </tr> <tr> <td>CreateInstance</td> <td>.NET 10.0</td> <td>6.676 ns</td> <td>0.82</td> </tr> </tbody> </table><p><a href="https://github.com/dotnet/runtime/pull/112350">dotnet/runtime#112350</a> reduces some overheads and allocations as part of parsing and rendering <code>TypeName</code>s.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Reflection.Metadata; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "t")] public partial class Tests { [Benchmark] [Arguments(typeof(Dictionary<list>[,], List<int>&gt;[]))] public string ParseAndGetName(Type t) =&gt; TypeName.Parse(t.FullName).FullName; }</int></list></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>ParseAndGetName</td> <td>.NET 9.0</td> <td>5.930 us</td> <td>1.00</td> <td>12.25 KB</td> <td>1.00</td> </tr> <tr> <td>ParseAndGetName</td> <td>.NET 10.0</td> <td>4.305 us</td> <td>0.73</td> <td>5.75 KB</td> <td>0.47</td> </tr> </tbody> </table><p>And <a href="https://github.com/dotnet/runtime/pull/113803">dotnet/runtime#113803</a> from <a href="https://github.com/teo-tsirpanis">@teo-tsirpanis</a> improves how <code>DebugDirectoryBuilder</code> in <code>System.Reflection.Metadata</code> uses <code>DeflateStream</code> to embed a PDB. The code was previously buffering the compressed output into an intermediate <code>MemoryStream</code>, and then that <code>MemoryStream</code> was being written to the <code>BlobBuilder</code>. With this change, the <code>DeflateStream</code> is wrapped directly around the <code>BlobBuilder</code>, enabling the compressed data to be propagated directly to <code>builder.WriteBytes</code>.</p><h2>Primitives and Numerics</h2><p>Every time I write one of these &ldquo;Performance Improvements in .NET&rdquo; posts, a part of me thinks &ldquo;how could there possibly be more next time.&rdquo; That&rsquo;s especially true for core data types, which have received so much scrutiny over the years. Yet, here we are, with more to look at for .NET 10.</p><p><code>DateTime</code> and <code>DateTimeOffset</code> get some love in <a href="https://github.com/dotnet/runtime/pull/111112">dotnet/runtime#111112</a>, in particular with micro-optimizations around how instances are initialized. Similar tweaks show up in <a href="https://github.com/dotnet/runtime/pull/111244">dotnet/runtime#111244</a> for <code>DateOnly</code>, <code>TimeOnly</code>, and <code>ISOWeek</code>.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private DateTimeOffset _dto = new DateTimeOffset(2025, 9, 10, 0, 0, 0, TimeSpan.Zero); [Benchmark] public DateTimeOffset GetFutureTime() =&gt; _dto + TimeSpan.FromDays(1); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>GetFutureTime</td> <td>.NET 9.0</td> <td>6.012 ns</td> <td>1.00</td> </tr> <tr> <td>GetFutureTime</td> <td>.NET 10.0</td> <td>1.029 ns</td> <td>0.17</td> </tr> </tbody> </table><p><code>Guid</code> gets several notable performance improvements in .NET 10. <a href="https://github.com/dotnet/runtime/pull/105654">dotnet/runtime#105654</a> from <a href="https://github.com/SirCxyrtyx">@SirCxyrtyx</a> imbues <code>Guid</code> with an implementation of <code>IUtf8SpanParsable</code>. This not only allows <code>Guid</code> to be used in places where a generic parameter is constrained to <code>IUtf8SpanParsable</code>, it gives <code>Guid</code> overloads of <code>Parse</code> and <code>TryParse</code> that operate on UTF8 bytes. This means if you have UTF8 data, you don&rsquo;t first need to transcode it to UTF16 in order to parse it, nor use <code>Utf8Parser.TryParse</code>, which isn&rsquo;t as optimized as is <code>Guid.TryParse</code> (but which does enable parsing out a <code>Guid</code> from the beginning of a larger input).</p><pre><code>// dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Buffers.Text; using System.Text; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private byte[] _utf8 = Encoding.UTF8.GetBytes(Guid.NewGuid().ToString("N")); [Benchmark(Baseline = true)] public Guid TranscodeParse() { Span<char> scratch = stackalloc char[64]; ReadOnlySpan<char> input = Encoding.UTF8.TryGetChars(_utf8, scratch, out int charsWritten) ? scratch.Slice(0, charsWritten) : Encoding.UTF8.GetString(_utf8); return Guid.Parse(input); } [Benchmark] public Guid Utf8ParserParse() =&gt; Utf8Parser.TryParse(_utf8, out Guid result, out _, 'N') ? result : Guid.Empty; [Benchmark] public Guid GuidParse() =&gt; Guid.Parse(_utf8); }</char></char></code></pre> <table> <thead> <tr> <th>Method</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>TranscodeParse</td> <td>24.72 ns</td> <td>1.00</td> </tr> <tr> <td>Utf8ParserParse</td> <td>19.34 ns</td> <td>0.78</td> </tr> <tr> <td>GuidParse</td> <td>16.47 ns</td> <td>0.67</td> </tr> </tbody> </table><p><code>Char</code>, <code>Rune</code>, and <code>Version</code> also gained <code>IUtf8SpanParsable</code> implementations, in <a href="https://github.com/dotnet/runtime/pull/105773">dotnet/runtime#105773</a> from <a href="https://github.com/lilinus">@lilinus</a> and <a href="https://github.com/dotnet/runtime/pull/109252">dotnet/runtime#109252</a> from <a href="https://github.com/lilinus">@lilinus</a>. There&rsquo;s not much of a performance benefit here for <code>char</code> and <code>Rune</code>; implementing the interface mainly yields consistency and the ability to use these types with generic routines parameterized based on that interface. But <code>Version</code> gains the same kinds of performance (and usability) benefits as did <code>Guid</code>: it now sports support for parsing directly from UTF8, rather than needing to transcode first to UTF16 and then parse that.</p><pre><code>// dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Text; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private byte[] _utf8 = Encoding.UTF8.GetBytes(new Version("123.456.789.10").ToString()); [Benchmark(Baseline = true)] public Version TranscodeParse() { Span<char> scratch = stackalloc char[64]; ReadOnlySpan<char> input = Encoding.UTF8.TryGetChars(_utf8, scratch, out int charsWritten) ? scratch.Slice(0, charsWritten) : Encoding.UTF8.GetString(_utf8); return Version.Parse(input); } [Benchmark] public Version GuidParse() =&gt; Version.Parse(_utf8); }</char></char></code></pre> <table> <thead> <tr> <th>Method</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>TranscodeParse</td> <td>46.48 ns</td> <td>1.00</td> </tr> <tr> <td>GuidParse</td> <td>35.75 ns</td> <td>0.77</td> </tr> </tbody> </table><p>Sometimes performance improvements come about as a side-effect of other work. <a href="https://github.com/dotnet/runtime/pull/110923">dotnet/runtime#110923</a> was intending to remove some pointer use from <code>Guid</code>&lsquo;s formatting implementation, but in doing so, it ended up also slightly improving throughput of the (admittedly rarely used) &ldquo;X&rdquo; format.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private char[] _dest = new char[64]; private Guid _g = Guid.NewGuid(); [Benchmark] public void FormatX() =&gt; _g.TryFormat(_dest, out int charsWritten, "X"); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>FormatX</td> <td>.NET 9.0</td> <td>3.0584 ns</td> <td>1.00</td> </tr> <tr> <td>FormatX</td> <td>.NET 10.0</td> <td>0.7873 ns</td> <td>0.26</td> </tr> </tbody> </table><p><code>Random</code> (and its cryptographically-secure counterpart <code>RandomNumberGenerator</code>) continues to improve in .NET 10, with new methods (such as <code>Random.GetString</code> and <code>Random.GetHexString</code> from <a href="https://github.com/dotnet/runtime/pull/112162">dotnet/runtime#112162</a>) for usability, but also importantly with performance improvements to existing methods. Both <code>Random</code> and <code>RandomNumberGenerator</code> were given a handy <code>GetItems</code> method in .NET 8; this method allows a caller to supply a set of choices and the number of items desired, allowing <code>Random{NumberGenerator}</code> to perform &ldquo;sampling with replacement&rdquo;, selecting an item from the set that number of times. In .NET 9, these implementations were optimized to special-case a power-of-2 number of choices that&rsquo;s less than or equal to 256. In such a case, we can avoid many trips to the underlying source of randomness by requesting bytes in bulk, rather than requesting an <code>int</code> per element. With the power-of-2 choice count, we can simply mask each byte to produce the index into the choices while not introducing bias. In .NET 10, <a href="https://github.com/dotnet/runtime/pull/107988">dotnet/runtime#107988</a> extends this to apply to non-power-of-2 cases, as well. We can&rsquo;t just mask off bits as in the power-of-2 case, but we can do &ldquo;rejection sampling,&rdquo; which is just a fancy way of saying &ldquo;if you randomly get a value outside of the allowed range, try again&rdquo;.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Security.Cryptography; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private const string Base58 = "123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz"; [Params(30)] public int Length { get; set; } [Benchmark] public char[] WithRandom() =&gt; Random.Shared.GetItems<char>(Base58, Length); [Benchmark] public char[] WithRandomNumberGenerator() =&gt; RandomNumberGenerator.GetItems<char>(Base58, Length); }</char></char></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Length</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>WithRandom</td> <td>.NET 9.0</td> <td>30</td> <td>144.42 ns</td> <td>1.00</td> </tr> <tr> <td>WithRandom</td> <td>.NET 10.0</td> <td>30</td> <td>73.68 ns</td> <td>0.51</td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>WithRandomNumberGenerator</td> <td>.NET 9.0</td> <td>30</td> <td>23,179.73 ns</td> <td>1.00</td> </tr> <tr> <td>WithRandomNumberGenerator</td> <td>.NET 10.0</td> <td>30</td> <td>853.47 ns</td> <td>0.04</td> </tr> </tbody> </table><p><code>decimal</code> operations, specifically multiplication and division, get a performance bump, thanks to <a href="https://github.com/dotnet/runtime/pull/99212">dotnet/runtime#99212</a> from <a href="https://github.com/Daniel-Svensson">@Daniel-Svensson</a>.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private decimal _n = 9.87654321m; private decimal _d = 1.23456789m; [Benchmark] public decimal Divide() =&gt; _n / _d; }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Divide</td> <td>.NET 9.0</td> <td>27.09 ns</td> <td>1.00</td> </tr> <tr> <td>Divide</td> <td>.NET 10.0</td> <td>23.68 ns</td> <td>0.87</td> </tr> </tbody> </table><p><code>UInt128</code> division similarly gets some assistance in <a href="https://github.com/dotnet/runtime/pull/99747">dotnet/runtime#99747</a> from <a href="https://github.com/Daniel-Svensson">@Daniel-Svensson</a>, utilizing the X86 <code>DivRem</code> hardware intrinsic when dividing a value that&rsquo;s larger than a <code>ulong</code> by a value that could fit in a <code>ulong</code>.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private UInt128 _n = new UInt128(123, 456); private UInt128 _d = new UInt128(0, 789); [Benchmark] public UInt128 Divide() =&gt; _n / _d; }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Divide</td> <td>.NET 9.0</td> <td>27.3112 ns</td> <td>1.00</td> </tr> <tr> <td>Divide</td> <td>.NET 10.0</td> <td>0.5522 ns</td> <td>0.02</td> </tr> </tbody> </table><p><code>BigInteger</code> gets a few improvements as well. <a href="https://github.com/dotnet/runtime/pull/115445">dotnet/runtime#115445</a> from <a href="https://github.com/Rob-Hague">@Rob-Hague</a> augments its <code>TryWriteBytes</code> method to use a direct memory copy when viable, namely when the number is non-negative such that it doesn&rsquo;t need twos-complement tweaks.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Numerics; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private BigInteger _value = BigInteger.Parse(string.Concat(Enumerable.Repeat("1234567890", 20))); private byte[] _bytes = new byte[256]; [Benchmark] public bool TryWriteBytes() =&gt; _value.TryWriteBytes(_bytes, out _); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>TryWriteBytes</td> <td>.NET 9.0</td> <td>27.814 ns</td> <td>1.00</td> </tr> <tr> <td>TryWriteBytes</td> <td>.NET 10.0</td> <td>5.743 ns</td> <td>0.21</td> </tr> </tbody> </table><p>Also rare but fun, if you tried using <code>BigInteger.Parse</code> exactly with the string representation of <code>int.MinValue</code>, you&rsquo;d end up allocating unnecessarily. That&rsquo;s addressed by <a href="https://github.com/dotnet/runtime/pull/104666">dotnet/runtime#104666</a> from <a href="https://github.com/kzrnm">@kzrnm</a>, which tweaks the handling of this corner-case so that it&rsquo;s appropriately recognized as a case that can be represented using a singleton for <code>int.MinValue</code> (the singleton already existed, it just wasn&rsquo;t applied in this case).</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Numerics; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private string _int32min = int.MinValue.ToString(); [Benchmark] public BigInteger ParseInt32Min() =&gt; BigInteger.Parse(_int32min); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>ParseInt32Min</td> <td>.NET 9.0</td> <td>80.54 ns</td> <td>1.00</td> <td>32 B</td> <td>1.00</td> </tr> <tr> <td>ParseInt32Min</td> <td>.NET 10.0</td> <td>71.59 ns</td> <td>0.89</td> <td>&ndash;</td> <td>0.00</td> </tr> </tbody> </table><p>One area that got a lot of attention in .NET 10 is <code>System.Numerics.Tensors</code>. The <code>System.Numerics.Tensors</code> library was introduced in .NET 8, focusing on a <code>TensorPrimitives</code> class that provided various numerical routines on spans of <code>float</code>. .NET 9 then expanded <code>TensorPrimitives</code> with more operations and generic versions of them. Now in .NET 10, <code>TensorPrimitives</code> gains even more operations, with many of the existing ones also made faster for various scenarios.</p><p>To start, <a href="https://github.com/dotnet/runtime/pull/112933">dotnet/runtime#112933</a> adds over 70 new overloads to <code>TensorPrimitives</code>, including operations like <code>StdDev</code>, <code>Average</code>, <code>Clamp</code>, <code>DivRem</code>, <code>IsNaN</code>, <code>IsPow2</code>, <code>Remainder</code>, and many more. The majority of these operations are also vectorized, using shared implementations that are parameterized with generic operators. For example, the entirety of the <code>Decrement<t></t></code> implementation is:</p><pre><code>public static void Decrement<t>(ReadOnlySpan<t> x, Span<t> destination) where T : IDecrementOperators<t> =&gt; InvokeSpanIntoSpan<t>&gt;(x, destination);</t></t></t></t></t></code></pre><p>where <code>InvokeSpanIntoSpan</code> is a shared routine used by almost 60 methods, each of which supplies its own operator that&rsquo;s then used in the heavily-optimized routine. In this case, the <code>DecrementOperator<t></t></code> is simply this:</p><pre><code>private readonly struct DecrementOperator<t> : IUnaryOperator<t> where T : IDecrementOperators<t> { public static bool Vectorizable =&gt; true; public static T Invoke(T x) =&gt; --x; public static Vector128<t> Invoke(Vector128<t> x) =&gt; x - Vector128<t>.One; public static Vector256<t> Invoke(Vector256<t> x) =&gt; x - Vector256<t>.One; public static Vector512<t> Invoke(Vector512<t> x) =&gt; x - Vector512<t>.One; }</t></t></t></t></t></t></t></t></t></t></t></t></code></pre><p>With that minimal implementation, which provides a decrement implementation for vectorized widths of 128 bits, 256 bits, 512 bits, and scalar, the workhorse routine is able to provide a very efficient implementation.</p><pre><code>// Update benchmark.csproj with a package reference to System.Numerics.Tensors. // dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Numerics.Tensors; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private float[] _src = Enumerable.Range(0, 1000).Select(i =&gt; (float)i).ToArray(); private float[] _dest = new float[1000]; [Benchmark(Baseline = true)] public void DecrementManual() { ReadOnlySpan<float> src = _src; Span<float> dest = _dest; for (int i = 0; i &lt; src.Length; i++) { dest[i] = src[i] - 1f; } } [Benchmark] public void DecrementTP() =&gt; TensorPrimitives.Decrement(_src, _dest); }</float></float></code></pre> <table> <thead> <tr> <th>Method</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>DecrementManual</td> <td>288.80 ns</td> <td>1.00</td> </tr> <tr> <td>DecrementTP</td> <td>22.46 ns</td> <td>0.08</td> </tr> </tbody> </table><p>Wherever possible, these methods also utilize APIs on the underlying <code>Vector128</code>, <code>Vector256</code>, and <code>Vector512</code> types, including new corresponding methods introduced in <a href="https://github.com/dotnet/runtime/pull/111179">dotnet/runtime#111179</a> and <a href="https://github.com/dotnet/runtime/pull/115525">dotnet/runtime#115525</a>, such as <code>IsNaN</code>.</p><p>Existing methods are also improved. <a href="https://github.com/dotnet/runtime/pull/111615">dotnet/runtime#111615</a> from <a href="https://github.com/BarionLP">@BarionLP</a> improves <code>TensorPrimitives.SoftMax</code> by avoiding unnecessary recomputation of <code>T.Exp</code>. The softmax function involves computing <code>exp</code> for every element and summing them all together. The output for an element with value <code>x</code> is then the <code>exp(x)</code> divided by that sum. The previous implementation was following that outline, resulting in computing <code>exp</code> twice for each element. We can instead compute <code>exp</code> just once for each element, caching them temporarily in the destination while creating the sum, and then reusing those for the subsequent division, overwriting each with the actual result. The net result is close to doubling the throughput:</p><pre><code>// Update benchmark.csproj with a package reference to System.Numerics.Tensors. // dotnet run -c Release -f net9.0 --filter ** using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Configs; using BenchmarkDotNet.Environments; using BenchmarkDotNet.Jobs; using BenchmarkDotNet.Running; using System.Numerics.Tensors; var config = DefaultConfig.Instance .AddJob(Job.Default.WithRuntime(CoreRuntime.Core90).WithNuGet("System.Numerics.Tensors", "9.0.9").AsBaseline()) .AddJob(Job.Default.WithRuntime(CoreRuntime.Core10_0).WithNuGet("System.Numerics.Tensors", "10.0.0-rc.1.25451.107")); BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args, config); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "NuGetReferences")] public partial class Tests { private float[] _src, _dst; [GlobalSetup] public void Setup() { Random r = new(42); _src = Enumerable.Range(0, 1000).Select(_ =&gt; r.NextSingle()).ToArray(); _dst = new float[_src.Length]; } [Benchmark] public void SoftMax() =&gt; TensorPrimitives.SoftMax(_src, _dst); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>SoftMax</td> <td>.NET 9.0</td> <td>1,047.9 ns</td> <td>1.00</td> </tr> <tr> <td>SoftMax</td> <td>.NET 10.0</td> <td>649.8 ns</td> <td>0.62</td> </tr> </tbody> </table><p><a href="https://github.com/dotnet/runtime/pull/111505">dotnet/runtime#111505</a> from <a href="https://github.com/alexcovington">@alexcovington</a> enables <code>TensorPrimitives.Divide<t></t></code> to be vectorized for <code>int</code>. The operation already supported vectorization for <code>float</code> and <code>double</code>, for which there&rsquo;s SIMD hardware-accelerated support for division, but it didn&rsquo;t support <code>int</code>, which lacks SIMD hardware-accelerated support. This PR teaches the JIT how to emulate SIMD integer division, by converting the <code>int</code>s to <code>double</code>s, doing <code>double</code> division, and then converting back.</p><pre><code>// Update benchmark.csproj with a package reference to System.Numerics.Tensors. // dotnet run -c Release -f net9.0 --filter ** using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Configs; using BenchmarkDotNet.Environments; using BenchmarkDotNet.Jobs; using BenchmarkDotNet.Running; using System.Numerics.Tensors; var config = DefaultConfig.Instance .AddJob(Job.Default.WithRuntime(CoreRuntime.Core90).WithNuGet("System.Numerics.Tensors", "9.0.9").AsBaseline()) .AddJob(Job.Default.WithRuntime(CoreRuntime.Core10_0).WithNuGet("System.Numerics.Tensors", "10.0.0-rc.1.25451.107")); BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args, config); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "NuGetReferences")] public partial class Tests { private int[] _n, _d, _dst; [GlobalSetup] public void Setup() { Random r = new(42); _n = Enumerable.Range(0, 1000).Select(_ =&gt; r.Next(1000, int.MaxValue)).ToArray(); _d = Enumerable.Range(0, 1000).Select(_ =&gt; r.Next(1, 1000)).ToArray(); _dst = new int[_n.Length]; } [Benchmark] public void Divide() =&gt; TensorPrimitives.Divide(_n, _d, _dst); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Divide</td> <td>.NET 9.0</td> <td>1,293.9 ns</td> <td>1.00</td> </tr> <tr> <td>Divide</td> <td>.NET 10.0</td> <td>458.4 ns</td> <td>0.35</td> </tr> </tbody> </table><p><a href="https://github.com/dotnet/runtime/pull/116945">dotnet/runtime#116945</a> further updates <code>TensorPrimitives.Divide</code> (as well as <code>TensorPrimitives.Sign</code> and <code>TensorPrimitives.ConvertToInteger</code>) to be vectorizable when used with <code>nint</code> or <code>nuint</code>. <code>nint</code> can be treated identically to <code>int</code> when in a 32-bit process and to <code>long</code> when in a 64-bit process; same for <code>nuint</code> with <code>uint</code> and <code>ulong</code>, respectively. So anywhere we&rsquo;re successfully vectorizing for <code>int</code>/<code>uint</code> on 32-bit or <code>long</code>/<code>ulong</code> on 64-bit, we can also successfully vectorize for <code>nint</code>/<code>nuint</code>. <a href="https://github.com/dotnet/runtime/pull/116895">dotnet/runtime#116895</a> also enables vectorizing <code>TensorPrimitives.ConvertTruncating</code> when used to convert <code>float</code> to <code>int</code> or <code>uint</code> and <code>double</code> to <code>long</code> or <code>ulong</code>. Vectorization hadn&rsquo;t previously been enabled because the underlying operations used had some undefined behavior; that behavior was fixed late in the .NET 9 cycle, such that this vectorization can now be enabled.</p><pre><code>// Update benchmark.csproj with a package reference to System.Numerics.Tensors. // dotnet run -c Release -f net9.0 --filter ** using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Configs; using BenchmarkDotNet.Environments; using BenchmarkDotNet.Jobs; using BenchmarkDotNet.Running; using System.Numerics.Tensors; var config = DefaultConfig.Instance .AddJob(Job.Default.WithRuntime(CoreRuntime.Core90).WithNuGet("System.Numerics.Tensors", "9.0.9").AsBaseline()) .AddJob(Job.Default.WithRuntime(CoreRuntime.Core10_0).WithNuGet("System.Numerics.Tensors", "10.0.0-rc.1.25451.107")); BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args, config); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "NuGetReferences")] public partial class Tests { private float[] _src; private int[] _dst; [GlobalSetup] public void Setup() { Random r = new(42); _src = Enumerable.Range(0, 1000).Select(_ =&gt; r.NextSingle() * 1000).ToArray(); _dst = new int[_src.Length]; } [Benchmark] public void ConvertTruncating() =&gt; TensorPrimitives.ConvertTruncating(_src, _dst); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>ConvertTruncating</td> <td>.NET 9.0</td> <td>933.86 ns</td> <td>1.00</td> </tr> <tr> <td>ConvertTruncating</td> <td>.NET 10.0</td> <td>41.99 ns</td> <td>0.04</td> </tr> </tbody> </table><p>Not to be left out, <code>TensorPrimitives.LeadingZeroCount</code> is also improved in <a href="https://github.com/dotnet/runtime/pull/110333">dotnet/runtime#110333</a> from <a href="https://github.com/alexcovington">@alexcovington</a>. When AVX512 is available, the change utilizes AVX512 instructions like <code>PermuteVar16x8x2</code> to vectorize <code>LeadingZeroCount</code> for all types supported by <code>Vector512<t></t></code>.</p><pre><code>// Update benchmark.csproj with a package reference to System.Numerics.Tensors. // dotnet run -c Release -f net9.0 --filter ** using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Configs; using BenchmarkDotNet.Environments; using BenchmarkDotNet.Jobs; using BenchmarkDotNet.Running; using System.Numerics.Tensors; var config = DefaultConfig.Instance .AddJob(Job.Default.WithRuntime(CoreRuntime.Core90).WithNuGet("System.Numerics.Tensors", "9.0.9").AsBaseline()) .AddJob(Job.Default.WithRuntime(CoreRuntime.Core10_0).WithNuGet("System.Numerics.Tensors", "10.0.0-rc.1.25451.107")); BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args, config); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "NuGetReferences")] public partial class Tests { private byte[] _src, _dst; [GlobalSetup] public void Setup() { _src = new byte[1000]; _dst = new byte[_src.Length]; new Random(42).NextBytes(_src); } [Benchmark] public void LeadingZeroCount() =&gt; TensorPrimitives.LeadingZeroCount(_src, _dst); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>LeadingZeroCount</td> <td>.NET 9.0</td> <td>401.60 ns</td> <td>1.00</td> </tr> <tr> <td>LeadingZeroCount</td> <td>.NET 10.0</td> <td>12.33 ns</td> <td>0.03</td> </tr> </tbody> </table><p>In terms of changes that affected the most operations, <a href="https://github.com/dotnet/runtime/pull/116898">dotnet/runtime#116898</a> and <a href="https://github.com/dotnet/runtime/pull/116934">dotnet/runtime#116934</a> take the cake. Together, these PRs extend vectorization for almost 60 distinct operations to also accelerate for <code>Half</code>: <code>Abs</code>, <code>Add</code>, <code>AddMultiply</code>, <code>BitwiseAnd</code>, <code>BitwiseOr</code>, <code>Ceiling</code>, <code>Clamp</code>, <code>CopySign</code>, <code>Cos</code>, <code>CosPi</code>, <code>Cosh</code>, <code>CosineSimilarity</code>, <code>Decrement</code>, <code>DegreesToRadians</code>, <code>Divide</code>, <code>Exp</code>, <code>Exp10</code>, <code>Exp10M1</code>, <code>Exp2</code>, <code>Exp2M1</code>, <code>ExpM1</code>, <code>Floor</code>, <code>FusedAddMultiply</code>, <code>Hypot</code>, <code>Increment</code>, <code>Lerp</code>, <code>Log</code>, <code>Log10</code>, <code>Log10P1</code>, <code>Log2</code>, <code>Log2P1</code>, <code>LogP1</code>, <code>Max</code>, <code>MaxMagnitude</code>, <code>MaxMagnitudeNumber</code>, <code>MaxNumber</code>, <code>Min</code>, <code>MinMagnitude</code>, <code>MinMagnitudeNumber</code>, <code>MinNumber</code>, <code>Multiply</code>, <code>MultiplyAdd</code>, <code>MultiplyAddEstimate</code>, <code>Negate</code>, <code>OnesComplement</code>, <code>Reciprocal</code>, <code>Remainder</code>, <code>Round</code>, <code>Sigmoid</code>, <code>Sin</code>, <code>SinPi</code>, <code>Sinh</code>, <code>Sqrt</code>, <code>Subtract</code>, <code>Tan</code>, <code>TanPi</code>, <code>Tanh</code>, <code>Truncate</code>, and <code>Xor</code>. The challenge here is that <code>Half</code> doesn&rsquo;t have accelerated hardware support, and today is not even supported by the vector types. In fact, even for its scalar operations, <code>Half</code> is manipulated internally by converting it to a <code>float</code>, performing the relevant operation as <code>float</code>, and then casting back, e.g. here&rsquo;s the implementation of the <code>Half</code> multiplication operator:</p><pre><code>public static Half operator *(Half left, Half right) =&gt; (Half)((float)left * (float)right);</code></pre><p>For all of these <code>TensorPrimitives</code> operations, they previously would treat <code>Half</code> like any other unaccelerated type, and would just run a scalar loop that performed the operation on each <code>Half</code>. That means for each element, we&rsquo;re converting it to <code>float</code>, then performing the operation, and then converting it back. As luck would have it, though, <code>TensorPrimitives</code> already defines the <code>ConvertToSingle</code> and <code>ConvertToHalf</code> methods, which are accelerated. We can then reuse those methods to do the same thing that&rsquo;s already done for scalar operations but do it vectorized: take a vector of <code>Half</code>s, convert them all to <code>float</code>s, process all the <code>float</code>s, and convert them all back to <code>Half</code>s. Of course, I already stated that the vector types don&rsquo;t support <code>Half</code>, so how can we &ldquo;take a vector of <code>Half</code>&ldquo;? By reinterpret casting the <code>Span<half></half></code> to <code>Span<short></short></code> (or <code>Span<ushort></ushort></code>), which allows us to smuggle the <code>Half</code>s through. And, as it turns out, even for scalar, the very first thing <code>Half</code>&lsquo;s <code>float</code> cast operator does is convert it to a <code>short</code>.</p><p>The net result is that a ton of operations can now be accelerated for <code>Half</code>.</p><pre><code>// Update benchmark.csproj with a package reference to System.Numerics.Tensors. // dotnet run -c Release -f net9.0 --filter ** using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Configs; using BenchmarkDotNet.Environments; using BenchmarkDotNet.Jobs; using BenchmarkDotNet.Running; using System.Numerics.Tensors; var config = DefaultConfig.Instance .AddJob(Job.Default.WithRuntime(CoreRuntime.Core90).WithNuGet("System.Numerics.Tensors", "9.0.9").AsBaseline()) .AddJob(Job.Default.WithRuntime(CoreRuntime.Core10_0).WithNuGet("System.Numerics.Tensors", "10.0.0-rc.1.25451.107")); BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args, config); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "NuGetReferences")] public partial class Tests { private Half[] _x, _y, _dest; [GlobalSetup] public void Setup() { _x = new Half[1000]; _y = new Half[_x.Length]; _dest = new Half[_x.Length]; var random = new Random(42); for (int i = 0; i &lt; _x.Length; i++) { _x[i] = (Half)random.NextSingle(); _y[i] = (Half)random.NextSingle(); } } [Benchmark] public void Add() =&gt; TensorPrimitives.Add(_x, _y, _dest); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Add</td> <td>.NET 9.0</td> <td>5,984.3 ns</td> <td>1.00</td> </tr> <tr> <td>Add</td> <td>.NET 10.0</td> <td>481.7 ns</td> <td>0.08</td> </tr> </tbody> </table><p>The <code>System.Numerics.Tensors</code> library in .NET 10 now also includes stable APIs for tensor types (which use <code>TensorPrimitives</code> in their implementations). This includes a <code>Tensor<t></t></code>, <code>ITensor&lt;,&gt;</code>, <code>TensorSpan<t></t></code>, and <code>ReadOnlyTensorSpan<t></t></code>. One of the really interesting things about these types is that they take advantage of the new C# 14 compound operators feature, and do so for a significant performance benefit. In previous versions of C#, you&rsquo;re able to write custom operators, for example an addition operator:</p><pre><code>public class C { public int Value; public static C operator +(C left, C right) =&gt; new() { Value = left.Value + right.Value }; }</code></pre><p>With that type, I can write code like:</p><pre><code>C a = new() { Value = 42 }; C b = new() { Value = 84 }; C c = a + b; Console.WriteLine(c.Value);</code></pre><p>which will print out <code>126</code>. I can also change the code to use a compound operator, <code>+=</code>, like this:</p><pre><code>C a = new() { Value = 42 }; C b = new() { Value = 84 }; a += b; Console.WriteLine(a.Value);</code></pre><p>which will also print out <code>126</code>, because the <code>a += b</code> is always identical to <code>a = a + b</code>&hellip; or, at least it was. Now with C# 14, it&rsquo;s possible for a type to not only define a <code>+</code> operator, it can also define a <code>+=</code> operator. If a type defines a <code>+=</code> operator, it will be used rather than expanding <code>a += b</code> as shorthand for <code>a = a + b</code>. And that has performance ramifications.</p><p>A tensor is basically a multidimensional array, and as with arrays, these can be big&hellip; really big. If I have a sequence of operations:</p><pre><code>Tensor<int> t1 = ...; Tensor<int> t2 = ...; for (int i = 0; i &lt; 3; i++) { t1 += t2; }</int></int></code></pre><p>and each of those <code>t1 += t2</code>s exands into <code>t1 = t1 + t2</code>, then for each I&rsquo;m allocating a brand new tensor. If they&rsquo;re big, that gets expensive right quick. But C# 14&rsquo;s new user-defined compound operators, as initially added to the compiler in <a href="https://github.com/dotnet/roslyn/pull/78400">dotnet/roslyn#78400</a>, enable mutation of the target.</p><pre><code>public class C { public int Value; public static C operator +(C left, C right) =&gt; new() { Value = left.Value + right.Value }; public static void operator +=(C other) =&gt; left.Value += other.Value; }</code></pre><p>And that means that such compound operators on the tensor types can just update the target tensor in place rather than allocating a whole new (possibly very large) data structure for each computation. <a href="https://github.com/dotnet/runtime/pull/117997">dotnet/runtime#117997</a> adds all of these compound operators for the tensor types. (Not only are these using C# 14 user-defined compound operators, they&rsquo;re doing so as extension operators, using the new C# 14 extension types feature. Fun!)</p><h2>Collections</h2><p>Handling collections of data is the lifeblood of any application, and as such every .NET release tries to eke out even more performance from collections and collection processing.</p><h3>Enumeration</h3><p>Iterating through collections is one of the most common things developers do. To make this as efficient as possible, the most prominent collection types in .NET (e.g. <code>List<t></t></code>) expose struct-based enumerators (e.g. <code>List<t>.Enumerator</t></code>) which their public <code>GetEnumerator()</code> methods then return in a strongly-typed manner:</p><pre><code>public Enumerator GetEnumerator() =&gt; new Enumerator(this);</code></pre><p>This is in addition to their <code>IEnumerable<t>.GetEnumerator()</t></code> implementation, which ends up being implemented via an &ldquo;explicit&rdquo; interface implementation (&ldquo;explicit&rdquo; means the relevant method provides the interface method implementation but does not show up as a public method on the type itself), e.g. <code>List<t></t></code>&lsquo;s implementation:</p><pre><code>IEnumerator<t> IEnumerable<t>.GetEnumerator() =&gt; Count == 0 ? SZGenericArrayEnumerator<t>.Empty : GetEnumerator();</t></t></t></code></pre><p>Directly <code>foreach</code>&lsquo;ing over the collection allows the C# compiler to bind to the struct-based enumerator, enabling avoiding the enumerator allocation and being able to directly see the non-virtual methods on the enumerator, rather than working with an <code>IEnumerator<t></t></code> and the interface dispatch required to invoke methods on it. That, however, falls apart once the collection is used polymorphically as an <code>IEnumerable<t></t></code>; at that point, the <code>IEnumerable<t>.GetEnumerator()</t></code> is used, which is forced to allocate a new enumerator instance (except for special-cases, such as how <code>List<t></t></code>&lsquo;s implementation shown above returns a singleton enumerator when the collection is empty).</p><p>Thankfully, as noted earlier in the JIT section, the JIT has been gaining super powers around dynamic PGO, escape analysis, and stack allocation. This means that in many situations, the JIT is now able to see that the most common concrete type for a given call site is a specific enumerator type and generate code specific to when it is that type, devirtualizing the calls, possibly inlining them, and then, if it&rsquo;s able to do so sufficiently, stack allocating the enumerator. With the progress that&rsquo;s been made in .NET 10, this now happens very frequently for arrays and <code>List<t></t></code>. While the JIT is able to do this in general regardless of an object&rsquo;s type, the ubiquity of enumeration makes it all that much more important for <code>IEnumerator<t></t></code>, so <a href="https://github.com/dotnet/runtime/pull/116978">dotnet/runtime#116978</a> marks <code>IEnumerator<t></t></code> as an <code>[Intrinsic]</code>, giving the JIT the ability to better reason about it.</p><p>However, some enumerators still needed a bit of help. Besides <code>T[]</code>, <code>List<t></t></code> is the most popular collection type in .NET, and with the JIT changes, many <code>foreach</code>s of an <code>IEnumerable<t></t></code> that are actually <code>List<t></t></code> will successfully have the enumerator stack allocated. Awesome. That awesomeness dwindled, however, when trying out different sized lists. This is a benchmark that tests out enumerating a <code>List<t></t></code> typed as <code>IEnumerable<t></t></code>, with different lengths, along with benchmark results from early August 2025 (around .NET 10 Preview 7).</p><pre><code>// dotnet run -c Release -f net10.0 --filter ** using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private IEnumerable<int> _enumerable; [Params(500, 5000, 15000)] public int Count { get; set; } [GlobalSetup] public void Setup() =&gt; _enumerable = Enumerable.Range(0, Count).ToList(); [Benchmark] public int Sum() { int sum = 0; foreach (int item in _enumerable) sum += item; return sum; } }</int></code></pre> <table> <thead> <tr> <th>Method</th> <th>Count</th> <th>Mean</th> <th>Allocated</th> </tr> </thead> <tbody> <tr> <td>Sum</td> <td>500</td> <td>214.1 ns</td> <td>&ndash;</td> </tr> <tr> <td>Sum</td> <td>5000</td> <td>4,767.1 ns</td> <td>40 B</td> </tr> <tr> <td>Sum</td> <td>15000</td> <td>13,824.4 ns</td> <td>40 B</td> </tr> </tbody> </table><p>Note that for the <code>500</code> element <code>List<t></t></code>, the allocation column shows that nothing was allocated on the heap, as the enumerator was successfully stack allocated. Fabulous. But then just increasing the size of the list caused it to no longer be stack-allocated. Why? The reason for the allocation in the jump from <code>500</code> to <code>5000</code> has to do with dynamic PGO combined with how <code>List<t></t></code>&lsquo;s enumerator was written oh so many years ago.</p><p><code>List<t></t></code>&lsquo;s enumerator&rsquo;s <code>MoveNext</code> was structured like this:</p><pre><code>public bool MoveNext() { if (_version == _list._version &amp;&amp; ((uint)_index &lt; (uint)_list._size)) { ... // handle successfully getting next element return true; } return MoveNextRare(); } private bool MoveNextRare() { ... // handle version mismatch and/or returning false for completed enumeration }</code></pre><p>The <code>Rare</code> in the name gives a hint as to why it&rsquo;s split like this. The <code>MoveNext</code> method was kept as thin as possible for the common case of invoking <code>MoveNext</code>, namely all successful calls that return <code>true</code>; the only time <code>MoveNextRare</code> is needed, other than when the enumerator is misused, is for the final call to it after all elements have been yielded. That streamlining of <code>MoveNext</code> itself was done to make <code>MoveNext</code> inlineable. However, a lot has changed since this code was written, making it less important, and the separating out of <code>MoveNextRare</code> has a really interesting interaction with dynamic PGO. One of the things dynamic PGO looks for is whether code is considered hot (used a lot) or cold (used rarely), and that data influences whether a method should be considered for inlining. For shorter lists, dynamic PGO will see <code>MoveNextRare</code> invoked a reasonable number of times, and will consider it for inlining. And if all of the calls to the enumerator are inlined, the enumerator instance can avoid escaping the call frame, and can then be stack allocated. But once the list length grows to a much larger amount, that <code>MoveNextRare</code> method will start to look really cold, will struggle to be inlined, and will then allow the enumerator instance to escape, preventing it from being stack allocated. <a href="https://github.com/dotnet/runtime/pull/118425">dotnet/runtime#118425</a> recognizes that times have changed since this enumerator was written, with many changes to inlining heuristics and PGO and the like; it undoes the separating out of <code>MoveNextRare</code> and simplifies the enumerator. With how the system works today, the re-combined <code>MoveNext</code> is still inlineable, with or without PGO, and we&rsquo;re able to stack allocate at the larger size.</p><table> <thead> <tr> <th>Method</th> <th>Count</th> <th>Mean</th> <th>Allocated</th> </tr> </thead> <tbody> <tr> <td>Sum</td> <td>500</td> <td>221.2 ns</td> <td>&ndash;</td> </tr> <tr> <td>Sum</td> <td>5000</td> <td>2,153.6 ns</td> <td>&ndash;</td> </tr> <tr> <td>Sum</td> <td>15000</td> <td>14,724.9 ns</td> <td>40 B</td> </tr> </tbody> </table><p>With that fix, we still had an issue, though. We&rsquo;re now avoiding the allocation at lengths 500 and 5000, but at 15,000 we still see the enumerator being allocated. Now why? This has to do with <a href="https://devblogs.microsoft.com/dotnet/performance_improvements_in_net_7/#on-stack-replacement">OSR (on-stack replacement)</a>, which was introduced in .NET 7 as a key enabler for allowing tiered compilation to be used with methods containing loops. OSR allows for a method to be recompiled with optimizations even while it&rsquo;s executing, and for an invocation of the method to jump from the unoptimized code for the method to the corresponding location in the newly optimized method. While OSR is awesome, it unfortunately causes some complications here. Once the list gets long enough, an invocation of the tier 0 (unoptimized) method will transition to the OSR optimized method&hellip; but OSR methods don&rsquo;t contain dynamic PGO instrumentation (they used to, but it was removed because it led to problems if the instrumented code never got recompiled again and thus suffered regressions due to forever-more running with the instrumentation probes in place). Without the instrumentation, and in particular without the instrumentation for the tail portion of the method (where the enumerator&rsquo;s <code>Dispose</code> method is invoked), even though <code>List<t>.Dispose</t></code> is a nop, the JIT may not be able to do the guarded devirtualization that enables the <code>IEnumerator<t>.Dispose</t></code> to be devirtualized and inlined. Meaning, ironically, that the nop <code>Dispose</code> causes escape analysis to see the enumerator instance escape, such that it can&rsquo;t be stack allocated. Whew.</p><p>Thankfully, <a href="https://github.com/dotnet/runtime/pull/118461">dotnet/runtime#118461</a> addresses that in the JIT. Specifically for enumerators, this PR enables dynamic PGO to infer the missing instrumentation based on the earlier probes used with the other enumerator methods, which then enables it to successfully devirtualize and inline <code>Dispose</code>. So, for .NET 10, and the same benchmark, we end up with this lovely sight:</p><table> <thead> <tr> <th>Method</th> <th>Count</th> <th>Mean</th> <th>Allocated</th> </tr> </thead> <tbody> <tr> <td>Sum</td> <td>500</td> <td>216.5 ns</td> <td>&ndash;</td> </tr> <tr> <td>Sum</td> <td>5000</td> <td>2,082.4 ns</td> <td>&ndash;</td> </tr> <tr> <td>Sum</td> <td>15000</td> <td>6,525.3 ns</td> <td>&ndash;</td> </tr> </tbody> </table><p>Other types needed a bit of help as well. <a href="https://github.com/dotnet/runtime/pull/118467">dotnet/runtime#118467</a> addresses <code>PriorityQueue<telement></telement></code>&lsquo;s enumerator; it&rsquo;s enumerator was a port of <code>List<t></t></code>&lsquo;s and so was changed similarly.</p><p>Separately, <a href="https://github.com/dotnet/runtime/pull/117328">dotnet/runtime#117328</a> streamline&rsquo;s <code>Stack<t></t></code>&lsquo;s enumerator type, removing around half the lines of code that previously composed it. The previous enumerator&rsquo;s <code>MoveNext</code> incurred five branches on the way to grabbing most next elements:</p><ul> <li>It first did a version check, comparing the stack&rsquo;s version number against the enumerator&rsquo;s captured version number, to ensure the stack hadn&rsquo;t been mutated since the time the enumerator was grabbed.</li> <li>It then checked to see whether this was the first call to the enumerator, taking one path that lazily-initialized some state if it was and another path assuming already-initialized state if not.</li> <li>Assuming this wasn&rsquo;t the first call, it then checked whether enumeration had previously ended.</li> <li>Assuming it hadn&rsquo;t, it then checked whether there&rsquo;s anything left to enumerate.</li> <li>And finally, it dereferenced the underlying array, incurring a bounds check.</li> </ul><p>The new implementation cuts that in half. It relies on the enumerator&rsquo;s constructor initializing the current index to the length of the stack, such that each <code>MoveNext</code> call just decrements this value. When the data is exhausted, the count will go negative. This means that we can combine a whole bunch of these checks into a single check:</p><pre><code>if ((uint)index &lt; (uint)array.Length)</code></pre><p>and we&rsquo;re left with just two branches on the way to reading any element: the version check and whether the index is in bounds. That reduction not only means there&rsquo;s less code to process and fewer branches that might be improperly predicted, it also shrinks the size of the members to the point where they&rsquo;re much more likely to be inlined, which in turns makes it much more likely that the enumerator object can be stack allocated.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private Stack<int> _direct = new Stack<int>(Enumerable.Range(0, 10)); private IEnumerable<int> _enumerable = new Stack<int>(Enumerable.Range(0, 10)); [Benchmark] public int SumDirect() { int sum = 0; foreach (int item in _direct) sum += item; return sum; } [Benchmark] public int SumEnumerable() { int sum = 0; foreach (int item in _enumerable) sum += item; return sum; } }</int></int></int></int></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Code Size</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>SumDirect</td> <td>.NET 9.0</td> <td>23.317 ns</td> <td>1.00</td> <td>331 B</td> <td>&ndash;</td> <td>NA</td> </tr> <tr> <td>SumDirect</td> <td>.NET 10.0</td> <td>4.502 ns</td> <td>0.19</td> <td>55 B</td> <td>&ndash;</td> <td>NA</td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>SumEnumerable</td> <td>.NET 9.0</td> <td>30.893 ns</td> <td>1.00</td> <td>642 B</td> <td>40 B</td> <td>1.00</td> </tr> <tr> <td>SumEnumerable</td> <td>.NET 10.0</td> <td>7.906 ns</td> <td>0.26</td> <td>381 B</td> <td>&ndash;</td> <td>0.00</td> </tr> </tbody> </table><p><a href="https://github.com/dotnet/runtime/pull/117341">dotnet/runtime#117341</a> does something similar but for <code>Queue<t></t></code>. <code>Queue<t></t></code> has an interesting complication when compared to <code>Stack<t></t></code>, which is that it can wrap around the length of the underlying array. Whereas with <code>Stack<t></t></code>, we can always start at a particular index and just count down to 0, using that index as the offset into the array, with <code>Queue<t></t></code>, the starting index can be anywhere in the array, and when walking from that index to the last element, we might need to wrap around back to the beginning. Such wrapping can be accomplished using <code>% array.Length</code> (which is what <code>Queue<t></t></code> does on .NET Framework), but such a division operation can be relatively costly. An alternative, since we know the count can never be more than the array&rsquo;s length, is to check whether we&rsquo;ve already walked past the end of the array, and if we have, then subtract the array&rsquo;s length to get to the corresponding location from the start of the array. The existing implementation in .NET 9 did just that:</p><pre><code>if (index &gt;= array.Length) { index -= array.Length; // wrap around if needed } _currentElement = array[index];</code></pre><p>That is two branches, one for the check against the array length, and one for the bounds check. The bounds check can&rsquo;t be eliminated here because the JIT hasn&rsquo;t seen proof that the index is actually in-bounds and thus needs to be defensive. Instead, we can write it like this:</p><pre><code>if ((uint)index &lt; (uint)array.Length) { _currentElement = array[index]; } else { index -= array.Length; _currentElement = array[index]; }</code></pre><p>An enumeration of a queue can logically be split into two parts: the elements from the head index to the end of the array, and the elements from the beginning of the array to the tail. All of the former now fall into the first block, which incurs only one branch because the JIT can use the knowledge gleaned from the comparison to eliminate the bounds check. It only incurs a bounds check when in the second portion of the enumeration.</p><p>We can more easily visualize the branch savings by using benchmarkdotnet&rsquo;s <code>HardwareCounters</code> diagnoser, asking it to track <code>HardwareCounter.BranchInstructions</code> (this diagnoser only works on Windows). Note here, as well, that the changes not only improve throughput, they also enable the boxed enumerator to be stack allocated.</p><pre><code>// This benchmark was run on Windows for the HardwareCounters diagnoser to work. // dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using BenchmarkDotNet.Diagnosers; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HardwareCounters(HardwareCounter.BranchInstructions)] [MemoryDiagnoser(displayGenColumns: false)] [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private Queue<int> _direct; private IEnumerable<int> _enumerable; [GlobalSetup] public void Setup() { _direct = new Queue<int>(Enumerable.Range(0, 10)); for (int i = 0; i &lt; 5; i++) { _direct.Enqueue(_direct.Dequeue()); } _enumerable = _direct; } [Benchmark] public int SumDirect() { int sum = 0; foreach (int item in _direct) sum += item; return sum; } [Benchmark] public int SumEnumerable() { int sum = 0; foreach (int item in _enumerable) sum += item; return sum; } }</int></int></int></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>BranchInstructions/Op</th> <th>Code Size</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>SumDirect</td> <td>.NET 9.0</td> <td>24.340 ns</td> <td>1.00</td> <td>79</td> <td>251 B</td> <td>&ndash;</td> <td>NA</td> </tr> <tr> <td>SumDirect</td> <td>.NET 10.0</td> <td>7.192 ns</td> <td>0.30</td> <td>37</td> <td>96 B</td> <td>&ndash;</td> <td>NA</td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>SumEnumerable</td> <td>.NET 9.0</td> <td>30.695 ns</td> <td>1.00</td> <td>103</td> <td>531 B</td> <td>40 B</td> <td>1.00</td> </tr> <tr> <td>SumEnumerable</td> <td>.NET 10.0</td> <td>8.672 ns</td> <td>0.28</td> <td>50</td> <td>324 B</td> <td>&ndash;</td> <td>0.00</td> </tr> </tbody> </table><p><code>ConcurrentDictionary<tkey></tkey></code> also gets in on the fun. The dictionary is implemented as a collection of &ldquo;buckets&rdquo;, each of which of which is a linked list of entries. It had a fairly complicated enumerator for processing these structures, relying on jumping between cases of a switch statement, e.g.</p><pre><code>switch (_state) { case StateUninitialized: ... // Initialize on first MoveNext. goto case StateOuterloop; case StateOuterloop: // Check if there are more buckets in the dictionary to enumerate. if ((uint)i &lt; (uint)buckets.Length) { // Move to the next bucket. ... goto case StateInnerLoop; } goto default; case StateInnerLoop: ... // Yield elements from the current bucket. goto case StateOuterloop; default: // Done iterating. ... }</code></pre><p>If you squint, there are nested loops here, where we&rsquo;re enumerating each bucket and for each bucket enumerating its contents. With how this is structured, however, from the JIT&rsquo;s perspective, we could enter those loops from any of those <code>case</code>s, depending on the current value of <code>_state</code>. That produces something referred to as an &ldquo;irreducible loop,&rdquo; which is a loop that has multiple possible entry points. Imagine you have:</p><pre><code>A: if (someCondition) goto B; ... B: if (someOtherCondition) goto A;</code></pre><p>Labels <code>A</code> and <code>B</code> form a loop, but that loop can be entered by jumping to either <code>A</code> or to <code>B</code>. If the compiler could prove that this loop were only ever enterable from <code>A</code> or only ever enterable from <code>B</code>, then the loop would be &ldquo;reducible.&rdquo; Irreducible loops are much more complex than reducible loops for a compiler to deal with, as they have more complex control and data flow and in general are harder to analyze. <a href="https://github.com/dotnet/runtime/pull/116949">dotnet/runtime#116949</a> rewrites the <code>MoveNext</code> method to be a more typical <code>while</code> loop, which is not only easier to read and maintain, it&rsquo;s also reducible and more efficient, and because it&rsquo;s more streamlined, it&rsquo;s also inlineable and enables possible stack allocation.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Collections.Concurrent; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private ConcurrentDictionary<int> _ints = new(Enumerable.Range(0, 1000).ToDictionary(i =&gt; i, i =&gt; i)); [Benchmark] public int EnumerateInts() { int sum = 0; foreach (var kvp in _ints) sum += kvp.Value; return sum; } }</int></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>EnumerateInts</td> <td>.NET 9.0</td> <td>4,232.8 ns</td> <td>1.00</td> <td>56 B</td> <td>1.00</td> </tr> <tr> <td>EnumerateInts</td> <td>.NET 10.0</td> <td>664.2 ns</td> <td>0.16</td> <td>&ndash;</td> <td>0.00</td> </tr> </tbody> </table> <h3>LINQ</h3><p>All of these examples show enumerating collections using a <code>foreach</code> loop, and while that&rsquo;s obviously incredibly common, so too is using LINQ (Language Integrated Query) to enumerate and process collections. For in-memory collections, LINQ provides literally hundreds of extension methods for performing maps, filters, sorts, and a plethora of other operations over enumerables. It is incredibly handy, is thus used <em>everywhere</em>, and is thus important to optimize. Every release of .NET has seen improvements to LINQ, and that continues in .NET 10.</p><p>Most prominent from a performance perspective in this release are the changes to <code>Contains</code>. As discussed in depth in <a href="https://www.youtube.com/watch?v=xKr96nIyCFM">Deep .NET: Deep Dive on LINQ with Stephen Toub and Scott Hanselman</a> and <a href="https://www.youtube.com/watch?v=W4-NVVNwCWs">Deep .NET: An even DEEPER Dive into LINQ with Stephen Toub and Scott Hanselman</a>, the LINQ methods are able to pass information between them by using specialized internal <code>IEnumerable<t></t></code> implementations. When you call <code>Select</code>, that might return an <code>ArraySelectIterator<tsource></tsource></code> or an <code>IListSelectIterator<tsource></tsource></code> or an <code>IListSkipTakeSelectIterator<tsource></tsource></code> or one of any number of other types. Each of these types has fields that carry information about the source (e.g. the <code>IListSkipTakeSelectIterator<tsource></tsource></code> has fields not only for the <code>IList<tsource></tsource></code> source and the <code>Func<tsource></tsource></code> selector, but also for the tracked min and max bounds based on previous <code>Skip</code> and <code>Take</code> calls), and they have overrides of virtual methods that allow for various operations to be specialized. This means sequences of LINQ methods can be optimized. For example, <code>source.Where(...).Select(...)</code> is optimized a) to combine both the filter and the map delegates into a single <code>IEnumerable<t></t></code>, thus removing the overhead of an extra layer of interface dispatch, and b) to perform operations specific to the original source data type (e.g. if <code>source</code> was an array, the processing can be done directly on that array rather than via <code>IEnumerator<t></t></code>).</p><p>Many of these optimizations make the most sense when a method returns an <code>IEnumerable<t></t></code> that happens to be the result of a LINQ query. The producer of that method doesn&rsquo;t know how the consumer will be consuming it, and the consumer doesn&rsquo;t know the details of how the producer produced it. But since the LINQ methods flow context via the concrete implementations of <code>IEnumerable<t></t></code>, significant optimizations are possible for interesting combinations of consumer and producer methods. For example, let&rsquo;s say a producer of an <code>IEnumerable<t></t></code> decides they want to always return data in ascending order, so they do:</p><pre><code>public static IEnumerable<t> GetData() { ... return data.OrderBy(s =&gt; s.CreatedAt); }</t></code></pre><p>But as it turns out, the consumer won&rsquo;t be looking at all of the elements, and instead just wants the first:</p><pre><code>T value = GetData().First();</code></pre><p>LINQ optimizes this by having the enumerable returned from <code>OrderBy</code> provide a specialized implementation of <code>First</code>/<code>FirstOrDefault</code>: it doesn&rsquo;t need to perform an <code>O(N log N)</code> sort (or allocate a lot of memory to hold all of the keys), it can instead just do an <code>O(N)</code> search for the smallest element in the source, because the smallest element would be the first to be yielded from <code>OrderBy</code>.</p><p><code>Contains</code> is ripe for these kinds of optimizations as well, e.g. <code>OrderBy</code>, <code>Distinct</code>, and <code>Reverse</code> all entail non-trivial processing and/or allocation, but if followed by a <code>Contains</code>, all that work can be skipped, as the <code>Contains</code> can just search the source directly. With <a href="https://github.com/dotnet/runtime/pull/112684">dotnet/runtime#112684</a>, this set of optimizations is extended to <code>Contains</code>, with almost 30 specialized implementations of <code>Contains</code> across the various iterator specializations.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private IEnumerable<int> _source = Enumerable.Range(0, 1000).ToArray(); [Benchmark] public bool AppendContains() =&gt; _source.Append(100).Contains(999); [Benchmark] public bool ConcatContains() =&gt; _source.Concat(_source).Contains(999); [Benchmark] public bool DefaultIfEmptyContains() =&gt; _source.DefaultIfEmpty(42).Contains(999); [Benchmark] public bool DistinctContains() =&gt; _source.Distinct().Contains(999); [Benchmark] public bool OrderByContains() =&gt; _source.OrderBy(x =&gt; x).Contains(999); [Benchmark] public bool ReverseContains() =&gt; _source.Reverse().Contains(999); [Benchmark] public bool UnionContains() =&gt; _source.Union(_source).Contains(999); [Benchmark] public bool SelectManyContains() =&gt; _source.SelectMany(x =&gt; _source).Contains(999); [Benchmark] public bool WhereSelectContains() =&gt; _source.Where(x =&gt; true).Select(x =&gt; x).Contains(999); }</int></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>AppendContains</td> <td>.NET 9.0</td> <td>2,931.97 ns</td> <td>1.00</td> <td>88 B</td> <td>1.00</td> </tr> <tr> <td>AppendContains</td> <td>.NET 10.0</td> <td>52.06 ns</td> <td>0.02</td> <td>56 B</td> <td>0.64</td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>ConcatContains</td> <td>.NET 9.0</td> <td>3,065.17 ns</td> <td>1.00</td> <td>88 B</td> <td>1.00</td> </tr> <tr> <td>ConcatContains</td> <td>.NET 10.0</td> <td>54.58 ns</td> <td>0.02</td> <td>56 B</td> <td>0.64</td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>DefaultIfEmptyContains</td> <td>.NET 9.0</td> <td>39.21 ns</td> <td>1.00</td> <td>&ndash;</td> <td>NA</td> </tr> <tr> <td>DefaultIfEmptyContains</td> <td>.NET 10.0</td> <td>32.89 ns</td> <td>0.84</td> <td>&ndash;</td> <td>NA</td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>DistinctContains</td> <td>.NET 9.0</td> <td>16,967.31 ns</td> <td>1.000</td> <td>58656 B</td> <td>1.000</td> </tr> <tr> <td>DistinctContains</td> <td>.NET 10.0</td> <td>46.72 ns</td> <td>0.003</td> <td>64 B</td> <td>0.001</td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>OrderByContains</td> <td>.NET 9.0</td> <td>12,884.28 ns</td> <td>1.000</td> <td>12280 B</td> <td>1.000</td> </tr> <tr> <td>OrderByContains</td> <td>.NET 10.0</td> <td>50.14 ns</td> <td>0.004</td> <td>88 B</td> <td>0.007</td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>ReverseContains</td> <td>.NET 9.0</td> <td>479.59 ns</td> <td>1.00</td> <td>4072 B</td> <td>1.00</td> </tr> <tr> <td>ReverseContains</td> <td>.NET 10.0</td> <td>51.80 ns</td> <td>0.11</td> <td>48 B</td> <td>0.01</td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>UnionContains</td> <td>.NET 9.0</td> <td>16,910.57 ns</td> <td>1.000</td> <td>58664 B</td> <td>1.000</td> </tr> <tr> <td>UnionContains</td> <td>.NET 10.0</td> <td>55.56 ns</td> <td>0.003</td> <td>72 B</td> <td>0.001</td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>SelectManyContains</td> <td>.NET 9.0</td> <td>2,950.64 ns</td> <td>1.00</td> <td>192 B</td> <td>1.00</td> </tr> <tr> <td>SelectManyContains</td> <td>.NET 10.0</td> <td>60.42 ns</td> <td>0.02</td> <td>128 B</td> <td>0.67</td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>WhereSelectContains</td> <td>.NET 9.0</td> <td>1,782.05 ns</td> <td>1.00</td> <td>104 B</td> <td>1.00</td> </tr> <tr> <td>WhereSelectContains</td> <td>.NET 10.0</td> <td>260.25 ns</td> <td>0.15</td> <td>104 B</td> <td>1.00</td> </tr> </tbody> </table><p>LINQ in .NET 10 also gains some new methods, including <code>Sequence</code> and <code>Shuffle</code>. While the primary purpose of these new methods is not performance, they can have a meaningful impact on performance, due to how they&rsquo;ve been implemented and how they integrate with the rest of the optimizations in LINQ. Take <code>Sequence</code>, for example. <code>Sequence</code> is similar to <code>Range</code>, in that its a source for numbers:</p><pre><code>public static IEnumerable<t> Sequence<t>(T start, T endInclusive, T step) where T : INumber<t></t></t></t></code></pre><p>Whereas <code>Range</code> only works with <code>int</code> and produces a contiguous series of non-overflowing numbers starting at the initial value, <code>Sequence</code> works with any <code>INumber&lt;&gt;</code>, supports <code>step</code> values other than <code>1</code> (including negative values), and allows for wrapping around <code>T</code>&lsquo;s maximum or minimum. However, when appropriate (e.g. <code>step</code> is <code>1</code>), <code>Sequence</code> will try to utilize <code>Range</code>&lsquo;s implementation, which has internally been updated to work with any <code>T : INumber<t></t></code>, even though its public API is still tied to <code>int</code>. That means that all of the optimizations afforded to <code>Range<t></t></code> propagate to <code>Sequence<t></t></code>.</p><pre><code>// dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private List<short> _values = new(); [Benchmark(Baseline = true)] public void Fill1() { _values.Clear(); for (short i = 42; i &lt;= 1042; i++) { _values.Add(i); } } [Benchmark] public void Fill2() { _values.Clear(); _values.AddRange(Enumerable.Sequence<short>(42, 1042, 1)); } }</short></short></code></pre> <table> <thead> <tr> <th>Method</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Fill1</td> <td>1,479.99 ns</td> <td>1.00</td> </tr> <tr> <td>Fill2</td> <td>37.42 ns</td> <td>0.03</td> </tr> </tbody> </table><p>My favorite new LINQ method, though, is <code>Shuffle</code> (introduced in <a href="https://github.com/dotnet/runtime/pull/112173">dotnet/runtime#112173</a>), in part because it&rsquo;s very handy, but in part because of its implementation and performance focus. The purpose of <code>Shuffle</code> is to randomize the source input, and logically, it&rsquo;s akin to a very simple implementation:</p><pre><code>public static IEnumerable<t> Shuffle<t>(IEnumerable<t> source) { T[] arr = source.ToArray(); Random.Shared.Shuffle(arr); foreach (T item in arr) yield return item; }</t></t></t></code></pre><p>Worst case, this implementation is effectively what&rsquo;s in LINQ. Just as in the worst case <code>OrderBy</code> needs to buffer up the whole input because it&rsquo;s possible any item might be the smallest and thus need to be yielded first, <code>Shuffle</code> similarly needs to support the possibility that the last element should probabilistically be yielded first. However, there are a variety of special-cases in the implementation that allow it to perform significantly better than such a hand-rolled <code>Shuffle</code> implementation you might be using today.</p><p>First, <code>Shuffle</code> has some of the same characteristics as <code>OrderBy</code>, in that they&rsquo;re both creating permutations of the input. That means that many of the ways we can specialize subsequent operations on the result of an <code>OrderBy</code> also apply to <code>Shuffle</code>. For example, <code>Shuffle.First</code> on an <code>IList<t></t></code> can just select an element from the list at random. <code>Shuffle.Count</code> can just count the underlying source, since the order of the elements is irrelevant to the result. <code>Shuffle.Contains</code> can just perform the contains on the underlying source. Etc. But my two favorite sequences are <code>Shuffle.Take</code> and <code>Shuffle.Take.Contains</code>.</p><p><code>Shuffle.Take</code> provides an interesting optimization opportunity: whereas with <code>Shuffle</code> by itself we need to build the whole shuffled sequence, with a <code>Shuffle</code> followed immediately by a <code>Take(N)</code>, we only need to sample <code>N</code> items from the source. We still need those <code>N</code> items to be a uniformly random distribution, akin to what we&rsquo;d get if we performed the buffering shuffle and then selected the first <code>N</code> items in the resulting array, but we can do so using an algorithm that allows us to avoid buffering everything. We need an algorithm that will let us iterate through the source data once, picking out elements as we go, and only ever buffering <code>N</code> items at a time. Enter &ldquo;reservoir sampling.&rdquo; I previously discussed reservoir sampling in <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-8/">Performance Improvements in .NET 8</a>, as it&rsquo;s employed by the JIT as part of its dynamic PGO implementation, and we can use the algorithm here in <code>Shuffle</code> as well. Reservoir sampling provides exactly the single-pass, low-memory path we want: initialize a &ldquo;reservoir&rdquo; (an array) with the first <code>N</code> items, then as we scan the rest of the sequence, probabilistically overwrite one of the elements in our reservoir with the current item. The algorithm ensures that every element ends up in the reservoir with equal probability, yielding the same distribution as fully shuffling and taking <code>N</code>, but using only <code>O(N)</code> space and only making a single pass over an otherwise unknown-length source.</p><pre><code>// dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private IEnumerable<int> _source = Enumerable.Range(1, 1000).ToList(); [Benchmark(Baseline = true)] public List<int> ShuffleTakeManual() =&gt; ShuffleManual(_source).Take(10).ToList(); [Benchmark] public List<int> ShuffleTakeLinq() =&gt; _source.Shuffle().Take(10).ToList(); private static IEnumerable<int> ShuffleManual(IEnumerable<int> source) { int[] arr = source.ToArray(); Random.Shared.Shuffle(arr); foreach (var item in arr) { yield return item; } } }</int></int></int></int></int></code></pre> <table> <thead> <tr> <th>Method</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>ShuffleTakeManual</td> <td>4.150 us</td> <td>1.00</td> <td>4232 B</td> <td>1.00</td> </tr> <tr> <td>ShuffleTakeLinq</td> <td>3.801 us</td> <td>0.92</td> <td>192 B</td> <td>0.05</td> </tr> </tbody> </table><p><code>Shuffle.Take.Contains</code> is even more fun. We now have a probability problem that reads like a brain teaser or an SAT question. &ldquo;I have <code>totalCount</code> items of which <code>equalCount</code> match my target value, and we&rsquo;re going to pick <code>takeCount</code> items at random. What is the probability that at least one of those <code>takeCount</code> items is one of the <code>equalCount</code> items?&rdquo; This is called a hypergeometric distribution, and we can use an implementation of it for <code>Shuffle.Take.Contains</code>.</p><p>To make this easier to reason about, let&rsquo;s talk candy. Imagine you have a jar of 100 jelly beans, of which 20 are your favorite flavor, Watermelon, and you&rsquo;re going to pick 5 of the 100 beans at random; what are the chances you get at least one Watermelon? To solve this, we could reason through all the different ways we might get 1, 2, 3, 4, or 5 Watermelons, but instead, let&rsquo;s do the opposite and think through how likely it is that we don&rsquo;t get any (sad panda):</p><ul> <li>The chance that our first pick isn&rsquo;t a Watermelon is the number of non-Watermelons divided by the total number of beans, so <code>(100-20)/100</code>.</li> <li>Once we&rsquo;ve picked a bean out of the jar, we&rsquo;re not putting it back, so the chance that our second pick isn&rsquo;t a Watermelon is now <code>(99-20)/99</code> (we have one fewer bean, but our first pick wasn&rsquo;t a Watermelon, so there&rsquo;s the same number of Watermelons as there was before).</li> <li>For a third pick, it&rsquo;s now <code>(98-20)/98</code>.</li> <li>And so on.</li> </ul><p>After five rounds, we end up with <code>(80/100) * (79/99) * (78/98) * (77/97) * (76/96)</code>, which is ~32%. If the chances I don&rsquo;t get a Watermelon are ~32%, then the chances I do get a Watermelon are ~68%. Jelly beans aside, that&rsquo;s our algorithm:</p><pre><code>double probOfDrawingZeroMatches = 1; for (long i = 0; i &lt; _takeCount; i++) { probOfDrawingZeroMatches *= (double)(totalCount - i - equalCount) / (totalCount - i); } return Random.Shared.NextDouble() &gt; probOfDrawingZeroMatches;</code></pre><p>The net effect is we can compute the answer much more efficiently than with a naive implementation that shuffles and then separately takes and separately contains.</p><pre><code>// dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private IEnumerable<int> _source = Enumerable.Range(1, 1000).ToList(); [Benchmark(Baseline = true)] public bool ShuffleTakeContainsManual() =&gt; ShuffleManual(_source).Take(10).Contains(2000); [Benchmark] public bool ShuffleTakeContainsLinq() =&gt; _source.Shuffle().Take(10).Contains(2000); private static IEnumerable<int> ShuffleManual(IEnumerable<int> source) { int[] arr = source.ToArray(); Random.Shared.Shuffle(arr); foreach (var item in arr) { yield return item; } } }</int></int></int></code></pre> <table> <thead> <tr> <th>Method</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>ShuffleTakeContainsManual</td> <td>3,900.99 ns</td> <td>1.00</td> <td>4136 B</td> <td>1.00</td> </tr> <tr> <td>ShuffleTakeContainsLinq</td> <td>79.12 ns</td> <td>0.02</td> <td>96 B</td> <td>0.02</td> </tr> </tbody> </table><p><code>LINQ</code> in .NET 10 also sports some new methods that <em>are</em> about performance (at least in part), in particular <code>LeftJoin</code> and <code>RightJoin</code>, from <a href="https://github.com/dotnet/runtime/pull/110872">dotnet/runtime#110872</a>. I say these are about performance because it&rsquo;s already possible to achieve the left and right join semantics using existing LINQ surface area, and the new methods do it more efficiently.</p><p><code>Enumerable.Join</code> implements an &ldquo;inner join,&rdquo; meaning only matching pairs from the two supplied collections appear in the output. For example, this code, which is joining based on the first letter in each string:</p><pre><code>IEnumerable<string> left = ["apple", "banana", "cherry", "date", "grape", "honeydew"]; IEnumerable<string> right = ["aardvark", "dog", "elephant", "goat", "gorilla", "hippopotamus"]; foreach (string result in left.Join(right, s =&gt; s[0], s =&gt; s[0], (s1, s2) =&gt; $"{s1} {s2}")) { Console.WriteLine(result); }</string></string></code></pre><p>outputs:</p><pre><code>apple aardvark date dog grape goat grape gorilla honeydew hippopotamus</code></pre><p>In contrast, a &ldquo;left join&rdquo; (also known as a &ldquo;left outer join&rdquo;) would yield the following:</p><pre><code>apple aardvark banana cherry date dog grape goat grape gorilla honeydew hippopotamus</code></pre><p>Note that it has all of the same output as with the &ldquo;inner join,&rdquo; except it has at least one row for every <code>left</code> element, even if there&rsquo;s no matching element in the <code>right</code> row. And then a &ldquo;right join&rdquo; (also known as a &ldquo;right outer join&rdquo;) would yield the following:</p><pre><code>apple aardvark date dog elephant grape goat grape gorilla honeydew hippopotamus</code></pre><p>Again, all the same output as with the &ldquo;inner join,&rdquo; except it has at least one row for every <code>right</code> element, even if there&rsquo;s no matching element in the <code>left</code> row.</p><p>Prior to .NET 10, there was no <code>LeftJoin</code> or <code>RightJoin</code>, but their semantics could be achieved using a combination of <code>GroupJoin</code>, <code>SelectMany</code>, and <code>DefaultIfEmpty</code>:</p><pre><code>public static IEnumerable<tresult> LeftJoin<touter>( this IEnumerable<touter> outer, IEnumerable<tinner> inner, Func<touter> outerKeySelector, Func<tinner> innerKeySelector, Func<touter> resultSelector) =&gt; outer .GroupJoin(inner, outerKeySelector, innerKeySelector, (o, inners) =&gt; (o, inners)) .SelectMany(x =&gt; x.inners.DefaultIfEmpty(), (x, i) =&gt; resultSelector(x.o, i));</touter></tinner></touter></tinner></touter></touter></tresult></code></pre><p><code>GroupJoin</code> creates a group for each <code>outer</code> (&ldquo;left&rdquo;) element, where the group contains all matching items from <code>inner</code> (&ldquo;right&rdquo;). We can flatten those results by using <code>SelectMany</code>, such that we end up with an output for each pairing, using <code>DefaultIfEmpty</code> to ensure that there&rsquo;s always at least a default inner element to pair. We can do the exact same thing for a <code>RightJoin</code>: in fact, we can implement the right join just by delegating to the left join and flipping all the arguments:</p><pre><code>public static IEnumerable<tresult> RightJoin<touter>( this IEnumerable<touter> outer, IEnumerable<tinner> inner, Func<touter> outerKeySelector, Func<tinner> innerKeySelector, Func<touter> resultSelector) =&gt; inner.LeftJoin(outer, innerKeySelector, outerKeySelector, (i, o) =&gt; resultSelector(o, i));</touter></tinner></touter></tinner></touter></touter></tresult></code></pre><p>Thankfully, you no longer need to do that yourself, and this isn&rsquo;t how the new <code>LeftJoin</code> and <code>RightJoin</code> methods are implemented in .NET 10. We can see the difference with a benchmark:</p><pre><code>// dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Linq; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private IEnumerable<int> Outer { get; } = Enumerable.Sequence(0, 1000, 2); private IEnumerable<int> Inner { get; } = Enumerable.Sequence(0, 1000, 3); [Benchmark(Baseline = true)] public void LeftJoin_Manual() =&gt; ManualLeftJoin(Outer, Inner, o =&gt; o, i =&gt; i, (o, i) =&gt; o + i).Count(); [Benchmark] public int LeftJoin_Linq() =&gt; Outer.LeftJoin(Inner, o =&gt; o, i =&gt; i, (o, i) =&gt; o + i).Count(); private static IEnumerable<tresult> ManualLeftJoin<touter>( IEnumerable<touter> outer, IEnumerable<tinner> inner, Func<touter> outerKeySelector, Func<tinner> innerKeySelector, Func<touter> resultSelector) =&gt; outer .GroupJoin(inner, outerKeySelector, innerKeySelector, (o, inners) =&gt; (o, inners)) .SelectMany(x =&gt; x.inners.DefaultIfEmpty(), (x, i) =&gt; resultSelector(x.o, i)); }</touter></tinner></touter></tinner></touter></touter></tresult></int></int></code></pre> <table> <thead> <tr> <th>Method</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>LeftJoin_Manual</td> <td>29.02 us</td> <td>1.00</td> <td>65.84 KB</td> <td>1.00</td> </tr> <tr> <td>LeftJoin_Linq</td> <td>15.23 us</td> <td>0.53</td> <td>36.95 KB</td> <td>0.56</td> </tr> </tbody> </table><p>Moving on from new methods, existing methods were also improved in other ways. <a href="https://github.com/dotnet/runtime/pull/112401">dotnet/runtime#112401</a> from <a href="https://github.com/miyaji255">@miyaji255</a> improved the performance of <code>ToArray</code> and <code>ToList</code> following <code>Skip</code> and/or <code>Take</code> calls. In the specialized iterator implementation used for <code>Take</code> and <code>Skip</code>, this PR simply checks in the <code>ToList</code> and <code>ToArray</code> implementations whether the source is something from which we can easily get a <code>ReadOnlySpan<t></t></code> (namely a <code>T[]</code> or <code>List<t></t></code>). If it is, rather than copying elements one by one into the destination, it can slice the retrieved span and use its <code>CopyTo</code>, which, depending on the <code>T</code>, may even be vectorized.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private readonly IEnumerable<string> _source = Enumerable.Range(0, 1000).Select(i =&gt; i.ToString()).ToArray(); [Benchmark] public List<string> SkipTakeToList() =&gt; _source.Skip(200).Take(200).ToList(); }</string></string></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>SkipTakeToList</td> <td>.NET 9.0</td> <td>1,218.9 ns</td> <td>1.00</td> </tr> <tr> <td>SkipTakeToList</td> <td>.NET 10.0</td> <td>257.4 ns</td> <td>0.21</td> </tr> </tbody> </table><p>LINQ in .NET 10 also sees a few notable enhancements for Native AOT. The code for LINQ has grown over time, as all of these various specializations have found their way into the codebase. These optimizations are generally implemented by deriving specialized iterators from a base <code>Iterator<t></t></code>, which has a bunch of <code>abstract</code> or <code>virtual</code> methods for performing the subsequent operation (e.g. <code>Contains</code>). With Native AOT, any use of a method like <code>Enumerable.Contains</code> then prevents the corresponding implementations on <em>all</em> of those specializations from being trimmed away, leading to non-trivial increase in assembly code size. As such, years ago multiple builds of <code>System.Linq.dll</code> were introduced into the <code>dotnet/runtime</code> build system: one focused on speed, and one focused on size. When building <code>System.Linq.dll</code> to go with coreclr, you&rsquo;d end up with the speed-optimized build that has all of these specializations. When building <code>System.Linq.dll</code> to go with other flavors, like Native AOT, you&rsquo;d instead get the size-optimized build, which eschews many of the LINQ optimizations that have been added in the last decade. And as this was a build-time decision, developers using one of these platforms didn&rsquo;t get a choice; as you learn in kindergarten, &ldquo;you get what you get and you don&rsquo;t get upset.&rdquo; Now in .NET 10, if you do forget what you learned in kindergarten and you do get upset, you have recourse: thanks to <a href="https://github.com/dotnet/runtime/pull/111743">dotnet/runtime#111743</a> and <a href="https://github.com/dotnet/runtime/pull/109978">dotnet/runtime#109978</a>, this setting is now a feature switch rather than a build-time configuration. So, in particular if you&rsquo;re publishing for Native AOT and you&rsquo;d prefer all the speed-focused optimizations, you can add <code><usesizeoptimizedlinq>false</usesizeoptimizedlinq></code> to your project file and be happy.</p><p>However, the need for that switch is now also reduced significantly by <a href="https://github.com/dotnet/runtime/pull/118156">dotnet/runtime#118156</a>. When this size/speed split was previously introduced into the <code>System.Linq.dll</code> build, all of these specializations were eschewed, without a lot of an analysis for tradeoffs involved; as this was focused on optimizing for size, any specialized overrides were removed, no matter how much space they actually saved. Many of those savings turned out to be minimal, however, and in a variety of situations, the throughput cost was significant. This PR brings back some of the more impactful specializations where the throughput gains significantly outweigh the relatively-minimal size cost.</p><h3>Frozen Collections</h3><p>The <code>FrozenDictionary<tkey></tkey></code> and <code>FrozenSet<t></t></code> collection types were introduced in .NET 8 as collections optimized for the common scenario of creating a long-lived collection that&rsquo;s then read from <em>a lot</em>. They spend more time at construction in exchange for faster read operations. Under the covers, this is achieved in part by having specializations of the implementations that are optimized for different types of data or shapes of input. .NET 9 improved upon the implementations, and .NET 10 takes it even further.</p><p><code>FrozenDictionary<tkey></tkey></code> exerts a lot of energy for <code>TKey</code> as <code>string</code>, as that is such a common use case. It also has specializations for <code>TKey</code> as <code>Int32</code>. <a href="https://github.com/dotnet/runtime/pull/111886">dotnet/runtime#111886</a> and <a href="https://github.com/dotnet/runtime/pull/112298">dotnet/runtime#112298</a> extend that further by adding specializations for when <code>TKey</code> is any primitive integral type that&rsquo;s the size of an <code>int</code> or smaller (e.g. <code>byte</code>, <code>char</code>, <code>ushort</code>, etc.) as well as enums backed by such primitives (which represent the vast, vast majority of enums used in practice). In particular, they handle the common case where these values are densely packed, in which case they implement the dictionary as an array that it can index into based on the integer&rsquo;s value. This makes for a very efficient lookup, while not consuming too much additional space: it&rsquo;s only used when the values are dense and thus won&rsquo;t be wasting many empty slots in the array.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Collections.Frozen; using System.Net; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "status")] public partial class Tests { private static readonly FrozenDictionary<httpstatuscode> s_statusDescriptions = Enum.GetValues<httpstatuscode>().Distinct() .ToFrozenDictionary(status =&gt; status, status =&gt; status.ToString()); [Benchmark] [Arguments(HttpStatusCode.OK)] public string Get(HttpStatusCode status) =&gt; s_statusDescriptions[status]; }</httpstatuscode></httpstatuscode></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Get</td> <td>.NET 9.0</td> <td>2.0660 ns</td> <td>1.00</td> </tr> <tr> <td>Get</td> <td>.NET 10.0</td> <td>0.8735 ns</td> <td>0.42</td> </tr> </tbody> </table><p>Both <code>FrozenDictionary<tkey></tkey></code> and <code>FrozenSet<t></t></code> also improve with regards to the alternate lookup functionality introduced in .NET 9. Alternate lookups are a mechanism that enables getting a proxy for a dictionary or set that&rsquo;s keyed with a different key from <code>TKey</code>, most commonly a <code>ReadOnlySpan<char></char></code> when <code>TKey</code> is <code>string</code>. As noted, both <code>FrozenDictionary<tkey></tkey></code> and <code>FrozenSet<t></t></code> achieve their goals by having different implementations based on the nature of the indexed data, and that specialization is achieved by virtual methods that derived specializations override. The JIT is typically able to minimize the costs of such virtuals, especially if the collections are stored in <code>static readonly</code> fields. However, the alternate lookup support complicated things, as it introduced a virtual method with a generic method parameter (the alternate key type), otherwise known as GVM. &ldquo;GVM&rdquo; might as well be a four-letter word in performance circles, as they&rsquo;re hard for the runtime to optimize. The purpose of these alternate lookups is primarily performance, but the use of a GVM significantly reduced those performance gains. <a href="https://github.com/dotnet/runtime/pull/108732">dotnet/runtime#108732</a> from <a href="https://github.com/andrewjsaid">@andrewjsaid</a> addresses this by changing the frequency with which a GVM needs to be invoked. Rather than the lookup operation itself being a generic virtual method, the PR introduces a separate generic virtual method that retrieves a delegate for performing the lookup; the retrieval of that delegate still incurs GVM penalties, but once the delegate is retrieved, it can be cached, and invoking it does not incur said overheads. This results in measurable improvements on throughput.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Collections.Frozen; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private static readonly FrozenDictionary<string> s_d = new Dictionary<string> { ["one"] = 1, ["two"] = 2, ["three"] = 3, ["four"] = 4, ["five"] = 5, ["six"] = 6, ["seven"] = 7, ["eight"] = 8, ["nine"] = 9, ["ten"] = 10, ["eleven"] = 11, ["twelve"] = 12, }.ToFrozenDictionary(); [Benchmark] public int Get() { var alternate = s_d.GetAlternateLookup<readonlyspan>&gt;(); return alternate["one"] + alternate["two"] + alternate["three"] + alternate["four"] + alternate["five"] + alternate["six"] + alternate["seven"] + alternate["eight"] + alternate["nine"] + alternate["ten"] + alternate["eleven"] + alternate["twelve"]; } }</readonlyspan></string></string></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Get</td> <td>.NET 9.0</td> <td>133.46 ns</td> <td>1.00</td> </tr> <tr> <td>Get</td> <td>.NET 10.0</td> <td>81.39 ns</td> <td>0.61</td> </tr> </tbody> </table> <h3>BitArray</h3><p><code>BitArray</code> provides support for exactly what its name says, a bit array. You create it with the desired number of values and can then read and write a <code>bool</code> for each index, turning the corresponding bit to <code>1</code> or <code>0</code> accordingly. It also provides a variety of helper operations for processing the whole bit array, such as for Boolean logic operations like <code>And</code> and <code>Not</code>. Where possible, those operations are vectorized, taking advantage of SIMD to process many bits per instruction.</p><p>However, for situations where you want to write custom manipulations of the bits, you only have two options: use the indexer (or corresponding <code>Get</code> and <code>Set</code> methods), which means multiple instructions required to process each bit, or use <code>CopyTo</code> to extract all of the bits to a separate array, which means you need to allocate (or at least rent) such an array and pay for the memory copy before you can then manipulate the bits. There&rsquo;s also not a great way to then copy those bits back if you wanted to manipulate the <code>BitArray</code> in place.</p><p><a href="https://github.com/dotnet/runtime/pull/116308">dotnet/runtime#116308</a> adds a <code>CollectionsMarshal.AsBytes(BitArray)</code> method that returns a <code>Span<byte></byte></code> directly referencing the <code>BitArray</code>&lsquo;s underlying storage. This provides a very efficient way to get access to all the bits, which then makes it possible to write (or reuse) vectorized algorithms. Say, for example, you wanted to use a <code>BitArray</code> to represent a binary embedding (an &ldquo;embedding&rdquo; is a vector representation of the semantic meaning of some data, basically an array of numbers, each one corresponding to some aspect of the data; a binary embedding uses a single bit for each number). To determine how semantically similar two inputs are, you get an embedding for each and then perform a distance or similarity calculation on the two. For binary embeddings, a common distance metric is &ldquo;hamming distance,&rdquo; which effectively lines up the bits and tells you the number of positions that have different values, e.g. <code>0b1100</code> and <code>0b1010</code> have a hamming distance of 2. Helpfully, <code>TensorPrimitives.HammingBitDistance</code> provides an implementation of this, accepting two <code>ReadOnlySpan<t></t></code>s and computing the number of bits that differ between them. With <code>CollectionsMarshal.AsBytes</code>, we can now utilize that helper directly with the contents of <code>BitArray</code>s, both saving us the effort of having to write it manually and benefiting from any optimizations in <code>HammingBitDistance</code> itself.</p><pre><code>// Update benchmark.csproj with a package reference to System.Numerics.Tensors. // dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Collections; using System.Numerics.Tensors; using System.Runtime.InteropServices; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private BitArray _bits1, _bits2; [GlobalSetup] public void Setup() { Random r = new(42); byte[] bytes = new byte[128]; r.NextBytes(bytes); _bits1 = new BitArray(bytes); r.NextBytes(bytes); _bits2 = new BitArray(bytes); } [Benchmark(Baseline = true)] public long HammingDistanceManual() { long distance = 0; for (int i = 0; i &lt; _bits1.Length; i++) { if (_bits1[i] != _bits2[i]) { distance++; } } return distance; } [Benchmark] public long HammingDistanceTensorPrimitives() =&gt; TensorPrimitives.HammingBitDistance( CollectionsMarshal.AsBytes(_bits1), CollectionsMarshal.AsBytes(_bits2)); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>HammingDistanceManual</td> <td>1,256.72 ns</td> <td>1.00</td> </tr> <tr> <td>HammingDistanceTensorPrimitives</td> <td>63.29 ns</td> <td>0.05</td> </tr> </tbody> </table><p>The main motivation for this PR was adding the <code>AsBytes</code> method, but doing so triggered a series of other modifications that themselves help with performance. For example, rather than backing the <code>BitArray</code> with an <code>int[]</code> as was previously done, it&rsquo;s now backed by a <code>byte[]</code>, and rather than reading elements one by one in the <code>byte[]</code>-based constructor, vectorized copy operations are now being used (they were already being used and continue to be used in the <code>int[]</code>-based constructor).</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Collections; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private byte[] _byteData = Enumerable.Range(0, 512).Select(i =&gt; (byte)i).ToArray(); [Benchmark] public BitArray ByteCtor() =&gt; new BitArray(_byteData); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>ByteCtor</td> <td>.NET 9.0</td> <td>160.10 ns</td> <td>1.00</td> </tr> <tr> <td>ByteCtor</td> <td>.NET 10.0</td> <td>83.07 ns</td> <td>0.52</td> </tr> </tbody> </table> <h3>Other Collections</h3><p>There are a variety of other notable improvements in collections:</p><ul> <li><strong><code>List<t></t></code></strong>. <a href="https://github.com/dotnet/runtime/pull/107683">dotnet/runtime#107683</a> from <a href="https://github.com/karakasa">@karakasa</a> builds on a change that was made in .NET 9 to improve the performance of using <code>InsertRange</code> on a <code>List<t></t></code> to insert a <code>ReadOnlySpan<t></t></code>. When a full <code>List<t></t></code> is appended to, the typical process is a new larger array is allocated, all of the existing elements are copied over (one array copy), and then the new element is stored into the array in the next available slot. If that same growth routine is used when <em>inserting</em> rather than <em>appending</em> an element, you possibly end up copying some elements twice: you first copy over all of the elements into the new array, and then to handle the insert, you may again need to copy some of the elements you already copied as part of shifting them to make room for the insertion at the new location. In the extreme, if you&rsquo;re inserting at index 0, you copy all of the elements into the new array, and then you copy all of the elements again to shift them by one slot. The same applies when inserting a range of elements, so with this PR, rather than first copying over all of the elements and then shifting a subset, <code>List<t></t></code> now grows by copying the elements above and below the target range for the insertion to their correct location and then fills in the target range with the inserted elements.<pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private readonly int[] _data = [1, 2, 3, 4]; [Benchmark] public List<int> Test() { List<int> list = new(4); list.AddRange(_data); list.InsertRange(0, _data); return list; } }</int></int></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Test</td> <td>.NET 9.0</td> <td>48.65 ns</td> <td>1.00</td> </tr> <tr> <td>Test</td> <td>.NET 10.0</td> <td>30.07 ns</td> <td>0.62</td> </tr> </tbody> </table> </li> <li><strong><code>ConcurrentDictionary<tkey></tkey></code></strong>. <a href="https://github.com/dotnet/runtime/pull/108065">dotnet/runtime#108065</a> from <a href="https://github.com/koenigst">@koenigst</a> changes how a <code>ConcurrentDictionary</code>&lsquo;s backing array is sized when it&rsquo;s cleared. <code>ConcurrentDictionary</code> is implemented with an array of linked lists, and when the collection is constructed, a constructor parameter allows for presizing that array. Due to the concurrent nature of the dictionary and its implementation, <code>Clear</code>&lsquo;ing it necessitates creating a new array rather than just using part of the old one. When that new array was created, it reset to using the default size. This PR tweaks that to remember the initial capacity requested by the user, and using that initial size again when constructing the new array.<pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Collections.Concurrent; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private ConcurrentDictionary<int> _data = new(concurrencyLevel: 1, capacity: 1024); [Benchmark] public void ClearAndAdd() { _data.Clear(); for (int i = 0; i &lt; 1024; i++) { _data.TryAdd(i, i); } } }</int></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>ClearAndAdd</td> <td>.NET 9.0</td> <td>51.95 us</td> <td>1.00</td> <td>134.36 KB</td> <td>1.00</td> </tr> <tr> <td>ClearAndAdd</td> <td>.NET 10.0</td> <td>30.32 us</td> <td>0.58</td> <td>48.73 KB</td> <td>0.36</td> </tr> </tbody> </table> </li> <li><strong><code>Dictionary<tkey></tkey></code></strong>. <code>Dictionary</code> is one of the most popular collection types across .NET, and <code>TKey</code> == <code>string</code> is one of (if not <em>the</em>) most popular forms. <a href="https://github.com/dotnet/runtime/pull/117427">dotnet/runtime#117427</a> makes dictionary lookups with constant <code>string</code>s much faster. You might expect it would be a complicated change, but it ends up being just a few strategic tweaks. A variety of methods for operating on <code>string</code>s are already known to the JIT and already have optimized implementations for when dealing with constants. All this PR needed to do was change which methods <code>Dictionary<tkey></tkey></code> was using in its optimized <code>TryGetValue</code> lookup path, and because that path is often inlined, a constant argument to <code>TryGetValue</code> can be exposed as a constant to these helpers, e.g. <code>string.Equals</code>.<pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private Dictionary<string> _data = new() { ["a"] = 1, ["b"] = 2, ["c"] = 3, ["d"] = 4, ["e"] = 5 }; [Benchmark] public int Get() =&gt; _data["a"] + _data["b"] + _data["c"] + _data["d"] + _data["e"]; }</string></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Get</td> <td>.NET 9.0</td> <td>33.81 ns</td> <td>1.00</td> </tr> <tr> <td>Get</td> <td>.NET 10.0</td> <td>14.02 ns</td> <td>0.41</td> </tr> </tbody> </table> </li> <li><strong><code>OrderedDictionary<tkey></tkey></code></strong>. <a href="https://github.com/dotnet/runtime/pull/109324">dotnet/runtime#109324</a> adds new overloads of <code>TryAdd</code> and <code>TryGetValue</code> that provide the index of the added or retrieved element in the collection. This index can then be used in subsequent operations on the dictionary to access the same slot. For example, if you want to implement an <code>AddOrUpdate</code> operation on top of <code>OrderedDictionary</code>, you need to perform one or two operations, first trying to add the item, and then if found to already exist, updating it, and that update can benefit from targeting the exact index that contains the element rather than it needing to do another keyed lookup.<pre><code>// dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private OrderedDictionary<string> _dictionary = new(); [Benchmark(Baseline = true)] public void Old() =&gt; AddOrUpdate_Old(_dictionary, "key", k =&gt; 1, (k, v) =&gt; v + 1); [Benchmark] public void New() =&gt; AddOrUpdate_New(_dictionary, "key", k =&gt; 1, (k, v) =&gt; v + 1); private static void AddOrUpdate_Old(OrderedDictionary<string> d, string key, Func<string> addFunc, Func<string> updateFunc) { if (d.TryGetValue(key, out int existing)) { d[key] = updateFunc(key, existing); } else { d.Add(key, addFunc(key)); } } private static void AddOrUpdate_New(OrderedDictionary<string> d, string key, Func<string> addFunc, Func<string> updateFunc) { if (d.TryGetValue(key, out int existing, out int index)) { d.SetAt(index, updateFunc(key, existing)); } else { d.Add(key, addFunc(key)); } } }</string></string></string></string></string></string></string></code></pre> <table> <thead> <tr> <th>Method</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Old</td> <td>6.961 ns</td> <td>1.00</td> </tr> <tr> <td>New</td> <td>4.201 ns</td> <td>0.60</td> </tr> </tbody> </table> </li> <li><strong><code>ImmutableArray<t></t></code></strong>. The <code>ImmutableCollectionsMarshal</code> class already exposes an <code>AsArray</code> method that enables retrieving the backing <code>T[]</code> from an <code>ImmutableArray<t></t></code>. However, if you had an <code>ImmutableArray<t>.Builder</t></code>, there was previously no way to access the backing store it was using. <a href="https://github.com/dotnet/runtime/pull/112177">dotnet/runtime#112177</a> enables doing so, with an <code>AsMemory</code> method that retrieves the underlying storage as a <code>Memory<t></t></code>.</li> <li><strong><code>InlineArray</code></strong>. .NET 8 introduced <code>InlineArrayAttribute</code>, which can be used to attribute a struct containing a single field; the attribute accepts a count, and the runtime replicates the struct&rsquo;s field that number of times, as if you&rsquo;d logically copy/pasted the field repeatedly. The runtime also ensures that the storage is contiguous and appropriately aligned, such that if you had an indexible collection that pointed to the beginning of the struct, you could use it as an array. And it so happens such a collection exists: <code>Span<t></t></code>. C# 12 then makes it easy to treat any such attributed struct as a span, e.g.<pre><code>[InlineArray(8)] internal struct EightStrings { private string _field; } ... EightStrings strings = default; Span<string> span = strings;</string></code></pre><p>The C# compiler will itself emit code that uses this capability. For example, if you use collection expressions to initialize a span, you&rsquo;re likely triggering the compiler to emit an <code>InlineArray</code>. When I write this:</p><pre><code>public void M(int a, int b, int c, int d) { Span<int> span = [a, b, c, d]; }</int></code></pre><p>the compiler emits something like the following equivalent:</p><pre><code>public void M(int a, int b, int c, int d) { &lt;&gt;y__InlineArray4<int> buffer = default(&lt;&gt;y__InlineArray4<int>);<privateimplementationdetails>.InlineArrayElementRef&lt;&lt;&gt;y__InlineArray4<int>, int&gt;(ref buffer, 0) = a;<privateimplementationdetails>.InlineArrayElementRef&lt;&lt;&gt;y__InlineArray4<int>, int&gt;(ref buffer, 1) = b;<privateimplementationdetails>.InlineArrayElementRef&lt;&lt;&gt;y__InlineArray4<int>, int&gt;(ref buffer, 2) = c;<privateimplementationdetails>.InlineArrayElementRef&lt;&lt;&gt;y__InlineArray4<int>, int&gt;(ref buffer, 3) = d;<privateimplementationdetails>.InlineArrayAsSpan&lt;&lt;&gt;y__InlineArray4<int>, int&gt;(ref buffer, 4); }</int></privateimplementationdetails></int></privateimplementationdetails></int></privateimplementationdetails></int></privateimplementationdetails></int></privateimplementationdetails></int></int></code></pre><p>where it has defined that <code>&lt;&gt;y__InlineArray4</code> like this:</p><pre><code>[StructLayout(LayoutKind.Auto)] [InlineArray(4)] internal struct &lt;&gt;y__InlineArray4<t> { [CompilerGenerated] private T _element0; }</t></code></pre><p>This shows up elsewhere, too. For example, C# 13 introduced support for using <code>params</code> with collections other than arrays, including spans, so now I can write this:</p><pre><code>public void Caller(int a, int b, int c, int d) =&gt; M(a, b, c, d); public void M(params ReadOnlySpan<int> span) { }</int></code></pre><p>and for <code>Caller</code> we&rsquo;ll see very similar code emitted to what I previously showed, with the compiler manufacturing such an <code>InlineArray</code> type. As you might imagine, the popularity of the features that cause the compiler to produce these types has caused there to be a lot of them emitted. Each type is specific to a particular length, so while the compiler will reuse them, a) it can end up needing to emit a lot to cover different lengths, and b) it emits them as internal to each assembly that needs them, so there can end up being a lot of duplication. Looking just at the shared framework for .NET 9 (the core libraries like <code>System.Private.CoreLib</code> that ship as part of the runtime), there are ~140 of these types&hellip; all of which are for sizes no larger than 8. For .NET 10, <a href="https://github.com/dotnet/runtime/pull/113403">dotnet/runtime#113403</a> adds a set of public <code>InlineArray2<t></t></code>, <code>InlineArray3<t></t></code>, etc., that should cover the vast majority of sizes the compiler would otherwise need to emit types. In the near future, the C# compiler will be updated to use those new types when available instead of emitting its own, thereby yielding non-trivial size savings.</p></li> </ul> <h2>I/O</h2><p>In previous .NET releases, there have been concerted efforts that have invested a lot in improving specific areas of I/O performance, such as completely rewriting <code>FileStream</code> in .NET 6. Nothing as comprehensive as that was done for I/O in .NET 10, but there are some nice one-off improvements that can still have a measurable impact on certain scenarios.</p><p>On Unix, when a <code>MemoryMappedFile</code> is created and it&rsquo;s not associated with a particular <code>FileStream</code>, it needs to create some kind of backing memory for the MMF&rsquo;s data. On Linux, it&rsquo;d try to use <code>shm_open</code>, which creates a shared memory object with appropriate semantics. However, in the years since <code>MemoryMappedFile</code> was initially enabled on Linux, the Linux kernel has added support for anonymous files and the <code>memfd_create</code> function that creates them. These are ideal for <code>MemoryMappedFile</code> and much more efficient, so <a href="https://github.com/dotnet/runtime/pull/105178">dotnet/runtime#105178</a> from <a href="https://github.com/am11">@am11</a> switches over to using <code>memfd_create</code> when it&rsquo;s available.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.IO.MemoryMappedFiles; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { [Benchmark] public void MMF() { using MemoryMappedFile mff = MemoryMappedFile.CreateNew(null, 12345); using MemoryMappedViewAccessor accessor = mff.CreateViewAccessor(); } }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>MMF</td> <td>.NET 9.0</td> <td>9.916 us</td> <td>1.00</td> </tr> <tr> <td>MMF</td> <td>.NET 10.0</td> <td>6.358 us</td> <td>0.64</td> </tr> </tbody> </table><p><code>FileSystemWatcher</code> is improved in <a href="https://github.com/dotnet/runtime/pull/116830">dotnet/runtime#116830</a>. The primary purpose for this PR was to fix a memory leak, where on Windows disposing of a <code>FileSystemWatcher</code> while it was in use could end up leaking some objects. However, it also addresses a performance issue specific to Windows. <code>FileSystemWatcher</code> needs to pass a buffer to the OS for the OS to populate with file-changed information. That meant that <code>FileSystemWatcher</code> was allocating a managed array and then immediately pinning that buffer so it could pass a pointer to it into native code. For certain consumption of <code>FileSystemWatcher</code>, especially in scenarios where lots of <code>FileSystemWatcher</code> instances are created, that pinning could contribute to non-trivial heap fragmentation. Interestingly, though, this array is effectively never consumed as an array: all of the writes into it are performed in native code via the pointer that was passed to the OS, and all consumption of it in managed code to read out the events are done via a span. That means the array nature of it doesn&rsquo;t really matter, and we&rsquo;re better off just allocating a native rather than managed buffer that then requires pinning.</p><pre><code>// Run on Windows. // dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { [Benchmark] public void FSW() { using FileSystemWatcher fsw = new(Environment.CurrentDirectory); fsw.EnableRaisingEvents = true; } }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>FSW</td> <td>.NET 9</td> <td>61.46 us</td> <td>1.00</td> <td>8944 B</td> <td>1.00</td> </tr> <tr> <td>FSW</td> <td>.NET 10</td> <td>61.21 us</td> <td>1.00</td> <td>744 B</td> <td>0.08</td> </tr> </tbody> </table><p><code>BufferedStream</code> gets a boost from <a href="https://github.com/dotnet/runtime/pull/104822">dotnet/runtime#104822</a> from <a href="https://github.com/ANahr">@ANahr</a>. There is a curious and problematic inconsistency in <code>BufferedStream</code> that&rsquo;s been there since, well, forever as far as I can tell. It&rsquo;s obviously been revisited in the past, and due to the super duper strong backwards compatibility concerns for .NET Framework (where a key feature is that the framework doesn&rsquo;t change), the issue was never fixed. There&rsquo;s even a <a href="https://github.com/microsoft/referencesource/blob/f7df9e2399ecd273e90908ac11caf1433e142448/mscorlib/system/io/bufferedstream.cs#L1263">comment in the code</a> to this point:</p><pre><code>// We should not be flushing here, but only writing to the underlying stream, but previous version flushed, so we keep this.</code></pre><p>A <code>BufferedStream</code> does what its name says. It wraps an underlying <code>Stream</code> and buffers access to it. So, for example, if it were configured with a buffer size of 1000, and you wrote 100 bytes to the <code>BufferedStream</code> at a time, your first 10 writes would just go to the buffer and the underlying <code>Stream</code> wouldn&rsquo;t be touched at all. Only on the 11th write would the buffer be full and need to be flushed (meaning written) to the underlying <code>Stream</code>. So far, so good. Moreover, there&rsquo;s a difference between flushing to the underlying stream and flushing the underlying stream. Those sound almost identical, but they&rsquo;re not: in the former case, we&rsquo;re effectively calling <code>_stream.Write(buffer)</code> to write the buffer to that stream, and in the latter case, we&rsquo;re effectively calling <code>_stream.Flush()</code> to force any buffering <em>that</em> stream was doing to propagate it to <em>its</em> underlying destination. <code>BufferedStream</code> really shouldn&rsquo;t be in the business of doing the latter when <code>Write</code>&lsquo;ing to the <code>BufferedStream</code>, and in general it wasn&rsquo;t&hellip; except in one case. Whereas most of the writing-related methods would not call <code>_stream.Flush()</code>, for some reason <code>WriteByte</code> did. In particular for cases where the <code>BufferedStream</code> is configured with a small buffer, and where the underlying stream&rsquo;s flush is relatively expensive (e.g. <code>DeflateStream.Flush</code> forces any buffered bytes to be compressed and emitted), that can be problematic for performance, nevermind the inconsistency. This change simply fixes the inconsistency, such that <code>WriteByte</code> no longer forces a flush on the underlying stream.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.IO.Compression; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private byte[] _bytes; [GlobalSetup] public void Setup() { _bytes = new byte[1024 * 1024]; new Random(42).NextBytes(_bytes); } [Benchmark] public void WriteByte() { using Stream s = new BufferedStream(new DeflateStream(Stream.Null, CompressionLevel.SmallestSize), 256); foreach (byte b in _bytes) { s.WriteByte(b); } } }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>WriteByte</td> <td>.NET 9.0</td> <td>73.87 ms</td> <td>1.00</td> </tr> <tr> <td>WriteByte</td> <td>.NET 10.0</td> <td>17.77 ms</td> <td>0.24</td> </tr> </tbody> </table><p>While on the subject of compression, it&rsquo;s worth calling out several improvements in <code>System.IO.Compression</code> in .NET 10, too. As noted in <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-9/">Performance Improvements in .NET 9</a>, <code>DeflateStream</code>/<code>GZipStream</code>/<code>ZLibStream</code> are managed wrappers around an underlying native <code>zlib</code> library. For a long time, that was the original <code>zlib</code> (<a href="https://github.com/madler/zlib">madler/zlib</a>). Then it was Intel&rsquo;s <code>zlib-intel</code> fork (<a href="https://github.com/intel/zlib">intel/zlib</a>), which is now archived and no longer maintained. In .NET 9, the library switched to using <code>zlib-ng</code> (<a href="https://github.com/zlib-ng/zlib-ng">zlib-ng/zlib-ng</a>), which is a modernized fork that&rsquo;s well-maintained and optimized for a large number of hardware architectures. .NET 9 is based on <code>zlib-ng</code> 2.2.1. <a href="https://github.com/dotnet/runtime/pull/118457">dotnet/runtime#118457</a> updates it to use <code>zlib-ng</code> 2.2.5. Compared with the 2.2.1 release, there are a variety of performance improvements in <code>zlib-ng</code> itself, which .NET 10 then inherits, such as improved used of AVX2 and AVX512. Most importantly, though, the update includes a <a href="https://github.com/zlib-ng/zlib-ng/pull/1938">revert</a> that undoes a cleanup change in the 2.2.0 release; the original change removed a workaround for a function that had been slow and was found to no longer be slow, but as it turns out, it&rsquo;s still slow in some circumstances (long, <em>highly</em> compressible data), resulting in a throughput regression. The fix in 2.2.5 puts back the workaround to fix the regression.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.IO.Compression; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private byte[] _data = new HttpClient().GetByteArrayAsync(@"https://raw.githubusercontent.com/dotnet/runtime-assets/8d362e624cde837ec896e7fff04f2167af68cba0/src/System.IO.Compression.TestData/DeflateTestData/xargs.1").Result; [Benchmark] public void Compress() { using ZLibStream z = new(Stream.Null, CompressionMode.Compress); for (int i = 0; i &lt; 100; i++) { z.Write(_data); } } }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Compress</td> <td>.NET 9.0</td> <td>202.79 us</td> <td>1.00</td> </tr> <tr> <td>Compress</td> <td>.NET 10.0</td> <td>70.45 us</td> <td>0.35</td> </tr> </tbody> </table><p>The managed wrapper for <code>zlib</code> also gains some improvements. <a href="https://github.com/dotnet/runtime/pull/113587">dotnet/runtime#113587</a> from <a href="https://github.com/edwardneal">@edwardneal</a> improves the case where multiple gzip payloads are being read from the underlying <code>Stream</code>. Due to its nature, multiple complete gzip payloads can be written one after the other, and a single <code>GZipStream</code> can be used to decompress all of them as if they were one. Each time it hit a boundary between payloads, the managed wrapper was throwing away the old interop handles and creating new ones, but it can instead take advantage of reset capabilities in the underlying <code>zlib</code> library, shaving off some cycles associated with freeing and re-allocating the underlying data structures. This is a very biased micro-benchmark (a stream containing a 1000 gzip payloads that each decompresses into a single byte), highlighting the worst case, but it exemplifies the issue:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.IO.Compression; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private MemoryStream _data; [GlobalSetup] public void Setup() { _data = new MemoryStream(); for (int i = 0; i &lt; 1000; i++) { using GZipStream gzip = new(_data, CompressionMode.Compress, leaveOpen: true); gzip.WriteByte(42); } } [Benchmark] public void Decompress() { _data.Position = 0; using GZipStream gzip = new(_data, CompressionMode.Decompress, leaveOpen: true); gzip.CopyTo(Stream.Null); } }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Decompress</td> <td>.NET 9.0</td> <td>331.3 us</td> <td>1.00</td> </tr> <tr> <td>Decompress</td> <td>.NET 10.0</td> <td>104.3 us</td> <td>0.31</td> </tr> </tbody> </table><p>Other components that sit above these streams, like <code>ZipArchive</code>, have also improved. <a href="https://github.com/dotnet/runtime/pull/103153">dotnet/runtime#103153</a> from <a href="https://github.com/edwardneal">@edwardneal</a> updates <code>ZipArchive</code> to not rely on <code>BinaryReader</code> and <code>BinaryWriter</code>, avoiding their underlying buffer allocations and having more fine-grained control over how and when exactly data is encoded/decoded and written/read. And <a href="https://github.com/dotnet/runtime/pull/102704">dotnet/runtime#102704</a> from <a href="https://github.com/edwardneal">@edwardneal</a> reduces memory consumption and allocation when updating <code>ZipArchive</code>s. A <code>ZipArchive</code> update used to be &ldquo;rewrite the world&rdquo;: it loaded every entry&rsquo;s data into memory and rewrote all the file headers, all entry data, and the &ldquo;central directory&rdquo; (what the zip format calls its catalog of all the entries in the archive). A large archive would have proportionally large allocation. This PR introduces change tracking plus ordering of entries so that only the portion of the file from the first actually affected entry (or one whose variable&#8209;length metadata/data changed) is rewritten, rather than always rewriting the whole thing. The effects can be significant.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.IO.Compression; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private Stream _zip = new MemoryStream(); [GlobalSetup] public void Setup() { using ZipArchive zip = new(_zip, ZipArchiveMode.Create, leaveOpen: true); Random r = new(42); for (int i = 0; i &lt; 1000; i++) { byte[] fileBytes = new byte[r.Next(512, 2048)]; r.NextBytes(fileBytes); using Stream s = zip.CreateEntry($"file{i}.txt").Open(); s.Write(fileBytes); } } [Benchmark] public void Update() { _zip.Position = 0; using ZipArchive zip = new(_zip, ZipArchiveMode.Update, leaveOpen: true); zip.GetEntry("file987.txt")?.Delete(); } }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>Update</td> <td>.NET 9.0</td> <td>987.8 us</td> <td>1.00</td> <td>2173.9 KB</td> <td>1.00</td> </tr> <tr> <td>Update</td> <td>.NET 10.0</td> <td>354.7 us</td> <td>0.36</td> <td>682.22 KB</td> <td>0.31</td> </tr> </tbody> </table><p>(<code>ZipArchive</code> and <code>ZipFile</code> also gain async APIs in <a href="https://github.com/dotnet/runtime/pull/114421">dotnet/runtime#114421</a>, a long requested feature that allows using async I/O while loading, manipulating, and saving zips.)</p><p>Finally, somewhere between performance and reliability, <a href="https://github.com/dotnet/roslyn-analyzers/pull/7390">dotnet/roslyn-analyzers#7390</a> from <a href="https://github.com/mpidash">@mpidash</a> adds a new analyzer for <code>StreamReader.EndOfStream</code>. <code>StreamReader.EndOfStream</code> seems like it should be harmless, but it&rsquo;s quite the devious little property. The intent is to determine whether the reader is at the end up of the underlying <code>Stream</code>. Seems easy enough. If the <code>StreamReader</code> still has previously read data buffered, obviously it&rsquo;s not at the end. And if the reader has previously seen EOF, e.g. <code>Read</code> returned <code>0</code>, then it obviously is at the end. But in all other situations, there&rsquo;s no way to know you&rsquo;re at the end of the stream (at least in the general case) without performing a read, which means this property does something properties should never do: perform I/O. Worse than just performing I/O, that read can be a blocking operation, e.g. if the <code>Stream</code> represents a network stream for a <code>Socket</code>, and performing a read actually means blocking until data is received. Even worse, though, is when it&rsquo;s used in an asynchronous method, e.g.</p><pre><code>while (!reader.EndOfStream) { string? line = await reader.ReadLineAsync(); ... }</code></pre><p>Now not only might <code>EndOfStream</code> do I/O and block, it&rsquo;s doing that in a method that&rsquo;s supposed to do all of its waiting asynchronously.</p><p>What makes this even more frustrating is that <code>EndOfStream</code> isn&rsquo;t even useful in a loop like that above. <code>ReadLineAsync</code> will return a <code>null</code> string if it&rsquo;s at the end of the stream, so the loop would instead be better as:</p><pre><code>while (await reader.ReadLineAsync() is string line) { ... }</code></pre><p>Simpler, cheaper, and no ticking time bombs of synchronous I/O. Thanks to this new analyzer, any such use of <code>EndOfStream</code> in an async method will trigger <code>CA2024</code>:</p><h2>Networking</h2><p>Networking-related operations show up in almost every modern workload. Past releases of .NET have seen a lot of energy exerted on whittling away at networking overheads, as these components are used over and over and over, often in critical paths, and the overheads can add up. .NET 10 continues the streamlining trend.</p><p>As was seen with core primitives earlier, <code>IPAddress</code> and <code>IPNetwork</code> are both imbued with UTF8 parsing capabilities, thanks to <a href="https://github.com/dotnet/runtime/pull/102144">dotnet/runtime#102144</a> from <a href="https://github.com/edwardneal">@edwardneal</a>. As is the case with most other such types in the core libraries, the UTF8-based implementation and the UTF16-based implementation are mostly the same implementation, sharing most of their code via generic methods parameterized on <code>byte</code> vs <code>char</code>. And as a result of the focus on enabling UTF8, not only can you parse UTF8 bytes directly rather than needing to transcode first, the existing code actually gets a bit faster.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Net; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD", "s")] public partial class Tests { [Benchmark] [Arguments("Fe08::1%13542")] public IPAddress Parse(string s) =&gt; IPAddress.Parse(s); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Parse</td> <td>.NET 9.0</td> <td>71.35 ns</td> <td>1.00</td> </tr> <tr> <td>Parse</td> <td>.NET 10.0</td> <td>54.60 ns</td> <td>0.77</td> </tr> </tbody> </table><p><code>IPAddress</code> is also imbued with <code>IsValid</code> and <code>IsValidUtf8</code> methods, thanks to <a href="https://github.com/dotnet/runtime/pull/111433">dotnet/runtime#111433</a>. It was previously possible to test the validity of an address via <code>TryParse</code>, but when successful, that would allocate the <code>IPAddress</code>; if you don&rsquo;t need the resulting object but just need to know whether it&rsquo;s valid, the extra allocation is wasteful.</p><pre><code>// dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Net; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private string _address = "123.123.123.123"; [Benchmark(Baseline = true)] public bool TryParse() =&gt; IPAddress.TryParse(_address, out _); [Benchmark] public bool IsValid() =&gt; IPAddress.IsValid(_address); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>TryParse</td> <td>26.26 ns</td> <td>1.00</td> <td>40 B</td> <td>1.00</td> </tr> <tr> <td>IsValid</td> <td>21.88 ns</td> <td>0.83</td> <td>&ndash;</td> <td>0.00</td> </tr> </tbody> </table><p><code>Uri</code>, used in the above benchmark, also gets some notable improvements. In fact, one of my favorite improvements in all of .NET 10 is in <code>Uri</code>. The feature itself isn&rsquo;t a performance improvement, but there are some interesting performance-related ramifications for it. In particular, since forever, <code>Uri</code> has had a length limitation due to implementation details. <code>Uri</code> keeps track of various offsets in the input, such as where the host portion starts, where the path starts, where the query starts, and so on. The implementer chose to use <code>ushort</code> for each of these values rather than <code>int</code>. That means the maximum length of a <code>Uri</code> is then constrained to the lengths a <code>ushort</code> can describe, namely 65,535 characters. That sounds like a ridiculously long <code>Uri</code>, one no one would ever need to go beyond&hellip; until you consider data URIs. Data URIs embed a representation of arbitrary bytes, typically Base64 encoded, in the URI itself. This allows for files to be represented directly in links, and it&rsquo;s become a common way for AI-related services to send and receive data payloads, like images. It doesn&rsquo;t take a very large image to exceed 65K characters, however, especially with Base64 encoding increasing the payload size by ~33%. <a href="https://github.com/dotnet/runtime/pull/117287">dotnet/runtime#117287</a> finally removes that limitation, so now <code>Uri</code> can be used to represent very large data URIs, if desired. This, however, has some performance ramifications (beyond the few percentage increase in the size of <code>Uri</code>, to accomodate the extra <code>ushort</code> to <code>int</code> bytes). In particular, <code>Uri</code> implements path compression, so for example this:</p><pre><code>Console.WriteLine(new Uri("http://test/hello/../hello/../hello"));</code></pre><p>prints out:</p><pre><code>http://test/hello</code></pre><p>As it turns out, the algorithm implementing that path compression is <code>O(N^2)</code>. Oops. With a limit of 65K characters, such a quadratic complexity isn&rsquo;t a security concern (as <code>O(N^2)</code> operations can sometimes be, as if <code>N</code> is unbounded, it creates an attack vector where an attacker can do <code>N</code> work and get the attackee to do disproportionately more). But once the limit is removed entirely, it could be. As such, <a href="https://github.com/dotnet/runtime/pull/117820">dotnet/runtime#117820</a> compensates by making the path compression <code>O(N)</code>. And while in the general case, we don&rsquo;t expect path compression to be a meaningfully impactful part of constructing <code>Uri</code>, in degenerate cases, even under the old limit, the change can still make a measurable improvement.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private string _input = $"http://host/{string.Concat(Enumerable.Repeat("a/../", 10_000))}{new string('a', 10_000)}"; [Benchmark] public Uri Ctor() =&gt; new Uri(_input); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Ctor</td> <td>.NET 9.0</td> <td>18.989 us</td> <td>1.00</td> </tr> <tr> <td>Ctor</td> <td>.NET 10.0</td> <td>2.228 us</td> <td>0.12</td> </tr> </tbody> </table><p>In the same vein, the longer the URI, the more effort is required to do whatever validation is needed in the constructor. <code>Uri</code>&lsquo;s constructor needs to check whether the input has any Unicode characters that might need to be handled. Rather than checking all the characters one at a time, with <a href="https://github.com/dotnet/runtime/pull/107357">dotnet/runtime#107357</a>, <code>Uri</code> can now use <code>SearchValues</code> to more quickly rule out or find the first location of a character that needs to be looked at more deeply.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private string _uri; [GlobalSetup] public void Setup() { byte[] bytes = new byte[40_000]; new Random(42).NextBytes(bytes); _uri = $"data:application/octet-stream;base64,{Convert.ToBase64String(bytes)}"; } [Benchmark] public Uri Ctor() =&gt; new Uri(_uri); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Ctor</td> <td>.NET 9.0</td> <td>19.354 us</td> <td>1.00</td> </tr> <tr> <td>Ctor</td> <td>.NET 10.0</td> <td>2.041 us</td> <td>0.11</td> </tr> </tbody> </table><p>Other changes were made to <code>Uri</code> that further reduce construction costs in various other cases, too. For cases where the URI host is an IPv6 address, e.g. <code>http://[2603:1020:201:10::10f]</code>, <a href="https://github.com/dotnet/runtime/pull/117292">dotnet/runtime#117292</a> recognizes that scope IDs are relatively rare and makes the cases without a scope ID cheaper in exchange for making the cases with a scope ID a little more expensive.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { [Benchmark] public string CtorHost() =&gt; new Uri("http://[2603:1020:201:10::10f]").Host; }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>CtorHost</td> <td>.NET 9.0</td> <td>304.9 ns</td> <td>1.00</td> <td>208 B</td> <td>1.00</td> </tr> <tr> <td>CtorHost</td> <td>.NET 10.0</td> <td>254.2 ns</td> <td>0.83</td> <td>216 B</td> <td>1.04</td> </tr> </tbody> </table><p>(Note that the .NET 10 allocation is 8 bytes larger than the .NET 9 allocation due to the extra space required in this case for dropping the length limitation, as discussed earlier.)</p><p><a href="https://github.com/dotnet/runtime/pull/117289">dotnet/runtime#117289</a> also improves construction for cases where the URI requires normalization, saving some allocations by using normalization routines over spans (which were added in <a href="https://github.com/dotnet/runtime/pull/110465">dotnet/runtime#110465</a>) instead of needing to allocate <code>string</code>s for the inputs.</p><pre><code>using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { [Benchmark] public Uri Ctor() =&gt; new("http://some.host.with.&uuml;mlauts/"); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>Ctor</td> <td>.NET 9.0</td> <td>377.6 ns</td> <td>1.00</td> <td>440 B</td> <td>1.00</td> </tr> <tr> <td>Ctor</td> <td>.NET 10.0</td> <td>322.0 ns</td> <td>0.85</td> <td>376 B</td> <td>0.85</td> </tr> </tbody> </table><p>Various improvements have also found their way into the HTTP stack. For starters, the download helpers on <code>HttpClient</code> and <code>HttpContent</code> have improved. These types expose helper methods for some of the most common forms of grabbing data; while a developer can grab the response <code>Stream</code> and consume that efficiently, for simple and common cases like &ldquo;just get the whole response as a <code>string</code>&rdquo; or &ldquo;just get the whole response as a <code>byte[]</code>&ldquo;, the <code>GetStringAsync</code> and <code>GetByteArrayAsync</code> make that really easy to do. <a href="https://github.com/dotnet/runtime/pull/109642">dotnet/runtime#109642</a> changes how these methods operate in order to better manage the temporary buffers that are required, especially in the case where the server hasn&rsquo;t advertised a <code>Content-Length</code>, such that the client doesn&rsquo;t know ahead of time how much data to expect and thus how much space to allocate.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Net; using System.Net.Sockets; using System.Text; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private HttpClient _client = new(); private Uri _uri; [GlobalSetup] public void Setup() { Socket listener = new(AddressFamily.InterNetwork, SocketType.Stream, ProtocolType.Tcp); listener.Bind(new IPEndPoint(IPAddress.Loopback, 0)); listener.Listen(int.MaxValue); _ = Task.Run(async () =&gt; { byte[] header = "HTTP/1.1 200 OK\r\nTransfer-Encoding: chunked\r\n\r\n"u8.ToArray(); byte[] chunkData = Enumerable.Range(0, 100).SelectMany(_ =&gt; "abcdefghijklmnopqrstuvwxyz").Select(c =&gt; (byte)c).ToArray(); byte[] chunkHeader = Encoding.UTF8.GetBytes($"{chunkData.Length:X}\r\n"); byte[] chunkFooter = "\r\n"u8.ToArray(); byte[] footer = "0\r\n\r\n"u8.ToArray(); while (true) { var server = await listener.AcceptAsync(); server.NoDelay = true; using StreamReader reader = new(new NetworkStream(server), Encoding.ASCII); while (true) { while (!string.IsNullOrEmpty(await reader.ReadLineAsync())) ; await server.SendAsync(header); for (int i = 0; i &lt; 100; i++) { await server.SendAsync(chunkHeader); await server.SendAsync(chunkData); await server.SendAsync(chunkFooter); } await server.SendAsync(footer); } } }); var ep = (IPEndPoint)listener.LocalEndPoint!; _uri = new Uri($"http://{ep.Address}:{ep.Port}/"); } [Benchmark] public async Task<byte> ResponseContentRead_ReadAsByteArrayAsync() { using HttpResponseMessage resp = await _client.GetAsync(_uri); return await resp.Content.ReadAsByteArrayAsync(); } [Benchmark] public async Task<string> ResponseHeadersRead_ReadAsStringAsync() { using HttpResponseMessage resp = await _client.GetAsync(_uri, HttpCompletionOption.ResponseHeadersRead); return await resp.Content.ReadAsStringAsync(); } }</string></byte></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>ResponseContentRead_ReadAsByteArrayAsync</td> <td>.NET 9.0</td> <td>1.438 ms</td> <td>1.00</td> <td>912.71 KB</td> <td>1.00</td> </tr> <tr> <td>ResponseContentRead_ReadAsByteArrayAsync</td> <td>.NET 10.0</td> <td>1.166 ms</td> <td>0.81</td> <td>519.12 KB</td> <td>0.57</td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>ResponseHeadersRead_ReadAsStringAsync</td> <td>.NET 9.0</td> <td>1.528 ms</td> <td>1.00</td> <td>1166.77 KB</td> <td>1.00</td> </tr> <tr> <td>ResponseHeadersRead_ReadAsStringAsync</td> <td>.NET 10.0</td> <td>1.306 ms</td> <td>0.86</td> <td>773.3 KB</td> <td>0.66</td> </tr> </tbody> </table><p><a href="https://github.com/dotnet/runtime/pull/117071">dotnet/runtime#117071</a> reduces overheads associated with HTTP header validation. In the <code>System.Net.Http</code> implementation, some headers have dedicated parsers for them, while many (the majority of custom ones that services define) don&rsquo;t. This PR recognizes that for these, the validation that needs to be performed amounts to only checking for forbidden newline characters, and the objects that were being created for all headers weren&rsquo;t necessary for these.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Net.Http.Headers; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private readonly HttpResponseHeaders _headers = new HttpResponseMessage().Headers; [Benchmark] public void Add() { _headers.Clear(); _headers.Add("X-Custom", "Value"); } [Benchmark] public object GetValues() { _headers.Clear(); _headers.TryAddWithoutValidation("X-Custom", "Value"); return _headers.GetValues("X-Custom"); } }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>Add</td> <td>.NET 9.0</td> <td>28.04 ns</td> <td>1.00</td> <td>32 B</td> <td>1.00</td> </tr> <tr> <td>Add</td> <td>.NET 10.0</td> <td>12.61 ns</td> <td>0.45</td> <td>&ndash;</td> <td>0.00</td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>GetValues</td> <td>.NET 9.0</td> <td>82.57 ns</td> <td>1.00</td> <td>64 B</td> <td>1.00</td> </tr> <tr> <td>GetValues</td> <td>.NET 10.0</td> <td>23.97 ns</td> <td>0.29</td> <td>32 B</td> <td>0.50</td> </tr> </tbody> </table><p>For folks using HTTP/2, <a href="https://github.com/dotnet/runtime/pull/112719">dotnet/runtime#112719</a> decreases per-connection memory consumption, by changing the <code>HPackDecoder</code> to lazily grow its buffers, starting from expected-case sizing rather than worst-case. (&ldquo;HPACK&rdquo; is the header compression algorithm used by HTTP/2, utilizing a table shared between client and server for managing commonly transmitted headers.) It&rsquo;s a little hard to measure in a micro-benchmark, since in a real app the connections get reused (and the benefits here aren&rsquo;t about temporary allocation but rather connection density and overall working set), but we can get a glimpse of it by doing what you&rsquo;re not supposed to do and create a new <code>HttpClient</code> for each request (you&rsquo;re not supposed to do that, or more specifically not supposed to create a new handler for each request, because doing so tears down the connection pool and the connections it contains&hellip; which is bad for an app but exactly what we want for our micro-benchmark).</p><pre><code>// For this benchmark, change the benchmark.csproj to start with: //<project> // instead of: //<project> // dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using System.Net; using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using Microsoft.AspNetCore.Server.Kestrel.Core; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private WebApplication _app; [GlobalSetup] public async Task Setup() { var builder = WebApplication.CreateBuilder(); builder.Logging.SetMinimumLevel(LogLevel.Warning); builder.WebHost.ConfigureKestrel(o =&gt; o.ListenLocalhost(5000, listen =&gt; listen.Protocols = HttpProtocols.Http2)); _app = builder.Build(); _app.MapGet("/hello", () =&gt; Results.Text("hi from kestrel over h2c\n")); var serverTask = _app.RunAsync(); await Task.Delay(300); } [GlobalCleanup] public async Task Cleanup() { await _app.StopAsync(); await _app.DisposeAsync(); } [Benchmark] public async Task Get() { using var client = new HttpClient() { DefaultRequestVersion = HttpVersion.Version20, DefaultVersionPolicy = HttpVersionPolicy.RequestVersionExact }; var response = await client.GetAsync("http://localhost:5000/hello"); } }</project></project></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>Get</td> <td>.NET 9.0</td> <td>485.9 us</td> <td>1.00</td> <td>83.19 KB</td> <td>1.00</td> </tr> <tr> <td>Get</td> <td>.NET 10.0</td> <td>445.0 us</td> <td>0.92</td> <td>51.79 KB</td> <td>0.62</td> </tr> </tbody> </table><p>Also, on Linux and macOS, all HTTP use (and, more generally, all socket interactions) gets a tad cheaper from <a href="https://github.com/dotnet/runtime/pull/109052">dotnet/runtime#109052</a>, which eliminates a <code>ConcurrentDictionary&lt;&gt;</code> lookup for each asynchronous operation that completes on a <code>Socket</code>.</p><p>And for all you Native AOT fans, <a href="https://github.com/dotnet/runtime/pull/117012">dotnet/runtime#117012</a> also adds a feature switch that enables trimming out the HTTP/3 implementation from <code>HttpClient</code>, which can represent a very sizeable and &ldquo;free&rdquo; space savings if you&rsquo;re not using HTTP/3 at all.</p><h2>Searching</h2><p>Someone once told me that computer science was &ldquo;all about sorting and searching.&rdquo; That&rsquo;s not far off. Searching in one way, shape, or form is an integral part of many applications and services.</p><h3>Regex</h3><p>Whether you love or hate the terse syntax, regular expressions (regex) continue to be an integral part of software development, with applications as part of both software and the software development process. As such, it&rsquo;s had robust support in .NET since the early days of the platform, with the <code>System.Text.RegularExpressions</code> namespace providing a feature-rich set of regex capabilities. The performance of <code>Regex</code> was improved significantly in .NET 5 (<a href="https://devblogs.microsoft.com/dotnet/regex-performance-improvements-in-net-5/">Regex Performance Improvements in .NET 5</a>) and then again in .NET 7, which also saw a significant amount of new functionality added (<a href="https://devblogs.microsoft.com/dotnet/regular-expression-improvements-in-dotnet-7/">Regular Expression Improvements in .NET 7</a>). It&rsquo;s continued to be improved in every release since, and .NET 10 is no exception.</p><p>As I&rsquo;ve discussed in previous blog posts about regex and performance, there are two high-level ways regex engines are implemented, either with backtracking or without. Non-backtracking engines typically work by creating some form of finite automata that represents the pattern, and then for each character consumed from the input, moves around the deterministic finite automata (DFA, meaning you can be in only a single state at a time) or non-deterministic finite automata (NFA, meaning you can be in multiple states at a time), transitioning from one state to another. A key benefit of a non-backtracking engine is that it can often make linear guarantees about processing time, where an input string of length <code>N</code> can be processed in worst-case <code>O(N)</code> time. A key downside of a non-backtracking engine is it can&rsquo;t support all of the features developers are familiar with in modern regex engines, like back references. Backtracking engines are named as such because they&rsquo;re able to &ldquo;backtrack,&rdquo; trying one approach to see if there&rsquo;s a match and then going back and trying another. If you have the regex pattern <code>\w*\d</code> (which matches any number of word characters followed by a single digit) and supply it with the string <code>"12"</code>, a backtracking engine is likely to first try treating both the <code>'1'</code> and the <code>'2'</code> as word characters, then find that it doesn&rsquo;t have anything to fulfill the <code>\d</code>, and thus backtrack, instead treating only the <code>'1'</code> as being consumed by the <code>\w*</code>, and leaving the <code>'2'</code> to be consumed by the <code>\d</code>. Backtracking is how engines support features like back references, variable-length lookarounds, conditional expressions, and more. They can also have excellent performance, especially on the average and best cases. A key downside, however, is their worst case, where on some patterns they can suffer from &ldquo;catastrophic backtracking.&rdquo; That happens when all of that backtracking leads to exploring the same input over and over and over again, possibly consuming much more than linear time.</p><p>Since .NET 7, .NET has had an opt-in non-backtracking engine, which is what you get with <code>RegexOptions.NonBacktracking</code>, Otherwise, it uses a backtracking engine, whether using the default interpreter, or a regex compiled to IL (<code>RegexOptions.Compiled</code>), or a regex emitted as a custom C# implementation with the regex source generator (<code>[GeneratedRegex(...)]</code>). These backtracking engines can yield exceptional performance, but due to their backtracking nature, they are susceptible to bad worst-case performance, which is why specifying timeouts to a <code>Regex</code> is often encouraged, especially when using patterns of unknown provenance. Still, there are things backtracking engines can do to help mitigate some such backtracking, in particular avoiding the need for some of the backtracking in the first place.</p><p>One of the main tools backtracking engines offer for reduced backtracking is an &ldquo;atomic&rdquo; construct. Some regex syntaxes surface this via &ldquo;possessive quantifiers,&rdquo; while others, including .NET, surface it via &ldquo;atomic groups.&rdquo; They&rsquo;re fundamentally the same thing, just expressed in the syntax differently. An atomic group in .NET&rsquo;s regex syntax is a group that is never backtracked into. If we take our previous <code>\w*\d</code> example, we could wrap the <code>\w*</code> loop in an atomic group like this: <code>(?&gt;\w*)\d</code>. In doing so, whatever that <code>\w*</code> consumes won&rsquo;t change via backtracking after exiting the group and moving on to whatever comes after it in the pattern. So if I try to match <code>"12"</code> with such a pattern, it&rsquo;ll fail, because the <code>\w*</code> will consume both characters, the <code>\d</code> will have nothing to match, and no backtracking will be applied, because the <code>\w*</code> is wrapped in an atomic group and thus exposes no backtracking opportunities.</p><p>In that example, wrapping the <code>\w*</code> with an atomic group changes the meaning of the pattern, and thus it&rsquo;s not something that a regex engine could choose to do automatically. However, there are many cases where wrapping otherwise backtracking constructs in an atomic group does not observably change behavior, because any backtracking that would otherwise happen would provably never be fruitful. Consider a pattern <code>a*b</code>. <code>a*b</code> is observably identical to <code>(?&gt;a*)b</code>, which says that the <code>a*</code> should not be backtracked into. That&rsquo;s because there&rsquo;s nothing the <code>a*</code> can &ldquo;give back&rdquo; (which can only be <code>a</code>s) that would satisfy what comes next in the pattern (which is only <code>b</code>). It&rsquo;s thus valid for a backtracking engine to transform how it processes <code>a*b</code> to instead be the equivalent of how it processes <code>(?&gt;a*)b</code>. And the .NET regex engine has been capable of such transformations since .NET 5. This can result in massive improvements to throughput. With backtracking, waving my hands, we effectively need to execute everything after the backtracking construct for each possible position we could backtrack to. So, for example, with <code>\w*SOMEPATTERN</code>, if the <code>w*</code> successfully initially consumes 100 characters, we then possibly need to try to match <code>SOMEPATTERN</code> up to 100 different times, as we may need to backtrack up to 100 times and re-evaluate <code>SOMEPATTERN</code> each time we give back one of the things initially matched. If we instead make that <code>(?&gt;\w*)</code>, we eliminate all but one of those! That makes improvements to this ability to automatically transform backtracking constructs to be non-backtracking possibly massive improvements in performance, and practically every release of .NET since .NET 5 has increased the set of patterns that are automatically transformed. .NET 10 included.</p><p>Let&rsquo;s start with <a href="https://github.com/dotnet/runtime/pull/117869">dotnet/runtime#117869</a>, which teaches the regex optimizer about more &ldquo;disjoint&rdquo; sets. Consider the previous example of <code>a*b</code>, and how I said we can make that <code>a*</code> loop atomic because there&rsquo;s nothing <code>a*</code> can &ldquo;give back&rdquo; that matches <code>b</code>. That is a general statement about auto-atomicity: a loop can be made atomic if it&rsquo;s guaranteed to end with something that can&rsquo;t possibly match the thing that comes after it. So, if I have <code>[abc]+[def]</code>, that loop can be made atomic, because there&rsquo;s nothing <code>[abc]</code> can match that <code>[def]</code> can also match. In contrast, if the expression were instead <code>[abc]+[cef]</code>, that loop must not be made atomic automatically, as doing so could change behavior. The sets <em>do</em> overlap, as both can match <code>'c'</code>. So, for example, if the input were just <code>"cc"</code>, the original expression should match it (the <code>[abc]*</code> loop would match <code>'c'</code> with one iteration of the loop and then the second <code>'c'</code> would satisfy the <code>[cef]</code> set), but if the expression were instead <code>(?&gt;[abc]+)[cef]</code>, it would no longer match, as the <code>[abc]+</code> would consume both <code>'c'</code>s, and there&rsquo;d be nothing left for the <code>[cef]</code> set to match. Two sets that don&rsquo;t have any overlap are referred to as being &ldquo;disjoint,&rdquo; and so the optimizer needs to be able to prove the disjointedness of sets in order to perform these kinds of auto-atomicity optimizations. The optimizer was already able to do so for many sets, in particular ones that were composed purely of characters or character ranges, e.g. <code>[ace]</code> or <code>[a-zA-Z0-9]</code>. But many sets are instead composed of entire Unicode categories. For example, when you write <code>\d</code>, unless you&rsquo;ve specified <code>RegexOptions.ECMAScript</code> that&rsquo;s the same as <code>\p{Nd}</code>, which says &ldquo;match any character in the Unicode category of Number decimal digits&rdquo;, aka all characters for which <code>char.GetUnicodeCategory</code> returns <code>UnicodeCategory.DecimalDigitNumber</code>. And the optimizer was unable to reason about overlap between such sets. So, for example, if you had the expression <code>\w*\p{Sm}</code>, that matches anything that&rsquo;s any number of word characters followed by a math symbol (<code>UnicodeCategory.MathSymbol</code>). <code>\w</code> is actually just a set of eight specific Unicode categories, such that the previous expression behaves identically to if I&rsquo;d written <code>[\p{Ll}\p{Lu}\p{Lt}\p{Lm}\p{Lo}\p{Mn}\p{Nd}\p{Pc}]*\p{Sm}</code> (<code>\w</code> is composed of <code>UnicodeCategory.UppercaseLetter</code>, <code>UnicodeCategory.LowercaseLetter</code>, <code>UnicodeCategory.TitlecaseLetter</code>, <code>UnicodeCategory.ModiferLetter</code>, <code>UnicodeCategory.OtherLetter</code>, <code>UnicodeCategory.NonSpacingMark</code>, <code>UnicodeCategory.ModiferLetter</code>, <code>UnicodeCategory.DecimalDigitNumber</code>, and <code>UnicodeCategory.ConnectorPunctuation</code>). Note that none of those eight categories is the same as <code>\p{Sm}</code>, which means they&rsquo;re disjoint, which means we can safely change that loop to being atomic without impacting behavior; it just makes it faster. One of the easiest ways to see the effect of this is to look at the output from the regex source generator. Before the change, if I look at the XML comment generated for that expression, I get this:</p><pre><code>/// &#9675; Match a word character greedily any number of times. /// &#9675; Match a character in the set [\p{Sm}].</code></pre><p>and after, I get this:</p><pre><code>/// &#9675; Match a word character atomically any number of times. /// &#9675; Match a character in the set [\p{Sm}].</code></pre><p>That one word change in the first sentence makes a huge difference. Here&rsquo;s the relevant portion of the C# code emitted by the source generator for the matching routine before the change:</p><pre><code>// Match a word character greedily any number of times. //{ charloop_starting_pos = pos; int iteration = 0; while ((uint)iteration &lt; (uint)slice.Length &amp;&amp; Utilities.IsWordChar(slice[iteration])) { iteration++; } slice = slice.Slice(iteration); pos += iteration; charloop_ending_pos = pos; goto CharLoopEnd; CharLoopBacktrack: if (Utilities.s_hasTimeout) { base.CheckTimeout(); } if (charloop_starting_pos &gt;= charloop_ending_pos) { return false; // The input didn't match. } pos = --charloop_ending_pos; slice = inputSpan.Slice(pos); CharLoopEnd: //}</code></pre><p>You can see how backtracking influences the emitted code. The core loop in there is iterating through as many word characters as it can match, but then before moving on, it remembers some position information about where it was. It also sets up a label for where subsequent code should jump to if it needs to backtrack; that code undoes one of the matched characters and then retries everything that came after it. If the code needs to backtrack again, it&rsquo;ll again undo one of the characters and retry. And so on. Now, here&rsquo;s what the code looks like after the change:</p><pre><code>// Match a word character atomically any number of times. { int iteration = 0; while ((uint)iteration &lt; (uint)slice.Length &amp;&amp; Utilities.IsWordChar(slice[iteration])) { iteration++; } slice = slice.Slice(iteration); pos += iteration; }</code></pre><p>All of that backtracking gunk is gone; the loop matches as much as it can, and that&rsquo;s that. You can see the effect this has one some cases with a micro-benchmark like this:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Text.RegularExpressions; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private static readonly string s_input = new string(' ', 100); private static readonly Regex s_regex = new Regex(@"\s+\S+", RegexOptions.Compiled); [Benchmark] public int Count() =&gt; s_regex.Count(s_input); }</code></pre><p>This is a simple test where we&rsquo;re trying to match any positive number of whitespace characters followed by any positive number of non-whitespace characters, giving it an input composed entirely of whitespace. Without atomicity, the engine is going to consume all of the whitespace as part of the <code>\s+</code> but will then find that there isn&rsquo;t any non-whitespace available to match the <code>\S+</code>. What does it do then? It backtracks, gives back one of the hundred spaces consumed by <code>\s+</code>, and tries again to match the <code>\S+</code>. It won&rsquo;t match, so it backtracks again. And again. And again. A hundred times, until it has nothing left to try and gives up. With atomicity, all that backtracking goes away, allowing it to fail faster.</p><table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Count</td> <td>.NET 9.0</td> <td>183.31 ns</td> <td>1.00</td> </tr> <tr> <td>Count</td> <td>.NET 10.0</td> <td>69.23 ns</td> <td>0.38</td> </tr> </tbody> </table><p><a href="https://github.com/dotnet/runtime/pull/117892">dotnet/runtime#117892</a> is a related improvement. In regex, <code>\b</code> is called a &ldquo;word boundary&rdquo;; it checks whether the wordness of the previous character (whether the previous character matches <code>\w</code>) matches the wordness of the next character, calling it a boundary if they differ. You can see this in the engine&rsquo;s <code>IsBoundary</code> helper&rsquo;s implementation, which follows (note that according to <a href="http://www.unicode.org/reports/tr18/">TR18</a> whether a character is considered a boundary word char is <em>almost</em> exactly the same as <code>\w</code>, except with two additional zero-width Unicode characters also included):</p><pre><code>internal static bool IsBoundary(ReadOnlySpan<char> inputSpan, int index) { int indexM1 = index - 1; return ((uint)indexM1 &lt; (uint)inputSpan.Length &amp;&amp; RegexCharClass.IsBoundaryWordChar(inputSpan[indexM1])) != ((uint)index &lt; (uint)inputSpan.Length &amp;&amp; RegexCharClass.IsBoundaryWordChar(inputSpan[index])); }</char></code></pre><p>The optimizer already had a special-case in its auto-atomicity logic that had knowledge of boundaries and their relationship to <code>\w</code> and <code>\d</code>, specifically. So, if you had <code>\w+\b</code>, the optimizer would recognize that in order for the <code>\b</code> to match, what comes after what the <code>\w+</code> matches must necessarily not match <code>\w</code>, because then it wouldn&rsquo;t be a boundary, and thus the <code>\w+</code> could be made atomic. Similarly, with a pattern of <code>\d+\b</code>, it would recognize that what came after must not be in <code>\d</code>, and could make the loop atomic. It didn&rsquo;t generalize this, though. Now in .NET 10, it does. This PR teaches the optimizer how to recognize subsets of <code>\w</code>, because, as with the special-case of <code>\d</code>, any subset of <code>\w</code> can similarly benefit: if what comes before the <code>\b</code> is a word character, what comes after must not be. Thus, with this PR, an expression like <code>[a-zA-Z]+\b</code> will now have the loop made atomic.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Text.RegularExpressions; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private static readonly string s_input = "Supercalifragilisticexpialidocious1"; private static readonly Regex s_regex = new Regex(@"^[A-Za-z]+\b", RegexOptions.Compiled); [Benchmark] public int Count() =&gt; s_regex.Count(s_input); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Count</td> <td>.NET 9.0</td> <td>116.57 ns</td> <td>1.00</td> </tr> <tr> <td>Count</td> <td>.NET 10.0</td> <td>21.74 ns</td> <td>0.19</td> </tr> </tbody> </table><p>Just doing a better job of set disjointedness analysis is helpful, but more so is actually recognizing whole new classes of things that can be made atomic. In prior releases, the auto-atomicity optimizations only kicked in for loops over single characters, e.g. <code>a*</code>, <code>[abc]*?</code>, <code>[^abc]*</code>. That is obviously only a subset of loops, as many loops are composed of more than just a single character; loops can surround any regex construct. Even a capture group thrown into the mix would knock the auto-atomicity behavior off the rails. Now with <a href="https://github.com/dotnet/runtime/pull/117943">dotnet/runtime#117943</a>, a significant number of loops involving more complicated constructs can be made atomic. Loops larger than a single character are tricky, though, as there are more things that need to be taken into account when reasoning through atomicity. With a single character, we only need to prove disjointedness for that one character with what comes after it. But, consider an expression like <code>([a-z][0-9])+a1</code>. Can that loop be made atomic? What comes after the loop (<code>'a'</code>) is provably disjoint from what ends the loop (<code>[0-9]</code>), and yet making this loop atomic automatically would change behavior and be a no-no. Imagine if the input were <code>"b2a1"</code>. That matches; if this expression is processed normally, the loop would match a single iteration, consuming the <code>"b2"</code>, and then the <code>a1</code> after the loop would consume the corresponding <code>a1</code> in the input. But, if the loop were made atomic, e.g. <code>(?&gt;([a-z][0-9])+)a1</code>, the loop would end up performing two iterations and consuming both the <code>"b2"</code> and the <code>"a1"</code>, leaving nothing for the <code>a1</code> in the pattern. As it turns out, we not only need to ensure what ends the loop is disjoint from what comes after it, we also need to ensure that what starts the loop is disjoint from what comes after it. That&rsquo;s not all, though. Now consider an expression <code>^(a|ab)+$</code>. This matches an entire input composed of <code>"a"</code>s and <code>"ab"</code>s. Given an input string like <code>"aba"</code>, this will match successfully, as it will consume the <code>"ab"</code> with the second branch of the alternation, and then consume the remaining <code>a</code> with the first branch of the alternation on the next iteration of the loop. But now consider what happens if we make the loop atomic: <code>^(?&gt;(a|ab)+)$</code>. Now on that same input, the initial <code>a</code> in the input will be consumed by the first branch of the alternation, and that will satisfy the loop&rsquo;s minimum bound of 1 iteration, exiting the loop. It&rsquo;ll then proceed to validate that it&rsquo;s at the end of the string, and fail, but with the loop now atomic, there&rsquo;s nothing to backtrack into, and the whole match fails. Oops. The problem here is that the loop&rsquo;s ending must not only be disjoint with what comes next, and the loop&rsquo;s beginning must not only be disjoint with what comes next, but because it&rsquo;s a loop, what comes next can actually be itself, which means the loop&rsquo;s beginning and ending must be disjoint from each other. Those criteria significantly limit to what patterns this can be applied, but even with that, it&rsquo;s still surprisingly common: <code>dotnet/runtime-assets</code> (which contains test assets for use with <code>dotnet/runtime</code>) contains a <a href="https://github.com/dotnet/runtime-assets/blob/f9ac0b368d930728d6740686de29b5276958d15b/src/System.Text.RegularExpressions.TestData/Regex_RealWorldPatterns.json">database of regex patterns</a> sourced from appropriately-licensed nuget packages, yielding almost 20,000 unique patterns, and more than 7% of those were positively impacted by this.</p><p>Here is an example that&rsquo;s searching <a href="https://www.gutenberg.org/cache/epub/3200/pg3200.txt">&ldquo;The Entire Project Gutenberg Works of Mark Twain&rdquo;</a> for sequences of all lowercase ASCII words, each followed by a space, and then all followed by an uppercase ASCII letter.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Text.RegularExpressions; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private static readonly string s_input = new HttpClient().GetStringAsync(@"https://www.gutenberg.org/cache/epub/3200/pg3200.txt").Result; private static readonly Regex s_regex = new Regex(@"([a-z]+ )+[A-Z]", RegexOptions.Compiled); [Benchmark] public int Count() =&gt; s_regex.Count(s_input); }</code></pre><p>In previous releases, that inner loop would be made atomic, but the outerloop would remain greedy (backtracking). From the XML comment generated by the source generator, we get this:</p><pre><code>/// &#9675; Loop greedily at least once. /// &#9675; 1st capture group. /// &#9675; Match a character in the set [a-z] atomically at least once. /// &#9675; Match ' '. /// &#9675; Match a character in the set [A-Z].</code></pre><p>Now in .NET 10, we get this:</p><pre><code>/// &#9675; Loop atomically at least once. /// &#9675; 1st capture group. /// &#9675; Match a character in the set [a-z] atomically at least once. /// &#9675; Match ' '. /// &#9675; Match a character in the set [A-Z].</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Count</td> <td>.NET 9.0</td> <td>573.4 ms</td> <td>1.00</td> </tr> <tr> <td>Count</td> <td>.NET 10.0</td> <td>504.6 ms</td> <td>0.88</td> </tr> </tbody> </table><p>As with any optimization, auto-atomicity should never change observable behavior; it should just make things faster. And as such, every case where atomicity is automatically applied requires it being reasoned through to ensure that the optimization is of sound logic. In some cases, the optimization was written to be conservative, as the relevant reasoning through the logic wasn&rsquo;t previously done. An example of that is addressed by <a href="https://github.com/dotnet/runtime/pull/118191">dotnet/runtime#118191</a>, which makes a few tweaks to how boundaries are handled in the auto-atomicity logic, removing some constraints that were put in place but which, as it turns out, are unnecessary. The core logic that implements the atomicity analysis is a method that looks like this:</p><pre><code>private static bool CanBeMadeAtomic(RegexNode node, RegexNode subsequent, ...)</code></pre><p><code>node</code> is the representation for the part of the regex that&rsquo;s being considered for becoming atomic (e.g. a loop) and <code>subsequent</code> is what comes immediately after it in the pattern; the method then proceeds to validate <code>node</code> against <code>subsequent</code> to see whether it can prove there wouldn&rsquo;t be any behavioral changes if <code>node</code> were made atomic. However, not all cases are sufficiently handled just by validating against <code>subsequent</code> itself. Consider a pattern like <code>a*b*\w</code>, where <code>node</code> represents <code>a*</code> and <code>subsequent</code> represents <code>b*</code>. <code>a</code> and <code>b</code> are obviously disjoint, and so <code>node</code> can be made atomic with regards to <code>subsequent</code>, but&hellip; here <code>subsequent</code> is also &ldquo;nullable,&rdquo; meaning it might successfully match 0 characters (the loop has a lower bound of 0). And in such a case, what comes after the <code>a*</code> won&rsquo;t necessarily be a <code>b</code> but could be what comes after the <code>b*</code>, which here is a <code>\w</code>, which overlaps with <code>a</code>, and as such, it would be a behavioral change to make this into <code>(?&gt;a*)b*\w</code>. Consider an input of just <code>"a"</code>. With the original pattern, <code>a*</code> would successfully match the empty string with 0 iterations, <code>b*</code> would successfully match the empty string with 0 iterations, and then <code>\w</code> would successfully match the input <code>'a'</code>. But with the atomicized pattern, <code>(?&gt;a*)</code> would successfully match the input <code>'a'</code> with a single iteration, leaving nothing to match the <code>\w</code>. As such, when <code>CanBeMadeAtomic</code> detects that <code>subsequent</code> may be nullable and successfully match the empty string, it needs to iterate to also validate against what comes after <code>subsequent</code> (and possibly again and again if what comes next itself keeps being nullable).</p><p><code>CanBeMadeAtomic</code> already factored in boundaries (<code>\b</code> and <code>\B</code>), but it did so with the conservative logic that since a boundary is &ldquo;zero-width&rdquo; (meaning it doesn&rsquo;t consume any input), it must always require checking what comes after it. But that&rsquo;s not actually the case. Even though a boundary is zero-width, it still makes guarantees about what comes next: if the prior character is a word character, the next is guaranteed to not be with a successful match. And as such, we can safely make this more liberal and not require checking what comes next.</p><p>This last example also highlights an interesting aspect of this auto-atomicity optimization in general. There is nothing this optimization provides that the developer writing the regex in the first place couldn&rsquo;t have done themselves. Instead of <code>a*b</code>, a developer can write <code>(?&gt;a*)b</code>. Instead of <code>[a-z]+(?= )</code>, a developer can write <code>(?&gt;[a-z]+)(?= )</code>. And so on. But when was the last time you explicitly added an atomic group to a regex you authored? Of the almost 20,000 regular expression patterns in the aforementioned database of real-world regexes sourced from nuget, care to guess how many include an explicitly written atomic group? The answer: ~100. It&rsquo;s just not something developers in general think to do, so although the optimization transforms the user&rsquo;s pattern into something they could have written themselves, it&rsquo;s an incredibly valuable optimization, especially since now in .NET 10 over 70% of those patterns have at least one construct upgraded to be atomic.</p><p>The auto-atomicity optimization is an example of the optimizer removing unnecessary work. A key example of that, but certainly not the only example. Several additional PRs in .NET 10 have also eliminated unnecessary work, in other ways.</p><p><a href="https://github.com/dotnet/runtime/pull/118084">dotnet/runtime#118084</a> is a fun example of this, but to understand it, we first need to understand lookarounds. A &ldquo;lookaround&rdquo; is a regex construct that makes its contents zero-width. Whereas when a set like &ldquo;[abc]&rdquo; matches it consumes a single character from the input, or when a loop like &ldquo;[abc]{3,5}&rdquo; matches it&rsquo;ll consume between 3-5 characters from the input, lookarounds (as with other zero-width constructs, like anchors) don&rsquo;t consume anything. You wrap a lookaround around a regex expression, and it effectively makes the consumption temporary, e.g. if I wrap <code>[abc]{3,5}</code> in a positive lookahead as <code>(?=[abc]{3,5})</code>, that will end up performing the whole match for the 3-5 set characters, but those characters won&rsquo;t remain consumed after exiting the lookaround; the lookaround is just performing a test to ensure the inner pattern matches but the position in the input is reset upon exiting the lookaround. This is again visualized easily by looking at the code emitted by the regex source generator for a pattern like <code>(?=[abc]{3,5})abc</code>:</p><pre><code>// Zero-width positive lookahead. { int positivelookahead_starting_pos = pos; // Match a character in the set [a-c] atomically at least 3 and at most 5 times. { int iteration = 0; while (iteration &lt; 5 &amp;&amp; (uint)iteration &lt; (uint)slice.Length &amp;&amp; char.IsBetween(slice[iteration], 'a', 'c')) { iteration++; } if (iteration &lt; 3) { return false; // The input didn't match. } slice = slice.Slice(iteration); pos += iteration; } pos = positivelookahead_starting_pos; slice = inputSpan.Slice(pos); } // Match the string "abc". if (!slice.StartsWith("abc")) { return false; // The input didn't match. }</code></pre><p>We can see that the lookaround is caching the starting position, then proceeding to try to match the loop it contains, and if successful, resetting the matching position to what it was when the lookaround was entered, then continuing on to perform the match for what comes after the lookaround.</p><p>These examples have been for a particular flavor of lookaround, called a positive lookahead. There are four variations of lookarounds composed of two choices: positive vs negative, and lookahead vs lookbehind. Lookaheads validate the pattern starting from the current position and proceeding forwards (as matching typically is), while lookbehinds validate the pattern starting from just before the current position and extending backwards. Positive indicates that the pattern should match, while negative indicates that the pattern should not match. So, for example, the negative lookbehind <code>(? will match if what comes before the current position is not a word character.</code></p><p>Negative lookarounds are particularly interesting, because, unlike every other regex construct, they guarantee that the pattern they contain <em>doesn&rsquo;t</em> match. That also makes them special in other regards, in particular around capture groups. For a positive lookaround, even though they&rsquo;re zero width, anything capture groups inside of the lookaround capture still remain to outside of the lookaround, e.g. <code>^(?=(abc))\1$</code>, which entails a backreference successfully matching what&rsquo;s captured by the capture group inside of the positive lookahead, will successfully match the input <code>"abc"</code>. But because <em>negative</em> lookarounds guarantee their content doesn&rsquo;t match, it would be counter-intuitive if anything captured inside of a negative lookaround persisted past the lookaround&hellip; so it doesn&rsquo;t. The capture groups inside of a negative lookaround are still possibly meaningful, in particular if there&rsquo;s a backreference also <em>inside of</em> the same lookaround that refers back to the capture group, e.g. the pattern <code>^(?!(ab)\1cd)ababc</code> is checking to see whether the input does not begin with <code>ababcd</code> but does begin with <code>ababc</code>. But if there&rsquo;s no backreference, the capture group is useless, and we don&rsquo;t need to do any work for it as part of processing the regex (work like remembering where the capture occurred). Such capture groups can be completely eliminated from the node tree as part of the optimization phase, and that&rsquo;s exactly what <a href="https://github.com/dotnet/runtime/pull/118084">dotnet/runtime#118084</a> does. Just as developers often use backtracking constructs without thinking to make them atomic, developers also often use capture groups purely as a grouping mechanism without thinking of the possibility of making them non-capturing groups. Since captures in general need to persist to be examined by the <code>Match</code> object returned from a <code>Regex</code>, we can&rsquo;t just eliminate all capture groups that aren&rsquo;t used internally in the pattern, but we can for these negative lookarounds. Consider a pattern like <code>(?, which is looking for the word <code>"token"</code> when it&rsquo;s <em>not</em> preceeded by <code>"access "</code> or <code>"auth "</code>; the developer here (me, in this case) did what&rsquo;s fairly natural, putting a group around the alternation so that the <code>\s</code> that follows either word can be factored out (if it were instead <code>access|auth\s</code>, the whitespace set would only be in the second branch of the alternation and wouldn&rsquo;t apply to the first). But my &ldquo;simple&rdquo; grouping here is actually a capture group by default; to get it to be non-capturing, I&rsquo;d either need to write it as a non-capturing group, i.e. <code>(?, or I&rsquo;d need to use <code>RegexOptions.ExplicitCapture</code>, which turns all non-named capture groups into non-capturing groups.</code></code></p><p>We can similarly remove other work related to lookarounds. As noted, positive lookarounds exist to transform any pattern into a zero-width pattern, i.e. don&rsquo;t consume anything. That&rsquo;s all they do. If the pattern being wrapped by the positive lookaround is already zero-width, the lookaround contributes nothing to the behavior of the expression and can be removed. So, for example, if you have <code>(?=$)</code>, that can be transformed into just <code>$</code>. That&rsquo;s exactly what <a href="https://github.com/dotnet/runtime/pull/118091">dotnet/runtime#118091</a> does.</p><p><a href="https://github.com/dotnet/runtime/pull/118079">dotnet/runtime#118079</a> and <a href="https://github.com/dotnet/runtime/pull/118111">dotnet/runtime#118111</a> handle other transformations relative to zero-width assertions, in particular with regards to loops. For whatever reason, you&rsquo;ll see developers wrapping zero-width assertions inside of loops, either making such assertions optional (e.g. <code>\b?</code>) or with some larger upper bound (e.g. <code>(?=abc)*</code>). But these zero-width assertions don&rsquo;t consume anything; their sole purpose is to flag whether something is true or false at the current position. If you make such a zero-width assertion optional, then you&rsquo;re saying &ldquo;check whether it&rsquo;s true or false, and then immediately ignore the answer, because both answers are valid&rdquo;; as such, the whole expression can be removed as a nop. Similarly, if you wrap a loop with an upper bound greater than 1 around such an expression, you&rsquo;re saying &ldquo;check whether it&rsquo;s true or false, now without changing anything check again, and check again, and check again.&rdquo; There&rsquo;s a common English expression that&rsquo;s something along the lines of &ldquo;insanity is doing the same thing over and over again and expecting different results.&rdquo; That applies here. There may be behavioral benefits to invoking the zero-width assertion once, but repeating it beyond that is a pure waste: if it was going to fail, it would have failed the first time. Mostly. There&rsquo;s one specific case where the difference is actually observable, and that has to do with an interesting feature of .NET regexes: capture groups track <em>all</em> matched captures, not just the last. Consider this program:</p><pre><code>// dotnet run -c Release -f net10.0 using System.Diagnostics; using System.Text.RegularExpressions; Match m = Regex.Match("abc", "^(?=(\\w+)){3}abc$"); Debug.Assert(m.Success); foreach (Group g in m.Groups) { foreach (Capture c in g.Captures) { Console.WriteLine($"Group: {g.Name}, Capture: {c.Value}"); } }</code></pre><p>If you run that, you may be surprised to see that capture group #1 (the explicit group I have inside of the lookahead) provides three capture values:</p><pre><code>Group: 0, Capture: abc Group: 1, Capture: abc Group: 1, Capture: abc Group: 1, Capture: abc</code></pre><p>That&rsquo;s because the loop around the positive lookahead does three iterations, each iteration matches <code>"abc"</code>, and each successful capture is persisted for subsequent inspection via the <code>Regex</code> APIs. As such, we can&rsquo;t optimize any loop around zero-width assertions by lowering the upper bound from greater than 1 to 1; we can only do so if it doesn&rsquo;t contain any captures. And that&rsquo;s what these PRs do. Given a loop that wraps a zero-width assertion that does not contain a capture, if the lower bound of the loop is 0, the whole loop and its contents can be eliminated, and if the upper bound of the loop is greater than 1, the loop itself can be removed, leaving only its contents in its stead.</p><p>Any time work like this is eliminated, it&rsquo;s easy to construct monstrous, misleading micro-benchmarks&hellip; but it&rsquo;s also a lot of fun, so, I&rsquo;ll allow myself it this time.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Text.RegularExpressions; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private static readonly string s_input = new HttpClient().GetStringAsync(@"https://www.gutenberg.org/cache/epub/3200/pg3200.txt").Result; private static readonly Regex s_regex = new Regex(@"(?=.*\bTwain\b.*\bConnecticut\b)*.*Mark", RegexOptions.Compiled); [Benchmark] public int Count() =&gt; s_regex.Count(s_input); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Count</td> <td>.NET 9.0</td> <td>3,226.024 ms</td> <td>1.000</td> </tr> <tr> <td>Count</td> <td>.NET 10.0</td> <td>6.605 ms</td> <td>0.002</td> </tr> </tbody> </table><p><a href="https://github.com/dotnet/runtime/pull/118083">dotnet/runtime#118083</a> is similar. &ldquo;Repeaters&rdquo; are a name for a regex loop that has the same lower and upper bound, such that the contents of the loop &ldquo;repeats&rdquo; that fixed number of times. Typically you&rsquo;ll see these written out using the <code>{N}</code> syntax, e.g. <code>[abc]{3}</code> is a repeater that requires three characters, any of which can be <code>'a'</code>, <code>'b'</code>, or <code>'c'</code>. But of course it could also be written out in long-form, just by manually repeating the contents, e.g. <code>[abc][abc][abc]</code>. Just as we saw how we can condense loops around zero-width assertions when specified in loop form, we can do the exact same thing when manually written out. So, for example, <code>\b\b</code> is the same as just <code>\b{2}</code>, which is just <code>\b</code>.</p><p>Another nice example of removing unnecessary work is <a href="https://github.com/dotnet/runtime/pull/118105">dotnet/runtime#118105</a>. Boundary assertions are used in many expressions, e.g. it&rsquo;s quite common to see a simple pattern like <code>\b\w+\b</code>, which is trying to match an entire word. When the regex engine encounters such an assertion, historically it&rsquo;s delegated to the <code>IsBoundary</code> helper shown earlier. There is, however, some subtle unnecessary work here, which is more obvious when you see what the regex source generator outputs for an expression like <code>\b\w+\b</code>. This is what the output looks like on .NET 9:</p><pre><code>// Match if at a word boundary. if (!Utilities.IsBoundary(inputSpan, pos)) { return false; // The input didn't match. } // Match a word character atomically at least once. { int iteration = 0; while ((uint)iteration &lt; (uint)slice.Length &amp;&amp; Utilities.IsWordChar(slice[iteration])) { iteration++; } if (iteration == 0) { return false; // The input didn't match. } slice = slice.Slice(iteration); pos += iteration; } // Match if at a word boundary. if (!Utilities.IsBoundary(inputSpan, pos)) { return false; // The input didn't match. }</code></pre><p>Pretty straightforward: match the boundary, consume as many word characters as possible, then again match a boundary. Except if you look back at the definition of <code>IsBoundary</code>, you&rsquo;ll notice that it&rsquo;s doing two checks, one against the previous character and one against the next character.</p><pre><code>internal static bool IsBoundary(ReadOnlySpan<char> inputSpan, int index) { int indexM1 = index - 1; return ((uint)indexM1 &lt; (uint)inputSpan.Length &amp;&amp; RegexCharClass.IsBoundaryWordChar(inputSpan[indexM1])) != ((uint)index &lt; (uint)inputSpan.Length &amp;&amp; RegexCharClass.IsBoundaryWordChar(inputSpan[index])); }</char></code></pre><p>Now, look at that, and look back at the generated code, and look at this again, and back at the source generated code again. See anything unnecessary? When we perform the first boundary comparison, we are dutifully checking the previous character, which is necessary, but then we&rsquo;re checking the current character, which is about to checked against <code>\w</code> by the subsequent <code>\w+</code> loop. Similarly for the second boundary check, we just finished matching <code>\w+</code>, which will have only successfully matched if there was at least one word character. While we still need to validate that the subsequent character is not a boundary character (there are two characters considered boundary characters that aren&rsquo;t word characters), we don&rsquo;t need to re-validate the previous character. So, <a href="https://github.com/dotnet/runtime/pull/118105">dotnet/runtime#118105</a> overhauls boundary handling in the compiler and source generator to emit customized boundary checks based on surrounding knowledge. If it can prove that the subsequent construct will validate that a character is a word character, then it only needs to validate that the previous character is not a boundary character; similarly, if it can prove that the previous construct will have already validated that a character is a word character, then it only needs to validate that the next character isn&rsquo;t. This leads to this tweaked source generated code now on .NET 10:</p><pre><code>// Match if at a word boundary. if (!Utilities.IsPreWordCharBoundary(inputSpan, pos)) { return false; // The input didn't match. } // Match a word character atomically at least once. { int iteration = 0; while ((uint)iteration &lt; (uint)slice.Length &amp;&amp; Utilities.IsWordChar(slice[iteration])) { iteration++; } if (iteration == 0) { return false; // The input didn't match. } slice = slice.Slice(iteration); pos += iteration; } // Match if at a word boundary. if (!Utilities.IsPostWordCharBoundary(inputSpan, pos)) { return false; // The input didn't match. }</code></pre><p>Those <code>IsPreWordCharBoundary</code> and <code>IsPostWordCharBoundary</code> helpers are just half the checks in the main boundary helper. In cases where there are lots of boundary tests being performed, the reduced check count can add up.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Text.RegularExpressions; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private static readonly string s_input = new HttpClient().GetStringAsync(@"https://www.gutenberg.org/cache/epub/3200/pg3200.txt").Result; private static readonly Regex s_regex = new Regex(@"\ba\b", RegexOptions.Compiled | RegexOptions.IgnoreCase); [Benchmark] public int CountStandaloneAs() =&gt; s_regex.Count(s_input); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>CountStandaloneAs</td> <td>.NET 9.0</td> <td>20.58 ms</td> <td>1.00</td> </tr> <tr> <td>CountStandaloneAs</td> <td>.NET 10.0</td> <td>19.25 ms</td> <td>0.94</td> </tr> </tbody> </table><p>The <code>Regex</code> optimizer is all about pattern recognition: it looks for sequences and shapes it recognizes and performs transforms over those to put them into a more efficiently-processable form. One example of this is with alternations around coalescable branches. Let&rsquo;s say you have an alternation <code>a|e|i|o|u</code>. You could process that as an alternation, but it&rsquo;s also much more efficiently represented and processed as the equivalent set <code>[aeiou]</code>. There is an optimization that does such transformations as part of handling alternations. However, through .NET 9, it only handled single characters and sets, but not negated sets. For example, it would transform <code>a|e|i|o|u</code> into <code>[aeiou]</code>, and it would transform <code>[aei]|[ou]</code> into <code>[aeiou]</code>, but it would not merge negations like <code>[^\n]</code>, otherwise known as <code>.</code> (when not in <code>RegexOptions.Singleline</code> mode). When developers want a set that represents all characters, there are various idioms they employ, such as <code>[\s\S]</code>, which says &ldquo;this is a set of all whitespace and non-whitespace characters&rdquo;, aka everything. Another common idiom is <code>\n|.</code>, which is the same as <code>\n|[^\n]</code>, which says &ldquo;this is an alternation that matches either a newline or anything other than a newline&rdquo;, aka also everything. Unfortunately, while examples like <code>[\d\D]</code> have been handled well, <code>.|\n</code> has not, because of the gap in the alternation optimization. <a href="https://github.com/dotnet/runtime/pull/118109">dotnet/runtime#118109</a> improves that, such that such &ldquo;not&rdquo; cases are mergable as part of the existing optimization. That takes a relatively expensive alternation and converts it into a super fast set check. And while, in general, set containment checks are very efficient, this one is as efficient as you can get, as it&rsquo;s always true. We can see an example of this with a pattern intended to match C-style comment blocks.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Text.RegularExpressions; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private const string Input = """ /* This is a comment. */ /* Another comment */ /* Multi-line comment */ """; private static readonly Regex s_regex = new Regex(@"/\*(?:.|\n)*?\*/", RegexOptions.Compiled); [Benchmark] public int Count() =&gt; s_regex.Count(Input); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Count</td> <td>.NET 9.0</td> <td>344.80 ns</td> <td>1.00</td> </tr> <tr> <td>Count</td> <td>.NET 10.0</td> <td>93.59 ns</td> <td>0.27</td> </tr> </tbody> </table><p>Note that there&rsquo;s another change that helps .NET 10 here, <a href="https://github.com/dotnet/runtime/pull/118373">dotnet/runtime#118373</a>, though I hesitate to call it out as a performance improvement since it&rsquo;s really more of a bug fix. As part of writing this post, these benchmark numbers were showing some oddities (it&rsquo;s important in general to be skeptical of benchmark results and to investigate anything that doesn&rsquo;t align with reason and expectations). The result of investigating was a one-word change that yielded significant speedups on this test, specifically when using <code>RegexOptions.Compiled</code> (the bug didn&rsquo;t exist in the source generator). As part of handling lazy loops, there&rsquo;s a special-case for when the lazy loop is around a set that matches any character, which, thanks to the previous PR, <code>(?:.|\n)</code> now does. That special-case recognizes that if the lazy loop matches anything, we can efficiently find the end of the lazy loop by searching for whatever comes after the loop (e.g. in this test, the loop is followed by the literal <code>"*/"</code>). Unfortunately, the helper that emits that <code>IndexOf</code> call was passed the wrong node from the pattern: it was being passed the object representing the <code>(?:.|\n)</code> any-set rather than the <code>"*/"</code> literal, which resulted in it emitting the equivalent of <code>IndexOfAnyInRange((char)0, '\uFFFF')</code> rather than the equivalent of <code>IndexOf("*/")</code>. Oops. It was still functionally correct, in that the <code>IndexOfAnyInRange</code> call would successfully match the first character and the loop would re-evaluate from that location, but that means that rather than efficiently skipping using SIMD over a bunch of positions that couldn&rsquo;t possibly match, we were doing non-trivial work for each and every position along the way.</p><p><a href="https://github.com/dotnet/runtime/pull/118087">dotnet/runtime#118087</a> represents another interesting transformation related to alternations. It&rsquo;s very common to come across alternations with empty branches, possibly because that&rsquo;s what the developer wrote, but more commonly as an outcome of other transformations that have happened. For example, given the pattern <code>\r\n|\r</code>, which is trying to match line endings that begin with <code>\r</code>, there is an optimization that will factor out a common prefix of all of the branches, producing the equivalent of <code>\r(?:\n|)</code>; in other words, <code>\r</code> followed by either a line feed or empty. Such an alternation is a perfectly valid representation for this concept, but there&rsquo;s a more natural one: <code>?</code>. Behaviorally, this pattern is identical to <code>\r\n?</code>, and because the latter is more common and more canonical, the regex engine has more optimizations that recognize this loop-based form, for example coalescing with other loops, or auto-atomicity. As such, this PR finds all alternations of the form <code>X|</code> and transforms them into <code>X?</code>. Similarly, it finds all alternations of the form <code>|X</code> and transforms them into <code>X??</code>. The difference between <code>X|</code> and <code>|X</code> is whether <code>X</code> is tried first or empty is tried first; similarly, the difference between the greedy <code>X?</code> loop and the lazy <code>X??</code> loop is whether <code>X</code> is tried first or empty is tried first. The impact of this can be seen in the code generated for the previously cited example. Here is the source-generated code for the heart of the matching routine for <code>\r\n|\r</code> on .NET 9:</p><pre><code>// Match '\r'. if (slice.IsEmpty || slice[0] != '\r') { return false; // The input didn't match. } // Match with 2 alternative expressions, atomically. { int alternation_starting_pos = pos; // Branch 0 { // Match '\n'. if ((uint)slice.Length &lt; 2 || slice[1] != '\n') { goto AlternationBranch; } pos += 2; slice = inputSpan.Slice(pos); goto AlternationMatch; AlternationBranch: pos = alternation_starting_pos; slice = inputSpan.Slice(pos); } // Branch 1 { pos++; slice = inputSpan.Slice(pos); } AlternationMatch:; }</code></pre><p>Now, here&rsquo;s what&rsquo;s produced on .NET 10:</p><pre><code>// Match '\r'. if (slice.IsEmpty || slice[0] != '\r') { return false; // The input didn't match. } // Match '\n' atomically, optionally. if ((uint)slice.Length &gt; (uint)1 &amp;&amp; slice[1] == '\n') { slice = slice.Slice(1); pos++; }</code></pre><p>The optimizer recognized that the <code>\r\n|\r</code> was the same as <code>\r(?:\n|)</code>, which is the same as <code>\r\n?</code>, which is the same as <code>\r(?&gt;\n?)</code>, which it can produce much simplified code for, given that it no longer needs any backtracking.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Text.RegularExpressions; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private static readonly string s_input = new HttpClient().GetStringAsync(@"https://www.gutenberg.org/cache/epub/3200/pg3200.txt").Result; private static readonly Regex s_regex = new Regex(@"ab|a", RegexOptions.Compiled); [Benchmark] public int Count() =&gt; s_regex.Count(s_input); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Count</td> <td>.NET 9.0</td> <td>23.35 ms</td> <td>1.00</td> </tr> <tr> <td>Count</td> <td>.NET 10.0</td> <td>18.73 ms</td> <td>0.80</td> </tr> </tbody> </table><p>.NET 10 also features improvements to <code>Regex</code> that go beyond just this form of work elimination. <code>Regex</code>&lsquo;s matching routines are logically factored into two pieces: finding as quickly as possible the next place that could possibly match (<code>TryFindNextPossibleStartingPosition</code>), and then performing the full matching routine at that location (<code>TryMatchAtCurrentPosition</code>). It&rsquo;s desirable that <code>TryFindNextPossibleStartingPosition</code> both does its work as quickly as possible while also significantly limiting the number of locations a full match should be performed. <code>TryFindNextPossibleStartingPosition</code>, for example, could operate very quickly just by always saying that the next index in the input should be tested, which would result in the full matching logic being performed at every index in the input; that&rsquo;s not great for performance. Instead, the optimizer analyzes the pattern looking for things that would allow it to quickly search for viable starting locations, e.g. fixed strings or sets at known offsets in the pattern. Anchors are some of the most valuable things the optimizer can find, as they significantly inhibit the possible places matching is valid; the ideal pattern begins with a beginning anchor (<code>^</code>), which then means the only possible place matching can be successful is at index 0.</p><p>We previously discussed lookarounds, but as it turns out, until .NET 10, lookarounds weren&rsquo;t factored into what <code>TryFindNextPossibleStartingPosition</code> should look for. <a href="https://github.com/dotnet/runtime/pull/112107">dotnet/runtime#112107</a> changes that. It teaches the optimizer when and how to explore positive lookaheads at the beginning of a pattern for constructs that could help it more efficiently find starting locations. For example, in .NET 9, for the pattern <code>(?=^)hello</code>, here&rsquo;s what the source generator emits for <code>TryFindNextPossibleStartingPosition</code>:</p><pre><code>private bool TryFindNextPossibleStartingPosition(ReadOnlySpan<char> inputSpan) { int pos = base.runtextpos; // Any possible match is at least 5 characters. if (pos &lt;= inputSpan.Length - 5) { // The pattern has the literal "hello" at the beginning of the pattern. Find the next occurrence. // If it can't be found, there's no match. int i = inputSpan.Slice(pos).IndexOfAny(Utilities.s_indexOfString_hello_Ordinal); if (i &gt;= 0) { base.runtextpos = pos + i; return true; } } // No match found. base.runtextpos = inputSpan.Length; return false; }</char></code></pre><p>The optimizer found the <code>"hello"</code> string in the pattern and is thus searching for that as part of finding the next possible place to do the full match. That would be excellent, if it weren&rsquo;t for the lookahead that also says any match must happen at the beginning of the input. Now in .NET 10, we get this:</p><pre><code>private bool TryFindNextPossibleStartingPosition(ReadOnlySpan<char> inputSpan) { int pos = base.runtextpos; // Any possible match is at least 5 characters. if (pos &lt;= inputSpan.Length - 5) { // The pattern leads with a beginning (\A) anchor. if (pos == 0) { return true; } } // No match found. base.runtextpos = inputSpan.Length; return false; }</char></code></pre><p>That <code>pos == 0</code> check is critical, because it means we will only ever attempt the full match in one location and we can avoid the search that would happen even if we never found a good location to perform the match. Again, any time you eliminate work like this, you can construct tantalizing micro-benchmarks&hellip;</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Text.RegularExpressions; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private static readonly string s_input = new HttpClient().GetStringAsync(@"https://www.gutenberg.org/cache/epub/3200/pg3200.txt").Result; private static readonly Regex s_regex = new Regex(@"(?=^)hello", RegexOptions.Compiled); [Benchmark] public int Count() =&gt; s_regex.Count(s_input); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Count</td> <td>.NET 9.0</td> <td>2,383,784.95 ns</td> <td>1.000</td> </tr> <tr> <td>Count</td> <td>.NET 10.0</td> <td>17.43 ns</td> <td>0.000</td> </tr> </tbody> </table><p>That same PR also improved optimizations over alternations. It&rsquo;s already the case that the branches of alternations are analyzed looking for common prefixes that can be factored out. For example, given the pattern <code>abc|abd</code>, the optimizer will spot the shared <code>"ab"</code> prefix at the beginning of each branch and factor that out, resulting in <code>ab(?:c|d)</code>, and will then see that each branch of the remaining alternation are individual characters, which it can convert into a set, <code>ab[cd]</code>. If, however, the branches began with anchors, these optimizations wouldn&rsquo;t be applied. Given the pattern <code>^abc|^abd</code>, the code generators would end up emitting this exactly as it&rsquo;s written, with an alternation with two branches, the first branch checking for the beginning and then matching <code>"abc"</code>, the second branch also checking for the beginning and then matching <code>"abd"</code>. Now in .NET 10, the anchor can be factored out, such that <code>^abc|^abd</code> ends up being rewritten as <code>^ab[cd]</code>.</p><p>As a small tweak, <a href="https://github.com/dotnet/runtime/pull/112065">dotnet/runtime#112065</a> also helps improve the source generated code for repeaters by using a more efficient searching routine. Let&rsquo;s take the pattern <code>[0-9a-f]{32}</code> as an example. This is looking for sequences of 32 lowercase hex digits. In .NET 9, the implementation of that ends up looking like this:</p><pre><code>// Match a character in the set [0-9a-f] exactly 32 times. { if ((uint)slice.Length &lt; 32) { return false; // The input didn't match. } if (slice.Slice(0, 32).IndexOfAnyExcept(Utilities.s_asciiHexDigitsLower) &gt;= 0) { return false; // The input didn't match. } }</code></pre><p>Simple, clean, fairly concise, and utilizing the vectorized <code>IndexOfAnyExcept</code> to very efficiently validate that the whole sequence of 32 characters are lowercase hex. We can do a tad bit better, though. The <code>IndexOfAnyExcept</code> method not only needs to find whether the span contains something other than one of the provided values, it needs to specify the index at which that found value occurs. That&rsquo;s only a few instructions, but it&rsquo;s a few unnecessary instructions, since here that exact index isn&rsquo;t utilized&hellip; the implementation only cares whether it&rsquo;s <code>&gt;= 0</code>, meaning whether anything was found or not. As such, we can instead use the <code>Contains</code> variant of this method, which doesn&rsquo;t need to spend extra cycles determining the exact index. Now in .NET 10, this is generated:</p><pre><code>// Match a character in the set [0-9a-f] exactly 32 times. if ((uint)slice.Length &lt; 32 || slice.Slice(0, 32).ContainsAnyExcept(Utilities.s_asciiHexDigitsLower)) { return false; // The input didn't match. }</code></pre><p>Finally, the .NET 10 SDK includes a new analyzer related to <code>Regex</code>. It&rsquo;s oddly common to see code that determines whether an input matches a <code>Regex</code> written like this: <code>Regex.Match(...).Success</code>. While functionally correct, that&rsquo;s much more expensive than <code>Regex.IsMatch(...)</code>. For all of the engines, <code>Regex.Match(...)</code> requires allocating a new <code>Match</code> object and supporting data structures (except when there isn&rsquo;t a match found, in which case it&rsquo;s able to use an empty singleton); in contrast, <code>IsMatch</code> doesn&rsquo;t need to allocate such an instance because it doesn&rsquo;t need to return such an instance (as an implementation detail, it may still use a <code>Match</code> object, but it can reuse one rather than creating a new one each time). It can also avoid other inefficiencies. <code>RegexOptions.NonBacktracking</code> is &ldquo;pay-for-play&rdquo; with the information it needs to gather. Determining just <em>whether</em> there&rsquo;s a match is cheaper than determining exactly where the match begins and ends, which is cheaper still than determining all of the captures that make up that match. <code>IsMatch</code> is thus the cheapest, only needing to determine that there is a match, not exactly where it is or what the exact captures are, whereas <code>Match</code> needs to determine all of that. <code>Regex.Matches(...).Count</code> is similar; it&rsquo;s having to gather all of the relevant details and allocate a whole bunch of objects, whereas <code>Regex.Count(...)</code> can do so in a much more efficient manner. <a href="https://github.com/dotnet/roslyn-analyzers/pull/7547">dotnet/roslyn-analyzers#7547</a> adds CA1874 and CA1875, which flag these cases and recommend use of <code>IsMatch</code> and <code>Count</code>, respectively.</p><p><img src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2025/09/CA1875.png" alt="Analyzer and fixer for CA1875"></p><pre><code>// dotnet run -c Release -f net10.0 --filter ** using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Text.RegularExpressions; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private static readonly string s_input = new HttpClient().GetStringAsync(@"https://www.gutenberg.org/cache/epub/3200/pg3200.txt").Result; private static readonly Regex s_regex = new Regex(@"\b\w+\b", RegexOptions.NonBacktracking); [Benchmark(Baseline = true)] public int MatchesCount() =&gt; s_regex.Matches(s_input).Count; [Benchmark] public int Count() =&gt; s_regex.Count(s_input); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>MatchesCount</td> <td>680.4 ms</td> <td>1.00</td> <td>665530176 B</td> <td>1.00</td> </tr> <tr> <td>Count</td> <td>219.0 ms</td> <td>0.32</td> <td>&ndash;</td> <td>0.00</td> </tr> </tbody> </table><p><code>Regex</code> is one form of searching, but there are other primitives and helpers throughout .NET for various forms of searching, and they&rsquo;ve seen meaningful improvements in .NET 10, as well.</p><h3>SearchValues</h3><p>When discussing performance improvements in .NET 8, I called out two changes that were my favorites. The first was dynamic PGO. The second was <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-8/#searchvalues"><code>SearchValues</code></a>.</p><p><code>SearchValues</code> provides a mechanism for precomputing optimal strategies for searching. .NET 8 introduced overloads of <code>SearchValues.Create</code> that produce <code>SearchValues<byte></byte></code> and <code>SearchValues<char></char></code>, and corresponding overloads of <code>IndexOfAny</code> and friends that accept such instances. If there&rsquo;s a set of values you&rsquo;ll be searching for over and over and over, you can create one of these instances once, cache it, and then use it for all subsequent searches for those values, e.g.</p><pre><code>private static readonly SearchValues<char> s_validBase64Chars = SearchValues.Create("ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/"); internal static bool IsValidBase64(ReadOnlySpan<char> input) =&gt; input.ContainsAnyExcept(s_validBase64Chars);</char></char></code></pre><p>There are a plethora of different implementations used by <code>SearchValues<t></t></code> behind the scenes, each of which is selected and configured based on the <code>T</code> and the exact nature of the target values for which we&rsquo;re searching. <a href="https://github.com/dotnet/runtime/pull/106900">dotnet/runtime#106900</a> adds another, which both helps to shave off several instructions in the core vectorized search loop, and helps to highlight just how nuanced these different algorithms can be. Previously, if four target <code>byte</code> values were provided, and they weren&rsquo;t in a contiguous range, <code>SearchValues.Create</code> would choose an implementation that just uses four vectors, one per target byte, and does four comparisons (one against each target vector) for each input vector being tested. However, there&rsquo;s already a specialization that&rsquo;s used for more than five target bytes when all of the target bytes are ASCII. This PR allows that specialization to be used for both four or five targets when the lower nibble (the bottom four bits) of each of the targets is unique, and in doing so, it becomes several instructions cheaper: rather than doing four comparisons, it can do a single shuffle and equality check.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Buffers; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private static readonly byte[] s_haystack = new HttpClient().GetByteArrayAsync(@"https://www.gutenberg.org/cache/epub/3200/pg3200.txt").Result; private static readonly SearchValues<byte> s_needle = SearchValues.Create("\0\r&amp;&lt;"u8); [Benchmark] public int Count() { int count = 0; ReadOnlySpan<byte> haystack = s_haystack.AsSpan(); int pos; while ((pos = haystack.IndexOfAny(s_needle)) &gt;= 0) { count++; haystack = haystack.Slice(pos + 1); } return count; } }</byte></byte></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Count</td> <td>.NET 9.0</td> <td>3.704 ms</td> <td>1.00</td> </tr> <tr> <td>Count</td> <td>.NET 10.0</td> <td>2.668 ms</td> <td>0.72</td> </tr> </tbody> </table><p><a href="https://github.com/dotnet/runtime/pull/107798">dotnet/runtime#107798</a> improves another such algorithm, when AVX512 is available. One of the fallback strategies used by <code>SearchValues.Create<char></char></code> is a vectorized &ldquo;probabilistic map&rdquo;, basically a Bloom filter. It has a bitmap that stores a bit for each <code>byte</code> of the <code>char</code>; when testing to see whether the <code>char</code> is in the target set, it checks to see whether the bit for each of the <code>char</code>&lsquo;s <code>byte</code>s is set. If at least one isn&rsquo;t set, the <code>char</code> definitely isn&rsquo;t in the target set. If both are set, more validation will need to be done to determine the actual inclusion of that value in the set. This can make it very efficient to rule out large amounts of input that definitely are not in the set and then only spend more effort on input that might be. The implementation involves various shuffle, shift, and permute operations, and this change is able to use a better set of instructions that reduce the number needed.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Buffers; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private static readonly SearchValues<char> s_searchValues = SearchValues.Create("&szlig;&auml;&ouml;&uuml;&Auml;&Ouml;&Uuml;"); private string _input = new string('\n', 10_000); [Benchmark] public int IndexOfAny() =&gt; _input.AsSpan().IndexOfAny(s_searchValues); }</char></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>IndexOfAny</td> <td>.NET 9.0</td> <td>437.7 ns</td> <td>1.00</td> </tr> <tr> <td>IndexOfAny</td> <td>.NET 10.0</td> <td>404.7 ns</td> <td>0.92</td> </tr> </tbody> </table><p>While .NET 8 introduced support for <code>SearchValues<byte></byte></code> and <code>SearchValues<char></char></code>, .NET 9 introduced support for <code>SearchValues<string></string></code>. <code>SearchValues<string></string></code> is used a bit differently from <code>SearchValues<byte></byte></code> and <code>SearchValues<char></char></code>; whereas <code>SearchValues<byte></byte></code> is used to search for target <code>byte</code>s within a collection of <code>byte</code>s and <code>SearchValues<char></char></code> is used to search for target <code>char</code>s within a collection of <code>char</code>s, <code>SearchValues<string></string></code> is used to search for target <code>string</code>s within a single <code>string</code> (or span of <code>char</code>s). In other words, it&rsquo;s a multi-substring search. Let&rsquo;s say you have the regular expression <code>(?i)hello|world</code>; that is specifying that it should look for either &ldquo;hello&rdquo; or &ldquo;world&rdquo; in a case-insensitive manner; the <code>SearchValues</code> equivalent of that is <code>SearchValues.Create(["hello", "world"], StringComparison.OrdinalIgnoreCase)</code> (in fact, if you specify that pattern, the <code>Regex</code> compiler and source generator will use such a <code>SearchValues.Create</code> call under the covers in order to optimize the search).</p><p><code>SearchValues<string></string></code> also gets better in .NET 10. A key algorithm used by <code>SearchValues<string></string></code> whenever possible and relevant is called &ldquo;Teddy,&rdquo; and enables performing a vectorized search for multiple substrings. In its core processing loop, when using AVX512, there are two instructions, a <code>PermuteVar8x64x2</code> and an <code>AlignRight</code>; <a href="https://github.com/dotnet/runtime/pull/107819">dotnet/runtime#107819</a> recognizes that those can be replaced by a single <code>PermuteVar64x8x2</code>. Similarly, when on Arm64, <a href="https://github.com/dotnet/runtime/pull/118110">dotnet/runtime#118110</a> plays the instructions game and replaces a use of <code>ExtractNarrowingSaturateUpper</code> with the slightly cheaper <code>UnzipEven</code>.</p><p><code>SearchValues<string></string></code> is also able to optimize searching for a single string, spending more time to come up with optimal search parameters than does a simpler <code>IndexOf(string, StringComparison)</code> call. Similar to the approach with the probabilistic maps employed earlier, the vectorized search can yield false positives that then need to be weeded out. In some cases by construction, however, we know that false positives aren&rsquo;t possible; <a href="https://github.com/dotnet/runtime/pull/108368">dotnet/runtime#108368</a> extends an existing optimization that was case-sensitive only to also apply in some case-insensitive uses, such that we can avoid doing the extra validation step in more cases. For the candidate verification that remains, <a href="https://github.com/dotnet/runtime/pull/108365">dotnet/runtime#108365</a> also significantly reduces overhead in a variety of cases, including adding specialized handling for needles (the things being searched for) of up to 16 characters (previously it was only up to 8), and precomputing more information to make the verification faster.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Buffers; using System.Text.RegularExpressions; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private static readonly string s_haystack = new HttpClient().GetStringAsync(@"https://www.gutenberg.org/cache/epub/3200/pg3200.txt").Result; private static readonly Regex s_the = new("the", RegexOptions.IgnoreCase | RegexOptions.Compiled); private static readonly Regex s_something = new("something", RegexOptions.IgnoreCase | RegexOptions.Compiled); [Benchmark] public int CountThe() =&gt; s_the.Count(s_haystack); [Benchmark] public int CountSomething() =&gt; s_something.Count(s_haystack); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>CountThe</td> <td>.NET 9.0</td> <td>9.881 ms</td> <td>1.00</td> </tr> <tr> <td>CountThe</td> <td>.NET 10.0</td> <td>7.799 ms</td> <td>0.79</td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>CountSomething</td> <td>.NET 9.0</td> <td>2.466 ms</td> <td>1.00</td> </tr> <tr> <td>CountSomething</td> <td>.NET 10.0</td> <td>2.027 ms</td> <td>0.82</td> </tr> </tbody> </table><p><a href="https://github.com/dotnet/runtime/pull/118108">dotnet/runtime#118108</a> also adds a &ldquo;packed&rdquo; variant of the single-string implementation, meaning it&rsquo;s able to handle common cases like ASCII more efficiently by ignoring a character&rsquo;s upper zero byte in order to fit twice as much into a vector.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Buffers; using System.Text.RegularExpressions; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private static readonly string s_haystack = string.Concat(Enumerable.Repeat("Sherlock Holm_s", 8_000)); private static readonly SearchValues<string> s_needles = SearchValues.Create(["Sherlock Holmes"], StringComparison.OrdinalIgnoreCase); [Benchmark] public bool ContainsAny() =&gt; s_haystack.AsSpan().ContainsAny(s_needles); }</string></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>ContainsAny</td> <td>.NET 9.0</td> <td>58.41 us</td> <td>1.00</td> </tr> <tr> <td>ContainsAny</td> <td>.NET 10.0</td> <td>16.32 us</td> <td>0.28</td> </tr> </tbody> </table> <h3>MemoryExtensions</h3><p>The searching improvements continue beyond <code>SearchValues</code>, of course. Prior to .NET 10, the <code>MemoryExtensions</code> class already had a wealth of support for searching and manipulating spans, with extension methods like <code>IndexOf</code>, <code>IndexOfAnyExceptInRange</code>, <code>ContainsAny</code>, <code>Count</code>, <code>Replace</code>, <code>SequenceCompare</code>, and more (the set was further extended as well by <a href="https://github.com/dotnet/runtime/pull/112951">dotnet/runtime#112951</a>, which added <code>CountAny</code> and <code>ReplaceAny</code>), but the vast majority of these were limited to work with <code>T</code> types constrained to be <code>IEquatable<t></t></code>. And in practice, many of the types you want to search do in fact implement <code>IEquatable<t></t></code>. However, you might be in a generic context with an unconstrained <code>T</code>, such that even if the <code>T</code> used to instatiate the generic type or method is equatable, it&rsquo;s not evident in the type system and thus the <code>MemoryExtensions</code> method couldn&rsquo;t be used. And of course there are scenarios where you want to be able to supply a different comparison routine. Both of these scenarios show up, for example, in the implementation of LINQ&rsquo;s <code>Enumerable.Contains</code>; if the source <code>IEnumerable<tsource></tsource></code> is actually something we could treat as a span, like <code>TSource[]</code> or <code>List<tsource></tsource></code>, it&rsquo;d be nice to be able to just delegate to the optimized <code>MemoryExtensions.Contains<t></t></code>, but a) <code>Enumerable.Contains</code> doesn&rsquo;t constrain its <code>TSource : IEquatable<tsource></tsource></code>, and b) <code>Enumerable.Contains</code> accepts an optional comparer.</p><p>To address this, <a href="https://github.com/dotnet/runtime/pull/110197">dotnet/runtime#110197</a> adds ~30 new overloads to the <code>MemoryExtensions</code> class. These overloads all parallel existing methods, but remove the <code>IEquatable<t></t></code> (or <code>IComparable<t></t></code>) constraint on the generic method parameter and accept an optional <code>IEqualityComparer<t>?</t></code> (or <code>IComparer<t></t></code>). When no comparer or a default comparer is supplied, they can fall back to using the same vectorized logic for relevant types, and otherwise can provide as optimal an implementation as they can muster, based on the nature of <code>T</code> and the supplied comparer.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private IEnumerable<int> _data = Enumerable.Range(0, 1_000_000).ToArray(); [Benchmark] public bool Contains() =&gt; _data.Contains(int.MaxValue, EqualityComparer<int>.Default); }</int></int></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Contains</td> <td>.NET 9.0</td> <td>213.94 us</td> <td>1.00</td> </tr> <tr> <td>Contains</td> <td>.NET 10.0</td> <td>67.86 us</td> <td>0.32</td> </tr> </tbody> </table><p>(It&rsquo;s also worth highlighting that with the &ldquo;first-class&rdquo; span support in C# 14, many of these extensions from <code>MemoryExtensions</code> now naturally show up directly on types like <code>string</code>.)</p><p>This kind of searching often shows up as part of other APIs. For example, encoding APIs often need to first find something to be encoded, and that searching can be accelerated by using one of these efficiently implemented search APIs. There are dozens and dozens of existing examples of that throughout the core libraries, many of the places using <code>SearchValues</code> or these various <code>MemoryExtensions</code> methods. <a href="https://github.com/dotnet/runtime/pull/110574">dotnet/runtime#110574</a> adds another, speeding up <code>string.Normalize</code>&lsquo;s argument validation. The current implementation walks character by character looking for the first surrogate. The new implementation gives that a jump start by using <code>IndexOfAnyInRange</code>.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private string _input = "This is a test. This is only a test. Nothing to see here. \u263A\uFE0F"; [Benchmark] public string Normalize() =&gt; _input.Normalize(); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Normalize</td> <td>.NET 9.0</td> <td>104.93 ns</td> <td>1.00</td> </tr> <tr> <td>Normalize</td> <td>.NET 10.0</td> <td>88.94 ns</td> <td>0.85</td> </tr> </tbody> </table><p><a href="https://github.com/dotnet/runtime/pull/110478">dotnet/runtime#110478</a> similarly updates <code>HttpUtility.UrlDecode</code> to use the vectorized <code>IndexOfAnyInRange</code>. It also avoids allocating the resulting <code>string</code> if nothing needs to be decoded.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Web; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { [Benchmark] public string UrlDecode() =&gt; HttpUtility.UrlDecode("aaaaabbbbb%e2%98%ba%ef%b8%8f"); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>UrlDecode</td> <td>.NET 9.0</td> <td>59.42 ns</td> <td>1.00</td> </tr> <tr> <td>UrlDecode</td> <td>.NET 10.0</td> <td>54.26 ns</td> <td>0.91</td> </tr> </tbody> </table><p>Similarly, <a href="https://github.com/dotnet/runtime/pull/114494">dotnet/runtime#114494</a> employs <code>SearchValues</code> in <code>OptimizedInboxTextEncoder</code>, which is the core implementation that backs the various encoders like <code>JavaScriptEncoder</code> and <code>HtmlEncoder</code> in the <code>System.Text.Encodings.Web</code> library.</p><h2>JSON</h2><p>JSON is at the heart of many different domains, having become the lingua franca of data interchange on the web. With <code>System.Text.Json</code> as the recommended library for working with JSON in .NET, it is constantly evolving to meet additional performance requirements. .NET 10 sees it updated with both improvements to the performance of existing methods as well as new methods specifically geared towards helping with performance.</p><p>The <code>JsonSerializer</code> type is layered on top of the lower-level <code>Utf8JsonReader</code> and <code>Utf8JsonWriter</code> types. When serializing, <code>JsonSerializer</code> needs an instance of <code>Utf8JsonWriter</code>, which is a <code>class</code>, and any associated objects, such as an <code>IBufferWriter</code> instance. For any temporary buffers it requires, it&rsquo;ll use rented buffers from <code>ArrayPool<byte></byte></code>, but for these helper objects, it maintains its own cache, to avoid needing to recreate them at very high frequencies. That cache was being used for all asynchronous streaming serialization operations, but as it turns out, it wasn&rsquo;t being used for synchronous streaming serialization operations. <a href="https://github.com/dotnet/runtime/pull/112745">dotnet/runtime#112745</a> fixes that to make the use of the cache consistent, avoiding these intermediate allocations.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Text.Json; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private Data _data = new(); private MemoryStream _stream = new(); [Benchmark] public void Serialize() { _stream.Position = 0; JsonSerializer.Serialize(_stream, _data); } public class Data { public int Value1 { get; set; } } }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>Serialize</td> <td>.NET 9.0</td> <td>115.36 ns</td> <td>1.00</td> <td>176 B</td> <td>1.00</td> </tr> <tr> <td>Serialize</td> <td>.NET 10.0</td> <td>77.73 ns</td> <td>0.67</td> <td>&ndash;</td> <td>0.00</td> </tr> </tbody> </table><p>Earlier when discussing collections, it was noted that <code>OrderedDictionary<tkey></tkey></code> now exposes overloads of methods like <code>TryAdd</code> that return the relevant item&rsquo;s index, which then allows subsequent access to avoid the more costly key-based lookup. As it turns out, <code>JsonObject</code>&lsquo;s indexer needs to do that, first indexing into the dictionary by key, doing some checks, and then indexing again. It&rsquo;s now been updated to use these new overloads. As those lookups typically dominate the cost of using the setter, this can upwards of double throughput of <code>JsonObject</code>&lsquo;s indexer:</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Text.Json.Nodes; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private JsonObject _obj = new(); [Benchmark] public void Set() =&gt; _obj["key"] = "value"; }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Set</td> <td>.NET 9.0</td> <td>40.56 ns</td> <td>1.00</td> </tr> <tr> <td>Set</td> <td>.NET 10.0</td> <td>16.96 ns</td> <td>0.42</td> </tr> </tbody> </table><p>Most of the improvements in <code>System.Text.Json</code>, however, are actually via new APIs. This same &ldquo;avoid a double lookup&rdquo; issue shows up in other places, for example wanting to add a property to a <code>JsonObject</code> but only if it doesn&rsquo;t yet exist. With <a href="https://github.com/dotnet/runtime/pull/111229">dotnet/runtime#111229</a> from <a href="https://github.com/Flu">@Flu</a>, that&rsquo;s addressed with a new <code>TryAdd</code> method (as well as a <code>TryAdd</code> overload and an overload of the existing <code>TryGetPropertyValue</code> that, as with <code>OrderedDictionary&lt;&gt;</code>, returns the index of the property).</p><pre><code>// dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Text.Json.Nodes; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private JsonObject _obj = new(); private JsonNode _value = JsonValue.Create("value"); [Benchmark(Baseline = true)] public void NonOverwritingSet_Manual() { _obj.Remove("key"); if (!_obj.ContainsKey("key")) { _obj.Add("key", _value); } } [Benchmark] public void NonOverwritingSet_TryAdd() { _obj.Remove("key"); _obj.TryAdd("key", _value); } }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>NonOverwritingSet_Manual</td> <td>16.59 ns</td> <td>1.00</td> </tr> <tr> <td>NonOverwritingSet_TryAdd</td> <td>14.31 ns</td> <td>0.86</td> </tr> </tbody> </table><p><a href="https://github.com/dotnet/runtime/pull/109472">dotnet/runtime#109472</a> from <a href="https://github.com/karakasa">@karakasa</a> also imbues <code>JsonArray</code> with new <code>RemoveAll</code> and <code>RemoveRange</code> methods. In addition to the usability benefits these can provide, they have the same performance benefits they have on <code>List<t></t></code> (which is not a coincidence, given that <code>JsonArray</code> is, as an implementation detail, a wrapper for a <code>List<jsonnode></jsonnode></code>). Removing &ldquo;incorrectly&rdquo; from a <code>List<t></t></code> can end up being an <code>O(N^2)</code> endeavor, e.g. when I run this:</p><pre><code>// dotnet run -c Release -f net10.0 using System.Diagnostics; for (int i = 100_000; i &lt; 700_000; i += 100_000) { List<int> items = Enumerable.Range(0, i).ToList(); Stopwatch sw = Stopwatch.StartNew(); while (items.Count &gt; 0) { items.RemoveAt(0); // uh oh } Console.WriteLine($"{i} =&gt; {sw.Elapsed}"); }</int></code></pre><p>I get output like this:</p><pre><code>100000 =&gt; 00:00:00.2271798 200000 =&gt; 00:00:00.8328727 300000 =&gt; 00:00:01.9820088 400000 =&gt; 00:00:03.9242008 500000 =&gt; 00:00:06.9549009 600000 =&gt; 00:00:11.1104903</code></pre><p>Note how as the list length grows linearly, the elapsed time is growing non-linearly. That&rsquo;s primarily because each <code>RemoveAt(0)</code> is requiring the entire remainder of the list to shift down, which is <code>O(N)</code> in the length of the list. That means we get <code>N + (N-1) + (N-2) + ... + 1</code> operations, which is <code>N(N+1)/2</code>, which is <code>O(N^2)</code>. Both <code>RemoveRange</code> and <code>RemoveAll</code> are able to avoid those costs by doing the shifting only once per element. Of course, even without such methods, I could have written my previous removal loop in a way that keeps it linear, namely by repeatedly removing the last element rather than the first (and, of course, if I <em>really</em> intended on removing everything, I could have just used <code>Clear</code>). Typical use, however, ends up removing a smattering of elements, and being able to just delegate and not worry about accidentally incurring a non-linear overhead is helpful.</p><pre><code>// dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Text.Json.Nodes; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private JsonArray _arr; [IterationSetup] public void Setup() =&gt; _arr = new JsonArray(Enumerable.Range(0, 100_000).Select(i =&gt; (JsonNode)i).ToArray()); [Benchmark] public void Manual() { int i = 0; while (i &lt; _arr.Count) { if (_arr[i]!.GetValue<int>() % 2 == 0) { _arr.RemoveAt(i); } else { i++; } } } [Benchmark] public void RemoveAll() =&gt; _arr.RemoveAll(static n =&gt; n!.GetValue<int>() % 2 == 0); }</int></int></code></pre> <table> <thead> <tr> <th>Method</th> <th>Mean</th> <th>Allocated</th> </tr> </thead> <tbody> <tr> <td>Manual</td> <td>355.230 ms</td> <td>&ndash;</td> </tr> <tr> <td>RemoveAll</td> <td>2.022 ms</td> <td>24 B</td> </tr> </tbody> </table><p>(Note that while <code>RemoveAll</code> in this micro-benchmark is more than 150x faster, it does have that small allocation that the manual implementation doesn&rsquo;t. That&rsquo;s due to a closure in the implementation while delegating to <code>List<t>.RemoveAll</t></code>. This could be avoided in the future if necessary.)</p><p>Another frequently-requested new method is from <a href="https://github.com/dotnet/runtime/pull/116363">dotnet/runtime#116363</a>, which adds new <code>Parse</code> methods to <code>JsonElement</code>. If a developer wants a <code>JsonElement</code> and only needs it temporarily, the most efficient mechanism available today is still the right answer: <code>Parse</code> a <code>JsonDocument</code>, use its <code>RootElement</code>, and then <em>only</em> when done with the <code>JsonElement</code>, dispose of the <code>JsonDocument</code>, e.g.</p><pre><code>using (JsonDocument doc = JsonDocument.Parse(json)) { DoSomething(doc.RootElement); }</code></pre><p>That, however, is really only viable when the <code>JsonElement</code> is used in a scoped manner. If a developer needs to hand out the <code>JsonElement</code>, they&rsquo;re left with three options:</p><ol> <li><code>Parse</code> into a <code>JsonDocument</code>, clone its <code>RootElement</code>, dispose of the <code>JsonDocument</code>, hand out the clone. While using <code>JsonDocument</code> is good for the temporary case, making a clone like this entails a fair bit of overhead:<pre><code>JsonElement clone; using (JsonDocument doc = JsonDocument.Parse(json)) { clone = doc.RootElement.Clone(); } return clone;</code></pre> </li> <li><code>Parse</code> into a <code>JsonDocument</code> and just hand out its <code>RootElement</code>. Please <em>do not do this</em>! <code>JsonDocument.Parse</code> creates a <code>JsonDocument</code> that&rsquo;s backed by an array from the <code>ArrayPool&lt;&gt;</code>. If you don&rsquo;t <code>Dispose</code> of the <code>JsonDocument</code> in this case, an array will be rented and then never returned to the pool. That&rsquo;s not the end of the world; if someone else requests an array from the pool and the pool doesn&rsquo;t have one cached to give them, it&rsquo;ll just manufacture one, so eventually the pool&rsquo;s arrays will be replenished. But the arrays in the pool are generally &ldquo;more valuable&rdquo; than others, because they&rsquo;ve generally been around longer, and are thus more likely to be in higher generations. By using an <code>ArrayPool</code> array rather than a new array for a shorter-lived <code>JsonDocument</code>, you&rsquo;re more likely throwing away an array that&rsquo;ll have net more impact on the overall system. The impact of that is not easily seen in a micro-benchmark.<pre><code>return JsonDocument.Parse(json).RootElement; // please don't do this</code></pre> </li> <li>Use <code>JsonSerializer</code> to deserialize a <code>JsonElement</code>. This is a simple and reasonable one-liner, but it does invoke the <code>JsonSerializer</code> machinery, which brings in more overhead.<pre><code>return JsonSerializer.Deserialize<jsonelement>(json);</jsonelement></code></pre> </li> </ol><p>Now in .NET 10, there&rsquo;s a fourth option:</p><ul> <li>Use <code>JsonElement.Parse</code>. This is the right answer. Use this instead of (1), (2), or (3).<pre><code>// dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Text.Json; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private const string JsonString = """{ "name": "John", "age": 30, "city": "New York" }"""; [Benchmark] public JsonElement WithClone() { using JsonDocument d = JsonDocument.Parse(JsonString); return d.RootElement.Clone(); } [Benchmark] public JsonElement WithoutClone() =&gt; JsonDocument.Parse(JsonString).RootElement; // please don't do this in production code [Benchmark] public JsonElement WithDeserialize() =&gt; JsonSerializer.Deserialize<jsonelement>(JsonString); [Benchmark] public JsonElement WithParse() =&gt; JsonElement.Parse(JsonString); }</jsonelement></code></pre> <table> <thead> <tr> <th>Method</th> <th>Mean</th> <th>Allocated</th> </tr> </thead> <tbody> <tr> <td>WithClone</td> <td>303.7 ns</td> <td>344 B</td> </tr> <tr> <td>WithoutClone</td> <td>249.6 ns</td> <td>312 B</td> </tr> <tr> <td>WithDeserialize</td> <td>397.3 ns</td> <td>272 B</td> </tr> <tr> <td>WithParse</td> <td>261.9 ns</td> <td>272 B</td> </tr> </tbody> </table> </li> </ul><p>With JSON being used as an encoding for many modern protocols, streaming large JSON payloads has become very common. And for most use cases, it&rsquo;s already possible to stream JSON well with <code>System.Text.Json</code>. However, in previous releases there wasn&rsquo;t been a good way to stream partial string properties; string properties had to have their values written in one operation. If you&rsquo;ve got small strings, that&rsquo;s fine. If you&rsquo;ve got really, really large strings, and those strings are lazily-produced in chunks, however, you ideally want the ability to write those chunks of the property as you have them, rather than needing to buffer up the value in its entirety. <a href="https://github.com/dotnet/runtime/pull/101356">dotnet/runtime#101356</a> augmented <code>Utf8JsonWriter</code> with a <code>WriteStringValueSegment</code> method, which enables such partial writes. That addresses the majority case, however there&rsquo;s a very common case where additional encoding of the value is desirable, and an API that automatically handles that encoding helps to be both efficient and easy. These modern protocols often transmit large blobs of binary data within the JSON payloads. Typically, these blobs end up being Base64 strings as properties on some JSON object. Today, outputting such blobs requires Base64-encoding the whole input and then writing the resulting <code>byte</code>s or <code>char</code>s in their entirety into the <code>Utf8JsonWriter</code>. To address that, <a href="https://github.com/dotnet/runtime/pull/111041">dotnet/runtime#111041</a> adds a <code>WriteBase64StringSegment</code> method to <code>Utf8JsonWriter</code>. For those sufficiently motivated to reduce memory overheads, and to enable the streaming of such payloads, <code>WriteBase64StringSegment</code> enables passing in a span of bytes, which the implementation will Base64-encode and write to the JSON property; it can be called multiple times with <code>isFinalSegment=false</code>, such that the writer will continue appending the resulting Base64 data to the property, until it&rsquo;s called with a final segment that ends the property. (<code>Utf8JsonWriter</code> has long had a <code>WriteBase64String</code> method, this new <code>WriteBase64StringSegment</code> simply enables it to be written in pieces.) The primary benefit of such a method is reduced latency and working set, as the entirety of the data payload needn&rsquo;t be buffered before being written out, but we can still come up with a throughput benchmark that shows benefits:</p><pre><code>// dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Buffers; using System.Text.Json; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private Utf8JsonWriter _writer = new(Stream.Null); private Stream _source = new MemoryStream(Enumerable.Range(0, 10_000_000).Select(i =&gt; (byte)i).ToArray()); [Benchmark] public async Task Buffered() { _source.Position = 0; _writer.Reset(); byte[] buffer = ArrayPool<byte>.Shared.Rent(0x1000); int totalBytes = 0; int read; while ((read = await _source.ReadAsync(buffer.AsMemory(totalBytes))) &gt; 0) { totalBytes += read; if (totalBytes == buffer.Length) { byte[] newBuffer = ArrayPool<byte>.Shared.Rent(buffer.Length * 2); Array.Copy(buffer, newBuffer, totalBytes); ArrayPool<byte>.Shared.Return(buffer); buffer = newBuffer; } } _writer.WriteStartObject(); _writer.WriteBase64String("data", buffer.AsSpan(0, totalBytes)); _writer.WriteEndObject(); await _writer.FlushAsync(); ArrayPool<byte>.Shared.Return(buffer); } [Benchmark] public async Task Streaming() { _source.Position = 0; _writer.Reset(); byte[] buffer = ArrayPool<byte>.Shared.Rent(0x1000); _writer.WriteStartObject(); _writer.WritePropertyName("data"); int read; while ((read = await _source.ReadAsync(buffer)) &gt; 0) { _writer.WriteBase64StringSegment(buffer.AsSpan(0, read), isFinalSegment: false); } _writer.WriteBase64StringSegment(default, isFinalSegment: true); _writer.WriteEndObject(); await _writer.FlushAsync(); ArrayPool<byte>.Shared.Return(buffer); } }</byte></byte></byte></byte></byte></byte></code></pre> <table> <thead> <tr> <th>Method</th> <th>Mean</th> </tr> </thead> <tbody> <tr> <td>Buffered</td> <td>3.925 ms</td> </tr> <tr> <td>Streaming</td> <td>1.555 ms</td> </tr> </tbody> </table><p>.NET 9 saw the introduction of the <code>JsonMarshal</code> class and the <code>GetRawUtf8Value</code> method, which provides raw access to the underlying bytes of property values fronted by a <code>JsonElement</code>. For situations where the name of the property is also needed, <a href="https://github.com/dotnet/runtime/pull/107784">dotnet/runtime#107784</a> from <a href="https://github.com/mwadams">@mwadams</a> provides a corresponding <code>JsonMarshal.GetRawUtf8PropertyName</code> method.</p><h2>Diagnostics</h2><p>Over the years, I&rsquo;ve seen a fair number of codebases introduce a <code>struct</code>-based <code>ValueStopwatch</code>; I think there are even a few still floating around the <code>Microsoft.Extensions</code> libraries. The premise behind these is that <code>System.Diagnostics.Stopwatch</code> is a <code>class</code>, but it simply wraps a <code>long</code> (a timestamp), so rather than writing code like the following that allocates:</p><pre><code>Stopwatch sw = Stopwatch.StartNew(); ... // something being measured sw.Stop(); TimeSpan elapsed = sw.Elapsed;</code></pre><p>you could write:</p><pre><code>ValueStopwatch sw = ValueStopwatch.StartNew(); ... // something being measured sw.Stop(); TimeSpan elapsed = sw.Elapsed;</code></pre><p>and avoid the allocation. <code>Stopwatch</code> subsequently gained helpers that make such a <code>ValueStopwatch</code> less appealing, since as of .NET 7, I can write it instead like this:</p><pre><code>long start = Stopwatch.GetTimestamp(); ... // something being measured long end = Stopwatch.GetTimestamp(); TimeSpan elapsed = Stopwatch.GetElapsedTime(start, end);</code></pre><p>However, that&rsquo;s not quite as natural as the original example, that just uses <code>Stopwatch</code>. Wouldn&rsquo;t it be nice if you could write the original example and have it executed as if it were the latter? With all the investments in .NET 9 and .NET 10 around escape analysis and stack allocation, you now can. <a href="https://github.com/dotnet/runtime/pull/111834">dotnet/runtime#111834</a> streamlines the <code>Stopwatch</code> implementation so that <code>StartNew</code>, <code>Elapsed</code>, and <code>Stop</code> are fully inlineable. At that point, the JIT can see that the allocated <code>Stopwatch</code> instance never escapes the frame, and it can be stack allocated.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Diagnostics; using System.Runtime.CompilerServices; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [DisassemblyDiagnoser] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { [Benchmark] public TimeSpan WithGetTimestamp() { long start = Stopwatch.GetTimestamp(); Nop(); long end = Stopwatch.GetTimestamp(); return Stopwatch.GetElapsedTime(start, end); } [Benchmark] public TimeSpan WithStartNew() { Stopwatch sw = Stopwatch.StartNew(); Nop(); sw.Stop(); return sw.Elapsed; } [MethodImpl(MethodImplOptions.NoInlining)] private static void Nop() { } }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Code Size</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>WithGetTimestamp</td> <td>.NET 9.0</td> <td>28.95 ns</td> <td>1.00</td> <td>148 B</td> <td>&ndash;</td> <td>NA</td> </tr> <tr> <td>WithGetTimestamp</td> <td>.NET 10.0</td> <td>28.32 ns</td> <td>0.98</td> <td>130 B</td> <td>&ndash;</td> <td>NA</td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>WithStartNew</td> <td>.NET 9.0</td> <td>38.62 ns</td> <td>1.00</td> <td>341 B</td> <td>40 B</td> <td>1.00</td> </tr> <tr> <td>WithStartNew</td> <td>.NET 10.0</td> <td>28.21 ns</td> <td>0.73</td> <td>130 B</td> <td>&ndash;</td> <td>0.00</td> </tr> </tbody> </table><p><a href="https://github.com/dotnet/runtime/pull/117031">dotnet/runtime#117031</a> is a nice improvement that helps reduce working set for anyone using an <code>EventSource</code> and that has events with really large IDs. For efficiency purposes, <code>EventSource</code> was using an array to map event ID to the data for that ID; lookup needs to be really fast, since the lookup is performed on every event write in order to look up the metadata for the event being written. In many <code>EventSource</code>s, the developer authors events with a small, contiguous range of IDs, and the array ends up being very dense. But if a developer authors any event with a really large ID (which we&rsquo;ve seen happen in multiple real-world projects, due to splitting events into multiple partial class definitions shared between different projects and selecting IDs for each file unlikely to conflict with each other), an array is still created with a length to accomodate that large ID, which can result in a really big allocation that persists for the lifetime of the event source, and a lot of that allocation ends up just being wasted space. Thankfully, since <code>EventSource</code> was written years ago, the performance of <code>Dictionary<tkey></tkey></code> has increased significantly, to the point where it&rsquo;s able to efficiently handle the lookups without needing the event IDs to be dense. Note that there should really only ever be one instance of a given <code>EventSource</code>-derived type; the recommended pattern is to store one into a static readonly field and just use that one. So the overheads incurred as part of this are primarily about the impact that single large allocation has on working set for the duration of the process. To make it easier to demonstrate, though, I&rsquo;m doing something you&rsquo;d never, ever do, and creating a new instance per event. Don&rsquo;t try this at home, or at least not in production.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Diagnostics; using System.Diagnostics.Tracing; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private MyListener _listener = new(); [Benchmark] public void Oops() { using OopsEventSource oops = new(); oops.Oops(); } [EventSource(Name = "MyTestEventSource")] public sealed class OopsEventSource : EventSource { [Event(12_345_678, Level = EventLevel.Error)] public void Oops() =&gt; WriteEvent(12_345_678); } private sealed class MyListener : EventListener { protected override void OnEventSourceCreated(EventSource eventSource) =&gt; EnableEvents(eventSource, EventLevel.Error); } }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>Oops</td> <td>.NET 9.0</td> <td>1,876.21 us</td> <td>1.00</td> <td>1157428.01 KB</td> <td>1.000</td> </tr> <tr> <td>Oops</td> <td>.NET 10.0</td> <td>22.06 us</td> <td>0.01</td> <td>19.21 KB</td> <td>0.000</td> </tr> </tbody> </table><p><a href="https://github.com/dotnet/runtime/pull/107333">dotnet/runtime#107333</a> from <a href="https://github.com/AlgorithmsAreCool">@AlgorithmsAreCool</a> reduces thread contention involved in starting and stopping an <code>Activity</code>. <code>ActivitySource</code> maintains a thread-safe list of listeners, which only changes on the rare occasion that a listener is registered or unregistered. Any time an <code>Activity</code> is created or destroyed (which can happen at very high frequency), each listener gets notified, which requires walking through the list of listeners. The previous code used a lock to protect that listeners list, and to avoid notifying the listener while holding the lock, the implementation would take the lock, determine the next listener, release the lock, notify the listener, and rinse and repeat until it had notified all listeners. This could result in significant contention, as multiple threads started and stopped <code>Activity</code>s. Now with this PR, the list switches to be an immutable array. Each time the list changes, a new array is created with the modified set of listeners. This makes the act of changing the listeners list much more expensive, but, as noted, that&rsquo;s generally a rarity. And in exchange, notifying listeners becomes much cheaper.</p><p><a href="https://github.com/dotnet/runtime/pull/117334">dotnet/runtime#117334</a> from <a href="https://github.com/petrroll">@petrroll</a> avoids the overheads of callers needing to interact with null loggers by excluding them in <code>LoggerFactory.CreateLoggers</code>, while <a href="https://github.com/dotnet/runtime/pull/117342">dotnet/runtime#117342</a> seals the <code>NullLogger</code> type so type checks against <code>NullLogger</code> (e.g. <code>if (logger is NullLogger</code>) can be made more efficient by the JIT. And <a href="https://github.com/dotnet/roslyn-analyzers/pull/7290">dotnet/roslyn-analyzers#</a> from <a href="https://github.com/mpidash">@mpidash</a> will help developers to realize that their logging operations aren&rsquo;t as cheap as they thought they might be. Consider this code:</p><pre><code>[LoggerMessage(Level = LogLevel.Information, Message = "This happened: {Value}")] private static partial void Oops(ILogger logger, string value); public static void UnexpectedlyExpensive() { Oops(NullLogger.Instance, $"{Guid.NewGuid()} {DateTimeOffset.UtcNow}"); }</code></pre><p>It&rsquo;s using the logger source generator, which will emit an implementation dedicated to this log method, including a log level check so that it doesn&rsquo;t pay the bulk of the costs associated with logging unless the associated level is enabled:</p><pre><code>[global::System.CodeDom.Compiler.GeneratedCodeAttribute("Microsoft.Extensions.Logging.Generators", "6.0.5.2210")] private static partial void Oops(global::Microsoft.Extensions.Logging.ILogger logger, global::System.String value) { if (logger.IsEnabled(global::Microsoft.Extensions.Logging.LogLevel.Information)) { __OopsCallback(logger, value, null); } }</code></pre><p>Except, the call site is doing non-trivial work, creating a new <code>Guid</code>, fetching the current time, and allocating a string via string interpolation, even though it might be wasted work if <code>LogLevel.Information</code> isn&rsquo;t available. This CA1873 analyzer flags that: <img src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2025/09/CA1873.png" alt="Analyzer for expensive logging sites"></p><h2>Cryptography</h2><p>A ton of effort went into cryptography in .NET 10, almost entirely focused on post&#8209;quantum cryptography (PQC). PQC refers to a class of cryptographic algorithms designed to resist attacks from quantum computers, machines that could one day render classic cryptographic algorithms like Rivest&ndash;Shamir&ndash;Adleman (RSA) or Elliptic Curve Cryptography (ECC) insecure by efficiently solving problems such as integer factorization and discrete logarithms. With the looming threat of &ldquo;harvest now, decrypt later&rdquo; attacks (where a well-funded attacker idly captures encrypted internet traffic, expecting that they&rsquo;ll be able to decrypt and read it later) and the multi-year process required to migrate critical infrastructure, the transition to quantum&#8209;safe cryptographic standards has become an urgent priority. In this light, .NET 10 adds support for ML-DSA (a National Institute of Standards and Technology PQC digital signature algorithm), Composite ML-DSA (a draft Internet Engineering Task Force specification for creating signatures that combine ML-DSA with a classical crypto algorithm like RSA), SLH-DSA (another NIST PQC signature algorithm), and ML-KEM (a NIST PQC key encapsulation algorithm). This is an important step towards quantum-resistant security, enabling developers to begin experimenting with and planning for post-quantum identity and authenticity scenarios. While this PQC effort is not about performance, the design of them is very much focused on more modern sensibilities that have performance as a key motivator. While older types, like those that derive from <code>AsymmetricAlgorithm</code>, are design around arrays, with support for spans tacked on later, the new types are design with spans at the center, and with array-based APIs available only for convenience.</p><p>There are, however, some cryptography-related changes in .NET 10 that are focused squarely on performance. One is around improving OpenSSL &ldquo;digest&rdquo; performance. .NET&rsquo;s cryptography stack is built on top of the underlying platform&rsquo;s native cryptographic libraries; on Linux, that means using OpenSSL, making it a hot path for common operations like hashing, signing, and TLS. &ldquo;Digest algorithms&rdquo; are the family of cryptographic hash functions (for example, SHA&#8209;256, SHA&#8209;512, SHA&#8209;3) that turn arbitrary input into fixed&#8209;size fingerprints; they&rsquo;re used all of the place, from verifying packages to TLS handshakes to content de-duplication. While .NET can use OpenSSL 1.x if that&rsquo;s what&rsquo;s offered by the OS, since .NET 6 it&rsquo;s been focusing more and more on optimizing for and lighting-up with OpenSSL 3 (the previously-discussed PQC support requires OpenSSL 3.5 or later). With OpenSSL 1.x, OpenSSL exposed getter functions like <code>EVP_sha256()</code>, which were cheap functions that just returned a direct pointer to the <code>EVP_MD</code> for the relevant hash implementation. OpenSSL 3.x introduced a provider model, with a fetch function (<code>EVP_MD_fetch</code>) for retrieving the provider-backed implementation. To keep source compatibility, the 1.x-era getter functions were changed to return pointers to compatibility shims: when you pass one of these legacy <code>EVP_MD</code> pointers into operations like <code>EVP_DigestInit_ex</code>, OpenSSL performs an &ldquo;implicit fetch&rdquo; under the covers to resolve the actual implementation. That implicit fetch path adds extra work, on each use. Instead, OpenSSL recommends consumers do an explicit fetch and then cache the result for reuse. That&rsquo;s what <a href="https://github.com/dotnet/runtime/pull/118613">dotnet/runtime#118613</a> does. The result is leaner and faster cryptographic hash operations on OpenSSL&#8209;based platforms.</p><pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Security.Cryptography; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private byte[] _src = new byte[1024]; private byte[] _dst = new byte[SHA256.HashSizeInBytes]; [GlobalSetup] public void Setup() =&gt; new Random(42).NextBytes(_src); [Benchmark] public void Hash() =&gt; SHA256.HashData(_src, _dst); }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Hash</td> <td>.NET 9.0</td> <td>1,206.8 ns</td> <td>1.00</td> </tr> <tr> <td>Hash</td> <td>.NET 10.0</td> <td>960.6 ns</td> <td>0.80</td> </tr> </tbody> </table><p>A few other performance niceties have also found their way in.</p><ul> <li><strong><code>AsnWriter.Encode</code></strong>. <a href="https://github.com/dotnet/runtime/pull/106728">dotnet/runtime#106728</a> and <a href="https://github.com/dotnet/runtime/pull/112638">dotnet/runtime#112638</a> add and then use throughout the crypto stack a callback-based mechanism to <code>AsnWriter</code> that enables encoding without forced allocation for the temporary encoded state.</li> <li><strong><code>SafeHandle</code> singleton</strong>. <a href="https://github.com/dotnet/runtime/pull/109391">dotnet/runtime#109391</a> employs a singleton <code>SafeHandle</code> in more places in <code>X509Certificate</code> to avoid temporary handle allocation.</li> <li><strong>Span-based <code>ProtectedData</code></strong>. <a href="https://github.com/dotnet/runtime/pull/109529">dotnet/runtime#109529</a> from <a href="https://github.com/ChadNedzlek">@ChadNedzlek</a> adds <code>Span<byte></byte></code>-based overloads to the <code>ProtectedData</code> class that enable protecting data without requiring the source or destinations to be in allocated arrays.</li> <li><strong><code>PemEncoding</code> UTF-8</strong>. <a href="https://github.com/dotnet/runtime/pull/109438">dotnet/runtime#109438</a> adds UTF-8 support to <code>PemEncoding</code>. <code>PemEncoding</code>, a utility class for parsing and formatting PEM (Privacy-Enhanced Mail)-encoded data such as that used in certificates and keys, previously worked only with <code>char</code>s. As was then done in <a href="https://github.com/dotnet/runtime/pull/109564">dotnet/runtime#109564</a>, this change makes it possible to parse UTF8 data directly without first needing to transcode to UTF16.</li> <li><strong><code>FindByThumbprint</code></strong>. <a href="https://github.com/dotnet/runtime/pull/109130">dotnet/runtime#109130</a> adds an <code>X509Certification2Collection.FindByThumbprint</code> method. The implementation uses a stack-based buffer for the thumbprint value for each candidate certificate, eliminating the arrays that would otherwise be created in a naive manual implementation. <a href="https://github.com/dotnet/runtime/pull/113606">dotnet/runtime#113606</a> then utilized this in <code>SslStream</code>.</li> <li><strong><code>SetKey</code></strong> <a href="https://github.com/dotnet/runtime/pull/113146">dotnet/runtime#113146</a> adds a span-based <code>SymmetricAlgorithm.SetKey</code> method which can then be used to avoid creating unnecessary arrays.</li> </ul> <h2>Peanut Butter</h2><p>As in every .NET release, there are a large number of PRs that help with performance in some fashion. The more of these that are addressed, the more the overall overhead for applications and services is lowered. Here are a smattering from this release:</p><ul> <li><strong>GC</strong>. DATAS (Dynamic Adaptation To Application Sizes) was introduced in .NET 8 and enabled by default in .NET 9. Now in .NET 10, <a href="https://github.com/dotnet/runtime/pull/105545">dotnet/runtime#105545</a> tuned DATAS to improve its overall behavior, cutting unnecessary work, smoothing out pauses (especially under high allocation rates), correcting fragmentation accounting that could cause extra short collections (gen1), and other such tweaks. The net result is fewer unnecessary collections, steadier throughput, and more predictable latency for allocation-heavy workloads. <a href="https://github.com/dotnet/runtime/pull/118762">dotnet/runtime#118762</a> also adds several knobs for configuring how DATAS behaves, and in particular settings to fine-tune how Gen0 grows.</li> <li><strong>GCHandle</strong>. The GC supports various types of &ldquo;handles&rdquo; that allow for explicit management of resources in relation to GC operation. For example, you can create a &ldquo;pinning handle,&rdquo; which ensures that the GC will not move the object in question. Historically, these handles were surfaced to developers via the <code>GCHandle</code> type, but it has a variety of issues, including that it&rsquo;s really easy to misuse due to lack of strong typing. To help address that, <a href="https://github.com/dotnet/runtime/pull/111307">dotnet/runtime#111307</a> introduces a few new strongly-typed flavors of handles, with <code>GCHandle<t></t></code>, <code>PinnedGCHandle<t></t></code>, and <code>WeakGCHandle<t></t></code>. These should not only address some of the usability issues, they&rsquo;re also able to shave off a bit of the overheads incurred by the old design.<pre><code>// dotnet run -c Release -f net10.0 --filter "*" using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using System.Runtime.InteropServices; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private byte[] _array = new byte[16]; [Benchmark(Baseline = true)] public void Old() =&gt; GCHandle.Alloc(_array, GCHandleType.Pinned).Free(); [Benchmark] public void New() =&gt; new PinnedGCHandle<byte>(_array).Dispose(); }</byte></code></pre> <table> <thead> <tr> <th>Method</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Old</td> <td>27.80 ns</td> <td>1.00</td> </tr> <tr> <td>New</td> <td>22.73 ns</td> <td>0.82</td> </tr> </tbody> </table> </li> <li><strong>Mono interpreter</strong>. The mono interpreter gained optimized support for several opcodes, including switches (<a href="https://github.com/dotnet/runtime/pull/107423">dotnet/runtime#107423</a>), new arrays (<a href="https://github.com/dotnet/runtime/pull/107430">dotnet/runtime#107430</a>), and memory barriers (<a href="https://github.com/dotnet/runtime/pull/107325">dotnet/runtime#107325</a>). But arguably more impactful was a series of more than a dozen PRs that enabled the interpreter to vectorize more operations with WebAssembly (Wasm). This included contributions like <a href="https://github.com/dotnet/runtime/pull/114669">dotnet/runtime#114669</a>, which enabled vectorization of shift operations, and <a href="https://github.com/dotnet/runtime/pull/113743">dotnet/runtime#113743</a>, which enabled vectorization of a plethora of operations like <code>Abs</code>, <code>Divide</code>, and <code>Truncate</code>. Other PRs used the Wasm-specific intrinsic APIs in more places, in order to accelerate on Wasm routines that were already accelerated on other architectures using architecture-specific intrinsics, e.g. <a href="https://github.com/dotnet/runtime/pull/115062">dotnet/runtime#115062</a> used <code>PackedSimd</code> in the workhorse methods behind the hex conversion routines on <code>Convert</code>, like <code>Convert.FromBase64String</code>.</li> <li><strong>FCALLs</strong>. There are many places in the lower-layers of <code>System.Private.CoreLib</code> where managed code needs to call into native code in the runtime. There are two primary ways this transition from managed to native has happened, historically. One method is through what&rsquo;s called a &ldquo;QCALL&rdquo;, essentially just a DllImport (P/Invoke) into native functions exposed by the runtime. The other, which historically was the dominant mechansim, is an &ldquo;FCALL,&rdquo; which is a more complex and specialized pathway that allows direct access to managed objects from native code. FCALLs were once the standard, but over time, more of them were converted to QCALLs. This shift improves reliability (since FCALLs are notoriously tricky to implement correctly) and can also boost performance, as FCALLs require helper method frames, which QCALLs can often avoid. A ton of PRs in .NET 10 went into removing FCALLs, like <a href="https://github.com/dotnet/runtime/pull/107218">dotnet/runtime#107218</a> for helper method frames in <code>Exception</code>, <code>GC</code>, and <code>Thread</code>, <a href="https://github.com/dotnet/runtime/pull/106497">dotnet/runtime#106497</a> for helper method frames in <code>object</code>, <a href="https://github.com/dotnet/runtime/pull/107152">dotnet/runtime#107152</a> for those used in connecting to profilers, <a href="https://github.com/dotnet/runtime/pull/108415">dotnet/runtime#108415</a> and <a href="https://github.com/dotnet/runtime/pull/108535">dotnet/runtime#108535</a> for ones in reflection, and over a dozen others. In the end, all FCALLS that touched managed memory or threw exceptions were removed.</li> <li><strong>Converting hex.</strong> Recent .NET releases added methods to <code>Convert</code> like <code>FromHexString</code> and <code>TryToHexStringLower</code>, but such methods all used UTF16. <a href="https://github.com/dotnet/runtime/pull/117965">dotnet/runtime#117965</a> adds overloads of these that work with UTF8 bytes.</li> <li><strong>Formatting.</strong> String interpolation is backed by &ldquo;interpolated string handlers.&rdquo; When you interpolate with a string target type, by default you get the <code>DefaultInterpolatedStringHandler</code> that comes from <code>System.Runtime.CompilerServices</code>. That implementation is able to use stack-allocated memory and the <code>ArrayPool&lt;&gt;</code> for reduced allocation overheads as it&rsquo;s buffering up text formatted to it. While very advanced, other code, including other interpolated string handlers, can use <code>DefaultInterpolatedStringHandler</code> as an implementation detail. However, when doing so, such code only could get access to the final output as a <code>string</code>; the underlying buffer wasn&rsquo;t exposed. <a href="https://github.com/dotnet/runtime/pull/112171">dotnet/runtime#112171</a> adds a <code>Text</code> property to <code>DefaultInterpolatedStringHandler</code> for code that wants access to the already formatted text in a <code>ReadOnlySpan<char></char></code>.</li> <li><strong>Enumeration-related allocations.</strong> <a href="https://github.com/dotnet/runtime/pull/118288">dotnet/runtime#118288</a> removes a handful of allocations related to enumeration, for example removing a <code>string.Split</code> call in <code>EnumConverter</code> and replacing it with a <code>MemoryExtensions.Split</code> call that doesn&rsquo;t need to allocate either the <code>string[]</code> or the individual <code>string</code> instances.</li> <li><strong>NRBF decoding.</strong> <a href="https://github.com/dotnet/runtime/pull/107797">dotnet/runtime#107797</a> from <a href="https://github.com/teo-tsirpanis">@teo-tsirpanis</a> removes an array allocation used in a <code>decimal</code> constructor call, replacing it instead with a collection expression targeting a span, which will result in the state being stack allocated.</li> <li><strong>TypeConverter allocations.</strong> <a href="https://github.com/dotnet/runtime/pull/111349">dotnet/runtime#111349</a> from <a href="https://github.com/AlexRadch">@AlexRadch</a> reduces some parsing overheads in the <code>TypeConverter</code>s for <code>Size</code>, <code>SizeF</code>, <code>Point</code>, and <code>Rectangle</code> by using more modern APIs and constructs, such as the span-based <code>Split</code> method and string interpolation.</li> <li><strong>Generic math conversions.</strong> Most of the <code>TryConvertXx</code> methods using the various primitive&rsquo;s implementations of the generic math interfaces are marked as <code>MethodImplOptions.AggressiveInlining</code>, to help the JIT realize they should always be inlined, but a few stragglers were left out. <a href="https://github.com/dotnet/runtime/pull/112061">dotnet/runtime#112061</a> from <a href="https://github.com/hez2010">@hez2010</a> fixes that.</li> <li><strong>ThrowIfNull.</strong> C# 14 now supports the ability to write extension static methods. This is a huge boon for libraries that need to support downlevel targeting, as it means static methods can be polyfilled just as instance methods can be. There are many libraries in .NET that build not only for the latest runtimes but also for .NET Standard 2.0 and .NET Framework, and those libraries have been unable to use helper static methods like <code>ArgumentNullException.ThrowIfNull</code>, which can help to streamline call sites and make methods more inlineable (in addition, of course, to tidying up the code). Now that the dotnet/runtime repo builds with a C# 14 compiler, <a href="https://github.com/dotnet/runtime/pull/114644">dotnet/runtime#114644</a> replaced ~2500 call sites in such libraries with use of a <code>ThrowIfNull</code> polyfill.</li> <li><strong>FileProvider Change Tokens</strong>. <a href="https://github.com/dotnet/runtime/pull/116175">dotnet/runtime#116175</a> reduces allocation in <code>PollingWildCardChangeToken</code> by using allocation-free mechanisms for computing hashes, while <a href="https://github.com/dotnet/runtime/pull/115684">dotnet/runtime#115684</a> from <a href="https://github.com/rameel">@rameel</a> reduces allocation in <code>CompositeFileProvider</code> by avoiding taking up space for nop <code>NullChangeToken</code>s.</li> <li><strong>String interpolation.</strong> <a href="https://github.com/dotnet/runtime/pull/114497">dotnet/runtime#114497</a> removes a variety of null checks when dealing with nullable inputs, shaving off some overheads of the interpolation operation.<pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { private string _value = " "; [Benchmark] public string Interpolate() =&gt; $"{_value} {_value} {_value} {_value}"; }</code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> </tr> </thead> <tbody> <tr> <td>Interpolate</td> <td>.NET 9.0</td> <td>34.21 ns</td> <td>1.00</td> </tr> <tr> <td>Interpolate</td> <td>.NET 10.0</td> <td>29.47 ns</td> <td>0.86</td> </tr> </tbody> </table> </li> <li><strong><code>AssemblyQualifiedName</code></strong>. <code>Type.AssemblyQualifiedName</code> previously recomputed the result on every access. As of <a href="https://github.com/dotnet/runtime/issues/118389">dotnet/runtime#118389</a>, it&rsquo;s now cached.<pre><code>// dotnet run -c Release -f net9.0 --filter "*" --runtimes net9.0 net10.0 using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; BenchmarkSwitcher.FromAssembly(typeof(Tests).Assembly).Run(args); [MemoryDiagnoser(displayGenColumns: false)] [HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")] public partial class Tests { [Benchmark] public string AQN() =&gt; typeof(Dictionary<int>).AssemblyQualifiedName!; }</int></code></pre> <table> <thead> <tr> <th>Method</th> <th>Runtime</th> <th>Mean</th> <th>Ratio</th> <th>Allocated</th> <th>Alloc Ratio</th> </tr> </thead> <tbody> <tr> <td>AQN</td> <td>.NET 9.0</td> <td>132.345 ns</td> <td>1.007</td> <td>712 B</td> <td>1.00</td> </tr> <tr> <td>AQN</td> <td>.NET 10.0</td> <td>1.218 ns</td> <td>0.009</td> <td>&ndash;</td> <td>0.00</td> </tr> </tbody> </table> </li> </ul> <h2>What&rsquo;s Next?</h2><p>Whew! After all of that, I hope you&rsquo;re as excited as I am about .NET 10, and more generally, about the future of .NET.</p><p>As you&rsquo;ve seen in this tour (and in those for previous releases), the story of .NET performance is one of relentless iteration, systemic thinking, and the compounding effect of many targeted improvements. While I&rsquo;ve highlighted micro-benchmarks to show specific gains, the real story isn&rsquo;t about these benchmarks&hellip; it&rsquo;s about making real-world applications more responsive, more scalable, more sustainable, more economical, and ultimately, more enjoyable to build and use. Whether you&rsquo;re shipping high-throughput services, interactive desktop apps, or resource-constrained mobile experiences, .NET 10 offers tangible performance benefits to you and your users.</p><p>The best way to appreciate these improvements is to try <a href="https://dotnet.microsoft.com/download/dotnet/10.0">.NET 10 RC1</a> yourself. Download it, run your workloads, measure the impact, and share your experiences. See awesome gains? Find a regression that needs fixing? Spot an opportunity for further improvement? Shout it out, open an issue, even send a PR. Every bit of feedback helps make .NET better, and we look forward to continuing to build with you.</p><p>Happy coding!</p></div></article><div class="gallery"><p><img src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2025/09/CA2024.png"></p><p><img src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2025/09/CA1874.png"></p></div></section>]]></description><pubDate>Wed, 10 Sep 2025 19:15:08 +0530</pubDate></item><item><link>https://www.reddit.com/r/java/comments/1nckdwr/jep_401_value_classes_and_objects_preview_has/</link><title>JEP 401: Value classes and Objects (Preview) has just been submitted! (/r/programming)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nd8vob/jep_401_value_classes_and_objects_preview_has/</guid><comments>https://www.reddit.com/r/programming/comments/1nd8vob/jep_401_value_classes_and_objects_preview_has/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/programming/comments/1nd8vob/jep_401_value_classes_and_objects_preview_has/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>The JDK it is coming out in is still not known. However, this is a major milestone to have crossed. Plus, a new Early Access build of Valhalla (up-to-date with the current JDK, presumably) will go live soon too. Details in the linked post.</p><p>And for those unfamiliar, <a href="https://www.reddit.com/u/brian_goetz">u/brian_goetz</a> is the person leading the Project Valhalla effort. So, comments by him in the linked post can help you separate between assumptions by your average user vs the official words from the Open JDK Team themselves. <a href="https://www.reddit.com/u/pron98">u/pron98</a> is another OpenJDK Team member commenting in the linked post.</p></div><!-- SC_ON --></section><section class='separator separator-after-selftext'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><div><p><span> <faceplate-tracker> <a href="https://www.reddit.com/r/java/">r/java</a> </faceplate-tracker> </span></p><faceplate-tracker> <shreddit-join-button></shreddit-join-button> </faceplate-tracker> </div><p>News, Technical discussions, research papers and assorted things of interest related to the Java programming language NO programming help, NO learning Java related questions, NO installing or downloading Java questions, NO JVM languages - Exclusively Java </p><hr><p><span> <span>Members</span> </span> <span> <span> <span>Online</span> </span> </span> </p></div><div class="gallery"><p><img src="https://styles.redditmedia.com/t5_2qhd7/styles/communityIcon_tzuj58g8ax451.png?width=96&amp;height=96&amp;frame=1&amp;auto=webp&amp;crop=96%3A96%2Csmart&amp;s=75ed46fab979da9835d3c7bcda4ee5b3d4ec4a88"></p></div></section>]]></description><pubDate>Wed, 10 Sep 2025 14:16:19 +0530</pubDate></item><item><link>https://tylercipriani.com/blog/2022/11/19/git-notes-gits-coolest-most-unloved-feature/</link><title>Git Notes: git's coolest, most unloved­ feature (tylercipriani.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nd8nsi/git_notes_gits_coolest_most_unloved_feature/</guid><comments>https://www.reddit.com/r/programming/comments/1nd8nsi/git_notes_gits_coolest_most_unloved_feature/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 6 min | <a href='https://www.reddit.com/r/programming/comments/1nd8nsi/git_notes_gits_coolest_most_unloved_feature/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Did YOU know...? And if you did, what do you use it for?</p></div><!-- SC_ON --></section><section class='separator separator-after-selftext'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><article> <section> <blockquote><p>the short of it is: they&rsquo;re cool for appending notes from automated systems (like ticket or build systems) but not really for having interactive conversations with other developers (at least not yet)</p><p>&ndash; Scott Chacon, <a href="https://github.blog/2010-08-25-git-notes-display/">GitHub.blog</a>, Aug.&nbsp;2010</p></blockquote><p>Git notes are almost a secret.</p><p>They&rsquo;re buried by their own distressing usability.</p><p>But git notes are continually rediscovered by engineers trying to stash metadata inside git.</p><figure> <img src="https://photos.tylercipriani.com/2022-10-30_simonw-git-notes.png" alt="Sun, 30 Oct 2022 11:05 @simonw"> </figure><p><strong>Git notes are powerful tools.</strong> And they could solve so many problems&mdash;if only they were better known and easier to use.</p><section> <h2><span>&#129488;</span> What are git notes?</h2><p>A common use of git notes is tacking metadata onto commits.</p><p>Once a commit cements itself in git&rsquo;s history&mdash;that&rsquo;s it. It&rsquo;s impossible to amend a commit message buried deep in a repo&rsquo;s log<a href="https://tylercipriani.com#fn1"><sup>1</sup></a>.</p><p>But git notes enable you to amend new information about old commits in a special namespace. And they&rsquo;re capable of so much more.</p><p><strong>Notes stow metadata about anything tracked by git</strong>&mdash;any object: commits, blobs, and trees. All without futzing with the object itself.</p><p>You add notes to the latest commit in a repo like this:</p><pre><code>git notes add -m 'Acked-by: <tyler>'</tyler></code></pre><p>And then it shows up in <code>git log</code>:</p><pre><code>commit 1ef8b30ab7fc218ccc85c9a6411b1d2dd2925a16 Author: Tyler Cipriani <thcipriani> Date: Thu Nov 17 16:51:43 2022 -0700 Initial commit Notes: Acked-by: <tyler></tyler></thcipriani></code></pre> </section> <section> <h2><span>&#129406;</span> Git notes in the wild</h2><p>The git project itself offers an example of git notes in the wild. They link each commit to its discussion on their mailing list.</p><p>For example:</p><pre><code>commit 00f09d0e4b1826ee0519ea64e919515032966450 Author: <redacted> Date: Thu Jan 28 02:05:55 2010 +0100 bash: support 'git notes' and its subcommands ... Notes (amlog): Message-Id: &lt;1264640755-22447-1-git-send-email-szeder@ira.uka.de&gt;</redacted></code></pre><p>This commit&rsquo;s notes point intrepid users to the <a href="https://lore.kernel.org/git/1264640755-22447-1-git-send-email-szeder@ira.uka.de/">thread where this patch was discussed</a>.</p><p>Other folks are using notes for things like:</p><ul> <li>Tracking time spent per commit or branch</li> <li>Adding review and testing information to git log</li> <li>And even fully distributed code review</li> </ul> </section> <section> <h2><span>&#128230;</span> Storing code reviews and test results in git notes</h2><p>Here is a plea for all forges: make code review metadata available offline, inside git.</p><p>The <a href="https://gerrit.googlesource.com/plugins/reviewnotes/+/refs/heads/master/src/main/resources/Documentation/refs-notes-review.md">reviewnotes</a> plugin for Gerrit<a href="https://tylercipriani.com#fn2"><sup>2</sup></a> is an example of how to do this well. It makes it easy to see who reviewed code in git log:</p><pre><code>git fetch origin refs/notes/review:refs/notes/review git log --show-notes=review</code></pre><p>The command above shows me all the standard git log info alongside information about what tests ran and who reviewed the code. All without forcing me into my browser.</p><pre><code>commit d1d17908d2a97f057887a4afbd99f6c40be56849 Author: User <user> Date: Sun Mar 27 18:10:51 2022 +0200 Change the thing Notes (review): Verified+1: SonarQube Bot Verified+2: jenkins-bot Code-Review+2: Reviewer Human <reviewerhuman> Submitted-by: jenkins-bot Submitted-at: Tue, 14 Jun 2022 21:59:58 +0000 Reviewed-on: https://gerrit.wikimedia.org/r/c/mediawiki/core/+/774005 Project: mediawiki/core Branch: refs/heads/master</reviewerhuman></user></code></pre> </section> <section> <h2><span>&#128160;</span> Distributed code review <u>inside</u> git notes</h2><p>Motivated hackers can knead and extend git notes. Using them as distributed storage for any madcap idea.</p><p>Someone at Google cobbled together a full-on code review system teetering atop git notes called <a href="https://github.com/google/git-appraise">git-appraise</a>.</p><p>Its authors have declared it a &ldquo;fully distributed code review&rdquo;&mdash;independent of GitHub, GitLab, or any other code forge.</p><p>This system lets you:</p><ul> <li>Request review of a change</li> <li>Comment on a change</li> <li>Review and merge a change</li> </ul><p>And you can do all this from your local computer, even if GitHub is down.</p><p>Plus, it&rsquo;s equipped with an affectedly unaesthetic web interface, if that&rsquo;s your thing.</p><figure> <img src="https://photos.tylercipriani.com/2022-11-27_git-appraise-web.png" alt="The git-appraise web interface, in all its NaN-line-numbering glory."> </figure> </section> <section> <h2><span>&#128557;</span> No one uses git notes</h2><p>Git notes are a pain to use.</p><p>And GitHub <a href="https://github.blog/2010-08-25-git-notes-display/">opted to stop displaying commit notes in 2014</a> without much explanation.</p><p>For commits, you can make viewing and adding notes easier using fancy options in your gitconfig<a href="https://tylercipriani.com#fn3"><sup>3</sup></a>. But for storing notes about blobs or trees? Forget it. You&rsquo;d need to be comfortable rooting around in git&rsquo;s <a href="https://git-scm.com/book/en/v2/Git-Internals-Plumbing-and-Porcelain">plumbing</a> first.</p><p>So, for now: <strong>git notes are relegated to obscurity</strong>. Forever hamstrung by an obscure and clunky interface and limited adoption&mdash;I often forget they&rsquo;re there.</p></section> <section> <h2><span>&#128509;</span> Forge independence</h2><p>Git is a distributed code review system. But much of the value of git repos ends up locked into forges, like GitHub.</p><p>Git notes are a path toward an alternative.</p><p>Git distributes the history of a piece of code. <strong>Git notes could make it possible to distribute the history of an entire project.</strong></p></section> <section> <hr> <ol> <li><p>Without having to endure the <a href="https://groups.google.com/g/jenkinsci-dev/c/-myjRIPcVwU/m/mrwn8VkyXagJ">perils of a force push</a>, anyway.<a href="https://tylercipriani.com#fnref1">&#8617;&#65038;</a></p></li> <li><p>The code review system used for <a href="https://gerrit.wikimedia.org/r/">a</a> <a href="https://go-review.googlesource.com/">couple</a> of <a href="https://android-review.googlesource.com/">bigish</a> <a href="https://chromium-review.googlesource.com/">projects</a>.<a href="https://tylercipriani.com#fnref2">&#8617;&#65038;</a></p></li> <li><p>Noteably by automagically fetching notes and displaying them in <code>git log</code> via:</p><pre><code>$ git config --add \ remote.origin.fetch \ '+refs/notes/*:refs/notes/*' $ git config \ notes.displayRef \ 'refs/notes/*'</code></pre> <a href="https://tylercipriani.com#fnref3">&#8617;&#65038;</a></li> </ol> </section> </section> </article> </section>]]></description><pubDate>Wed, 10 Sep 2025 14:01:55 +0530</pubDate></item><item><link>https://github.com/Voultapher/sort-research-rs/blob/main/writeup/unreasonable/text.md</link><title>The unreasonable effectiveness of modern sort algorithms (github.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nd7bby/the_unreasonable_effectiveness_of_modern_sort/</guid><comments>https://www.reddit.com/r/programming/comments/1nd7bby/the_unreasonable_effectiveness_of_modern_sort/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 3 min | <a href='https://www.reddit.com/r/programming/comments/1nd7bby/the_unreasonable_effectiveness_of_modern_sort/'>Post permalink</a></p></section><section class='preview-image'><img src='https://opengraph.githubassets.com/88524f7c8a01d0d4944a3a40a78142db7ebeb7081d3e8a8aa8381149d161c5fd/Voultapher/sort-research-rs' /></section><section class='parsed-content'><div><qbsearch-input><div><dialog-helper> <dialog><div><h2> Provide feedback </h2> </div><scrollable-region><div><form><p>We read every piece of feedback, and take your input very seriously.</p><label>Include my email address so I can be contacted</label> </form></div></scrollable-region> </dialog></dialog-helper> <custom-scopes> <dialog-helper> <dialog><div><h2> Saved searches </h2> <h2>Use saved searches to filter your results more quickly</h2> </div><scrollable-region> </scrollable-region> </dialog></dialog-helper> </custom-scopes> </div></qbsearch-input><div><p><a href="/login?return_to=https%3A%2F%2Fgithub.com%2FVoultapher%2Fsort-research-rs%2Fblob%2Fmain%2Fwriteup%2Funreasonable%2Ftext.md"> Sign in </a> </p></div><p><a href="https://github.com/signup?ref_cta=Sign+upref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=Voultapher%2Fsort-research-rs">/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out"}"&gt; Sign up </repo-name></a></p><p><react-partial-anchor> <tool-tip>Appearance settings</tool-tip><template> <react-partial> </react-partial> </template> </react-partial-anchor> </p></div></section>]]></description><pubDate>Wed, 10 Sep 2025 12:31:35 +0530</pubDate></item><item><link>https://medium.com/@ozdemir.zynl/beyond-the-code-lessons-that-make-you-senior-1ba44469aa42?source=friends_link&amp;sk=b26d67b2b81fe10a800da07bd3415931</link><title>Beyond the Code: Lessons That Make You Senior Software Engineer (medium.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1ncx9gw/beyond_the_code_lessons_that_make_you_senior/</guid><comments>https://www.reddit.com/r/programming/comments/1ncx9gw/beyond_the_code_lessons_that_make_you_senior/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 14 min | <a href='https://www.reddit.com/r/programming/comments/1ncx9gw/beyond_the_code_lessons_that_make_you_senior/'>Post permalink</a></p></section><section class='preview-image'><img src='https://miro.medium.com/v2/resize:fit:1020/1*br0DCLdFNA9EX2KqYGtPvQ.png' /></section><section class='parsed-content'><div><figure><div><p><span>Press enter or click to view image in full size</span></p></div></figure><p>Five years ago, I applied for a Senior Software Engineer role. I wasn&rsquo;t entirely sure if I wanted the job, but the product caught my interest. After passing the online assessment, I was invited to the first round of interviews &mdash; and to my surprise, the CTO was on the attendee list.</p><p>The meeting began with quick introductions, and then the CTO took the lead. She didn&rsquo;t waste time: <em>&ldquo;We think your experience isn&rsquo;t enough for a Senior role, but we still wanted to see how you would do.&rdquo;</em> I was a little offended. I was young, but I had already written a lot of code and worked at several companies. This felt like my chance to prove her wrong.</p><p>I braced myself for tough technical questions &mdash; dynamic programming, tricky data structures, maybe some bit manipulation. Instead, the first question was: <em>&ldquo;What is technical debt?&rdquo;</em> I froze. I gave a one-line textbook answer and waited, expecting the real test to begin. But she pressed on: <em>&ldquo;What do you do when deadlines are missed? How do you prioritize tasks? How do you handle conflict in a team?&rdquo;</em> I couldn&rsquo;t hide my frustration. Where were the hard technical problems, the system designs, the algorithm puzzles?</p><p>After the interview, I was convinced she didn&rsquo;t know how to evaluate engineers. But looking back now, I realize she knew exactly what she was doing. She wasn&rsquo;t hiring just a coder. She was looking for someone who could navigate trade-offs, lead people, and think beyond the code itself. I wasn&rsquo;t ready then. But that conversation left me with a lasting realization: <strong>being &ldquo;senior&rdquo; isn&rsquo;t about mastering every protocol or memorizing every algorithm. It&rsquo;s about judgment, working with people, making decisions, thinking long-term, and developing the subtle skills nobody teaches you in school.</strong></p><p>Over the years, I&rsquo;ve made mistakes, worked with incredible teams, and learned lessons the hard way. I once purged a production database. I once argued for days about a design that never shipped. I once ignored a teammate&rsquo;s warning and caused a production crash. Each mistake left a mark, but also a lesson. <strong>Today, I want to share the ones that, looking back, mattered most on the path to becoming a Senior Engineer.</strong></p><p><em>To illustrate these lessons, I&rsquo;ll use a recurring character &mdash; Eddie. He isn&rsquo;t a real person or someone I&rsquo;ve worked with, but a fictional stand-in inspired by my younger self and the situations I&rsquo;ve experienced. Eddie reflects the mistakes I made and the lessons that shaped me.</em></p></div><div><h2>Reasons Over Rules</h2><p>Once, we needed to add new fields to a DynamoDB table in production. Eddie, a bright new joiner, proposed a heavyweight migration &mdash; replicate the table, sync with streams, then cut over after a week of testing. The plan sounded complete, but he hadn&rsquo;t considered simpler options, so I asked, &ldquo;Why not update the existing records with a script?&rdquo; His answer was blunt: &ldquo;That is dangerous. You shouldn&rsquo;t update a prod database&rdquo;, with no reasoning or data.</p><p>Here&rsquo;s the thing: in that specific system, the database didn&rsquo;t serve customer-facing traffic; it powered offline jobs. We could throttle updates, monitor metrics, and roll back if necessary, so the real risk was low compared to the complexity of his plan. We built a simple background updater instead, finished in days, not weeks, and it worked.</p><p>Eddie&rsquo;s approach reflected a common trap: treating &ldquo;best practices&rdquo; as absolute truths. <em>Never touch production. Microservices are always better. Monoliths are bad.</em> Rules like these sound safe, but in reality, they are context-dependent. <strong>True seniority isn&rsquo;t about having a larger catalog of rules. It&rsquo;s about reasoning. Always try to explain why a decision makes sense in a given situation and weigh risks against trade-offs.</strong></p><h2>Don&rsquo;t have assumptions, verify</h2><p>One of the most humbling lessons of my career came during outages. When systems fail, it&rsquo;s easy to jump to conclusions: <em>&ldquo;It must be the database,&rdquo; &ldquo;The new deployment caused it,&rdquo; &ldquo;It&rsquo;s probably transient.&rdquo;</em> But assumptions waste precious hours &mdash; and when production is burning, every minute counts.</p><p>I once spent half a day convinced a feature flag rollout had broken our service. I rolled it back, combed through the code, and chased dead ends. Only later did we uncover the real cause: a dependency update shipped in the same deployment. My tunnel vision had cost the team valuable time.</p><p><strong>The best engineers don&rsquo;t rely on guesswork. They trust data, and use their experience to guide where to look first &mdash; but they always verify.</strong> The real skill isn&rsquo;t having the &ldquo;right hunch&rdquo;; it&rsquo;s having the discipline to check logs, compare graphs, reproduce issues, and ask teammates for sanity checks. Verification solves problems faster and builds credibility. Nothing erodes trust quicker than confidently blaming the wrong thing.</p><h2>Question the Good News</h2><p>Eddie once rushed to my desk, excited: &ldquo;our API latency had dropped 20% after my recent changes&rdquo; I was surprised because his task was just a simple refactor &mdash; it shouldn&rsquo;t have affected latency at all. On the surface, it looked like a win. But when something looks too good to be true, it often is. Instead of celebrating, I asked him, &ldquo;Why did it drop?&rdquo; He was so focused on the positive outcome that he hadn&rsquo;t stopped to question it.</p><p>After digging deeper, we discovered the real story: he had accidentally removed retry logic in our downstream calls. (Yes, it even passed through PR reviews.) The latency improvement wasn&rsquo;t a performance boost, it was because failed requests weren&rsquo;t being retried. Behind the shiny numbers, silent failures were piling up, until alarms went off the next day.</p><p>The bigger lesson here is: always treat sudden miracles with skepticism. When metrics improve overnight, the first question should always be, <em>&ldquo;What did we break?&rdquo;</em> <strong>Healthy skepticism prevents teams from celebrating false wins. It reinforces an essential truth: data is only valuable when you understand the <em>why</em> behind it.</strong></p><h2>Mechanism over good intentions</h2><p>In engineering, I&rsquo;ve lost count of how many times I&rsquo;ve heard, <em>&ldquo;We&rsquo;ll just remember to do X next time.&rdquo;</em> It always comes from good intentions, but good intentions don&rsquo;t protect systems. People get distracted, tired or even pressured by deadlines. I once saw a critical incident triggered because someone forgot to run a post-deploy script. It wasn&rsquo;t that the person was careless; but the process was fragile.</p><p><strong>The strongest teams don&rsquo;t rely on memory or promises. They build mechanisms to make mistakes less likely and recover faster. </strong>Automated checks, CI/CD pipelines, feature flags, two-person approvals, monitoring alerts &mdash; these aren&rsquo;t &ldquo;nice to haves,&rdquo; they are guardrails. Instead of hoping someone remembers to double-check a config, you design the pipeline so the config can&rsquo;t be deployed without validation. Instead of trusting someone to manually run a script, you make the script part of the deployment flow.</p><p>Senior engineers design systems where the safe path is also the easiest path. Mistakes will happen, that&rsquo;s inevitable. What defines a strong engineering culture is whether your system catches those mistakes before your customers do.</p><h2>The Discipline of Saying &ldquo;No&rdquo;</h2><p>Eddie once joined a meeting with one of our clients. They were using our APIs for a critical system and wanted to introduce a new filtering logic that required pulling data from multiple sources. Instead of implementing the logic on their side, they proposed offloading it to our backend. After a long discussion, they convinced Eddie to agree. The plan was that they would contribute the changes as an away team, but the logic would still live in our systems.</p><p>When Eddie shared the outcome the next day, every senior engineer on the team pushed back. The clients didn&rsquo;t have strong reasons for not making the change on their side, they simply wanted to shift the complexity onto ours. Eddie had to go back, explain the decision, and clarify that the proposal wasn&rsquo;t accepted. The back-and-forth caused delays and confusion, but it also left him with an important lesson: sometimes the hardest but most valuable thing you can do is say &ldquo;no.&rdquo;</p><p>I&rsquo;ll admit, this has been one of the toughest lessons in my own career as well.<strong> Senior engineers aren&rsquo;t the ones who say &ldquo;yes&rdquo; to everything. They are the ones who protect their teams from unnecessary complexity, push back when trade-offs don&rsquo;t make sense, and know that not every request deserves to be accepted.</strong></p><h2>Growth Starts With Ownership</h2><p>Early in my career, I leaned heavily on senior teammates. Whenever a decision came up, I defaulted to them, thinking, <em>&ldquo;They know better than me.&rdquo;</em> It felt safe. I had to be certain and confident before making the decision.</p><p>But the truth is, engineering is full of uncertainty. Rarely do we have perfect information, and waiting for it leads to paralysis.<strong> Seniors aren&rsquo;t people who always have the right answer; they are people who make informed calls, weigh trade-offs, and take responsibility</strong>. When those decisions turn out to be wrong, they don&rsquo;t deflect blame. Instead, they own the outcome, learn, and adjust.</p><h2>From Protector to Mentor</h2><p>When I first began mentoring juniors like Eddie, my instinct was to shield them from failure. I&rsquo;d leave long review comments, walk them through every design detail, or even step in and write the code myself. It felt efficient in the moment, but it slowed their growth. Protecting someone from mistakes may prevent short-term pain, but it also deprives them of the lessons that only come from firsthand experience.</p><p>With time, I realized my role wasn&rsquo;t to prevent errors but to create a safe space where mistakes could happen without catastrophic consequences. Real growth comes from stumbling, recovering, and carrying those lessons forward. <strong>The ultimate measure of leadership is whether the team can thrive without you. If you&rsquo;ve built a culture where people learn through safe failure, then you&rsquo;ve done your job as a mentor.</strong></p><h2>Simplicity Scales, Complexity Breaks</h2><p>One of the hardest lessons for ambitious engineers to learn is that simple almost always beats clever. In uncertain situations, it is tempting to design for every possible scenario, &ldquo;future-proofing&rdquo; the system with abstractions and extensibility. But more often than not, those extra layers turn into dead weight.</p><p>I once watched a team build a sophisticated plugin framework for &ldquo;future integrations.&rdquo; It had its own configuration DSL and multiple extension points. Three years later, not a single plugin had ever been written! But every change in the system had to navigate around that unused complexity. What looked like foresight in the moment turned into drag over time.</p><p><strong>Try to avoid premature complexity at all costs. Simple solutions aren&rsquo;t just easier to write; they are easier to test, maintain, and onboard new engineers onto. </strong>They age gracefully because they carry fewer assumptions.</p><h2>Every System Breaks Eventually, Be Prepared</h2><p>No system is truly fault-tolerant. Code is alive &mdash; it evolves, integrates with new dependencies, and adapts to changing requirements. Each modification, no matter how small and well tested, increases the chance of introducing failures.</p><p>I once worked on an API that returned the list of projects a user owned. We never documented any guarantees about ordering, but because of the way our SQL query was written, results always happened to come back in ascending order by creation date. Over time, some clients quietly built logic that relied on that order. When we upgraded the database and optimized the query, the ordering behavior changed, and all tests passed. The next day, we were paged: downstream systems were failing. Nothing was &ldquo;wrong&rdquo; with our code, but clients had built assumptions on API responses that we were unaware of.</p><p>That&rsquo;s the reality of distributed systems: sometimes you&rsquo;re the one breaking a dependency, and sometimes you&rsquo;re the one depending on an unstated behavior. Both roles carry risk. Senior engineers anticipate this by communicating guarantees clearly, versioning APIs carefully, and setting up monitoring that catches unexpected shifts in behavior. Tests are valuable, but they&rsquo;re not a shield. <strong>Given enough time, enough traffic, and enough change, every system will break. The question isn&rsquo;t <em>if</em> &mdash; it&rsquo;s <em>when</em>, and how prepared you are to respond.</strong></p><h2>Embrace Change and Adapt</h2><p>The longer you spend in this industry, the clearer it becomes that change is constant, and the pace is only accelerating. In today&rsquo;s world, LLMs have already become part of everyday workflows. They are used to summarize information, answer questions, and assist with routine tasks (including coding).<strong> Is there hype surrounding them? For sure, yes. But it would be a mistake to assume they will simply fade away and leave us with the old ways of working.</strong> These tools will continue to improve and gradually take on more responsibilities.</p><p>Unlike the other lessons I&rsquo;ve shared here, this one doesn&rsquo;t come with a classic story or decades of battle-tested examples. We&rsquo;re still at the very beginning of figuring out the long-term impact of LLMs, which means the patterns aren&rsquo;t fully written yet. What I can say is that I&rsquo;ve already seen them provide small but meaningful gains; generating service status reports, scanning through logs to link metrics to incidents, writing code/tests (always peer-reviewed), and even highlighting potential security concerns during code reviews. They&rsquo;re not a replacement for engineering judgment, but when used thoughtfully, they can speed things up.</p><p><strong>That is why embracing change here doesn&rsquo;t mean blindly trusting AI-generated code, nor assuming it solves every problem. It means staying informed, experimenting carefully, and integrating what genuinely adds value while being mindful of risks like reliability, bias, or over-reliance. Engineers who learn to collaborate effectively with LLMs will free themselves to focus on higher-level problems, and in my opinion, they&rsquo;ll be better prepared for whatever role this technology grows into next.</strong></p></div><div><h2>Closing Notes</h2><p>At the end of the day, becoming a senior software engineer isn&rsquo;t about memorizing every algorithm, mastering every framework, or pretending to have all the answers. It&rsquo;s about judgment, humility, and the ability to adapt when things don&rsquo;t go as planned. It&rsquo;s about pausing to ask <em>why</em>, verifying instead of assuming, and putting guardrails in place so the team doesn&rsquo;t have to rely on memory or luck. Most of all, it&rsquo;s about becoming someone others can trust &mdash; when systems fail, when trade-offs get messy, or simply when people need guidance.</p><p>None of us get there without mistakes. I certainly didn&rsquo;t. What matters is what you learn from those moments, and how you pass that knowledge on to others. <strong>In the long run, your real job is to grow people who can one day take your place</strong>. That&rsquo;s what leadership looks like. If there&rsquo;s one message I hope you take from this article, it&rsquo;s that seniority is not a title &mdash; it&rsquo;s a mindset. And it&rsquo;s built slowly, day by day, decision by decision.</p><p><strong><em>Disclaimer</em></strong><em>: The content of this article is based on my personal experiences and reflections. It should not be interpreted as representing the views of my current employer or any other organization I have worked for.</em></p></div><div class="gallery"><p><img src="https://miro.medium.com/v2/resize:fill:64:64/1*vwObCqzIwzvIzAHchI0eIA@2x.jpeg"></p></div></section>]]></description><pubDate>Wed, 10 Sep 2025 04:02:25 +0530</pubDate></item><item><link>https://cedardb.com/blog/doomql/</link><title>Building a DOOM-like multiplayer shooter in pure SQL (cedardb.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1ncoxl8/building_a_doomlike_multiplayer_shooter_in_pure/</guid><comments>https://www.reddit.com/r/programming/comments/1ncoxl8/building_a_doomlike_multiplayer_shooter_in_pure/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 30 min | <a href='https://www.reddit.com/r/programming/comments/1ncoxl8/building_a_doomlike_multiplayer_shooter_in_pure/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><section><header><p><span>September 8, 2025</span> <span>&bull;</span> <span>12 minutes</span></p></header><div><figure><img src="https://cedardb.com/img/team/vogel.jpg" alt="Lukas Vogel"></figure><p>Lukas Vogel</p></div></section><section><h2>DOOMQL: A DOOM-like multiplayer shooter in pure SQL</h2><section><p>I recently stumbled across Patrick&rsquo;s excellent <a href="https://www.hey.earth/posts/duckdb-doom">DOOM clone</a> running in a browser powered by DuckDB-WASM. Ever since I&rsquo;ve read that, I wanted to push his awesome idea to the logical extreme: Build a <strong>multiplayer</strong> DOOM-like shooter <strong>entirely</strong> in SQL with CedarDB doing all the heavy lifting. During a month of parental leave (i.e., a lot of sleepless nights), I tried exactly that.</p><p>Here&rsquo;s a sneak peek at DOOMQL:</p><video> <source src="https://cedardb.comdoomql.mp4">Your browser does not support the video tag.</source></video><p>DOOMQL in action</p><p>Okay, with the flashy demo out of the way, let&rsquo;s talk about details. What follows is a tour of the architecture, the SQL rendering pipeline, the game loop, and the fun metagame of cheating by issuing SQL commands against the database.</p></section><h2>Why even do this?</h2><section>Playing DuckDB DOOM in your browser is fun, but some things bugged me: First of all, having parts of the rendering pipeline in Javascript felt like cheating. It worked well for DuckDB-Doom where everything is contained in a single HTML page, but I wanted to see if I could do everything in SQL. DuckDB-Doom is also a little bit stuttery with just 8 frames per second and has a pretty tiny viewport. I wanted to see if I could speed that up by switching over to CedarDB. I also wanted real sprites with transparency and they should move around believably in 3D space. And most importantly, making the game multi-player should not just be possible, but easy, right? I got nerd-sniped by the perceived similarity of a database server to a traditional game server: <strong>Databases exist to synchronize shared state across clients.</strong> Thanks to transaction isolation, each player has a consistent view of the game world, no matter what the other clients are doing. Why not lean into that? I would love to lie to you and claim I did it all to push CedarDB as an awesome database system but to be honest the database nerd in me just wanted to turn all knobs up to 11 and see what breaks.</section><h2>Architectural overview</h2><section><p>At a high level</p><ul><li>State lives in tables (map, players, mobs, inputs, configs, sprites, &hellip;)</li><li>Rendering is a stack of SQL views that implement raycasting and sprite projection</li><li>The game loop is a tiny shell script that executes a SQL file ~ 30 times per second.</li><li>The client is ~ 150 lines of Python: It polls for input and queries the database for your 3D view.</li></ul><p>You can play, observe other players and even cheat (by sending raw SQL).</p></section><h2>Game state, or: Let&rsquo;s store everything in the database</h2><section><p>With a database at hand, it&rsquo;s natural to store all game configuration, state, and static data in the database:</p><p><strong>Config</strong>:</p><div><pre><code><span><span><span>CREATE</span><span>TABLE</span>config(<span> </span></span></span><span><span>player_move_speed<span> </span><span>NUMERIC</span><span>DEFAULT</span><span>0</span>.<span>3</span>,</span></span><span><span>player_turn_speed<span> </span><span>NUMERIC</span><span>DEFAULT</span><span>0</span>.<span>2</span>,</span></span><span><span>ammo_max<span> </span><span>INT</span><span>DEFAULT</span><span>10</span>,</span></span><span><span>ammo_refill_interval_seconds<span> </span><span>INT</span><span>DEFAULT</span><span>2</span></span></span><span><span>);<span> </span></span></span></code></pre></div><p><strong>Map</strong>:</p><div><pre><code><span><span><span>CREATE</span><span>TABLE</span><span>map</span>(x<span>INT</span>,y<span> </span><span>INT</span>,tile<span> </span><span>CHAR</span>);</span></span></code></pre></div><p><strong>Players and inputs</strong>:</p><div><pre><code><span><span><span>CREATE</span><span>TABLE</span>players<span> </span>(</span></span><span><span>id<span> </span><span>INT</span><span>REFERENCES</span>mobs(id),<span> </span></span></span><span><span>score<span> </span><span>INT</span><span>DEFAULT</span><span>0</span>,</span></span><span><span>hp<span> </span><span>INT</span><span>DEFAULT</span><span>100</span>,</span></span><span><span>ammo<span> </span><span>INT</span><span>DEFAULT</span><span>10</span>,</span></span><span><span>last_ammo_refill<span> </span><span>int</span><span>default</span><span>EXTRACT</span>(EPOCH<span>FROM</span>(now()))::<span>INT</span></span></span><span><span>);<span> </span></span></span><span><span><span>CREATE</span><span>TABLE</span>inputs(<span> </span></span></span><span><span>player_id<span> </span><span>INT</span><span>PRIMARY</span><span>KEY</span><span>REFERENCES</span>players(id),<span> </span></span></span><span><span>action<span> </span><span>CHAR</span>(<span>1</span>),<span>-- 'w', 'a', 's', 'd', 'x' for shooting </span></span></span><span><span><span> </span><span>timestamp</span><span>TIMESTAMP</span><span>DEFAULT</span>NOW()<span> </span></span></span><span><span>);<span> </span></span></span></code></pre></div><p>Because everything is data, modding a running match is trivial:</p><div><pre><code><span><span><span>-- Change a setting </span></span></span><span><span><span>update</span>config<span> </span><span>set</span>ammo_max<span> </span><span>=</span><span>20</span>;</span></span><span></span><span><span><span>-- Add a player </span></span></span><span><span><span>insert</span><span>into</span>players<span> </span><span>values</span>(...);<span> </span></span></span><span><span><span>-- Move forward </span></span></span><span><span><span>update</span><span>input</span><span>set</span>action<span> </span><span>=</span><span>'w'</span><span>where</span>player_id<span> </span><span>=</span><span>&lt;</span>your_id<span>&gt;</span>;</span></span><span></span><span><span><span>-- Cheat (pls be smarter about it) </span></span></span><span><span><span>update</span>players<span> </span><span>set</span>hp<span> </span><span>=</span><span>100000</span><span>where</span>player_id<span> </span><span>=</span><span>&lt;</span>your_id<span>&gt;</span>;</span></span><span></span><span><span><span>-- Ban cheaters (that weren't smart about it) </span></span></span><span><span><span>delete</span><span>from</span>players<span> </span><span>where</span>hp<span> </span><span>&gt;</span><span>100</span>;</span></span></code></pre></div></section><h2>Renderer: When a <code>VIEW</code> becomes your 3D view</h2><section><p>If you squint enough, in DOOM, a 3D (or more correct: 2.5D) view is just a <em>view over 2D state</em> (i.e., the level map and any players/enemies on it). Well, we&rsquo;ve got <code>VIEWS</code> in SQL as well. They&rsquo;re also just views on our (2D) state tables. What&rsquo;s stopping us from quite <em>literally</em> building a 3D &ldquo;view&rdquo; of our 2D map using a simple raycasting algorithm?</p><p>The pipeline:</p><ol><li>Send a set of rays from each player&rsquo;s eye into the world, and see which map tiles are visible</li><li>Check which walls the player sees, rendering them at the correct height and more or less solid based on the distance</li><li>Project mobs into the player&rsquo;s camera space</li><li>Select sprite LODs based on depth</li><li>Expand sprites into pixels, scaled to screen space</li><li>Occlude against walls and other sprites</li><li>Assemble frame buffer rows with <code>string_agg</code></li><li>Build a minimap reusing the visible tiles calculation from earlier</li><li>Combine the 3D view with minimap and HUD (HP/bullets/players) into a game view</li></ol><p>Let&rsquo;s take a more in-depth look at steps 2, 7, and 8.</p></section><h3>Raycasting</h3><section><p>The recursive ray&#8209;marching logic is adapted from <a href="https://www.hey.earth/posts/duckdb-doom">Patrick&rsquo;s DuckDB DOOM post</a>. Here is a simplified excerpt, adapted for multiplayer:</p><div><pre><code><span><span><span>CREATE</span><span>OR</span><span>REPLACE</span><span>VIEW</span>visible_tiles<span> </span><span>AS</span></span></span><span><span><span>WITH</span><span>RECURSIVE</span>raytrace<span> </span><span>AS</span>(<span> </span></span></span><span><span><span>-- Starting at the player's eye ... </span></span></span><span><span><span> </span><span>SELECT</span>r.player_id,<span> </span>r.col,<span>1</span><span>AS</span>step_count,<span> </span></span></span><span><span>r.player_x<span> </span><span>+</span>COS(r.angle)<span>*</span>s.step<span>AS</span>fx,<span> </span></span></span><span><span>r.player_y<span> </span><span>+</span>SIN(r.angle)<span>*</span>s.step<span>AS</span>fy,<span> </span></span></span><span><span>r.angle,<span> </span><span> </span><span>AS</span>dist<span> </span></span></span><span><span><span>FROM</span>rays<span> </span>r,settings<span> </span>s<span>-- rays are built in an earlier step </span></span></span><span><span><span> </span><span>UNION</span><span>ALL</span></span></span><span><span><span>-- ... we recursively march along the rays, 1 "step" at a time ... </span></span></span><span><span><span> </span><span>SELECT</span>rt.player_id,<span> </span>rt.col,rt.step_count<span> </span><span>+</span><span>1</span>,</span></span><span><span>rt.fx<span> </span><span>+</span>COS(rt.angle)<span>*</span>s.step,</span></span><span><span>rt.fy<span> </span><span>+</span>SIN(rt.angle)<span>*</span>s.step,</span></span><span><span>rt.angle,<span> </span></span></span><span><span>step_count<span> </span><span>*</span>s.step<span> </span><span>*</span>COS(rt.angle<span> </span><span>-</span>m.dir)<span> </span><span>AS</span>dist<span> </span></span></span><span><span><span>FROM</span>raytrace<span> </span>rt,settings<span> </span>s,players<span> </span>p,mobs<span> </span>m</span></span><span><span><span>WHERE</span>rt.step_count<span> </span><span>&lt;</span>s.max_steps<span> </span><span>-- ... stopping after our max render distance </span></span></span><span><span><span> </span><span>AND</span>rt.player_id<span> </span><span>=</span>p.id<span> </span></span></span><span><span><span>AND</span>m.id<span> </span><span>=</span>p.id<span> </span></span></span><span><span><span>AND</span><span>NOT</span><span>EXISTS</span>(<span> </span><span>-- or if we hit a wall </span></span></span><span><span><span> </span><span>SELECT</span><span>1</span><span>FROM</span><span>map</span>m<span> </span></span></span><span><span><span>WHERE</span>m.x<span> </span><span>=</span><span>CAST</span>(rt.fx<span>AS</span><span>INT</span>)<span>AND</span>m.y<span> </span><span>=</span><span>CAST</span>(rt.fy<span>AS</span><span>INT</span>)</span></span><span><span><span>AND</span>m.tile<span> </span><span>=</span><span>'#'</span>)<span>-- wall </span></span></span><span><span>)<span> </span></span></span><span><span><span>-- We then determine per player: </span></span></span><span><span><span>-- a) which tiles we hit </span></span></span><span><span><span>-- b) how far away these tiles are </span></span></span><span><span><span>-- c) the column of the screen each tile should correspond to </span></span></span><span><span><span>SELECT</span>player_id,<span> </span>tile,<span>CAST</span>(fx<span>AS</span><span>INT</span>)<span>AS</span>tile_x,<span> </span><span>CAST</span>(fy<span>AS</span><span>INT</span>)<span>AS</span>tile_y,<span> </span>col,<span>MIN</span>(dist)<span>AS</span>dist<span> </span></span></span><span><span><span>FROM</span>raytrace<span> </span>rt,<span>map</span>m<span> </span></span></span><span><span><span>WHERE</span>m.x<span> </span><span>=</span><span>CAST</span>(rt.fx<span>AS</span><span>INT</span>)<span>AND</span>m.y<span> </span><span>=</span><span>CAST</span>(rt.fy<span>AS</span><span>INT</span>)<span>-- We might hit the same tile multiple times, so we take the closest hit </span></span></span><span><span><span>GROUP</span><span>BY</span>player_id,<span> </span>tile_x,tile_y,<span> </span>tile,col;<span> </span></span></span></code></pre></div><p>And that&rsquo;s just the <em>first</em> step in the pipeline. For the rest, take a look at <a href="https://github.com/cedardb/DOOMQL/blob/main/renderer.sql">the code</a>.</p></section><h3>Final frame assembly</h3><section><p>After all the heavy lifting, the payoff is surprisingly simple:</p><div><pre><code><span><span><span>SELECT</span>player_id,<span> </span>y,string_agg(ch,<span> </span><span>''</span><span>ORDER</span><span>BY</span>x)<span> </span><span>AS</span><span>row</span></span></span><span><span><span>FROM</span>framebuffer<span> </span></span></span><span><span><span>GROUP</span><span>BY</span>player_id,<span> </span>y;</span></span></code></pre></div><p>This glues together character pixels into text rows.</p></section><h3>HUD + minimap</h3><section><p>The same trick builds the HUD and minimap. Here is the health bar:</p><div><pre><code><span><span><span>'HP: ['</span><span>||</span></span></span><span><span>repeat(<span>'&#9608;'</span>,LEAST(<span>20</span>,ROUND(<span>20</span><span>*</span>GREATEST(<span>0</span>,LEAST(p.hp,<span>100</span>))::<span>numeric</span><span>/</span><span>100</span>)::<span>int</span>))<span>||</span></span></span><span><span>repeat(<span>' '</span>,GREATEST(<span>0</span>,<span>20</span><span>-</span>ROUND(<span>20</span><span>*</span>GREATEST(<span>0</span>,LEAST(p.hp,<span>100</span>))::<span>numeric</span><span>/</span><span>100</span>)::<span>int</span>))<span>||</span></span></span><span><span><span>'] '</span><span>||</span>GREATEST(<span>0</span>,p.hp)<span> </span></span></span></code></pre></div><p>Add ammo dots with <code>repeat('&bull;', p.ammo)</code> and you&rsquo;ve got a HUD entirely in SQL:</p><pre><code> 1: Lukas (L) score: 1 HP: [&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;&#9608; ] 50 AMMO: &bull;&bull;&bull;&bull;&bull;&bull;&bull;&bull;&bull;&bull; 2: Foobar (F) score: 0 HP: [&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;&#9608;] 100 AMMO: &bull;&bull;&bull;&bull;&bull;&bull;&bull;&bull; </code></pre><p>We can also re-use our earlier <code>visible_tiles</code> view to build a minimap with a view cone:</p><pre><code>select * from minimap where player_id = 1 order by y; player_id | y | row -----------+----+------------------------------------------------------------------ 1 | 0 | ################################################################ 1 | 1 | ################################################################ 1 | 2 | ##....... ##### ############################# 1 | 3 | ##.....F. ##### ##### ### 1 | 4 | ##....... ##### ##### ### 1 | 5 | ## ..... ##### ##### ### 1 | 6 | ## ... ### 1 | 7 | ## .L ### 1 | 8 | ## ##### ##### ### 1 | 9 | ## ##### ##### ### 1 | 10 | ## ############# ########## ### 1 | 11 | ########## ################ ########## ### 1 | 12 | ########## ################ ########## ### 1 | 13 | ########## ################ ###################### ########## 1 | 14 | #### ####### ###################### ########## 1 | 15 | #### ####### ###################### ########## 1 | 16 | #### ##### ##### ### 1 | 17 | #### ##### ##### ### 1 | 18 | #### ##### ##### ### 1 | 19 | #### ##### ##### ### 1 | 20 | #### ##### ##### ### 1 | 21 | #### ##### ### 1 | 22 | #### ### 1 | 23 | #### ##### ### 1 | 24 | #### ##### ##### ### 1 | 25 | #### ##### ##### ### 1 | 26 | #### ##### ##### ### 1 | 27 | #### ##### ##### ### 1 | 28 | #### ##### ##### ### 1 | 29 | ################################################################ 1 | 30 | ################################################################ 1 | 31 | ################################################################ </code></pre></section><h2>The surprisingly elegant game loop</h2><section><p>The loop is just a shell script running raw SQL against the database:</p><div><pre><code><span><span><span># Game loop @ 30 ticks per second</span> </span></span><span><span><span>while</span> true; <span>do</span> </span></span><span><span> psql -qtAX -U <span>"</span><span>$DB_USER</span><span>"</span> -d <span>"</span><span>$DB_NAME</span><span>"</span> -h <span>"</span><span>$DB_HOST</span><span>"</span> -p <span>"</span><span>$DB_PORT</span><span>"</span> -f gameloop.sql </span></span><span><span> sleep 0.03 </span></span><span><span><span>done</span> </span></span></code></pre></div><p>Inside <code>gameloop.sql</code>, actions like bullet movement, collisions, kills, and respawns run in a single transaction, which keeps state consistent even if something fails mid-tick.</p><p>Here&rsquo;s the part processing interactions with bullets:</p><div><pre><code><span><span><span>-- Process all bullets </span></span></span><span><span><span>BEGIN</span><span>TRANSACTION</span>;</span></span><span></span><span><span><span>-- Move bullets forward </span></span></span><span><span><span>UPDATE</span>mobs<span> </span></span></span><span><span><span>SET</span>x<span> </span><span>=</span>x<span> </span><span>+</span>cos(dir)<span> </span><span>*</span><span>0</span>.<span>5</span>,y<span> </span><span>=</span>y<span> </span><span>+</span>sin(dir)<span> </span><span>*</span><span>0</span>.<span>5</span></span></span><span><span><span>WHERE</span>kind<span> </span><span>=</span><span>'bullet'</span>;</span></span><span></span><span><span><span>-- Delete bullets that are out of bounds </span></span></span><span><span><span>DELETE</span><span>FROM</span>mobs<span> </span></span></span><span><span><span>WHERE</span>(x<span> </span><span>&lt;</span><span>0</span></span></span><span><span><span>OR</span>x<span> </span><span>&gt;=</span>(<span>select</span><span>max</span>(x)<span>from</span><span>map</span>)</span></span><span><span><span>OR</span>y<span> </span><span>&lt;</span><span>0</span></span></span><span><span><span>OR</span>y<span> </span><span>&gt;=</span>(<span>select</span><span>max</span>(y)<span>from</span><span>map</span>))</span></span><span><span><span>AND</span>kind<span> </span><span>=</span><span>'bullet'</span>;</span></span><span></span><span><span><span>-- Delete bullets that hit walls </span></span></span><span><span><span>DELETE</span><span>FROM</span>mobs<span> </span>b</span></span><span><span><span>WHERE</span><span>EXISTS</span></span></span><span><span>(<span>SELECT</span><span>1</span></span></span><span><span><span>FROM</span><span>map</span>m<span> </span></span></span><span><span><span>WHERE</span>m.x<span> </span><span>=</span><span>CAST</span>(b.x<span>AS</span><span>INT</span>)</span></span><span><span><span>AND</span>m.y<span> </span><span>=</span><span>CAST</span>(b.y<span>AS</span><span>INT</span>)</span></span><span><span><span>AND</span>m.tile<span> </span><span>=</span><span>'#'</span>)</span></span><span><span><span>AND</span>kind<span> </span><span>=</span><span>'bullet'</span>;</span></span><span></span><span></span><span><span><span>-- Players hit by a bullet loses 50 HP </span></span></span><span><span><span>UPDATE</span>players<span> </span>p<span>SET</span>hp<span> </span><span>=</span>hp<span> </span><span>-</span><span>50</span></span></span><span><span><span>FROM</span>collisions<span> </span><span>c</span></span></span><span><span><span>WHERE</span>p.id<span> </span><span>=</span><span>c</span>.player_id;</span></span><span></span><span><span><span>-- If a player has 0 or less HP, the player killing them gets a point </span></span></span><span><span><span>UPDATE</span>players<span> </span>p<span>SET</span>score<span> </span><span>=</span>score<span> </span><span>+</span><span>1</span></span></span><span><span><span>FROM</span>collisions<span> </span><span>c</span></span></span><span><span><span>WHERE</span>p.id<span> </span><span>=</span><span>c</span>.bullet_owner</span></span><span><span><span>AND</span><span>EXISTS</span>(<span>SELECT</span><span>1</span><span>FROM</span>players<span> </span>p2<span>WHERE</span>p2.id<span> </span><span>=</span><span>c</span>.player_id<span>AND</span>p2.hp<span> </span><span>&lt;=</span><span>0</span>);</span></span><span></span><span><span><span>-- Delete bullets that hit players </span></span></span><span><span><span>DELETE</span><span>FROM</span>mobs<span> </span>m</span></span><span><span><span>USING</span>collisions<span> </span><span>c</span></span></span><span><span><span>WHERE</span>m.id<span> </span><span>=</span><span>c</span>.bullet_id;</span></span><span></span><span><span><span>-- Respawn players whose HP is 0 or less </span></span></span><span><span><span>UPDATE</span>mobs<span> </span>m</span></span><span><span><span>SET</span>x<span> </span><span>=</span>r.x,<span> </span>y<span>=</span>r.y,<span> </span>dir<span>=</span><span>0</span></span></span><span><span><span>FROM</span>players<span> </span>p</span></span><span><span><span>CROSS</span><span>JOIN</span>(<span> </span></span></span><span><span><span>SELECT</span>x,<span> </span>y</span></span><span><span><span>FROM</span><span>map</span></span></span><span><span><span>WHERE</span>tile<span> </span><span>=</span><span>'R'</span></span></span><span><span><span>ORDER</span><span>BY</span>random()<span> </span></span></span><span><span><span>LIMIT</span><span>1</span></span></span><span><span>)<span> </span><span>AS</span>r<span> </span></span></span><span><span><span>WHERE</span>m.id<span> </span><span>=</span>p.id<span> </span></span></span><span><span><span>AND</span>p.hp<span> </span><span>&lt;=</span><span>0</span>;</span></span><span></span><span><span><span>-- Reset players' HP to 100 and ammo to 10 after respawn </span></span></span><span><span><span>UPDATE</span>players<span> </span>p<span>SET</span></span></span><span><span>hp<span> </span><span>=</span><span>100</span>,</span></span><span><span>ammo<span> </span><span>=</span><span>10</span></span></span><span><span><span>FROM</span>mobs<span> </span>m</span></span><span><span><span>WHERE</span>p.id<span> </span><span>=</span>m.id<span> </span></span></span><span><span><span>AND</span>p.hp<span> </span><span>&lt;=</span><span>0</span>;</span></span><span></span><span><span><span>COMMIT</span>;</span></span></code></pre></div><p>On my machine, the game loop takes about 1 ms, so we could defintely improve the tick rate. That might be a way to get the Counterstrike snobs who scoff at everything below 128 Hz. It would require some refactoring on my part since I tied the movement speed to the game loop - a big no no in game design!</p><p>While only someone insane could think a pure SQL raycasting renderer is a good idea in an actual game, I&rsquo;ll happily defend this transactional game loop. I don&rsquo;t think this part would be much more concise or less brittle in a real game engine.</p></section><h2>Make it multiplayer in two queries</h2><section><p>The game client&rsquo;s job description is simple:</p><ol><li>Render</li></ol><div><pre><code><span><span><span>SELECT</span>full_row<span> </span><span>FROM</span>screen<span> </span><span>WHERE</span>player_id<span> </span><span>=</span><span>&lt;</span>your_id<span>&gt;</span><span>ORDER</span><span>BY</span>y<span> </span></span></span></code></pre></div><ol><li>Send input</li></ol><div><pre><code><span><span><span>INSERT</span><span>INTO</span>inputs(player_id,<span> </span>action)</span></span><span><span><span>VALUES</span>(<span>&lt;</span>your_id<span>&gt;</span>,<span>&lt;</span>pressed_key<span>&gt;</span>)</span></span><span><span><span>ON</span>CONFLICT(player_id)<span> </span></span></span><span><span><span>DO</span><span>UPDATE</span><span>SET</span>action<span> </span><span>=</span>EXCLUDED.action<span> </span></span></span></code></pre></div><p>The game loop periodically checks the input table and moves all players accordingly - inside a transaction, of course, so we don&rsquo;t run into any race conditions.</p><p>That&rsquo;s it (well, plus a one-time &ldquo;create player&rdquo; on first connect). The ~150 lines of Python in the client mostly handle keyboard input and reducing terminal flicker. Bonus: The client provides an observer mode. All it has to do is swap the <code><player_id></player_id></code> in the render call.</p></section><h2>Performance</h2><section><p>At 128 x 64 pixels, a single player view takes ~33 ms on my machine, which is <strong>enough for a breezy ~30 FPS</strong>, compared to DuckDB DOOM&rsquo;s 8 FPS at just 32 x 16 pixels. I&rsquo;m actually quite proud of that performance and quite happy with CedarDB here. I don&rsquo;t think any other database system can keep up with that. Let me know if you find one!</p><p>You might worry that rendering the views of all players and filtering late would be very wasteful. CedarDB&rsquo;s query optimizer pushes the <code>where player_id = &lt;...&gt;</code> predicate through view boundaries, avoiding unncessary work. You can easily check by running:</p><div><pre><code><span><span><span>select</span><span>*</span><span>from</span>screen<span> </span><span>order</span><span>by</span>y;<span> </span><span>-- render both users </span></span></span><span><span><span>-- Time: 57,907 ms (~2x single player 33ms) </span></span></span></code></pre></div></section><section><p>Because clients send raw SQL as superusers (I didn&rsquo;t bother setting up any role based access control or row level security, don&rsquo;t do that!), there&rsquo;s an <strong>emergent metagame: Cheat creatively and try not to get caught</strong>.</p><p>Low effort:</p><div><pre><code><span><span><span>update</span>players<span> </span><span>set</span>score<span> </span><span>=</span><span>0</span><span>where</span>id<span> </span><span>!=</span><span>&lt;</span>your_id<span>&gt;</span>;</span></span><span><span><span>update</span>players<span> </span><span>set</span>hp<span> </span><span>=</span><span>0</span><span>where</span>id<span> </span><span>!=</span><span>&lt;</span>your_id<span>&gt;</span>;</span></span></code></pre></div><p>Mischievous:</p><div><pre><code><span><span><span>update</span>inputs<span> </span><span>set</span>action<span> </span><span>=</span><span>null</span><span>where</span>player_id<span> </span><span>!=</span><span>&lt;</span>your_id<span>&gt;</span>;</span></span></code></pre></div><p>Steal kills:</p><div><pre><code><span><span><span>update</span>mobs<span> </span><span>set</span><span>owner</span><span>=</span><span>&lt;</span>your_id<span>&gt;</span><span>where</span>kind<span> </span><span>=</span><span>'bullet'</span>;</span></span></code></pre></div><p>Attempted but didn&rsquo;t work:</p><div><pre><code><span><span><span>DELETE</span><span>FROM</span>mobs<span> </span>m</span></span><span><span><span>USING</span>collisions<span> </span><span>c</span></span></span><span><span><span>WHERE</span>m.id<span> </span><span>=</span><span>c</span>.bullet_id<span>AND</span><span>c</span>.player_id<span>=</span><span>&lt;</span>your_id<span>&gt;</span>;</span></span></code></pre></div><p>This doesn&rsquo;t work because moving bullets, checking for collisions, and respawn happens in the same transaction. As transactions are atomic, you either see everything being applied at once, or nothing. By the time you see the hit, you&rsquo;re already dead. A property that&rsquo;s very useful for database systems (and not just to prevent cheating).</p></section><h2>What I learned</h2><section><p>I set out to see if I could push Patrick&rsquo;s demo to an extreme: Doing the entire rendering pipeline in SQL. And while it works, I have to admit that it is a pretty&hellip; bad idea? Fast enough, but horrible to maintain and debug.</p><p>The surprise was how <em>natural</em> it felt to express game state and logic in SQL. It even felt like accidentally re-invented the <a href="https://en.wikipedia.org/wiki/Entity_component_system">entity-component-system</a> pattern.<br>And multiplayer &ldquo;just worked&rdquo; because the database system which handles all the nasty concurrency is the source of truth.</p></section><h2>Try it yourself!</h2><section><p>All the code is on Github: <a href="https://github.com/cedardb/DOOMQL">DOOMQL Repo</a></p><p>Run:</p><div><pre><code><span><span>docker pull cedardb/cedardb:latest </span></span><span><span>docker run --rm -p 5432:5432 -e <span>CEDAR_PASSWORD</span><span>=</span>postgres --detach cedardb/cedardb:latest </span></span><span><span><span># Wait a few seconds for CedarDB to start</span> </span></span><span><span>./server.sh </span></span><span><span><span># in a second terminal window, zoom way out to have no line wraping issues</span> </span></span><span><span>python3 pyclient.py </span></span></code></pre></div><p>Want to discuss DOOMQL with me or find like-minded database nerds? <a href="https://bonsai.cedardb.com/slack">Join our Community Slack</a></p></section></section> </section>]]></description><pubDate>Tue, 09 Sep 2025 22:47:39 +0530</pubDate></item><item><link>https://blog.epsiolabs.com/i-love-uuid-i-hate-uuid</link><title>I love UUID, I hate UUID (blog.epsiolabs.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1ncht77/i_love_uuid_i_hate_uuid/</guid><comments>https://www.reddit.com/r/programming/comments/1ncht77/i_love_uuid_i_hate_uuid/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/programming/comments/1ncht77/i_love_uuid_i_hate_uuid/'>Post permalink</a></p></section><section class='preview-image'><img src='https://blog.epsiolabs.com/api/og/post?og=eyJ0aXRsZSI6IkklMjBsb3ZlJTIwVVVJRCUyQyUyMEklMjBoYXRlJTIwVVVJRCIsImF1dGhvciI6Ik1hb3IlMjBLZXJuIiwiZG9tYWluIjoiYmxvZy5lcHNpb2xhYnMuY29tIiwicGhvdG8iOiJodHRwczovL2Nkbi5oYXNobm9kZS5jb20vcmVzL2hhc2hub2RlL2ltYWdlL3VwbG9hZC92MTczNjQ0OTc1MDE5Ny8wMzU5YjBkNC1iYzIzLTRjMDAtODE4Zi0zM2JlN2ZiYjBkNjIucG5nIiwiYmdjb2xvciI6IiNmMWYyZjYiLCJyZWFkVGltZSI6Nn0=' /></section><section class='parsed-content'><div><p>Vercel Security Checkpoint</p><p>|</p><p>bom1::1757437374-6sa9o22QC2d5wOkKXHjcaEiqF0yACM9x</p></div></section>]]></description><pubDate>Tue, 09 Sep 2025 18:10:41 +0530</pubDate></item></channel></rss>
