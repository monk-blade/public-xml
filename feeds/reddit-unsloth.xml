<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><atom:link href="http://192.168.1.132/?platform=reddit&amp;subreddit=unsloth&amp;averagePostsPerDay=2&amp;content&amp;view=rss" rel="self" type="application/rss+xml"/><title>/r/unsloth</title><description>Hot posts in /r/unsloth (roughly 2 posts per day)</description><link>https://www.reddit.com/r/unsloth/</link><language>en-us</language><lastBuildDate>Sat, 06 Sep 2025 22:59:10 +0000</lastBuildDate><generator>Upvote RSS</generator><image><url>http://192.168.1.132//app/cache/images/styles-redditmedia-com-t5_aqipas-styles-communityIcon_86tg5vvk98lc1-144x400.png</url><title>/r/unsloth</title><link>https://www.reddit.com/r/unsloth/</link><width>144</width><height>144</height></image><item><link>https://www.reddit.com/r/unsloth/comments/1na24vc/request_q4_k_xl_quantization_for_the_new/</link><title>Request: Q4_K_XL quantization for the new distilled Qwen3 30B models</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1na24vc/request_q4_k_xl_quantization_for_the_new/</guid><comments>https://www.reddit.com/r/unsloth/comments/1na24vc/request_q4_k_xl_quantization_for_the_new/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1na24vc/request_q4_k_xl_quantization_for_the_new/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey everyone,</p><p>I recently saw that someone released some new distilled models on Hugging Face and I&#39;ve been testing them out:</p><p>BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32</p><p>BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Fp32</p><p>They seem really promising, especially for coding tasks ‚Äî in my initial experiments they perform quite well.</p><p>From my experience, however, Q4_K_XL quantization is noticeably faster and more efficient than the more common Q4_K_M quantizations.</p><p>Would it be possible for you to release Q4_K_XL versions of these distilled models? I think many people would benefit from the speed/efficiency gains.</p><p>Thank you very much in advance!</p></div><!-- SC_ON --></section>]]></description><pubDate>Sat, 06 Sep 2025 20:47:29 +0530</pubDate></item><item><link>https://i.redd.it/24uff9ie8fnf1.png</link><title>Dynamic 'Kimi-K2-Instruct-0905' Unsloth GGUFs out now! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n9iwbh/dynamic_kimik2instruct0905_unsloth_ggufs_out_now/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n9iwbh/dynamic_kimik2instruct0905_unsloth_ggufs_out_now/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n9iwbh/dynamic_kimik2instruct0905_unsloth_ggufs_out_now/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Most of the important ones including 1, 2, 4, 8-bit (full precision) etc. should be up now! <a href="https://huggingface.co/unsloth/Kimi-K2-Instruct-0905-GGUF">https://huggingface.co/unsloth/Kimi-K2-Instruct-0905-GGUF</a></p><p>You can follow our guide for more info, just make to to change the Kimi-K2 model name to &#39;Kimi-K2-Instruct-0905&#39; and it should work: <a href="https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locallyWe">https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locallyWe</a> recommend using Q2_K_XL or larger.</p><p>Thanks so much guys!</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/24uff9ie8fnf1.png' /></section>]]></description><pubDate>Sat, 06 Sep 2025 03:51:57 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n95oh9/is_it_possible_to_create_my_own_unsloth_dynamic/</link><title>Is it possible to create my own unsloth dynamic quants?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n95oh9/is_it_possible_to_create_my_own_unsloth_dynamic/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n95oh9/is_it_possible_to_create_my_own_unsloth_dynamic/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n95oh9/is_it_possible_to_create_my_own_unsloth_dynamic/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I can&#39;t find any documentation about how to replicate unsloth dynamic quants,for exemple, if I finetune my own model using unsloth, and then want to create quantized GGUFs to run it, could I do it the same way unsloth does with the dynamic GGUFs?</p><p>I know I can quantize each layer with a different quant using llama-quantize, but unsloth has a method to find the right quantization for each layer, and I am wondering if it&#39;s documented anywhere how to do it alongside the code necessary.</p></div><!-- SC_ON --></section>]]></description><pubDate>Fri, 05 Sep 2025 19:14:38 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n93onw/finetuning_deepseek_v31/</link><title>Finetuning Deepseek V3.1</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n93onw/finetuning_deepseek_v31/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n93onw/finetuning_deepseek_v31/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1n93onw/finetuning_deepseek_v31/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Is it possible to finetune Deepseek V3.1(not distill versions) using unsloth on a multi gpu setup?</p></div><!-- SC_ON --></section>]]></description><pubDate>Fri, 05 Sep 2025 17:47:56 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n92ut3/how_to_change_a_subtle_behavior_of_model_by_fine/</link><title>How to change a subtle behavior of model by fine tuning?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n92ut3/how_to_change_a_subtle_behavior_of_model_by_fine/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n92ut3/how_to_change_a_subtle_behavior_of_model_by_fine/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n92ut3/how_to_change_a_subtle_behavior_of_model_by_fine/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Situation</p><p>A model I&#39;m using keeps having two quirks, 1) it keeps providing citations when I pressed for it to quote (sources) and when it does start citing, it throws up hallucinated sources. 2) it keeps thinking that a concept is X when that concept is actually Y</p><p>Otherwise the model is perfect. Today after first fine tuning with 400 rows of data the model completely broken and became lowish IQ. The verbosity of the model became super brief as well to match the fine tune dataset.</p><p>Because I just need to shape the 2 small behaviors above, are there any advice for me?</p><p>Should I limit my dataset to even small and focus on these 2 points only and then lower the LR?</p></div><!-- SC_ON --></section>]]></description><pubDate>Fri, 05 Sep 2025 17:07:26 +0530</pubDate></item><item><link>https://i.redd.it/cqmnotmh96nf1.png</link><title>Unsloth Memory Efficient Reinforcement Learning (RL) is here! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n8efil/unsloth_memory_efficient_reinforcement_learning/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n8efil/unsloth_memory_efficient_reinforcement_learning/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n8efil/unsloth_memory_efficient_reinforcement_learning/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys, as you know RL used to be memory hungry, but we&#39;ve made lots of advancements this year to make it work on consumer hardware. Now, it&#39;s even more efficient! :)</p><p>We&#39;re introducing Unsloth&#39;s new kernels &amp; algorithms that allows faster RL training with 50% less VRAM, 10√ó more context length &amp; no accuracy loss.</p><p>Our main feature includes Unsloth Standby. Before, RL requires GPU splitting between training &amp; inference. With Unsloth Standby, you no longer have to.</p><p>‚≠êRead our educational blog for details, functionality and more: <a href="https://docs.unsloth.ai/basics/memory-efficient-rl">https://docs.unsloth.ai/basics/memory-efficient-rl</a></p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/cqmnotmh96nf1.png' /></section>]]></description><pubDate>Thu, 04 Sep 2025 21:39:56 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n80cno/updated_dynamic_deepseekv31_ggufs_upgraded/</link><title>Updated Dynamic DeepSeek-V3.1 GGUFs - upgraded performance! üêã</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n80cno/updated_dynamic_deepseekv31_ggufs_upgraded/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n80cno/updated_dynamic_deepseekv31_ggufs_upgraded/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n80cno/updated_dynamic_deepseekv31_ggufs_upgraded/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys, we reuploaded the DeepSeek-V3.1 quants and according to 3rd party Aider polyglot benchmarks, they&#39;re even better than before: <a href="https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF">https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF</a></p><p>We&#39;ll announce the amazing benchmark results likely next week, yes you will need to redownload.</p><p>The benchmarks are 90% done already and we compared them other quants and our previous quants and the results are clearly an improvement.</p><p>We converted DeepSeek-V3.1 using our normal conversion, however we needed to update it as we didn&#39;t know llama.cpp overrode some of our layer quantization for conversion so we needed to change reupload them. The quants should only be a few MB bigger but the increase in accuracy is very large.</p><p>Guide to run should remain the same: <a href="https://docs.unsloth.ai/basics/deepseek-v3.1-how-to-run-locally">https://docs.unsloth.ai/basics/deepseek-v3.1-how-to-run-locally</a></p></div><!-- SC_ON --></section>]]></description><pubDate>Thu, 04 Sep 2025 09:40:14 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n7sbak/new_to_llm_finetuning_and_trying_to_find_the_best/</link><title>New to LLM Fine-tuning and trying to find the best training method for my personal application.</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n7sbak/new_to_llm_finetuning_and_trying_to_find_the_best/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n7sbak/new_to_llm_finetuning_and_trying_to_find_the_best/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n7sbak/new_to_llm_finetuning_and_trying_to_find_the_best/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hello! I&#39;m looking to create an AI assistant for my personal planner app that has both canvas and g-cal integration, displays assignments, my daily schedule, and an organized calendar. I have already completed most of the UI for my app and the backend is nearly finished as well. I&#39;m currently looking to add an AI agent that I can use to control functionality on my app by running some methods I&#39;ve created that will edit the UI and also push assignments/events onto g-cal. Basically, I want to have the AI assistant both engage in conversation with me, and generate a formulaic reply that runs some of my methods and is readable by my application. Originally, I thought the best method to get this to work would be fine-tuning an existing LLM with a dataset that I created which replicated the functionality I needed. I also considered the option of simply feeding the API for my app to an LLM and instructing it with how to generate responses. What would you guys recommend in terms of the exact use case I&#39;m trying to fill? Any help is much appreciated, thanks in advance for your time.</p></div><!-- SC_ON --></section>]]></description><pubDate>Thu, 04 Sep 2025 03:28:49 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n6bfnf/how_to_run_unsloth_on_hpc/</link><title>How to run unsloth on HPC</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n6bfnf/how_to_run_unsloth_on_hpc/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n6bfnf/how_to_run_unsloth_on_hpc/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n6bfnf/how_to_run_unsloth_on_hpc/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey, I&#39;m a newbie to unsloth and AI in general, I&#39;ve gotten unsloth working on a local PC but need more firepower so hoping to run it on my university&#39;s HPC. I can give whatever details are needed about the system but not sure what&#39;s relevant that I can provide here so please tell me what I need to provide. </p><p>I tried writing and running the python code from the notebook on the HPC and it failed since unsloth wasn&#39;t installed in the python environment. Then I tried creating a singularity container as per HPC documentation and containering everything I thought was needed and that failed cuz the container couldn&#39;t access the GPU (needs Nvidia container toolkit or sthg and admins refused to install it for me). </p><p>Now I&#39;m lost. Idk what I should be doing to run unsloth and finetune my models on the HPC. Are there any other methods I have missed ? Or is there no other choice but to get the admins to help out ?</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 02 Sep 2025 11:13:29 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n3olwr/does_unsloth_support_mamba_architecture/</link><title>Does Unsloth support mamba architecture?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n3olwr/does_unsloth_support_mamba_architecture/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n3olwr/does_unsloth_support_mamba_architecture/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n3olwr/does_unsloth_support_mamba_architecture/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I&#39;m quite interested in the new Nvidia Nano models and Falcon H1 series. I&#39;m wondering if Unsloth support finetuning these models?</p></div><!-- SC_ON --></section>]]></description><pubDate>Sat, 30 Aug 2025 06:34:03 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n36aqg/can_someone_explain_to_me_why_the_number_of/</link><title>Can someone explain to me why the number of parameters are different in an unsloth quant?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n36aqg/can_someone_explain_to_me_why_the_number_of/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n36aqg/can_someone_explain_to_me_why_the_number_of/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n36aqg/can_someone_explain_to_me_why_the_number_of/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I thought quants were not supposed to change norms/biases/other parameters in a model. </p><p>However, when i look at the original Kimi K2, i see a lot of small tensors like size [5, 56]</p><p><a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct/blob/main/model-1-of-61.safetensors">https://huggingface.co/moonshotai/Kimi-K2-Instruct/blob/main/model-1-of-61.safetensors</a></p><p>These are missing in the unsloth quant: </p><p><a href="https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/blob/main/UD-Q4_K_XL/Kimi-K2-Instruct-UD-Q4_K_XL-00001-of-00013.gguf">https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/blob/main/UD-Q4_K_XL/Kimi-K2-Instruct-UD-Q4_K_XL-00001-of-00013.gguf</a></p><p>What&#39;s happening here? Why do these tensors disappear?</p></div><!-- SC_ON --></section>]]></description><pubDate>Fri, 29 Aug 2025 18:08:36 +0530</pubDate></item><item><link>https://i.redd.it/5q7trkshislf1.png</link><title>OpenAI gpt-oss Ultra Long Context is here! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n2hl06/openai_gptoss_ultra_long_context_is_here/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n2hl06/openai_gptoss_ultra_long_context_is_here/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1n2hl06/openai_gptoss_ultra_long_context_is_here/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys we&#39;ve got LOTS of updates for gpt-oss training today! We‚Äôre excited to introduce Unsloth Flex Attention support for OpenAI gpt-oss training that enables¬†<strong>&gt;8√ó longer context lengths</strong>,¬†<strong>&gt;50% less VRAM usage</strong>¬†and¬†<strong>&gt;1.5√ó faster training</strong>¬†vs. all implementations including those using Flash Attention 3 (FA3). Unsloth Flex Attention makes it possible to train with a¬†<strong>60K context length</strong>¬†on just 80GB of VRAM for BF16 LoRA. Also:</p><ul><li>You can now export/save your QLoRA fine-tuned gpt-oss model to llama.cpp, vLLM, Ollama or HF</li><li>We¬†<strong>fixed gpt-oss training losses going to infinity</strong>¬†on float16 GPUs (like T4 Colab)</li><li>We fixed gpt-oss implementation issues irrelevant to Unsloth, most notably ensuring that¬†<code>swiglu_limit = 7.0</code>¬†is properly applied during MXFP4 inference in transformers</li><li>Unsloth Flex Attention scales with context, longer sequences yield bigger savings in both VRAM and training time</li></ul><p>ü¶• Would highly recommend you guys to read our blog which has all the bug fixes, guides, details, explanations, findings etc. and it&#39;ll be really educational:¬†<a href="https://docs.unsloth.ai/basics/long-context-gpt-oss-training">https://docs.unsloth.ai/basics/long-context-gpt-oss-training</a></p><p>We&#39;ll likely release our gpt-oss training notebook with direct saving capabilities to GGUF, llama.cpp next week.<br/>And we&#39;ll be releasing third-party Aider polygot benchmarks for DeepSeek-V3.1 next week. You guys will be amazed at how well IQ1_M performs!<br/>And next week we&#39;ll have another great update for RL! üòâ<br/>And you can support our announcement tweet here: <a href="https://x.com/UnslothAI/status/1961108732361994248">https://x.com/UnslothAI/status/1961108732361994248</a></p><p>Thanks guys for reading and hope you all have a lovely Friday and long weekend,<br/>Mike! ü¶•</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/5q7trkshislf1.png' /></section>]]></description><pubDate>Thu, 28 Aug 2025 22:21:16 +0530</pubDate></item><item><link>https://www.reddit.com/gallery/1n1jq1l</link><title>Q5_K_XL and Q6_K_XL on 5-shot MMLU graph (Gallery)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n1jq1l/q5_k_xl_and_q6_k_xl_on_5shot_mmlu_graph/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n1jq1l/q5_k_xl_and_q6_k_xl_on_5shot_mmlu_graph/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n1jq1l/q5_k_xl_and_q6_k_xl_on_5shot_mmlu_graph/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>In the 5-shot MMLU graph on this page: <a href="https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs">https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs</a></p><p>Where do Q5_K_XL and Q6_K_XL fall? Curious how they compare to the other quants.</p><p>neolithic has been running the various unsloth quants of DeepSeek V3.1 in non-thinking mode under llama.cpp against the Aider Polyglot Benchmark and posting the results in Discord. So far the results seem to loosely match the MMLU graph (Q3 is a little weird), but we don&#39;t have MMLU graph data for these two quants.</p><p>Disclaimers: I&#39;m not an expert graph maker. The axis don&#39;t really line up and while the graph with pass_rate_1 and pass_rate_2  shows a good comparison between those two passes, I feel like it loses the plot if the goal is to compare against MMLU. I also don&#39;t know what MMLU means. lol. Further, I guessed the MMLU numbers because I didn&#39;t see a data table. I may have guessed wrong.</p></div><!-- SC_ON --></section><section class='embedded-media'><p><img src="https://preview.redd.it/hq9tejr6vklf1.png?width=1970&amp;format=png&amp;auto=webp&amp;s=f2c72ef87744ead5fd2154158175a3cf56b53a39" height="896" width="1970" /></p><p><img src="https://preview.redd.it/8giygjr6vklf1.png?width=1972&amp;format=png&amp;auto=webp&amp;s=4fddff2b4e5ee7b474bed7594ce8a3502c425409" height="902" width="1972" /></p></section>]]></description><pubDate>Wed, 27 Aug 2025 20:41:36 +0530</pubDate></item><item><link>https://www.reddit.com/gallery/1n0lt4r</link><title>[Experiment] 10-min QLoRA Fine-Tuning on 240 Q&amp;amp;As (ROUGE-L doubled, SARI +15) (Gallery)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n0lt4r/experiment_10min_qlora_finetuning_on_240_qas/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n0lt4r/experiment_10min_qlora_finetuning_on_240_qas/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1n0lt4r/experiment_10min_qlora_finetuning_on_240_qas/'>Post permalink</a></p></section><section class='embedded-media'><p><img src="https://preview.redd.it/xop6it558dlf1.png?width=1484&amp;format=png&amp;auto=webp&amp;s=655070f4ed341f8cddbf3e31a6464996909c3ed6" height="1068" width="1484" /></p><p><img src="https://preview.redd.it/qd0iou558dlf1.png?width=1420&amp;format=png&amp;auto=webp&amp;s=f519646d1f9de66e77d168392465e7642e13ad11" height="1068" width="1420" /></p></section>]]></description><pubDate>Tue, 26 Aug 2025 18:56:00 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n0ehqf/thank_you_for_the_5090_support/</link><title>Thank you for the 5090 support!</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n0ehqf/thank_you_for_the_5090_support/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n0ehqf/thank_you_for_the_5090_support/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1n0ehqf/thank_you_for_the_5090_support/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I was sooo happy tonight to have PyTorch and Unsloth do their magic on my 5090; it&#39;s amazing.</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 26 Aug 2025 12:01:19 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mzraar/facing_runtimeerror_unsloth_vllm_process_failed/</link><title>Facing "RuntimeError: Unsloth: vllm_process failed to load!"</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mzraar/facing_runtimeerror_unsloth_vllm_process_failed/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mzraar/facing_runtimeerror_unsloth_vllm_process_failed/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mzraar/facing_runtimeerror_unsloth_vllm_process_failed/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi, Can anyone help me to solve the below error while trying to use the predefined colab notebook of Unsloth for the synthetic data kit. I&#39;m even using an A100 GPU from Colab: </p><pre><code>ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.INFO 08-25 13:54:40 [__init__.py:241] Automatically detected platform cuda.ü¶• Unsloth Zoo will now patch everything to make training faster!Unsloth: Patching vLLM v1 graph captureUnsloth: Patching vLLM v0 graph captureUnsloth: Using dtype = torch.bfloat16 for vLLM.Unsloth: vLLM loading unsloth/Llama-3.2-3B-Instruct with actual GPU utilization = 89.06%Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 39.56 GB.Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 320.Unsloth: vLLM&#39;s KV Cache can use up to 29.25 GB. Also swap space = 6 GB.Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.vLLM STDOUT: INFO 08-25 13:55:04 [__init__.py:241] Automatically detected platform cuda.Stdout stream ended before readiness message detected.---------------------------------------------------------------------------RuntimeError                              Traceback (most recent call last) in &lt;cell line: 0&gt;()      1 from unsloth.dataprep import SyntheticDataKit      2 ----&gt; 3 generator = SyntheticDataKit.from_pretrained(      4     # Choose any model from       5     model_name = &quot;unsloth/Llama-3.2-3B-Instruct&quot;,/tmp/ipython-input-2164116524.pyhttps://huggingface.co/unsloth in __init__(self, model_name, max_seq_length, gpu_memory_utilization, float8_kv_cache, conservativeness, token, **kwargs)    147         while not self.check_vllm_status():    148             if trial &gt;= 100:--&gt; 149                 raise RuntimeError(&quot;Unsloth: vllm_process failed to load!&quot;)    150             trial += 1    151             time.sleep(1)/usr/local/lib/python3.12/dist-packages/unsloth/dataprep/synthetic.pyRuntimeError: Unsloth: vllm_process failed to load!</code></pre></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 25 Aug 2025 19:30:29 +0530</pubDate></item><item><link>https://huggingface.co/unsloth/Seed-OSS-36B-Instruct-GGUF</link><title>ByteDance Seed-OSS Dynamic GGUFs out now! (huggingface.co)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mzcpx4/bytedance_seedoss_dynamic_ggufs_out_now/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mzcpx4/bytedance_seedoss_dynamic_ggufs_out_now/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 15 min | <a href='https://www.reddit.com/r/unsloth/comments/1mzcpx4/bytedance_seedoss_dynamic_ggufs_out_now/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys due to high demand, we&#39;ve released Dynamic imatrix quantized GGUFs for seed-oss. Currently only works in llama.cpp or tools which support the latest version of llama.cpp.</p><p>Thanks and let us know how they are! :)</p></div><!-- SC_ON --></section><section class='separator separator-after-selftext'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><p>&#128075; Hi, everyone! <br> We are <b>ByteDance Seed Team.</b> </p><p>You can get to know us better through the following channels&#128071; <br> <a href="https://seed.bytedance.com/"> <img src="https://img.shields.io/badge/Website-%231e37ff?style=for-the-badge&amp;logo=bytedance&amp;logoColor=white"></a> </p><h2> <a href="https://huggingface.co#seed-oss-open-source-models"> </a> <span> Seed-OSS Open-Source Models </span> </h2><p><a href="https://github.com/ByteDance-Seed/seed-oss"> <img src="https://img.shields.io/badge/Seed-Project%20Page-yellow"></a> <a href="https://github.com/ByteDance-Seed/seed-oss"> <img src="https://img.shields.io/badge/Seed-Tech%20Report%20Coming%20Soon-red"></a> <a href="https://huggingface.co/ByteDance-Seed"> <img src="https://img.shields.io/badge/Seed-Hugging%20Face-orange"></a> <br> <a href="https://huggingface.co./LICENSE"> <img src="https://img.shields.io/badge/License-Apache2.0-blue"></a> </p><blockquote><p>This model card is dedicated to the <code>Seed-OSS-36B-Instruct</code> model.</p></blockquote> <h2> <a href="https://huggingface.co#news"> </a> <span> News </span> </h2> <ul> <li>[2025/08/20]&#128293;We release <code>Seed-OSS-36B-Base</code> (both with and without synthetic data versions) and <code>Seed-OSS-36B-Instruct</code>.</li> </ul> <h2> <a href="https://huggingface.co#introduction"> </a> <span> Introduction </span> </h2><p>Seed-OSS is a series of open-source large language models developed by ByteDance's Seed Team, designed for powerful long-context, reasoning, agent and general capabilities, and versatile developer-friendly features. Although trained with only 12T tokens, Seed-OSS achieves excellent performance on several popular open benchmarks.</p><p>We release this series of models to the open-source community under the Apache-2.0 license.</p><blockquote><p>Seed-OSS is primarily optimized for international (i18n) use cases.</p></blockquote> <h3> <a href="https://huggingface.co#key-features"> </a> <span> Key Features </span> </h3> <ul> <li><strong>Flexible Control of Thinking Budget</strong>: Allowing users to flexibly adjust the reasoning length as needed. This capability of dynamically controlling the reasoning length enhances inference efficiency in practical application scenarios.</li> <li><strong>Enhanced Reasoning Capability</strong>: Specifically optimized for reasoning tasks while maintaining balanced and excellent general capabilities.</li> <li><strong>Agentic Intelligence</strong>: Performs exceptionally well in agentic tasks such as tool-using and issue resolving.</li> <li><strong>Research-Friendly</strong>: Given that the inclusion of synthetic instruction data in pre-training may affect the post-training research, we released pre-trained models both with and without instruction data, providing the research community with more diverse options.</li> <li><strong>Native Long Context</strong>: Trained with up-to-512K long context natively.</li> </ul> <h3> <a href="https://huggingface.co#model-summary"> </a> <span> Model Summary </span> </h3><p>Seed-OSS adopts the popular causal language model architecture with RoPE, GQA attention, RMSNorm and SwiGLU activation.</p><div><table> <thead><tr> <th></th> <th></th> </tr> </thead><tbody><tr> <td></td> <td><strong>Seed-OSS-36B</strong></td> </tr> <tr> <td><strong>Parameters</strong></td> <td>36B</td> </tr> <tr> <td><strong>Attention</strong></td> <td>GQA</td> </tr> <tr> <td><strong>Activation Function</strong></td> <td>SwiGLU</td> </tr> <tr> <td><strong>Number of Layers</strong></td> <td>64</td> </tr> <tr> <td><strong>Number of QKV Heads</strong></td> <td>80 / 8 / 8</td> </tr> <tr> <td><strong>Head Size</strong></td> <td>128</td> </tr> <tr> <td><strong>Hidden Size</strong></td> <td>5120</td> </tr> <tr> <td><strong>Vocabulary Size</strong></td> <td>155K</td> </tr> <tr> <td><strong>Context Length</strong></td> <td>512K</td> </tr> <tr> <td><strong>RoPE Base Frequency</strong></td> <td>1e7</td> </tr> </tbody> </table> </div><h2> <a href="https://huggingface.co#evaluation-results"> </a> <span> Evaluation Results </span> </h2> <h3> <a href="https://huggingface.co#seed-oss-36b-base"> </a> <span> Seed-OSS-36B-Base </span> </h3><p>Incorporating synthetic instruction data into pretraining leads to improved performance on most benchmarks. We adopt the version augmented with synthetic instruction data (i.e., <em>w/ syn.</em>) as <code>Seed-OSS-36B-Base</code>. We also release <code>Seed-OSS-36B-Base-woSyn</code> trained without such data (i.e., <em>w/o syn.</em>), offering the community a high-performance foundation model unaffected by synthetic instruction data.</p><div><table> <thead> <tr> <th>Benchmark</th> <th><sup><a href="https://seed.bytedance.com/en/seed1_6">Seed1.6-Base</a></sup></th> <th><sup>Qwen3-30B-A3B-Base-2507*</sup></th> <th><sup>Qwen2.5-32B-Base*</sup></th> <th><sup>Seed-OSS-36B-Base<br>(<i>w/ syn.</i>)</sup></th> <th><sup>Seed-OSS-36B-Base-woSyn<br>(<i>w/o syn.</i>)</sup></th> </tr> </thead> <tbody> <tr> <td><strong>Knowledge</strong></td> </tr> <tr> <td>MMLU-Pro</td> <td>70</td> <td>59.8</td> <td>58.5 (55.1)</td> <td><b>65.1</b></td> <td>60.4</td> </tr> <tr> <td>MMLU</td> <td>88.8</td> <td>82.7</td> <td>84 (83.3)</td> <td><b>84.9</b></td> <td>84.8</td> </tr> <tr> <td>TriviaQA</td> <td>91</td> <td>76.2</td> <td>76</td> <td><b>82.1</b></td> <td>81.9</td> </tr> <tr> <td>GPQA-D</td> <td>43.4</td> <td><b>37</b></td> <td>29.3</td> <td>31.7</td> <td>35.2</td> </tr> <tr> <td>SimpleQA</td> <td>17.1</td> <td>7.2</td> <td>6.1</td> <td>5.8</td> <td><b>7.4</b></td> </tr> <tr> <td><strong>Reasoning</strong></td> </tr> <tr> <td>BBH</td> <td>92.1</td> <td>81.4</td> <td>79.1 (84.5)</td> <td><b>87.7</b></td> <td>87.2</td> </tr> <tr> <td>AGIEval-en</td> <td>78</td> <td>66.4</td> <td>65.6</td> <td><b>70.7</b></td> <td>70.1</td> </tr> <tr> <td><strong>Math</strong></td> </tr> <tr> <td>GSM8K</td> <td>93.1</td> <td>87</td> <td>87.5 (92.9)</td> <td><b>90.8</b></td> <td>90.3</td> </tr> <tr> <td>MATH</td> <td>72.9</td> <td>61.1</td> <td>63.5 (57.7)</td> <td><b>81.7</b></td> <td>61.3</td> </tr> <tr> <td><strong>Coding</strong></td> </tr> <tr> <td>MBPP</td> <td>83.6</td> <td>78.8</td> <td>77.8 (84.5)</td> <td><b>80.6</b></td> <td>74.6</td> </tr> <tr> <td>HumanEval</td> <td>78</td> <td>70.7</td> <td>47.6 (58.5)</td> <td><b>76.8</b></td> <td>75.6</td> </tr> </tbody> </table> </div><p><sup>- <b>Bold</b> denotes open-source SOTA. </sup><br><sup>- "*" indicates that the results in this column are presented in the format of "reproduced_results (reported_results_if_any)". </sup></p><h3> <a href="https://huggingface.co#seed-oss-36b-instruct"> </a> <span> Seed-OSS-36B-Instruct </span> </h3><div><table> <thead> <tr> <th>Benchmark</th> <th><sup><a href="https://console.volcengine.com/ark/region:ark+cn-beijing/model/detail?Id=doubao-seed-1-6-thinking">Seed1.6-Thinking-0715</a></sup></th> <th><sup>OAI-OSS-20B*</sup></th> <th><sup>Qwen3-30B-A3B-Thinking-2507*</sup></th> <th><sup>Qwen3-32B*</sup></th> <th><sup>Gemma3-27B</sup></th> <th><sup>Seed-OSS-36B-Instruct</sup></th> </tr> </thead> <tbody> <tr> <td><strong>Knowledge</strong></td> </tr> <tr> <td>MMLU-Pro</td> <td>86.6</td> <td>76.2</td> <td><ins>81.9</ins> (80.9)</td> <td>81.8</td> <td>67.5</td> <td><b>82.7</b></td> </tr> <tr> <td>MMLU</td> <td>90.6</td> <td>81.7 (85.3)</td> <td><ins>86.9</ins></td> <td>86.2</td> <td>76.9</td> <td><b>87.4</b></td> </tr> <tr> <td>GPQA-D</td> <td>80.7</td> <td><b>72.2</b> (71.5)</td> <td><ins>71.4</ins> (73.4)</td> <td>66.7 (68.4)</td> <td>42.4</td> <td><ins>71.4</ins></td> </tr> <tr> <td>SuperGPQA</td> <td>63.4</td> <td>50.1</td> <td><b>57.3</b> (56.8)</td> <td>49.3</td> <td>-</td> <td><ins>55.7</ins></td> </tr> <tr> <td>SimpleQA</td> <td>23.7</td> <td>6.7</td> <td><b>23.6</b></td> <td>8.6</td> <td><ins>10</ins></td> <td>9.7</td> </tr> <tr> <td><strong>Math</strong></td> </tr> <tr> <td>AIME24</td> <td>90.3</td> <td><b>92.7</b> (92.1)</td> <td>87.7</td> <td>82.7 (81.4)</td> <td>-</td> <td><ins>91.7</ins></td> </tr> <tr> <td>AIME25</td> <td>86</td> <td><b>90.3</b> (91.7)</td> <td>81.3 (85)</td> <td>73.3 (72.9)</td> <td>-</td> <td><ins>84.7</ins></td> </tr> <tr> <td>BeyondAIME</td> <td>60</td> <td><b>69</b></td> <td>56</td> <td>29</td> <td>-</td> <td><ins>65</ins></td> </tr> <tr> <td><strong>Reasoning</strong></td> </tr> <tr> <td>ArcAGI V2</td> <td>50.3</td> <td><b>41.7</b></td> <td>37.8</td> <td>14.4</td> <td>-</td> <td><ins>40.6</ins></td> </tr> <tr> <td>KORBench</td> <td>74.8</td> <td><b>72.3</b></td> <td>70.2</td> <td>65.4</td> <td>-</td> <td><ins>70.6</ins></td> </tr> <tr> <td>HLE</td> <td>13.9</td> <td><b>12.7</b> (10.9)</td> <td>8.7</td> <td>6.9</td> <td>-</td> <td><ins>10.1</ins></td> </tr> <tr> <td><strong>Coding</strong></td> </tr> <tr> <td>LiveCodeBench v6<br><sup>(02/2025-05/2025)</sup></td> <td>66.8</td> <td><ins>63.8</ins></td> <td>60.3 (66)</td> <td>53.4</td> <td>-</td> <td><b>67.4</b></td> </tr> <tr> <td><strong>Instruction Following</strong></td> </tr> <tr> <td>IFEval</td> <td>86.3</td> <td><b>92.8</b></td> <td>88 (88.9)</td> <td>88.4 (85)</td> <td><ins>90.4</ins></td> <td>85.8</td> </tr> <tr> <td><strong>Agent</strong></td> </tr> <tr> <td>TAU1-Retail</td> <td>63</td> <td>(54.8)</td> <td><ins>58.7</ins> (67.8)</td> <td>40.9</td> <td>-</td> <td><b>70.4</b></td> </tr> <tr> <td>TAU1-Airline</td> <td>49</td> <td>(38)</td> <td><b>47</b> (48)</td> <td>38</td> <td>-</td> <td><ins>46</ins></td> </tr> <tr> <td>SWE-Bench Verified<br><sup>(OpenHands)</sup></td> <td>41.8</td> <td><b>(60.7)</b></td> <td>31</td> <td>23.4</td> <td>-</td> <td><ins>56</ins></td> </tr> <tr> <td>SWE-Bench Verified<br><sup>(AgentLess 4*10)</sup></td> <td>48.4</td> <td>-</td> <td>33.5</td> <td><ins>39.7</ins></td> <td>-</td> <td><b>47</b></td> </tr> <tr> <td>Multi-SWE-Bench</td> <td>17.7</td> <td>-</td> <td><ins>9.5</ins></td> <td>7.7</td> <td>-</td> <td><b>17</b></td> </tr> <tr> <td><strong>Multilingualism</strong></td> </tr> <tr> <td>MMMLU</td> <td>84.3</td> <td>77.4 (75.7)</td> <td><b>79</b></td> <td><b>79</b> (80.6)</td> <td>-</td> <td><ins>78.4</ins></td> </tr> <tr> <td><strong>Long Context</strong></td> </tr> <tr> <td>RULER<br><sup>(128K)</sup></td> <td>94.5</td> <td>78.7</td> <td><ins>94.5</ins></td> <td>77.5</td> <td>-</td> <td><b>94.6</b></td> </tr> <tr> <td><strong>Safety</strong></td> </tr> <tr> <td>AIR-Bench</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>75.6</td> </tr> </tbody> </table> </div><p><sup>- <b>Bold</b> denotes open-source SOTA. <ins>Underlined</ins> indicates the second place in the open-source model. </sup><br><sup>- "*" indicates that the results in this column are presented in the format of "reproduced_results (reported_results_if_any)". Some results have been omitted due to the failure of the evaluation run. </sup><br><sup>- The results of Gemma3-27B are sourced directly from its technical report. </sup><br><sup>- Generation configs for Seed-OSS-36B-Instruct: temperature=1.1, top_p=0.95. Specifically, for Taubench, temperature=1, top_p=0.7. </sup><br><sup></sup></p><blockquote><p>We recommend sampling with <code>temperature=1.1</code> and <code>top_p=0.95</code>.</p></blockquote> <h3> <a href="https://huggingface.co#thinking-budget"> </a> <span> Thinking Budget </span> </h3><p>Users can flexibly specify the model's thinking budget. The figure below shows the performance curves across different tasks as the thinking budget varies. For simpler tasks (such as IFEval), the model's chain of thought (CoT) is shorter, and the score exhibits fluctuations as the thinking budget increases. For more challenging tasks (such as AIME and LiveCodeBench), the model's CoT is longer, and the score improves with an increase in the thinking budget.</p><p>Here is an example with a thinking budget set to 512: during the reasoning process, the model periodically triggers self-reflection to estimate the consumed and remaining budget, and delivers the final response once the budget is exhausted or the reasoning concludes.</p><pre><code><seed:think> Got it, let's try to solve this problem step by step. The problem says ... ... <seed:cot_budget_reflect>I have used 129 tokens, and there are 383 tokens remaining for use.</seed:cot_budget_reflect> Using the power rule, ... ... <seed:cot_budget_reflect>I have used 258 tokens, and there are 254 tokens remaining for use.</seed:cot_budget_reflect> Alternatively, remember that ... ... <seed:cot_budget_reflect>I have used 393 tokens, and there are 119 tokens remaining for use.</seed:cot_budget_reflect> Because if ... ... <seed:cot_budget_reflect>I have exhausted my token budget, and now I will start answering the question.</seed:cot_budget_reflect> </seed:think> To solve the problem, we start by using the properties of logarithms to simplify the given equations: (full answer omitted). </code></pre><p>If no thinking budget is set (default mode), Seed-OSS will initiate thinking with unlimited length. If a thinking budget is specified, users are advised to prioritize values that are integer multiples of 512 (e.g., 512, 1K, 2K, 4K, 8K, or 16K), as the model has been extensively trained on these intervals. Models are instructed to output a direct response when the thinking budget is 0, and we recommend setting any budget below 512 to this value.</p><h2> <a href="https://huggingface.co#quick-start"> </a> <span> Quick Start </span> </h2><pre><code>pip3 install -r requirements.txt pip install git+ssh://git@github.com/Fazziekey/transformers.git@seed-oss </code></pre><pre><code><span>from</span> transformers <span>import</span> AutoModelForCausalLM, AutoTokenizer <span>import</span> os <span>import</span> re model_name_or_path = <span>"ByteDance-Seed/Seed-OSS-36B-Instruct"</span> tokenizer = AutoTokenizer.from_pretrained(model_name_or_path) model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=<span>"auto"</span>) <span># You may want to use bfloat16 and/or move to GPU here</span> messages = [ {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"How to make pasta?"</span>}, ] tokenized_chat = tokenizer.apply_chat_template( messages, tokenize=<span>True</span>, add_generation_prompt=<span>True</span>, return_tensors=<span>"pt"</span>, thinking_budget=<span>512</span> <span># control the thinking budget</span> ) outputs = model.generate(tokenized_chat.to(model.device), max_new_tokens=<span>2048</span>) output_text = tokenizer.decode(outputs[]) </code></pre> <h2> <a href="https://huggingface.co#inference"> </a> <span> Inference </span> </h2> <h3> <a href="https://huggingface.co#download-model"> </a> <span> Download Model </span> </h3><p>Download Seed-OSS checkpoint to <code>./Seed-OSS-36B-Instruct</code></p><h3> <a href="https://huggingface.co#transformers"> </a> <span> Transformers </span> </h3><p>The <code>generate.py</code> script provides a simple interface for model inference with configurable options.</p><h4> <a href="https://huggingface.co#basic-usage"> </a> <span> Basic Usage </span> </h4><pre><code>cd inference python3 generate.py --model_path /path/to/model </code></pre> <h4> <a href="https://huggingface.co#key-parameters"> </a> <span> Key Parameters </span> </h4><div><table> <thead><tr> <th>Parameter</th> <th>Description</th> </tr> </thead><tbody><tr> <td><code>--model_path</code></td> <td>Path to the pretrained model directory (required)</td> </tr> <tr> <td><code>--prompts</code></td> <td>Input prompts (default: sample cooking/code questions)</td> </tr> <tr> <td><code>--max_new_tokens</code></td> <td>Maximum tokens to generate (default: 4096)</td> </tr> <tr> <td><code>--attn_implementation</code></td> <td>Attention mechanism: <code>flash_attention_2</code> (default) or <code>eager</code></td> </tr> <tr> <td><code>--load_in_4bit/8bit</code></td> <td>Enable 4-bit/8-bit quantization (reduces memory usage)</td> </tr> <tr> <td><code>--thinking_budget</code></td> <td>Thinking budget in tokens (default: -1 for unlimited budget)</td> </tr> </tbody> </table> </div><h4> <a href="https://huggingface.co#quantization-examples"> </a> <span> Quantization Examples </span> </h4><pre><code><span># </span><span>8-bit quantization</span> python3 generate.py --model_path /path/to/model --load_in_8bit True <span># </span><span>4-bit quantization</span> python3 generate.py --model_path /path/to/model --load_in_4bit True </code></pre> <h4> <a href="https://huggingface.co#custom-prompts"> </a> <span> Custom Prompts </span> </h4><pre><code>python3 generate.py --model_path /path/to/model --prompts "['What is machine learning?', 'Explain quantum computing']" </code></pre> <h3> <a href="https://huggingface.co#vllm"> </a> <span> vLLM </span> </h3><p>Use vllm &gt;= 0.10.0 or higher for inference.</p><ul> <li>First install vLLM with Seed-OSS support version:</li> </ul><pre><code>VLLM_USE_PRECOMPILED=1 VLLM_TEST_USE_PRECOMPILED_NIGHTLY_WHEEL=1 pip install git+ssh://git@github.com/FoolPlayer/vllm.git@seed-oss </code></pre> <ul> <li>Start vLLM API server:</li> </ul><pre><code>python3 -m vllm.entrypoints.openai.api_server \ --host localhost \ --port 4321 \ --enable-auto-tool-choice \ --tool-call-parser seed_oss \ --trust-remote-code \ --model ./Seed-OSS-36B-Instruct \ --chat-template ./Seed-OSS-36B-Instruct/chat_template.jinja \ --tensor-parallel-size 8 \ --dtype bfloat16 \ --served-model-name seed_oss </code></pre> <ul> <li>Test with OpenAI client:</li> </ul><p>Chat</p><pre><code><span># </span><span>no stream</span> python3 inference/vllm_chat.py --max_new_tokens 4096 --thinking_budget -1 <span># </span><span>stream</span> python3 inference/vllm_chat.py --max_new_tokens 4096 --thinking_budget -1 --stream </code></pre><p>Tool Call</p><pre><code><span># </span><span>no stream</span> python3 inference/vllm_tool_call.py --max_new_tokens 4096 --thinking_budget -1 <span># </span><span>stream</span> python3 inference/vllm_tool_call.py --max_new_tokens 4096 --thinking_budget -1 --stream </code></pre> <h2> <a href="https://huggingface.co#model-card"> </a> <span> Model Card </span> </h2><p>See <a href="https://huggingface.co/unsloth/Seed-OSS-36B-Instruct-GGUF/blob/main/./MODEL_CARD.md">MODEL_CARD</a>.</p><h2> <a href="https://huggingface.co#license"> </a> <span> License </span> </h2><p>This project is licensed under Apache-2.0. See the <a href="https://huggingface.co/unsloth/Seed-OSS-36B-Instruct-GGUF/blob/main/./LICENSE">LICENSE</a> flie for details.</p><h2> <a href="https://huggingface.co#citation"> </a> <span> Citation </span> </h2><pre><code>@misc{seed2025seed-oss, author={ByteDance Seed Team}, title={Seed-OSS Open-Source Models}, year={2025}, howpublished={\url{https://github.com/ByteDance-Seed/seed-oss}} } </code></pre> <h2> <a href="https://huggingface.co#about-bytedance-seed-team"> </a> <span> About <a href="https://seed.bytedance.com/">ByteDance Seed Team</a> </span> </h2><p>Founded in 2023, ByteDance Seed Team is dedicated to crafting the industry's most advanced AI foundation models. The team aspires to become a world-class research team and make significant contributions to the advancement of science and society.</p></div></section>]]></description><pubDate>Mon, 25 Aug 2025 06:20:09 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1myspiq/gemma3_unsloth_template_error/</link><title>Gemma-3 Unsloth template error</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1myspiq/gemma3_unsloth_template_error/</guid><comments>https://www.reddit.com/r/unsloth/comments/1myspiq/gemma3_unsloth_template_error/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1myspiq/gemma3_unsloth_template_error/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi guys... I try to make fintune of Gemma-3-270M but always get this error when i try to save it like gguf... Any ideas what is wrong with unsloth google collab template?</p><p>i</p></div><!-- SC_ON --></section>]]></description><pubDate>Sun, 24 Aug 2025 16:34:35 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1myrk9o/making_some_silly_mistake_while_saving_to_gguf/</link><title>Making some silly mistake while saving to GGUF from Lora?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1myrk9o/making_some_silly_mistake_while_saving_to_gguf/</guid><comments>https://www.reddit.com/r/unsloth/comments/1myrk9o/making_some_silly_mistake_while_saving_to_gguf/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 3 min | <a href='https://www.reddit.com/r/unsloth/comments/1myrk9o/making_some_silly_mistake_while_saving_to_gguf/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi</p><p>I ran a training run earlier on gemma3-270m and created a lora, which I saved in my google drive. I did not at that point save a gguf.</p><p>So now when I use colab and download the Lora and attempt to create a gguf, I&#39;m getting an error. I haven&#39;t done a save to gguf ever earlier, so I am not sure if I am making some silly mistake. Basically just copied the code from the official notebook and ran it, but not working. Can someone take a look.</p><p>My code:```</p><p>from google.colab import drive</p><p>drive.mount(&#39;/content/drive&#39;)</p><p>!cp -r /content/drive/MyDrive/stuff/lora_model .</p><p>from transformers import TextStreamer</p><p>from unsloth import FastModel</p><p>import torch</p><p>from unsloth import FastLanguageModel</p><p>from peft import PeftModel</p><p>max_seq_length = 3072</p><p>model, tokenizer = FastLanguageModel.from_pretrained(</p><pre><code>model_name = &quot;unsloth/gemma-3-270m-it&quot;, # YOUR MODELmax_seq_length = max_seq_length,load_in_4bit = False,  # 4 bit quantization to reduce memoryload_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memoryfull_finetuning = False, # [NEW!] We have full finetuning now!</code></pre><p>)</p><p>model = PeftModel.from_pretrained(model, &quot;lora_model&quot;)</p><pre><code>text = \[MY TESTING SAMPLE HERE\]</code></pre><p>_ = model.generate(</p><pre><code>**tokenizer(text, return_tensors = &quot;pt&quot;).to(&quot;cuda&quot;),max_new_tokens = 125,temperature = 1, top_p = 0.95, top_k = 64,streamer = TextStreamer(tokenizer, skip_prompt = True),</code></pre><p>)</p><p>print(&#39;\n+++++++++++++++++++++++++++++\n&#39;)</p><p>model.save_pretrained_merged(&quot;model&quot;, tokenizer, save_method = &quot;merged_16bit&quot;)</p><p>model.save_pretrained_gguf(&quot;model&quot;, tokenizer, quantization_method = &quot;q8_0&quot;)</p><p>```The load and inference run fine. Inference is in the finetuned format as expected. But when the GGUF part starts up, get this error.</p><p>If I run just the GGUF saving, then it says input folder not found, I guess because there is no model folder?</p><pre><code>/usr/local/lib/python3.12/dist-packages/unsloth\_zoo/saving\_utils.py:632: UserWarning: Model is not a PeftModel (no Lora adapters detected). Skipping Merge. Please use save\_pretrained() or push\_to\_hub() instead!warnings.warn(&quot;Model is not a PeftModel (no Lora adapters detected). Skipping Merge. Please use save\_pretrained() or push\_to\_hub() instead!&quot;)\---------------------------------------------------------------------------RuntimeError                              Traceback (most recent call last)/tmp/ipython-input-1119511992.py in &lt;cell line: 0&gt;()1 model.save\_pretrained\_merged(&quot;model&quot;, tokenizer, save\_method = &quot;merged\_16bit&quot;)\----&gt; 2 model.save\_pretrained\_gguf(&quot;model&quot;, tokenizer, quantization\_method = &quot;q8\_0&quot;)2 frames/usr/local/lib/python3.12/dist-packages/unsloth\_zoo/llama\_cpp.py in convert\_to\_gguf(input\_folder, output\_filename, quantization\_type, max\_shard\_size, print\_output, print\_outputs)654655     if not os.path.exists(input\_folder):\--&gt; 656         raise RuntimeError(f&quot;Unsloth: \`{input\_folder}\` does not exist?&quot;)657658     config\_file = os.path.join(input\_folder, &quot;config.json&quot;)RuntimeError: Unsloth: \`model\` does not exist?</code></pre><p>I also tried loading just the lora and then running inference.```    model, tokenizer = FastLanguageModel.from_pretrained(</p><pre><code>model_name = &quot;lora_model&quot;, # YOUR MODELmax_seq_length = max_seq_length,load_in_4bit = False,  # 4 bit quantization to reduce memoryload_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memoryfull_finetuning = False, # [NEW!] We have full finetuning now!)</code></pre><p>```</p><p>In such cases, the inference is the same as the vanilla untuned model and my  finetuning does not take effect.</p></div><!-- SC_ON --></section>]]></description><pubDate>Sun, 24 Aug 2025 15:26:45 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1myqw99/fine_tuned_qwen_model_following_grpo_notebook/</link><title>Fine tuned Qwen model following GRPO notebook sometimes infinitely repeats lines</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1myqw99/fine_tuned_qwen_model_following_grpo_notebook/</guid><comments>https://www.reddit.com/r/unsloth/comments/1myqw99/fine_tuned_qwen_model_following_grpo_notebook/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1myqw99/fine_tuned_qwen_model_following_grpo_notebook/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi all, </p><p>Getting into fine tuning LLMs and have currently been following the Qwen 4 GRPO notebook (<a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb">https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb</a> ) that shows how to train a model to have deepseek style reasoning traces. However, after training and when testing the model (exported model and run on llama.cpp), I notice that the model will more often than not end up repeating a sentence or two endlessly (e.g. in the reasoning CoT, model gets ‚Äústuck‚Äù and endlessly repeats a line, for example ‚Äústep 10: {some math calculation}\nstep 10: {some math calculation}\n‚Ä¶ ‚Äú, or something like sentence1\nsentence2\nsentence1‚Ä¶ etc.) on a prompt. It sometimes produces the correct answer in the expected format, but more often than not it does the above, even when on the right track.</p><p>I‚Äôve tried training from the qwen3 4b base model and the 2507 instruct variant (thinking that maybe since the instruct is trained for instruction following and already ‚Äúunderstands‚Äù the chat template but to no avail). I‚Äôve also rented an a100 for a bit to see if a larger model (qwen3-30b) would have same issue, but seems like I run into the same problem.</p><p>I‚Äôve currently been using a custom synthetically generated dataset with 665 rows, with approx. 30pct of them being general conversational text and the other 70% being domain specific questions (in this case mostly math and code related questions), in the same format as the unsloth/openmathreasoning-mini dataset used as a primer dataset. Settings for that part is left basically default (num epoch set to 2, etc). The GRPO trainer after uses dataset with both code and mathematical questions, with similar reward functions to the original notebook, with mathematical questions graded on correctness and code based on how much testcases passed (I‚Äôve also added a reward function to penalize constant repeat of lines), and I‚Äôve trained for about 500 steps.</p><p>I‚Äôve noticed a few issues similar to this, but the mentioned fixes seem to always be related to chat template issues, whereas my fine tuned model will have this issue sometimes but not always. I have been experimenting with using the qwen3 chat template with tool call support, but the issue is present on the base chatML style chat template used during finetuning as well.</p><p>I‚Äôm curious on any ideas how I can solve this issue. I‚Äôve tried presence/repeat/frequency penalty, but it doesn‚Äôt really work out and ultimately is only a bandaid fix. Is the ‚Äúprimer‚Äù dataset too large or overfitting the model? Do I need to run the GRPO trainer for more steps? I‚Äôm running it for ‚Äúonly‚Äù about 500 steps, is this too little/not enough? Should the dataset for my GRPO trainer be more diverse? </p><p>I‚Äôm only a traditional programmer and have only dabbled in computer vision before, a bit lost in LLM training lol, any suggestions and help would be extremely appreciated. Thanks!</p></div><!-- SC_ON --></section>]]></description><pubDate>Sun, 24 Aug 2025 14:44:43 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mxz9bg/ampere_issue/</link><title>Ampere Issue</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mxz9bg/ampere_issue/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mxz9bg/ampere_issue/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mxz9bg/ampere_issue/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I am trying to use unsloth for fine tuning. Unfortunately, I have trouble satisfying dependencies for a couple of days now. There is a conflict<br/>The Base Package (unsloth) requires xformers &gt;= 0.0.27.post2 while The GPU-Specific Package (unsloth[cu121-ampere]) requires xformers == 0.0.22.post7. Can anyone help? I have a paper submission deadline by end of month and without this, we will not be able to submit.</p><pre><code>+-----------------------------------------------------------------------------------------+| NVIDIA-SMI 570.169                Driver Version: 570.169        CUDA Version: 12.8     ||-----------------------------------------+------------------------+----------------------+| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC || Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. ||                                         |                        |               MIG M. ||=========================================+========================+======================||   0  NVIDIA RTX A6000               Off |   00000000:3B:00.0 Off |                  Off || 30%   28C    P8              9W /  300W |       4MiB /  49140MiB |      0%      Default ||                                         |                        |                  N/A |+-----------------------------------------+------------------------+----------------------++-----------------------------------------------------------------------------------------+| Processes:                                                                              ||  GPU   GI   CI              PID   Type   Process name                        GPU Memory ||        ID   ID                                                               Usage      ||=========================================================================================||  No running processes found                                                             |+-----------------------------------------------------------------------------------------+[project]# pyproject.toml[project]name = &quot;unsloth fine tuning&quot;version = &quot;0.1.0&quot;description = &quot;Local tools&quot;requires-python = &quot;&gt;=3.11&quot;dependencies = [    # --- Core Dependencies ---    &quot;pandas&quot;, &quot;sacrebleu&quot;, &quot;unbabel-comet&quot;, &quot;rouge-score&quot;,    &quot;sentence-transformers&quot;, &quot;openpyxl&quot;, &quot;nltk&gt;=3.9.1&quot;, &quot;httpx&quot;,    &quot;requests&quot;, &quot;pydantic&quot;, &quot;pydantic-settings&quot;,    &quot;unsloth[cu121-ampere]&quot;,    &quot;transformers&gt;=4.41&quot;, &quot;datasets&quot;, &quot;peft&quot;, &quot;bitsandbytes&quot;,    &quot;trl&quot;, &quot;accelerate&quot;, &quot;optuna&quot;,]</code></pre><p>This is my dockerfile  </p><pre><code>FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04ENV DEBIAN_FRONTEND=noninteractiveRUN apt-get update &amp;&amp; apt-get install -y \    python3.11 \    python3.11-venv \    python3-pip \    git \    curl \    gnupg \    lsb-release \    cmake \    &amp;&amp; rm -rf /var/lib/apt/lists/*# Install Docker CLIRUN curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg &amp;&amp; \    echo &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(. /etc/os-release &amp;&amp; echo $VERSION_CODENAME) stable&quot; | tee /etc/apt/sources.list.d/docker.list &gt; /dev/null &amp;&amp; \    apt-get update &amp;&amp; \    apt-get install -y docker-ce-cli &amp;&amp; \    rm -rf /var/lib/apt/lists/*# Install Ollama CLIRUN curl -fsSL https://ollama.com/install.sh | shWORKDIR /installCOPY pyproject.toml ./RUN python3.11 -m pip install --upgrade pip uvRUN uv venv /opt/venv --clearENV PATH=&quot;/opt/venv/bin:$PATH&quot;RUN uv sync --extra-index-url https://download.pytorch.org/whl/cu121 --index-strategy unsafe-best-match --prerelease=allowWORKDIR /workspaceRUN useradd --create-home --shell /bin/bash unslothRUN chown -R unsloth:unsloth /workspaceUSER unslothENV SHELL=/bin/bash</code></pre></div><!-- SC_ON --></section>]]></description><pubDate>Sat, 23 Aug 2025 17:14:04 +0530</pubDate></item><item><link>https://i.redd.it/2x5q1odxkmkf1.png</link><title>Run DeepSeek-V3.1 locally with Dynamic 1-bit GGUFs! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mxgjg8/run_deepseekv31_locally_with_dynamic_1bit_ggufs/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mxgjg8/run_deepseekv31_locally_with_dynamic_1bit_ggufs/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mxgjg8/run_deepseekv31_locally_with_dynamic_1bit_ggufs/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guy - you can now run DeepSeek-V3.1 locally on 170GB RAM with our Dynamic 1-bit GGUFs.üêã</p><p>The most popular GGUF sizes are now all i-matrix quantized! GGUFs: <a href="https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF">https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF</a>  </p><p>The 715GB model gets reduced to 170GB (-80% size) by smartly quantizing layers. This 162GB works for Ollama so you can run the command:</p><pre><code>OLLAMA_MODELS=unsloth_downloaded_models ollama serve &amp;ollama run hf.co/unsloth/DeepSeek-V3.1-GGUF:TQ1_0</code></pre><p>We also fixed the chat template for llama.cpp supported tools. The 1-bit IQ1_M GGUF passes all our coding tests, however 2-bit Q2_K_XL is recommended.</p><p>Guide + info: <a href="https://docs.unsloth.ai/basics/deepseek-v3.1">https://docs.unsloth.ai/basics/deepseek-v3.1</a></p><p>Thank you everyone and please let us know how it goes! :)</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/2x5q1odxkmkf1.png' /></section>]]></description><pubDate>Sat, 23 Aug 2025 01:21:12 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mxfnl3/can_someone_explain_whats_load_in_4bit/</link><title>Can someone explain what's "load_in_4bit"</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mxfnl3/can_someone_explain_whats_load_in_4bit/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mxfnl3/can_someone_explain_whats_load_in_4bit/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mxfnl3/can_someone_explain_whats_load_in_4bit/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>When do I use it, and when do I not?</p><p>I know it enables 4-bit quantization, but does it quantize a model by loading it into CPU memory first and then loading the quantized version into VRAM?</p><p>Does it decrease the quality of the LoRA?</p><p>Does it make the LoRA only compatible with the 4-bit quantized version of the model? o</p><p>I‚Äôm going to try fine-tuning qwen3-235b-a22b, and then during inference either serve it as Q4, Q8 or FP8, whichever has the best speed:quality ration I‚Äôm still not quite sure whether I should set this or load_in_8bit to True or False.</p></div><!-- SC_ON --></section>]]></description><pubDate>Sat, 23 Aug 2025 00:46:57 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mxbkjq/gguf_request_for_interns1mini8b/</link><title>GGUF Request for InternS1-Mini-8B !</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mxbkjq/gguf_request_for_interns1mini8b/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mxbkjq/gguf_request_for_interns1mini8b/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 3 min | <a href='https://www.reddit.com/r/unsloth/comments/1mxbkjq/gguf_request_for_interns1mini8b/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hello Unsloth community, <a href="https://www.reddit.com/u/danielhanchen">u/danielhanchen</a>, and <a href="https://www.reddit.com/u/yoracale">u/yoracale</a>,</p><p>I&#39;m a big fan of the amazing work you do in making powerful models accessible to everyone with your incredible quantization and training optimizations. The speed and memory savings you&#39;ve achieved for so many models are a game-changer for local inference. And with active collaborations, you have been able to bring zero-day ggufs for many latest models.</p><p>I&#39;m writing to request that you consider creating a GGUF quantization of a fascinating new model that was just released: <strong>InternS1-Mini-8B</strong> (<a href="https://huggingface.co/internlm/Intern-S1-mini">https://huggingface.co/internlm/Intern-S1-mini</a>) that may have gone under your radar. </p><p><strong><em>Edit-</em></strong> <a href="https://www.reddit.com/u/mortyspace">u/mortyspace</a> kindly made the quants for the model and they work great. Anyone interested can find them at <a href="https://huggingface.co/yarikdevcom/Intern-S1-mini-GGUF">https://huggingface.co/yarikdevcom/Intern-S1-mini-GGUF</a></p><h3><strong>What is InternS1-Mini-8B?</strong></h3><p>InternS1-Mini-8B is a new multimodal model from the same team behind the popular InternVL and InternLM models. While it&#39;s a smaller, more accessible version of their larger InternS1 model, it has a unique and powerful specialization.</p><ul><li><strong>Multimodal:</strong> It can process both text and images, which is essential for its primary use case.</li><li><strong>Built for Science:</strong> Unlike general-purpose multimodal models, InternS1-Mini-8B has been continuously pre-trained on a massive, 5 trillion token dataset, with over half of that data being scientific literature, diagrams, chemical formulas, and protein sequences. This deep domain expertise makes it a dedicated &quot;scientific research assistant.&quot;</li><li><strong>Efficient Architecture:</strong> The model uses a dense 8B-parameter language model (Qwen3-8B) and a 0.3B vision encoder, making it much more lightweight than its larger counterpart.</li></ul><h3><strong>Why is this model so interesting and important?</strong></h3><p>InternS1-Mini-8B isn&#39;t just another multimodal model‚Äîit&#39;s a specialized tool that could revolutionize local scientific research.</p><ul><li><strong>Interprets Complex Scientific Data:</strong> It can natively understand and reason about chemical structures, synthesis routes, protein sequences, and intricate diagrams. This goes beyond simple image captioning and allows for genuine scientific dialogue. It would also be fantastic in augmenting scientific RAG applications. </li><li><strong>Scientific Problem-Solving:</strong> Imagine a local model that can help you interpret a complex graph from a research paper, analyze a chemical structure from a picture, or even assist in brainstorming new experimental pathways. This is exactly what InternS1-Mini-8B is designed to do.</li><li><strong>Accessibility for Researchers:</strong> Having a locally runnable, quantized version of this model would make cutting-edge AI a reality for countless people working in chemistry, biology, materials science, and other fields.</li></ul><h3><strong>The Request:</strong></h3><p>I&#39;m aware that the Intern team has already released some GGUF quants, specifically <code>Q8_0</code> and <code>F16</code>. While this is a great start, these quants are still very large and can be challenging to run on typical consumer laptops with 8GB of VRAM.</p><p>This is where your work shines. The U-D quants you&#39;ve created are known to be far more memory-efficient and performant without a significant loss in quality. They would make InternS1-Mini-8B truly accessible to a much broader audience, including researchers and students who rely on more modest hardware.</p><p>We would be incredibly grateful if you could work your Unsloth magic on InternS1-Mini-8B. The efficiency and performance gains from your U-D quantizations would make this powerful scientific tool accessible on consumer hardware, democratizing AI for scientific research.</p></div><!-- SC_ON --></section>]]></description><pubDate>Fri, 22 Aug 2025 22:12:22 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mxaasb/question_about_rl/</link><title>Question about RL</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mxaasb/question_about_rl/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mxaasb/question_about_rl/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mxaasb/question_about_rl/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>So I was reading about RL and PPO and GRPO and their difference in Unsloth docs, and from my understanding, it works for tasks that are verifiable or closely verifiable or have a deterministic answer. What if I want the model to just generate better PDF outputs and layouts? I do have hand picked examples but in this case I assume RL would not work for me cuz there is no way really to have a reward function.</p><p>I have also noticed that it talks about thinking tokens coming up while training with GRPO, but lets say I wanna train a non thinking model instruction only, I should ditch this method?</p></div><!-- SC_ON --></section>]]></description><pubDate>Fri, 22 Aug 2025 21:24:43 +0530</pubDate></item><item><link>https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF</link><title>Run Preliminary DeepSeek-V3.1 Unsloth Dynamic GGUFs (huggingface.co)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mwo3zu/run_preliminary_deepseekv31_unsloth_dynamic_ggufs/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mwo3zu/run_preliminary_deepseekv31_unsloth_dynamic_ggufs/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 11 min | <a href='https://www.reddit.com/r/unsloth/comments/1mwo3zu/run_preliminary_deepseekv31_unsloth_dynamic_ggufs/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys we uploaded preliminary non-imatrix quants for those who want to run it. They&#39;re all still dynamic and run very well - just not i-matrix quantized: <a href="https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF">https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF</a></p><ul><li>UD-Q2_K_XL (247GB) is recommended</li><li>Read our guide on how to run it: <a href="https://docs.unsloth.ai/basics/deepseek-v3.1">https://docs.unsloth.ai/basics/deepseek-v3.1</a></li></ul><p>There&#39;s some issues we have to resolve for imatrix and we will likely announce the imatrix quants in like 15 hours or so.</p><p>Happy running and let us know how these preliminary quants perform :)</p></div><!-- SC_ON --></section><section class='separator separator-after-selftext'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><div><p><strong>Learn how to run DeepSeek-V3.1 correctly - <a href="https://docs.unsloth.ai/basics/deepseek-v3.1">Read our Guide</a>.</strong> </p><p><em><a href="https://docs.unsloth.ai/basics/unsloth-dynamic-v2.0-gguf">Unsloth Dynamic 2.0</a> achieves superior accuracy &amp; outperforms other leading quants.</em> </p><h2>&#128011; DeepSeek-V3.1 Usage Guidelines</h2> </div><p>These quants include our Unsloth chat template fixes, specifically for llama.cpp supported backends.</p><ul> <li>You must use --jinja for llama.cpp quants</li> <li>Set the temperature <strong>~0.6</strong> (recommended) and Top_P value of <strong>0.95</strong> (recommended)</li> <li>UD-Q2_K_XL (247GB) is recommended</li> <li>For complete detailed instructions, see our guide: <a href="https://docs.unsloth.ai/basics/deepseek-v3.1">unsloth.ai/blog/deepseek-v3.1</a></li> </ul> <hr><p><a href="https://www.deepseek.com/"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" alt="Homepage"> </a> <a href="https://chat.deepseek.com/"> <img src="https://img.shields.io/badge/%F0%9F%A4%96%20Chat-DeepSeek%20V3-536af5?color=536af5&amp;logoColor=white" alt="Chat"> </a> <a href="https://huggingface.co/deepseek-ai"> <img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;logoColor=white" alt="Hugging Face"> </a> </p><p><a href="https://huggingface.coLICENSE"> <img src="https://img.shields.io/badge/License-MIT-f5de53?&amp;color=f5de53" alt="License"> </a> </p><h2> <a href="https://huggingface.co#introduction"> </a> <span> Introduction </span> </h2><p>DeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode. Compared to the previous version, this upgrade brings improvements in multiple aspects:</p><ul> <li><p><strong>Hybrid thinking mode</strong>: One model supports both thinking mode and non-thinking mode by changing the chat template. </p></li> <li><p><strong>Smarter tool calling</strong>: Through post-training optimization, the model's performance in tool usage and agent tasks has significantly improved.</p></li> <li><p><strong>Higher thinking efficiency</strong>: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly.</p></li> </ul><p>DeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. We have expanded our dataset by collecting additional long documents and substantially extending both training phases. The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens. Additionally, DeepSeek-V3.1 is trained using the UE8M0 FP8 scale data format to ensure compatibility with microscaling data formats.</p><h2> <a href="https://huggingface.co#model-downloads"> </a> <span> Model Downloads </span> </h2> <h2> <a href="https://huggingface.co#chat-template"> </a> <span> Chat Template </span> </h2><p>The details of our chat template is described in <code>tokenizer_config.json</code> and <code>assets/chat_template.jinja</code>. Here is a brief description.</p><h3> <a href="https://huggingface.co#non-thinking"> </a> <span> Non-Thinking </span> </h3> <h4> <a href="https://huggingface.co#first-turn"> </a> <span> First-Turn </span> </h4><p>Prefix: <code>&lt;&#65372;begin&#9601;of&#9601;sentence&#65372;&gt;{system prompt}&lt;&#65372;User&#65372;&gt;{query}&lt;&#65372;Assistant&#65372;&gt;</code></p><p>With the given prefix, DeepSeek V3.1 generates responses to queries in non-thinking mode. Unlike DeepSeek V3, it introduces an additional token <code></code>.</p><h4> <a href="https://huggingface.co#multi-turn"> </a> <span> Multi-Turn </span> </h4><p>Context: <code>&lt;&#65372;begin&#9601;of&#9601;sentence&#65372;&gt;{system prompt}&lt;&#65372;User&#65372;&gt;{query}&lt;&#65372;Assistant&#65372;&gt;{response}&lt;&#65372;end&#9601;of&#9601;sentence&#65372;&gt;...&lt;&#65372;User&#65372;&gt;{query}&lt;&#65372;Assistant&#65372;&gt;{response}&lt;&#65372;end&#9601;of&#9601;sentence&#65372;&gt;</code></p><p>Prefix: <code>&lt;&#65372;User&#65372;&gt;{query}&lt;&#65372;Assistant&#65372;&gt;</code></p><p>By concatenating the context and the prefix, we obtain the correct prompt for the query.</p><h3> <a href="https://huggingface.co#thinking"> </a> <span> Thinking </span> </h3> <h4> <a href="https://huggingface.co#first-turn-1"> </a> <span> First-Turn </span> </h4><p>Prefix: <code>&lt;&#65372;begin&#9601;of&#9601;sentence&#65372;&gt;{system prompt}&lt;&#65372;User&#65372;&gt;{query}&lt;&#65372;Assistant&#65372;&gt;<think></think></code></p><p>The prefix of thinking mode is similar to DeepSeek-R1. </p><h4> <a href="https://huggingface.co#multi-turn-1"> </a> <span> Multi-Turn </span> </h4><p>Context: <code>&lt;&#65372;begin&#9601;of&#9601;sentence&#65372;&gt;{system prompt}&lt;&#65372;User&#65372;&gt;{query}&lt;&#65372;Assistant&#65372;&gt;{response}&lt;&#65372;end&#9601;of&#9601;sentence&#65372;&gt;...&lt;&#65372;User&#65372;&gt;{query}&lt;&#65372;Assistant&#65372;&gt;{response}&lt;&#65372;end&#9601;of&#9601;sentence&#65372;&gt;</code></p><p>Prefix: <code>&lt;&#65372;User&#65372;&gt;{query}&lt;&#65372;Assistant&#65372;&gt;<think></think></code></p><p>The multi-turn template is the same with non-thinking multi-turn chat template. It means the thinking token in the last turn will be dropped but the <code></code> is retained in every turn of context. </p><h3> <a href="https://huggingface.co#toolcall"> </a> <span> ToolCall </span> </h3><p>Toolcall is supported in non-thinking mode. The format is: </p><p><code>&lt;&#65372;begin&#9601;of&#9601;sentence&#65372;&gt;{system prompt}{tool_description}&lt;&#65372;User&#65372;&gt;{query}&lt;&#65372;Assistant&#65372;&gt;</code> where the tool_description is </p><pre><code>## Tools You have access to the following tools: ### {tool_name1} Description: {description} Parameters: {json.dumps(parameters)} IMPORTANT: ALWAYS adhere to this exact format for tool use: &lt;&#65372;tool&#9601;calls&#9601;begin&#65372;&gt;&lt;&#65372;tool&#9601;call&#9601;begin&#65372;&gt;tool_call_name&lt;&#65372;tool&#9601;sep&#65372;&gt;tool_call_arguments&lt;&#65372;tool&#9601;call&#9601;end&#65372;&gt;{{additional_tool_calls}}&lt;&#65372;tool&#9601;calls&#9601;end&#65372;&gt; Where: - `tool_call_name` must be an exact match to one of the available tools - `tool_call_arguments` must be valid JSON that strictly follows the tool's Parameters Schema - For multiple tool calls, chain them directly without separators or spaces </code></pre> <h3> <a href="https://huggingface.co#code-agent"> </a> <span> Code-Agent </span> </h3><p>We support various code agent frameworks. Please refer to the above toolcall format to create your own code agents. An example is shown in <code>assets/code_agent_trajectory.html</code>.</p><h3> <a href="https://huggingface.co#search-agent"> </a> <span> Search-Agent </span> </h3><p>We design a specific format for searching toolcall in thinking mode, to support search agent. </p><p>For complex questions that require accessing external or up-to-date information, DeepSeek-V3.1 can leverage a user-provided search tool through a multi-turn tool-calling process.</p><p>Please refer to the <code>assets/search_tool_trajectory.html</code> and <code>assets/search_python_tool_trajectory.html</code> for the detailed template.</p><h2> <a href="https://huggingface.co#evaluation"> </a> <span> Evaluation </span> </h2><div><table> <thead><tr> <th>Category</th> <th>Benchmark (Metric)</th> <th>DeepSeek V3.1-NonThinking</th> <th>DeepSeek V3 0324</th> <th>DeepSeek V3.1-Thinking</th> <th>DeepSeek R1 0528</th> </tr> </thead><tbody><tr> <td>General</td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td></td> <td>MMLU-Redux (EM)</td> <td>91.8</td> <td>90.5</td> <td>93.7</td> <td>93.4</td> </tr> <tr> <td></td> <td>MMLU-Pro (EM)</td> <td>83.7</td> <td>81.2</td> <td>84.8</td> <td>85.0</td> </tr> <tr> <td></td> <td>GPQA-Diamond (Pass@1)</td> <td>74.9</td> <td>68.4</td> <td>80.1</td> <td>81.0</td> </tr> <tr> <td></td> <td>Humanity's Last Exam (Pass@1)</td> <td>-</td> <td>-</td> <td>15.9</td> <td>17.7</td> </tr> <tr> <td>Search Agent</td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td></td> <td>BrowseComp</td> <td>-</td> <td>-</td> <td>30.0</td> <td>8.9</td> </tr> <tr> <td></td> <td>BrowseComp_zh</td> <td>-</td> <td>-</td> <td>49.2</td> <td>35.7</td> </tr> <tr> <td></td> <td>Humanity's Last Exam (Python + Search)</td> <td>-</td> <td>-</td> <td>29.8</td> <td>24.8</td> </tr> <tr> <td></td> <td>SimpleQA</td> <td>-</td> <td>-</td> <td>93.4</td> <td>92.3</td> </tr> <tr> <td>Code</td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td></td> <td>LiveCodeBench (2408-2505) (Pass@1)</td> <td>56.4</td> <td>43.0</td> <td>74.8</td> <td>73.3</td> </tr> <tr> <td></td> <td>Codeforces-Div1 (Rating)</td> <td>-</td> <td>-</td> <td>2091</td> <td>1930</td> </tr> <tr> <td></td> <td>Aider-Polyglot (Acc.)</td> <td>68.4</td> <td>55.1</td> <td>76.3</td> <td>71.6</td> </tr> <tr> <td>Code Agent</td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td></td> <td>SWE Verified (Agent mode)</td> <td>66.0</td> <td>45.4</td> <td>-</td> <td>44.6</td> </tr> <tr> <td></td> <td>SWE-bench Multilingual (Agent mode)</td> <td>54.5</td> <td>29.3</td> <td>-</td> <td>30.5</td> </tr> <tr> <td></td> <td>Terminal-bench (Terminus 1 framework)</td> <td>31.3</td> <td>13.3</td> <td>-</td> <td>5.7</td> </tr> <tr> <td>Math</td> <td></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td></td> <td>AIME 2024 (Pass@1)</td> <td>66.3</td> <td>59.4</td> <td>93.1</td> <td>91.4</td> </tr> <tr> <td></td> <td>AIME 2025 (Pass@1)</td> <td>49.8</td> <td>51.3</td> <td>88.4</td> <td>87.5</td> </tr> <tr> <td></td> <td>HMMT 2025 (Pass@1)</td> <td>33.5</td> <td>29.2</td> <td>84.2</td> <td>79.4</td> </tr> </tbody> </table> </div><p>Note: </p><ul> <li><p>Search agents are evaluated with our internal search framework, which uses a commercial search API + webpage filter + 128K context window. Seach agent results of R1-0528 are evaluated with a pre-defined workflow. </p></li> <li><p>SWE-bench is evaluated with our internal code agent framework.</p></li> <li><p>HLE is evaluated with the text-only subset.</p></li> </ul> <h3> <a href="https://huggingface.co#usage-example"> </a> <span> Usage Example </span> </h3><pre><code><span>import</span> transformers tokenizer = transformers.AutoTokenizer.from_pretrained(<span>"deepseek-ai/DeepSeek-V3.1"</span>) messages = [ {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>"You are a helpful assistant"</span>}, {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"Who are you?"</span>}, {<span>"role"</span>: <span>"assistant"</span>, <span>"content"</span>: <span>"<think>Hmm</think>I am DeepSeek"</span>}, {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"1+1=?"</span>} ] tokenizer.apply_chat_template(messages, tokenize=<span>False</span>, thinking=<span>True</span>, add_generation_prompt=<span>True</span>) <span># '&lt;&#65372;begin&#9601;of&#9601;sentence&#65372;&gt;You are a helpful assistant&lt;&#65372;User&#65372;&gt;Who are you?&lt;&#65372;Assistant&#65372;&gt;I am DeepSeek&lt;&#65372;end&#9601;of&#9601;sentence&#65372;&gt;&lt;&#65372;User&#65372;&gt;1+1=?&lt;&#65372;Assistant&#65372;&gt;<think>'</think></span> tokenizer.apply_chat_template(messages, tokenize=<span>False</span>, thinking=<span>False</span>, add_generation_prompt=<span>True</span>) <span># '&lt;&#65372;begin&#9601;of&#9601;sentence&#65372;&gt;You are a helpful assistant&lt;&#65372;User&#65372;&gt;Who are you?&lt;&#65372;Assistant&#65372;&gt;I am DeepSeek&lt;&#65372;end&#9601;of&#9601;sentence&#65372;&gt;&lt;&#65372;User&#65372;&gt;1+1=?&lt;&#65372;Assistant&#65372;&gt;'</span> </code></pre> <h2> <a href="https://huggingface.co#how-to-run-locally"> </a> <span> How to Run Locally </span> </h2><p>The model structure of DeepSeek-V3.1 is the same as DeepSeek-V3. Please visit <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a> repo for more information about running this model locally.</p><h2> <a href="https://huggingface.co#license"> </a> <span> License </span> </h2><p>This repository and the model weights are licensed under the <a href="https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF/blob/main/LICENSE">MIT License</a>.</p><h2> <a href="https://huggingface.co#citation"> </a> <span> Citation </span> </h2><pre><code>@misc{deepseekai2024deepseekv3technicalreport, title={DeepSeek-V3 Technical Report}, author={DeepSeek-AI}, year={2024}, eprint={2412.19437}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2412.19437}, } </code></pre> <h2> <a href="https://huggingface.co#contact"> </a> <span> Contact </span> </h2><p>If you have any questions, please raise an issue or contact us at <a href="https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF/blob/main/service@deepseek.com">service@deepseek.com</a>.</p></div><div class="gallery"><p><img src="https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png"></p><p><img src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;logoColor=white&amp;color=7289da"></p><p><img src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;logoColor=white"></p><p><img src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;logoColor=white"></p></div></section>]]></description><pubDate>Fri, 22 Aug 2025 03:09:26 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mw7zkb/google_colab_crashing_when_finetuning_qwen3_4b/</link><title>google colab crashing when finetuning qwen3 4b instruct</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mw7zkb/google_colab_crashing_when_finetuning_qwen3_4b/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mw7zkb/google_colab_crashing_when_finetuning_qwen3_4b/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mw7zkb/google_colab_crashing_when_finetuning_qwen3_4b/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I&#39;ve used the default settings and a custom dataset, trained for 60 steps (to test) and when I tried to push to hub as a merged model, it crashed and said &quot;Your session crashed after using all available RAM.&quot; Is there any fix for this?</p></div><!-- SC_ON --></section>]]></description><pubDate>Thu, 21 Aug 2025 16:43:24 +0530</pubDate></item><item><link>https://www.reddit.com/r/ollama/comments/1mv7sc0/qwen34binstruct2507gguf_template_fixed/</link><title>Qwen3-4B-Instruct-2507-GGUF template fixed</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mvco9u/qwen34binstruct2507gguf_template_fixed/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mvco9u/qwen34binstruct2507gguf_template_fixed/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 6 min | <a href='https://www.reddit.com/r/unsloth/comments/1mvco9u/qwen34binstruct2507gguf_template_fixed/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>The Unsloth team uploaded templates to: <a href="https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF">https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF</a></p><p>And how the model works out of box. Same should happen to the Thinking variant soon.</p><p>This model is amazing and having a drop-in working version is great.</p></div><!-- SC_ON --></section><section class='separator separator-after-selftext'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><div><shreddit-async-loader> <reddit-skip-to-sidebar></reddit-skip-to-sidebar> </shreddit-async-loader><div><main><shreddit-title title="Qwen3-4B-Instruct-2507-GGUF template fixed : r/ollama"></shreddit-title> <shreddit-post><div><p><span><pdp-back-button></pdp-back-button> <span> <faceplate-tracker> <a href="https://www.reddit.com/r/ollama/"> <faceplate-screen-reader-content> Go to ollama </faceplate-screen-reader-content> </a> </faceplate-tracker> </span><div><p><span> <span> <faceplate-tracker> <shreddit-async-loader> <faceplate-hovercard> <a href="https://www.reddit.com/r/ollama/">r/ollama</a><div><div><p><span> <faceplate-tracker> <a href="https://www.reddit.com/r/ollama/">r/ollama</a> </faceplate-tracker> </span></p><faceplate-tracker> <shreddit-join-button></shreddit-join-button> </faceplate-tracker> </div><hr><p><span> <span>Members</span> </span> <span> <span> <span>Online</span> </span> </span> </p></div></faceplate-hovercard> </shreddit-async-loader> </faceplate-tracker> </span> <span>&bull;</span> <faceplate-timeago></faceplate-timeago> </span></p><div><p><span><div><faceplate-hovercard> <faceplate-tracker><a href="https://www.reddit.com/user/Pjotrs/">Pjotrs</a></faceplate-tracker> </faceplate-hovercard> </div></span></p><shreddit-async-loader> <shreddit-distinguished-post-tags> </shreddit-distinguished-post-tags> </shreddit-async-loader> </div></div></span> </p></div><h2> Qwen3-4B-Instruct-2507-GGUF template fixed </h2> <shreddit-post-flair> </shreddit-post-flair><div><p>The Unsloth team uploaded templates to: <a href="https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF">https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF</a> </p><p>And how the model works out of box. Same should happen to the Thinking variant soon. </p><p>This model is amazing and having a drop-in working version is great. </p></div></shreddit-post> <faceplate-loader src="https://www.redditstatic.com/shreddit/sm/en-US/devvit-privacy-modal-client-js-305ac6a0.js"></faceplate-loader> <faceplate-partial src="https://www.reddit.com/svc/shreddit/devvit-privacy-modal/:subredditId/:postId/:appSlug/:appName/:appOwnerId"> </faceplate-partial> <faceplate-partial src="https://www.reddit.com/svc/shreddit/comments/r/ollama/t3_1mv7sc0?render-mode=partialseeker-session=true"> <shreddit-loading></shreddit-loading> </faceplate-partial> </main><div><div><p><span> <span> <faceplate-tracker> <a href="https://www.redditinc.com/policies/content-policy">Reddit Rules</a> </faceplate-tracker> </span><span> <faceplate-tracker> <a href="https://www.reddit.com/policies/privacy-policy">Privacy Policy</a> </faceplate-tracker> </span><span> <faceplate-tracker> <a href="https://www.redditinc.com/policies/user-agreement">User Agreement</a> </faceplate-tracker> </span><span> <faceplate-tracker> <a href="https://support.reddithelp.com/hc/sections/38303584022676-Accessibility">Accessibility</a> </faceplate-tracker> </span><span> <a href="https://redditinc.com">Reddit, Inc. &copy; 2025. All rights reserved.</a> </span> </span></p></div></div></div></div><flex-left-nav-container></flex-left-nav-container></div><div class="gallery"><p><img src="https://styles.redditmedia.com/t5_8sk7f8/styles/communityIcon_x5e6v387jf8c1.png?width=96&amp;height=96&amp;frame=1&amp;auto=webp&amp;crop=96%3A96%2Csmart&amp;s=07548cdd107f9833b93b2d8559abbd994f7dd84a"></p></div></section>]]></description><pubDate>Wed, 20 Aug 2025 17:24:40 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mv1ugc/valueerror_the_following_model_kwargs_are_not/</link><title>ValueError: The following `model_kwargs` are not used by the model: ['num_logits_to_keep'] (note: typos in the generate arguments will also show up in this list)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mv1ugc/valueerror_the_following_model_kwargs_are_not/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mv1ugc/valueerror_the_following_model_kwargs_are_not/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mv1ugc/valueerror_the_following_model_kwargs_are_not/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><pre><code>messages = [¬† ¬† {&quot;role&quot; : &quot;user&quot;, &quot;content&quot; : &quot;Continue the sequence: 1, 1, 2, 3, 5, 8,&quot;}]text = tokenizer.apply_chat_template(¬† ¬† messages,¬† ¬† tokenize = False,¬† ¬† add_generation_prompt = True, # Must add for generation)from transformers import TextStreamer_ = model.generate(¬† ¬† **tokenizer(text, return_tensors = &quot;pt&quot;).to(&quot;cuda&quot;),¬† ¬† max_new_tokens = 1000, # Increase for longer outputs!¬† ¬† temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking¬† ¬† streamer = TextStreamer(tokenizer, skip_prompt = True),)</code></pre><p>this is the error</p><p>---------------------------------------------------------------------------</p><p>ValueError                                Traceback (most recent call last)</p><p>/tmp/ipython-input-3930286668.py in &lt;cell line: 0&gt;()</p><p>10</p><p>11 from transformers import TextStreamer</p><p>---&gt; 12 _ = model.generate(</p><p>13     **tokenizer(text, return_tensors = &quot;pt&quot;).to(&quot;cuda&quot;),</p><p>14     max_new_tokens = 1000, # Increase for longer outputs!</p><p>4 frames</p><p>/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py in _validate_model_kwargs(self, model_kwargs)</p><p>1600</p><p>1601         if unused_model_args:</p><p>-&gt; 1602             raise ValueError(</p><p>1603                 f&quot;The following `model_kwargs` are not used by the model: {unused_model_args} (note: typos in the&quot;</p><p>1604                 &quot; generate arguments will also show up in this list)&quot;</p><p>ValueError: The following `model_kwargs` are not used by the model: [&#39;num_logits_to_keep&#39;] (note: typos in the generate arguments will also show up in this list)</p><p>I tried debugging with gemini 2.5 pro and gpt5 but they did not help at all and I have no idea what the issue could be because I literally kept almost all the nodes except the &quot;loading finetuned model&quot;  which I updated to this</p><pre><code>if True:¬† ¬† from unsloth import FastLanguageModel¬† ¬† base_model, tokenizer = FastLanguageModel.from_pretrained(¬† ¬† ¬† ¬† model_name = &quot;unsloth/Qwen3-4B-Instruct-2507&quot;,¬† ¬† ¬† ¬† max_seq_length = 2048,¬† ¬† ¬† ¬† load_in_4bit = True,¬† ¬† )¬† ¬† from peft import PeftModel¬† ¬† model = PeftModel.from_pretrained(base_model, &quot;lora_model&quot;)¬† ¬† FastLanguageModel.for_inference(model)</code></pre><p>because when I tried to run the default node I got this error</p><p>```</p><p>==((====))==  Unsloth 2025.8.8: Fast Qwen3 patching. Transformers: 4.55.2.</p><p>\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.</p><p>O^O/ \_/ \    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0</p><p>\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]</p><p>&quot;-____-&quot;     Free license: <a href="http://github.com/unslothai/unsloth">http://github.com/unslothai/unsloth</a></p><p>Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!</p><p>model.safetensors:‚Äá100%</p><p>‚Äá3.55G/3.55G‚Äá[00:25&lt;00:00,‚Äá78.2MB/s]</p><p>generation_config.json:‚Äá100%</p><p>‚Äá237/237‚Äá[00:00&lt;00:00,‚Äá28.3kB/s]</p><p>---------------------------------------------------------------------------</p><p>TypeError                                 Traceback (most recent call last)</p><p>/tmp/ipython-input-3850167755.py in &lt;cell line: 0&gt;()</p><p>1 if True:</p><p>2     from unsloth import FastLanguageModel</p><p>----&gt; 3     model, tokenizer = FastLanguageModel.from_pretrained(</p><p>4         model_name = &quot;lora_model&quot;, # YOUR MODEL YOU USED FOR TRAINING</p><p>5         max_seq_length = 2048,</p><p>1 frames</p><p>/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py in patch_peft_model(model, use_gradient_checkpointing)</p><p>2751         pass</p><p>2752         if not isinstance(model, PeftModelForCausalLM) and not isinstance(model, PeftModelForSequenceClassification):</p><p>-&gt; 2753             raise TypeError(</p><p>2754                 &quot;Unsloth: Your model needs to call `.get_peft_model` first!&quot;</p><p>2755             )</p><p>TypeError: Unsloth: Your model needs to call `.get_peft_model` first!</p><p>```</p></div><!-- SC_ON --></section>]]></description><pubDate>Wed, 20 Aug 2025 07:18:52 +0530</pubDate></item><item><link>https://i.redd.it/rsivghb2nzjf1.jpeg</link><title>Vision Tutorials failing (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mukcfg/vision_tutorials_failing/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mukcfg/vision_tutorials_failing/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mukcfg/vision_tutorials_failing/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi, </p><p>I am trying to eun the vision Tutorials at <a href="https://docs.unsloth.ai/basics/vision-fine-tuning">https://docs.unsloth.ai/basics/vision-fine-tuning</a> on Collab, specifically the one for Llama3.2 and I am getting memory issues on the T4. I last ran this tutorial a month ago and it ran fine, but now its getting OOM issues. Any reason why it&#39;s not working now? What can I do to overcome the OOM errors (besides paying for A100s).</p><p>Thanks for your help</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/rsivghb2nzjf1.jpeg' /></section>]]></description><pubDate>Tue, 19 Aug 2025 20:10:33 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mucie8/please_allow_me_to_unquantizeunfreeze_base_model/</link><title>Please allow me to unquantize/unfreeze base model params during LoRA tuning</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mucie8/please_allow_me_to_unquantizeunfreeze_base_model/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mucie8/please_allow_me_to_unquantizeunfreeze_base_model/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mucie8/please_allow_me_to_unquantizeunfreeze_base_model/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>This is something I am currently doing using HuggingFace code, and it works great, but VRAM is super tight.</p><p>I&#39;d sure love to free up some VRAM!! I noticed unsloth dropping my VRAM from 19-&gt;11 GB which is amazing, but also my setup just doesn&#39;t work though. I am really hoping some of those VRAM savings could be become possible in my hybrid setup!</p><p>Here is a summary of what I do:</p><ul><li>Load &quot;mistralai/Mistral-7B-Instruct-v0.3&quot;, 4bit quantized. Note that while much of the model is quantized, some parts of the model are still not quantized. e.g. Layernorm/embeddings/lm_head/modelnorm. HuggingFace customers can easily simply &#39;unfreeze&#39; these if they want, as long as they remember to save them to disk with torch.save afterwards (or merge). Unsloth, it appears... cannot, because it flat refuses to even train a &quot;fully quantized&quot; model (even though it is not really fully quantized...)</li><li>Add a Peft Model over the base model</li><li>Tune LoRA + embeddings + lm_head+modelnorm for 4 initial epochs.</li><li>After several initial epochs, I begin unquantizing and unfreezing layers (specifically just v_proj, o_proj, mlp), eventually layers 10-31 are tuned</li><li>Touch final layers/DPO at the end</li></ul><p>Anyway, when I tried it, I discovered unsloth will not update any modelnorm/layernorm in the base model for some reason. I filed a bug about this. <a href="https://github.com/unslothai/unsloth/issues/3178">https://github.com/unslothai/unsloth/issues/3178</a> But I wanted to confirm that there aren&#39;t other/bigger limitations relevant.</p><p>Is what I&#39;m asking technically feasible for unsloth? Would fully supporting this &#39;bloat&#39; unsloth too much, negating the savings? I hope it wouldn&#39;t, I suspect VRAM will increase but I am hopeful that HuggingFace can still be outperformed. I&#39;d love to see it if it can be done. I might even be able to help somewhat, but first I&#39;d like to know if what I&#39;m suggesting even makes sense when considering the internals unsloth&#39;s perf magic! Can it be done?</p><p>edit: I also tried to load Mistral with full_finetuning=True. but it seems it doesn&#39;t work even in the most basic case for Mistral. Also filed a bug about that.<a href="https://github.com/unslothai/unsloth/issues/3184">https://github.com/unslothai/unsloth/issues/3184</a>I don&#39;t actually want the model fully expanded anyway, but I suppose I could manually quantize some of the model as an alternative path?</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 19 Aug 2025 13:47:47 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mu5n8q/finetuning_a_code_generation_llm_on_bengali/</link><title>Fine-tuning a Code Generation LLM on Bengali Dataset - Need Model &amp;amp; Resource Recommendations</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mu5n8q/finetuning_a_code_generation_llm_on_bengali/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mu5n8q/finetuning_a_code_generation_llm_on_bengali/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mu5n8q/finetuning_a_code_generation_llm_on_bengali/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I want to fine-tune a code generation LLM on a dataset I created that looks like this:</p><p><code>csvid,instruction,response,test_list1,‡¶™‡ßç‡¶∞‡¶•‡¶Æ n ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡¶∞ ‡¶ï‡ßç‡¶∑‡ßÅ‡¶¶‡ßç‡¶∞‡¶§‡¶Æ ‡¶ó‡ßÅ‡¶£‡¶ø‡¶§‡¶ï ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®‡•§,&quot;def smallest_multiple(n):    if (n&lt;=2):      return n    i = n * 2    factors = [number  for number in range(n, 1, -1) if number * 2 &gt; n]    while True:        for a in factors:            if i % a != 0:                i += n                break            if (a == factors[-1] and i % a == 0):                return i&quot;,&quot;&quot;&quot;[&#39;assert smallest_multiple(13)==360360&#39;, &#39;assert smallest_multiple(2)==2&#39;, &#39;assert smallest_multiple(1)==1&#39;]&quot;&quot;&quot;2,‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡ßÄ‡¶ó‡ßÅ‡¶≤‡¶ø‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Æ‡¶æ‡¶® ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßá ‡¶¶‡ßÅ‡¶ü‡¶ø ‡¶Ö‡¶≠‡¶ø‡¶ß‡¶æ‡¶®‡¶ï‡ßá ‡¶è‡¶ï‡¶§‡ßç‡¶∞‡¶ø‡¶§ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®‡•§,&quot;from collections import Counterdef add_dict(d1,d2):   add_dict = Counter(d1) + Counter(d2)   return add_dict&quot;,&quot;&quot;&quot;[&quot;assert add_dict({&#39;a&#39;: 100, &#39;b&#39;: 200, &#39;c&#39;:300},{&#39;a&#39;: 300, &#39;b&#39;: 200, &#39;d&#39;:400})==({&#39;b&#39;: 400, &#39;d&#39;: 400, &#39;a&#39;: 400, &#39;c&#39;: 300}) &quot;, &quot;assert add_dict({&#39;a&#39;: 500, &#39;b&#39;: 700, &#39;c&#39;:900},{&#39;a&#39;: 500, &#39;b&#39;: 600, &#39;d&#39;:900})==({&#39;b&#39;: 1300, &#39;d&#39;: 900, &#39;a&#39;: 1000, &#39;c&#39;: 900}) &quot;, &quot;assert add_dict({&#39;a&#39;:900,&#39;b&#39;:900,&#39;d&#39;:900},{&#39;a&#39;:900,&#39;b&#39;:900,&#39;d&#39;:900})==({&#39;b&#39;: 1800, &#39;d&#39;: 1800, &#39;a&#39;: 1800})&quot;]&quot;&quot;&quot;</code></p><p><strong>Dataset Structure:</strong>- <code>instruction</code> ‚Üí coding task (in Bengali)- <code>response</code> ‚Üí Python function solution<br/>- <code>test_list</code> ‚Üí asserts to validate</p><p><strong>‚ö° Setup:</strong> I only plan to use Kaggle free GPU for training.</p><p><strong>üëâ Questions:</strong></p><ol><li>Which small/efficient model is best for this? (Qwen2.5-Coder, StarCoder, CodeLlama?)</li><li>Any good Kaggle notebook / resource for LoRA/QLoRA style finetuning on code datasets?</li></ol><p>Looking for something lightweight but useful for Bengali + code generation tasks. Any recommendations or experiences would be greatly appreciated!</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 19 Aug 2025 07:32:50 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mtyucz/promptcompletion_instruction_tuning_issue/</link><title>Prompt-Completion Instruction Tuning Issue</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mtyucz/promptcompletion_instruction_tuning_issue/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mtyucz/promptcompletion_instruction_tuning_issue/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mtyucz/promptcompletion_instruction_tuning_issue/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>There&#39;s a particular Instruction-finetuned model of &quot;Qwen2.5-Coder-7b-Instruct&quot; on Huggingface (unsloth model for which is not available) that I would like to instruction-finetune on my prompt-completion dataset</p><pre><code>train_dict={&quot;prompt&quot;: prompts, &quot;completion&quot;: completions}train_data = Dataset.from_dict(train_dict)</code></pre><p>I am passing in a Dataset object as above.</p><p>I load the model as</p><pre><code>model, tokenizer = FastLanguageModel.from_pretrained(.....model = FastLanguageModel.get_peft_model(......</code></pre><p>The training script is:</p><pre><code>from trl import SFTConfig, SFTTrainertrainer = SFTTrainer(¬† ¬† model = model,¬† ¬† tokenizer = tokenizer,¬† ¬† train_dataset = train_data,¬† ¬† max_seq_length = max_seq_length,¬† ¬† packing = False, # Can make training 5x faster for short sequences.¬† ¬† args = SFTConfig(¬† ¬† ¬† ¬† per_device_train_batch_size = BATCH_SIZE,¬† ¬† ¬† ¬† gradient_accumulation_steps = GRAD_ACCU, #4¬† ¬† ¬† ¬† # warmup_steps = 5,¬† ¬† ¬† ¬† # num_train_epochs = 1, # Set this for 1 full training run.¬† ¬† ¬† ¬† max_steps =2, #10,¬† ¬† ¬† ¬† learning_rate = 2e-4,¬† ¬† ¬† ¬† logging_steps = 1,¬† ¬† ¬† ¬† optim = &quot;adamw_8bit&quot;,¬† ¬† ¬† ¬† weight_decay = 0.01,¬† ¬† ¬† ¬† lr_scheduler_type = &quot;linear&quot;,¬† ¬† ¬† ¬† seed = 3407,¬† ¬† ¬† ¬† output_dir = OUTPUT_DIR,¬† ¬† ¬† ¬† report_to = &quot;wandb&quot; if USE_WANDB else &quot;none&quot;,¬† ¬† ¬† ¬† save_strategy=&quot;no&quot;,¬† ¬† ¬† ¬† completion_only_loss=True,¬† ¬† ),)trainer_stats = trainer.train()</code></pre><p>But, it is throwing in an error:</p><pre><code>RuntimeError: Unsloth: You must specify a `formatting_func`</code></pre><p>Note:  <strong>prompt and completion already contain chat template special tokens added using</strong></p><pre><code>tokenizer.apply_chat_template(..</code></pre><p>Could anyone please  suggest a way around how to train the model on completion only?</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 19 Aug 2025 02:47:02 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mtph9h/first_time_training_need_advice_about_optimizing/</link><title>First time training need advice about optimizing for humble rtx 4060</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mtph9h/first_time_training_need_advice_about_optimizing/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mtph9h/first_time_training_need_advice_about_optimizing/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mtph9h/first_time_training_need_advice_about_optimizing/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I know this gpu is not much, but I want to fine tune the gemma 270m.</p><p>Any optimizing tips? I used the offical notebook for gemma3 270b, but had to disable torch compile.</p></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 18 Aug 2025 21:07:03 +0530</pubDate></item><item><link>https://i.redd.it/f303p8l0csjf1.png</link><title>New gpt-oss Fine-tuning Guide! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mtn4yw/new_gptoss_finetuning_guide/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mtn4yw/new_gptoss_finetuning_guide/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mtn4yw/new_gptoss_finetuning_guide/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hello everyone! We made a new step-by-step guide for fine-tuning gpt-oss! ü¶•</p><p>You&#39;ll learn about:</p><ul><li>Locally training gpt-oss + inference FAQ &amp; tips</li><li>Reasoning effort &amp; Data prep </li><li>Evaluation, hyperparameters &amp; overfitting</li><li>Running &amp; saving your LLM to llama.cpp GGUF, HF etc.</li></ul><p>üîóGuide: <a href="https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune/">https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune/</a></p><p>Just a reminder we improved our fine-tuning and inference notebooks so if previously something wasn&#39;t working it should now!</p><p>Thank you for reading and let us know how we can improve guides in the future! :)</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/f303p8l0csjf1.png' /></section>]]></description><pubDate>Mon, 18 Aug 2025 19:40:30 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mthg2b/fine_tuning_gemma3_270m/</link><title>Fine Tuning Gemma3 270m</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mthg2b/fine_tuning_gemma3_270m/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mthg2b/fine_tuning_gemma3_270m/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mthg2b/fine_tuning_gemma3_270m/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi Greetings,</p><p>I want to fine tune gemma3 270m</p><p>I saw there is a google colab available</p><p>I cannot use it, I dont know how to use cloab notebooks</p><p>I would like simple python code to prepare data from normal text files</p><p>I would also like simple python code to train the model</p><p>And how to use the model once it is trained</p><p>I saw usecases where gemma could be trained to play chess</p><p>Can I give input of text files in text format and derived from books</p><p>So it would answer questions based on the book or information from text files</p><p>I am also interested in training gemma for games</p><p>Can I try a free approach, I have poor hardware , a GTX 1060</p><p>or I have to pay to get the fine tuning and training done</p><p>Regards.</p></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 18 Aug 2025 15:11:56 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mtfu6p/gptoss_export_to_vllm_in_mxfp4/</link><title>GPT-OSS export to vLLM in MXFP4</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mtfu6p/gptoss_export_to_vllm_in_mxfp4/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mtfu6p/gptoss_export_to_vllm_in_mxfp4/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mtfu6p/gptoss_export_to_vllm_in_mxfp4/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Dear Unsloth,</p><p>Thanks for all of the hard work incorporating GPT-OSS into unsloth. I was wondering, is there an estimated date as to when we would be able to export the weights in MXFP4 format?   </p><p>Thank you,</p><p>Cihan </p></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 18 Aug 2025 13:31:22 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mspb6l/looking_for_advice_finetuning_gemma_270m_for_chat/</link><title>Looking for advice finetuning Gemma 270m for chat titles</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mspb6l/looking_for_advice_finetuning_gemma_270m_for_chat/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mspb6l/looking_for_advice_finetuning_gemma_270m_for_chat/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mspb6l/looking_for_advice_finetuning_gemma_270m_for_chat/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi,</p><p>What sort of hyper params are suggested for this task?</p><p>I have a dataset of about 6000 examples. </p><p>I&#39;ve tried the default params (set epoch = 1) but somehow the title generation of the finetuned model is quite bad. I get spelling mistakes too here and there.</p><p>My loss curve kind of just flattens within about 0.3 epochs and then nothing much changes.</p><p>Should I up the learning rate. Currently it is 2e-5. </p><p>And drop the r and alpha to like 8 and 16 maybe?</p></div><!-- SC_ON --></section>]]></description><pubDate>Sun, 17 Aug 2025 17:43:38 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1ms6b17/how_are_you_running_kimi_k1/</link><title>How are you running Kimi K1?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1ms6b17/how_are_you_running_kimi_k1/</guid><comments>https://www.reddit.com/r/unsloth/comments/1ms6b17/how_are_you_running_kimi_k1/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1ms6b17/how_are_you_running_kimi_k1/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>It spawned, it got hyped and then... I am not reading anything about it since. Claude still seems to dominate the tool-using-models.</p><p>I got in touch with a vendor to order 2 Intel Pro B60s for my homelab and I am currently &quot;model shopping&quot;. And this reminded me that, hey, Kimi <em>does</em> exist, and Unsloth even made quant&#39;ed GGUFs.</p><p>But jeebus, it is impossible to fit into anything less than an entire shelf of servers. A 1T model is just... massive. So I am sure that offloading is basically required.</p><p>But how are you running Kimi K2? How is it? What&#39;s your t/s? It&#39;s capabilities, on plain paper, would make an absurdly amazing model to use for &quot;everything&quot; that isn&#39;t highly specialized. So it&#39;d be fun to run that. Originally I thought of using Deepseek R1 - but Kimi&#39;s MCP support seems to be much better. o.o</p></div><!-- SC_ON --></section>]]></description><pubDate>Sun, 17 Aug 2025 01:41:50 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mr5al2/so_about_finetuning/</link><title>So, about finetuning...</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mr5al2/so_about_finetuning/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mr5al2/so_about_finetuning/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mr5al2/so_about_finetuning/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I was getting (a little too...?) curious about the AI VTuber Neuro-sama - and in a spur of randomness, I dug into a rabbithole. Part of the result is here: <a href="https://www.reddit.com/r/LocalLLaMA/comments/1mq5cwq/so_what_is_neurosama_ai_vtuber_built_with/">https://www.reddit.com/r/LocalLLaMA/comments/1mq5cwq/so_what_is_neurosama_ai_vtuber_built_with/</a></p><p>But as someone there mentioned, there is a possibility that she is being continiously refined to include memory. Well that or RAG.</p><p>Either way; I never looked into actually finetuning. How do you do that - basically? I am planning to purchase the Intel Pro B60 and two of those - so I would have a pretty decent amount of VRAM at my disposal. How&#39;d I run finetune on that and what would I need? o.o</p><p>I am a complete noob in that and still have ways to go outside of inference and a few things involved in that (platform, api, ...).</p><p>Thanks in advance!</p></div><!-- SC_ON --></section>]]></description><pubDate>Fri, 15 Aug 2025 23:28:22 +0530</pubDate></item><item><link>https://i.redd.it/f667h5b9h0jf1.png</link><title>Google - Gemma 3 270M out now! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mq5hbb/google_gemma_3_270m_out_now/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mq5hbb/google_gemma_3_270m_out_now/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mq5hbb/google_gemma_3_270m_out_now/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Google releases Gemma 3 270M, a new model that runs locally on just 0.5 GB RAM. ‚ú®</p><p>GGUF to run: <a href="https://huggingface.co/unsloth/gemma-3-270m-it-GGUF">https://huggingface.co/unsloth/gemma-3-270m-it-GGUF</a></p><p>Trained on 6T tokens, it runs fast on phones &amp; handles chat, coding &amp; math tasks.</p><p>Run at ~50 t/s with our Dynamic GGUF, or fine-tune in a few mins via Unsloth &amp; export to your phone.</p><p>Our notebooks makes the 270M prameter model very smart at playing chess and can predict the next chess move.</p><p>Fine-tuning notebook: <a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(270M">https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(270M).ipynb</a>.ipynb)</p><p>Guide: <a href="https://docs.unsloth.ai/basics/gemma-3">https://docs.unsloth.ai/basics/gemma-3</a></p><p>Thanks to the Gemma team for providing Unsloth with Day Zero support! :)</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/f667h5b9h0jf1.png' /></section>]]></description><pubDate>Thu, 14 Aug 2025 21:57:40 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mpl382/gptoss_fixesupdates_for_finetuning_inference/</link><title>Gpt-oss Fixes/Updates for Fine-tuning &amp;amp; Inference</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mpl382/gptoss_fixesupdates_for_finetuning_inference/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mpl382/gptoss_fixesupdates_for_finetuning_inference/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mpl382/gptoss_fixesupdates_for_finetuning_inference/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys we noticed some of you having issues with the gpt-oss notebooks for fine-tuning &amp; inference. We did a large update to fix some issues and so you should see more stable runs.</p><p>Update Unsloth or Use our new updated finetuning notebook: <a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb">https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb</a>Or inference notebook: <a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb">https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb</a></p><p>And see instructions below to use the new update if local.</p><p>Keep in mind inference is still a bit iffy but it should work for the most part. We&#39;re still working on it.</p><p>As for saving and using the model to GGUF etc we&#39;re also working on that so stay tuned!</p><p>Use our new installation cell:<code>!pip install --upgrade -qqq uvtry: import numpy; install_numpy = f&quot;numpy=={numpy.__version__}&quot;except: install_numpy = &quot;numpy&quot;!uv pip install -qqq \    &quot;torch&gt;=2.8.0&quot; &quot;triton&gt;=3.4.0&quot; {install_numpy} \    &quot;unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo&quot; \    &quot;unsloth[base] @ git+https://github.com/unslothai/unsloth&quot; \    torchvision bitsandbytes \    git+https://github.com/huggingface/transformers \    git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels</code></p><p>Previous errors you might&#39;ve been getting included: GptOssTopKRouter or cuda error</p><p>Let us know if you&#39;re still having any issues! ü§ó</p></div><!-- SC_ON --></section>]]></description><pubDate>Thu, 14 Aug 2025 05:34:40 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1moylrq/need_help_torch_dynamoexcbackendcompilerfailed/</link><title>Need help: torch._dynamo.exc.BackendCompilerFailed</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1moylrq/need_help_torch_dynamoexcbackendcompilerfailed/</guid><comments>https://www.reddit.com/r/unsloth/comments/1moylrq/need_help_torch_dynamoexcbackendcompilerfailed/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1moylrq/need_help_torch_dynamoexcbackendcompilerfailed/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I ran into a very strange issue. The environment and the unsloth version are the same, the data is the same, and the model is also the same (gemma3). The code that could run last week can‚Äôt run this week. The error message is: torch._dynamo.exc.BackendCompilerFailed RuntimeError: Detected that you are using FX to symbolically trace a dynamo-optimized function. This is not supported at the moment.</p><p>Then, after I set the following, it can run normally: os.environ[&quot;UNSLOTH_COMPILE_DISABLE&quot;] = &quot;1&quot;</p><p>However, there‚Äôs a big difference in the start training loss: one is 10+, and the other is 1.9. The code is the same.</p><p>{&#39;loss&#39;: 15.0507, &#39;grad_norm&#39;: 26.66766929626465, &#39;learning_rate&#39;: 0.0, &#39;epoch&#39;: 0.0}</p><p>{&#39;loss&#39;: 1.8776, &#39;grad_norm&#39;: 5.469211101531982, &#39;learning_rate&#39;: 0.0, &#39;epoch&#39;: 0.0}</p></div><!-- SC_ON --></section>]]></description><pubDate>Wed, 13 Aug 2025 13:44:32 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1moc90n/some_grpo_questions/</link><title>Some GRPO questions</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1moc90n/some_grpo_questions/</guid><comments>https://www.reddit.com/r/unsloth/comments/1moc90n/some_grpo_questions/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1moc90n/some_grpo_questions/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Thank so much for the great fine-tuning tool, especially for memory saving.</p><p>I have been testing GRPO with qwen3.  I have a question.</p><p>Reward score gets improved. Yes, it seems working. I run it for 10 epochs. My question is about loss.  Loss is almost zero for first 1 epoch.  Then, it goes higher while reward goes up.</p><p>Is it normal that Loss = 0 for long time?</p><p>And, how multi gpu is going for GRPO?  I heard multi gpu is possible in unsloth except GRPO.  GRPO will be even better with multi gpu support.  Thanks again.</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 12 Aug 2025 21:14:31 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mo8nt7/bug_support_needed_on_mistral_small_32/</link><title>BUG / Support needed on mistral small 3.2</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mo8nt7/bug_support_needed_on_mistral_small_32/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mo8nt7/bug_support_needed_on_mistral_small_32/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mo8nt7/bug_support_needed_on_mistral_small_32/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><pre><code>from unsloth import FastLanguageModelmax_seq_length = 2048 ¬† dtype = None ¬†# or torch.float16 / torch.bfloat16 as your GPU supportsload_in_4bit = Truemodel, tokenizer = FastLanguageModel.from_pretrained(¬† ¬† model_name = &quot;mistralai/Mistral-Small-3.2-24B-Instruct-2506&quot;,¬† ¬† max_seq_length = max_seq_length,¬† ¬† dtype = dtype,¬† ¬† load_in_4bit = load_in_4bit,)</code></pre><p>i only loaded the model :  </p><pre><code>from unsloth.chat_templates import get_chat_template# Test promptmessages = [¬† ¬† {¬† ¬† ¬† ¬† &quot;role&quot;: &quot;system&quot;,¬† ¬† ¬† ¬† &quot;content&quot;: &quot;you area helpful assistant that can generate anagrams of words.&quot;¬† ¬† },¬† ¬† {¬† ¬† ¬† ¬† &quot;role&quot;: &quot;user&quot;,¬† ¬† ¬† ¬† &quot;content&quot;: &quot;make anagram of &#39;hello&#39;&quot;¬† ¬† }]tools = [¬† ¬† {¬† ¬† ¬† ¬† &quot;type&quot;: &quot;function&quot;,¬† ¬† ¬† ¬† &quot;function&quot;: {¬† ¬† ¬† ¬† ¬† ¬† &quot;name&quot;: &quot;generate_anagram&quot;,¬† ¬† ¬† ¬† ¬† ¬† &quot;description&quot;: &quot;Generate an anagram of a given word&quot;,¬† ¬† ¬† ¬† ¬† ¬† &quot;parameters&quot;: {¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† &quot;type&quot;: &quot;object&quot;,¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† &quot;properties&quot;: {¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† &quot;word&quot;: {¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† &quot;type&quot;: &quot;string&quot;,¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† &quot;description&quot;: &quot;The word to generate an anagram of&quot;¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† }¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† },¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† &quot;required&quot;: [&quot;word&quot;]¬† ¬† ¬† ¬† ¬† ¬† }¬† ¬† ¬† ¬† }¬† ¬† }]inputs = tokenizer.apply_chat_template(¬† ¬† messages,¬† ¬† tokenize=True,¬† ¬† padding=True,¬† ¬† add_generation_prompt=True,¬† ¬† return_tensors=&quot;pt&quot;,¬† ¬† return_attention_mask=True,¬† ¬† tools=tools,).to(&quot;cuda&quot;)outputs = model.generate(input_ids=inputs, max_new_tokens = 128, use_cache=True)decoded = tokenizer.batch_decode(outputs)print(decoded[0])</code></pre><p>thentried infenrece :  </p><p>and this error shows up:<br/>---------------------------------------------------------------------------</p><p>TypeError                                 Traceback (most recent call last)</p><p>Cell In[2], line 35</p><p>4 messages = [</p><p>5     {</p><p>6         &quot;role&quot;: &quot;system&quot;,</p><p>(...)     12     }</p><p>13 ]</p><p>15 tools = [</p><p>16     {</p><p>17         &quot;type&quot;: &quot;function&quot;,</p><p>(...)     32     }</p><p>33 ]</p><p>---&gt; 35 inputs = tokenizer.apply_chat_template(</p><p>36     messages,</p><p>37     tokenize=True,</p><p>38     padding=True,</p><p>39     add_generation_prompt=True,</p><p>40     return_tensors=&quot;pt&quot;,</p><p>41     return_attention_mask=True,</p><p>42     tools=tools,</p><p>43 ).to(&quot;cuda&quot;)</p><p>45 outputs = model.generate(input_ids=inputs, max_new_tokens = 128, use_cache=True)</p><p>47 decoded = tokenizer.batch_decode(outputs)</p><p>File ~/finetuning/venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172, in deprecate_kwarg.&lt;locals&gt;.wrapper.&lt;locals&gt;.wrapped_func(*args, **kwargs)</p><p>168 elif minimum_action in (Action.NOTIFY, Action.NOTIFY_ALWAYS) and not is_torchdynamo_compiling():</p><p>169     # DeprecationWarning is ignored by default, so we use FutureWarning instead</p><p>170     warnings.warn(message, FutureWarning, stacklevel=2)</p><p>--&gt; 172 return func(*args, **kwargs)</p><p>File ~/finetuning/venv/lib/python3.12/site-packages/transformers/processing_utils.py:1531, in ProcessorMixin.apply_chat_template(self, conversation, chat_template, **kwargs)</p><p>1529 video_metadata = []</p><p>1530 for message in conversation:</p><p>-&gt; 1531     visuals = [content for content in message[&quot;content&quot;] if content[&quot;type&quot;] in [&quot;image&quot;, &quot;video&quot;]]</p><p>1532     audio_fnames = [</p><p>1533         content[key]</p><p>1534         for content in message[&quot;content&quot;]</p><p>1535         for key in [&quot;audio&quot;, &quot;url&quot;, &quot;path&quot;]</p><p>1536         if key in content and content[&quot;type&quot;] == &quot;audio&quot;</p><p>1537     ]</p><p>1538     image_fnames = [</p><p>1539         vision_info[key]</p><p>1540         for vision_info in visuals</p><p>1541         for key in [&quot;image&quot;, &quot;url&quot;, &quot;path&quot;, &quot;base64&quot;]</p><p>1542         if key in vision_info and vision_info[&quot;type&quot;] == &quot;image&quot;</p><p>1543     ]</p><p>TypeError: string indices must be integers, not &#39;str&#39;</p><p>Is this a problem i have or in the unsloth library</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 12 Aug 2025 18:54:49 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mo86wf/error_in_the_latest_unslothgptoss_finetuning/</link><title>Error in the latest unsloth/gpt-oss finetuning script! How to fix?: NotImplementedError: Unsloth: Logits are empty from 2024.11 onwards. To get raw logits again, please set the environment variable `UNSLOTH_RETURN_LOGITS` to `"1" BEFORE starting to train ie before `trainer.train()`.</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mo86wf/error_in_the_latest_unslothgptoss_finetuning/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mo86wf/error_in_the_latest_unslothgptoss_finetuning/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 3 min | <a href='https://www.reddit.com/r/unsloth/comments/1mo86wf/error_in_the_latest_unslothgptoss_finetuning/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Complete Error:<br/>(.venv) wstf@gen-ai:~/finetune-gpt-oss-20b$ python finetune_with_unsloth.py<br/>/home/wstf/finetune-gpt-oss-20b/finetune_with_unsloth.py:19: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.  </p><p>Please restructure your imports with &#39;import unsloth&#39; at the top of your file.<br/>  from unsloth import FastLanguageModel, is_bfloat16_supported<br/>ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.<br/>ü¶• Unsloth Zoo will now patch everything to make training faster!<br/>Loading GPT-OSS 20B model with Unsloth...<br/>==((====))==  Unsloth 2025.8.4: Fast Gpt_Oss patching. Transformers: 4.55.0.<br/>   \\   /|    NVIDIA RTX 6000 Ada Generation. Num GPUs = 1. Max memory: 47.363 GB. Platform: Linux.<br/>O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1<br/>\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]<br/> &quot;-____-&quot;     Free license: <a href="http://github.com/unslothai/unsloth">http://github.com/unslothai/unsloth</a><br/>Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!<br/>Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01&lt;00:00,  2.07it/s\]  Adding LoRA adapters...  Unsloth: Making \`model.base\_model.model.model\` require gradients  Loading dataset...  Formatting dataset...  tokenizer eos token: &lt;|return|&gt;<br/>##################################<br/>tokenizer pad token: &lt;|reserved_200017|&gt;<br/>Setting up training configuration...<br/>GPU = NVIDIA RTX 6000 Ada Generation. Max memory = 47.363 GB.<br/>19.354 GB of memory reserved.<br/>Starting training...<br/>==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1<br/>   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 60<br/>O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4<br/>\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8<br/> &quot;-____-&quot;     Trainable parameters = 0 of 20,918,738,496 (0.00% trained)</p><p>wandb: Tracking run with wandb version 0.21.1<br/>wandb: Run data is saved locally in /home/wstf/finetune-gpt-oss-20b/wandb/run-20250812_155445-ksb3gy7i<br/>wandb: Run `wandb offline` to turn off syncing.  0%|                                         | 0/60 [00:00&lt;?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.<br/>Traceback (most recent call last):<br/>  File &quot;/home/wstf/finetune-gpt-oss-20b/finetune_with_unsloth.py&quot;, line 212, in &lt;module&gt;<br/>main()<br/>  File &quot;/home/wstf/finetune-gpt-oss-20b/finetune_with_unsloth.py&quot;, line 119, in main<br/>trainer_stats = trainer.train()<br/>^^^^^^^^^^^^^^^<br/>  File &quot;/home/wstf/finetune-gpt-oss-20b/.venv/lib/python3.12/site-packages/transformers/trainer.py&quot;, line 2238, in train<br/>return inner_training_loop(<br/>^^^^^^^^^^^^^^^^^^^^<br/>  File &quot;&lt;string&gt;&quot;, line 323, in _fast_inner_training_loop<br/>  File &quot;/home/wstf/finetune-gpt-oss-20b/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py&quot;, line 907, in training_step<br/>return super().training_step(*args, **kwargs)<br/>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br/>  File &quot;&lt;string&gt;&quot;, line 34, in _unsloth_training_step<br/>  File &quot;/home/wstf/finetune-gpt-oss-20b/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py&quot;, line 879, in compute_loss<br/>shift_logits = outputs.logits[..., :-1, :].contiguous()<br/>~~~~~~~~~~~~~~^^^^^^^^^^^^^<br/>  File &quot;/home/wstf/finetune-gpt-oss-20b/unsloth_compiled_cache/unsloth_compiled_module_gpt_oss.py&quot;, line 131, in raise_logits_error<br/>def raise_logits_error(*args, **kwargs): raise NotImplementedError(LOGITS_ERROR_STRING)<br/>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br/>NotImplementedError: Unsloth: Logits are empty from 2024.11 onwards. To get raw logits again, please set the environment variable `UNSLOTH_RETURN_LOGITS` to `&quot;1&quot; BEFORE starting to train ie before `trainer.train()`. For example:<br/>```<br/>import os<br/>os.environ[&#39;UNSLOTH_RETURN_LOGITS&#39;] = &#39;1&#39;<br/>trainer.train()<br/>```<br/>No need to restart your console - just add `os.environ[&#39;UNSLOTH_RETURN_LOGITS&#39;] = &#39;1&#39;` before trainer.train() and re-run the cell!  </p><p>Added &quot;os.environ[&#39;UNSLOTH_RETURN_LOGITS&#39;] = &#39;1&#39;&quot; before trainer.train() also called imports after &quot;os.environ[&#39;UNSLOTH_RETURN_LOGITS&#39;] = &#39;1&#39;&quot; but still getting the same error!<br/>Any solutions?</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 12 Aug 2025 18:34:56 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mnocsx/how_to_fix_this_attributeerror_gptosstopkrouter/</link><title>How to fix this? AttributeError: 'GptOssTopKRouter' object has no attribute 'weight'</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mnocsx/how_to_fix_this_attributeerror_gptosstopkrouter/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mnocsx/how_to_fix_this_attributeerror_gptosstopkrouter/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mnocsx/how_to_fix_this_attributeerror_gptosstopkrouter/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><pre><code>from unsloth import FastLanguageModelimport torchmax_seq_length = 1024dtype = None# 4bit pre quantized models we support for 4x faster downloading + no OOMs.fourbit_models = [¬† ¬† &quot;unsloth/gpt-oss-20b-unsloth-bnb-4bit&quot;, # 20B model using bitsandbytes 4bit quantization¬† ¬† &quot;unsloth/gpt-oss-120b-unsloth-bnb-4bit&quot;,¬† ¬† &quot;unsloth/gpt-oss-20b&quot;, # 20B model using MXFP4 format¬† ¬† &quot;unsloth/gpt-oss-120b&quot;,] # More models at https://huggingface.co/unslothmodel, tokenizer = FastLanguageModel.from_pretrained(¬† ¬† model_name = &quot;Guilherme34/GPT-OSS-UNCENSORED-20B&quot;,¬† ¬† dtype = dtype, # None for auto detection¬† ¬† max_seq_length = max_seq_length, # Choose any for long context!# 4 bit quantization to reduce memory¬† ¬† full_finetuning = False, # [NEW!] We have full finetuning now!¬† ¬† # token = &quot;hf_...&quot;, # use one if using gated models)==((====))==  Unsloth 2025.8.4: Fast Gpt_Oss patching. Transformers: 4.56.0.dev0.   \\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.4.0\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False] &quot;-____-&quot;     Free license: Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!http://github.com/unslothai/unsloth---------------------------------------------------------------------------AttributeError                            Traceback (most recent call last) in &lt;cell line: 0&gt;()     13 ] # More models at      14 ---&gt; 15 model, tokenizer = FastLanguageModel.from_pretrained(     16     model_name = &quot;Guilherme34/GPT-OSS-UNCENSORED-20B&quot;,     17     dtype = dtype, # None for auto detection/tmp/ipython-input-1559322843.pyhttps://huggingface.co/unsloth in __getattr__(self, name)   1960             if name in modules:   1961                 return modules[name]-&gt; 1962         raise AttributeError(   1963             f&quot;&#39;{type(self).__name__}&#39; object has no attribute &#39;{name}&#39;&quot;   1964         )/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.pyAttributeError: &#39;GptOssTopKRouter&#39; object has no attribute &#39;weight&#39;</code></pre></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 12 Aug 2025 02:03:59 +0530</pubDate></item><item><link>https://github.com/Ashx098/sft-play</link><title>From Data to Inference: Fully Automated QLoRA/LORA/Full Tuning for Local LLMs (github.com)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mn89jr/from_data_to_inference_fully_automated/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mn89jr/from_data_to_inference_fully_automated/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 36 min | <a href='https://www.reddit.com/r/unsloth/comments/1mn89jr/from_data_to_inference_fully_automated/'>Post permalink</a></p></section><section class='separator separator-before-parsed-content'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><article><h2>SFT-Play (QLoRA-ready, 8-GB Friendly) - QLoRA-ready SFT starter kit</h2><a href="https://github.com#sft-play-qlora-ready-8-gb-friendly---qlora-ready-sft-starter-kit"></a><p><a href="https://opensource.org/licenses/MIT"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT"></a> <a href="https://www.python.org/downloads/"><img src="https://camo.githubusercontent.com/ede359272474b146e713cf6845288105b689a7b203df440f6f8cda89bc50a897/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e382b2d626c75652e737667" alt="Python 3.8+"></a> <a href="https://pytorch.org/"><img src="https://camo.githubusercontent.com/56669d13a7aab35d88e7712080328edd396a12b8a87f55bc0cace80480cd09c3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5079546f7263682d322e302b2d7265642e737667" alt="PyTorch"></a> <a href="https://huggingface.co/transformers/"><img src="https://camo.githubusercontent.com/636b99ded7937a543acc4bbb99d829b90ebb7f308c1c8e938d35ac31426e1cd9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462541342539372532305472616e73666f726d6572732d342e34322b2d6f72616e67652e737667" alt="Transformers"></a> <a href="https://github.com/psf/black"><img src="https://camo.githubusercontent.com/5bf9e9fa18966df7cb5fac7715bef6b72df15e01a6efa9d616c83f9fcb527fe2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" alt="Code style: black"></a></p><p><strong>Plug-and-play Supervised Fine-Tuning</strong> on small GPUs. Single config, QLoRA/LoRA/Full switches, bitsandbytes/Unsloth backends, Jinja chat templating, TensorBoard live UI, and lean checkpoints (save adapters, not full models).</p><hr> <h2>&#127919; Who Should Use SFT-Play?</h2><a href="https://github.com#-who-should-use-sft-play"></a> <ul> <li><strong>AI hobbyists</strong> &mdash; fine-tune models on your own dataset without cloud GPUs</li> <li><strong>Researchers</strong> &mdash; run small-scale experiments before scaling to larger infrastructure</li> <li><strong>Educators</strong> &mdash; teach LLM fine-tuning with a minimal, reproducible setup</li> <li><strong>Open-source contributors</strong> &mdash; build datasets + share fine-tuned models efficiently</li> <li><strong>Developers</strong> &mdash; prototype AI features with custom models on local hardware</li> </ul> <hr><p></p><h2>&#10024; Features</h2><a href="https://github.com#-features"></a> <ul> <li><strong>Runs on any single GPU (8 GB+)</strong> &mdash; VRAM probe auto-tunes batch/grad-accum.</li> <li><strong>Two-config UX</strong> &mdash; <code>config_base.yaml</code> (defaults) + backend-specific configs (<code>run_bnb.yaml</code>, <code>run_unsloth.yaml</code>).</li> <li><strong>Tuning modes</strong> &mdash; <code>qlora | lora | full</code> (config switch).</li> <li><strong>Backends</strong> &mdash; <code>bitsandbytes</code> (default) or <code>unsloth</code> (optional; auto-fallback to bnb).</li> <li><strong>Data pipeline</strong> &mdash; raw &rarr; structured chat (<code>system,user,assistant</code>) &rarr; Jinja render on-the-fly.</li> <li><strong>UI</strong> &mdash; <strong>TensorBoard</strong> only (loss/metrics/LR; optional GPU stats).</li> <li><strong>Model Caching</strong> - Automatically download models from Hugging Face Hub and cache them locally.</li> <li><strong>Tiny checkpoints</strong> &mdash; LoRA adapters only (~50-200 MB vs. full model's multiple GB).</li> <li><strong>Complete automation</strong> &mdash; Makefile + workflows for zero-config setup.</li> </ul> <hr> <h2>&#128450;&#65039; Repo Layout</h2><a href="https://github.com#%EF%B8%8F-repo-layout"></a><div><pre><code>sft-play/ &#9500;&#9472; configs/ &#9474; &#9500;&#9472; config_base.yaml # reusable defaults (rarely change) &#9474; &#9500;&#9472; run_bnb.yaml # BitsAndBytes backend config (default) &#9474; &#9492;&#9472; run_unsloth.yaml # Unsloth backend config (optional) &#9500;&#9472; data/ &#9474; &#9500;&#9472; raw/ # input sources (json/csv/jsonl) &#9474; &#9500;&#9472; processed/ # structured chat (system,user,assistant) &#9474; &#9500;&#9472; processed_with_style/ # optional: after style injection &#9474; &#9492;&#9472; rendered/ # optional: materialized seq2seq (input,target) &#9500;&#9472; chat_templates/ &#9474; &#9492;&#9472; default.jinja # single Jinja template &#9500;&#9472; scripts/ &#9474; &#9500;&#9472; process_data.py # raw &rarr; structured chat + split &#9474; &#9500;&#9472; style_prompt.py # inject/override system/style rule &#9474; &#9500;&#9472; render_template.py # (optional) Jinja &rarr; seq2seq jsonl &#9474; &#9500;&#9472; train.py # QLoRA/LoRA/Full; bnb/Unsloth; TB logging &#9474; &#9500;&#9472; eval.py # ROUGE-L/SARI/Exact-Match (+ schema checks) &#9474; &#9500;&#9472; infer.py # batch/interactive inference (same template) &#9474; &#9500;&#9472; merge_lora.py # merge adapters &rarr; single FP16 model (optional) &#9474; &#9492;&#9472; utils/ &#9474; &#9492;&#9472; model_store.py # handles model downloading/caching &#9500;&#9472; env/ &#9474; &#9492;&#9472; accelerate_config.yaml # fp16, single-GPU defaults &#9500;&#9472; outputs/ # TB logs, metrics, sample preds &#9500;&#9472; adapters/ # LoRA adapter checkpoints &#9500;&#9472; workflows/ # automation scripts &#9474; &#9500;&#9472; quick_start.sh # interactive setup with sample data &#9474; &#9492;&#9472; batch_process.sh # batch processing automation &#9500;&#9472; Makefile # complete automation commands &#9500;&#9472; requirements.txt &#9492;&#9472; README.md </code></pre></div><hr><p></p><h2>&#9881;&#65039; Configs</h2><a href="https://github.com#%EF%B8%8F-configs"></a> <h3><code>configs/config_base.yaml</code> (defaults)</h3><a href="https://github.com#configsconfig_baseyaml-defaults"></a> <ul> <li>Training: epochs, warmup, weight_decay, fp16, gradient_checkpointing</li> <li>Checkpoint/eval: <code>save_strategy</code>, <code>save_steps</code>, <code>eval_strategy</code>, <code>save_total_limit</code>, <code>metric_for_best_model</code>, <code>load_best_model_at_end</code></li> <li>Data: <code>format: chat</code>, <code>template_path</code>, <strong>split ratios</strong> (train/val/test)</li> <li>Logging: <code>backend: tensorboard</code>, <code>log_interval</code></li> </ul><p></p><h3>Backend-Specific Configs (Recommended)</h3><a href="https://github.com#backend-specific-configs-recommended"></a><p><strong>For BitsAndBytes (Stable, Broad Compatibility):</strong></p><div><pre><span><span>#</span> configs/run_bnb.yaml</span> <span>include</span>: <span>configs/config_base.yaml</span> <span>tuning</span>: <span>backend</span>: <span>bnb </span><span><span>#</span> BitsAndBytes backend</span> <span>mode</span>: <span>qlora</span> <span>train</span>: <span>bf16</span>: <span>false </span><span><span>#</span> BitsAndBytes works best with fp16</span> <span>fp16</span>: <span>true</span> <span>output_dir</span>: <span>outputs/run-bnb</span></pre></div><p><strong>For Unsloth (Faster, Requires Compatible CUDA):</strong></p><div><pre><span><span>#</span> configs/run_unsloth.yaml</span> <span>include</span>: <span>configs/config_base.yaml</span> <span>tuning</span>: <span>backend</span>: <span>unsloth </span><span><span>#</span> Unsloth backend</span> <span>mode</span>: <span>qlora</span> <span>train</span>: <span>bf16</span>: <span>true </span><span><span>#</span> Unsloth works better with bfloat16</span> <span>fp16</span>: <span>false</span> <span>output_dir</span>: <span>outputs/run-unsloth</span></pre></div><h3>Data Format Support</h3><a href="https://github.com#data-format-support"></a><p>The training script supports <strong>both processed and rendered data formats</strong>:</p><p><strong>Processed Format (Default):</strong></p><div><pre>{<span>"system"</span>: <span><span>"</span>You are a helpful assistant.<span>"</span></span>, <span>"user"</span>: <span><span>"</span>What is 2+2?<span>"</span></span>, <span>"assistant"</span>: <span><span>"</span>2+2 equals 4.<span>"</span></span>}</pre></div><p><strong>Rendered Format (Optional):</strong></p><div>system\nYou are a helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nWhat is 2+2?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant", "target": "2+2 equals 4."}"&gt;<pre>{<span>"input"</span>: <span><span>"</span>&lt;|im_start|&gt;system<span>\n</span>You are a helpful assistant.&lt;|im_end|&gt;<span>\n</span>&lt;|im_start|&gt;user<span>\n</span>What is 2+2?&lt;|im_end|&gt;<span>\n</span>&lt;|im_start|&gt;assistant<span>"</span></span>, <span>"target"</span>: <span><span>"</span>2+2 equals 4.<span>"</span></span>}</pre></div><p><strong>To switch to rendered data:</strong></p><ol> <li>Generate rendered data: <code>make render</code></li> <li>Create config pointing to rendered paths:</li> </ol><div><pre><span>data</span>: <span>train_path</span>: <span>data/rendered/train.jsonl</span> <span>val_path</span>: <span>data/rendered/val.jsonl</span> <span>test_path</span>: <span>data/rendered/test.jsonl</span></pre></div><p><strong>Fallback behavior when no chat template:</strong> If <code>chat_templates/default.jinja</code> is missing, the system automatically creates simple format:</p><div><pre><code>System: You are a helpful assistant. User: What is 2+2? Assistant:2+2 equals 4. </code></pre></div><h3>Backend Safety Features</h3><a href="https://github.com#backend-safety-features"></a><p><strong>Automatic Backend Stamping:</strong></p><ul> <li>Each training run creates <code>outputs/<run>/backend.json</run></code> with backend info</li> <li>Prevents accidental resume across different backends</li> <li>Validates configuration consistency on resume</li> </ul><p><strong>XFormers Safety:</strong></p><ul> <li>Unsloth automatically disables XFormers to prevent compatibility issues</li> <li>BitsAndBytes uses standard PyTorch attention mechanisms</li> <li>Automatic fallback from Unsloth to BitsAndBytes if import fails</li> </ul><p><strong>Precision Auto-Detection:</strong></p><ul> <li>Auto-enables bf16 on Ada GPUs (RTX 40xx series) when neither precision is set</li> <li>Auto-enables fp16 on other GPUs as fallback</li> <li>Prevents both bf16 and fp16 being enabled simultaneously</li> </ul> <blockquote><p><strong>Recommended Usage:</strong> Use <code>configs/run_bnb.yaml</code> for stability or <code>configs/run_unsloth.yaml</code> for speed. The backend-specific configs ensure optimal settings and prevent configuration conflicts.</p></blockquote> <hr> <h2>&#129303; Hugging Face Token</h2><a href="https://github.com#-hugging-face-token"></a><p>To download models from the Hugging Face Hub, you need to provide an access token. You can do this in one of three ways:</p><ol> <li><p><strong>CLI Login (Recommended):</strong></p><p>This will store your token securely on your machine.</p></li> <li><p><strong>Environment Variable:</strong></p><div><pre><span>export</span> HUGGINGFACE_HUB_TOKEN=hf_...</pre></div><p>You can add this to your shell profile (e.g., <code>.bashrc</code>, <code>.zshrc</code>) or a <code>.env</code> file.</p></li> <li><p><strong>Offline Mode:</strong> After downloading a model once, you can work offline:</p></li> </ol> <h2>&#128200; TensorBoard</h2><a href="https://github.com#-tensorboard"></a><p>We log to a fixed path: <code>outputs/tb</code>.</p><p><strong>Automatic TensorBoard (Recommended):</strong></p><div><pre>make train-bnb-tb <span><span>#</span> BitsAndBytes training with TensorBoard auto-start</span> make train-unsloth-tb <span><span>#</span> Unsloth training with TensorBoard auto-start</span></pre></div><p><strong>Manual TensorBoard:</strong></p><div><pre>make tensorboard <span><span>#</span> uses port 6006</span> make tensorboard TB_PORT=6007</pre></div><p>Stop it:</p><p><strong>How it works:</strong></p><ul> <li>The <code>-tb</code> training targets automatically start TensorBoard in the background before training</li> <li>TensorBoard runs on port 6006 by default (configurable with TB_PORT)</li> <li>After training completes, TensorBoard continues running for you to review results</li> <li>Use <code>make tb-stop</code> to stop TensorBoard when you're done</li> </ul><p>If TB shows "No dashboards&hellip;" check you're pointing at the absolute path:</p><div><pre>tensorboard --logdir <span><span>"</span><span><span>$(</span>pwd<span>)</span></span>/outputs/tb<span>"</span></span> --port 6006</pre></div><h3>TensorBoard Screenshots</h3><a href="https://github.com#tensorboard-screenshots"></a><p><strong>Training Progress Monitoring:</strong> <a href="https://github.com/Ashx098/sft-play/blob/main/results/Screenshot%20from%202025-08-10%2001-40-11.png"><img src="https://github.com/Ashx098/sft-play/raw/main/results/Screenshot%20from%202025-08-10%2001-40-11.png" alt="TensorBoard Training"></a></p><p><strong>Loss and Learning Rate Tracking:</strong> <a href="https://github.com/Ashx098/sft-play/blob/main/results/Screenshot%20from%202025-08-10%2001-55-29.png"><img src="https://github.com/Ashx098/sft-play/raw/main/results/Screenshot%20from%202025-08-10%2001-55-29.png" alt="Loss and LR Tracking"></a></p><p><strong>Evaluation Metrics:</strong> <a href="https://github.com/Ashx098/sft-play/blob/main/results/Screenshot%20from%202025-08-10%2002-04-51.png"><img src="https://github.com/Ashx098/sft-play/raw/main/results/Screenshot%20from%202025-08-10%2002-04-51.png" alt="Evaluation Metrics"></a></p><h2>&#128640; Quickstart</h2><a href="https://github.com#-quickstart"></a><p></p><h3>Option 1: Automated Setup (Recommended)</h3><a href="https://github.com#option-1-automated-setup-recommended"></a><p><strong>Complete setup in one command:</strong></p><div><pre>./workflows/quick_start.sh</pre></div><p>This interactive script will:</p><ul> <li>Install dependencies (auto-detects uv or pip)</li> <li>Create all necessary directories</li> <li>Generate sample data if none exists</li> <li>Process data through the complete pipeline</li> <li>Guide you to training</li> </ul><p><strong>Setup and Training Screenshots:</strong> <a href="https://github.com/Ashx098/sft-play/blob/main/results/Screenshot%20from%202025-08-10%2002-07-35.png"><img src="https://github.com/Ashx098/sft-play/raw/main/results/Screenshot%20from%202025-08-10%2002-07-35.png" alt="Setup Process"></a></p><p><strong>Or use individual Makefile commands:</strong></p><div><pre>make <span>help</span> <span><span>#</span> See all available commands</span> make install <span><span>#</span> Install dependencies</span> make setup-dirs <span><span>#</span> Create directories</span> make full-pipeline <span><span>#</span> Complete data processing</span> make check <span><span>#</span> Validate setup before training</span> make train-with-tb <span><span>#</span> Train with TensorBoard monitoring</span></pre></div><p>You can also pre-download models using the Makefile:</p><div><pre>make download-model MODEL=Qwen/Qwen2.5-3B-Instruct</pre></div><h3>Option 2: Manual Step-by-Step</h3><a href="https://github.com#option-2-manual-step-by-step"></a><p></p><h4>0) Install</h4><a href="https://github.com#0-install"></a><div><pre>pip install -r requirements.txt <span><span>#</span> or</span> <span><span>#</span> uv venv &amp;&amp; uv pip install -e .</span></pre></div><p>(Optional) Configure Accelerate:</p><div><pre>accelerate config <span><span>#</span> or use env/accelerate_config.yaml</span></pre></div><h4>1) Add raw data</h4><a href="https://github.com#1-add-raw-data"></a><p>Create <code>data/raw/raw.jsonl</code> or <code>raw.json</code>. Example (JSONL):</p><div><pre>{<span>"system"</span>:<span><span>"</span>You are a helpful assistant.<span>"</span></span>,<span>"user"</span>:<span><span>"</span>What is machine learning?<span>"</span></span>,<span>"assistant"</span>:<span><span>"</span>Machine learning is...<span>"</span></span>}</pre></div><p>(Your <code>process_data.py</code> also supports simple dicts like <code>{"question":"...","answer":"..."}</code>.)</p><h4>2) Process raw &rarr; structured chat</h4><a href="https://github.com#2-process-raw--structured-chat"></a><div><pre>python scripts/process_data.py --config configs/run_bnb.yaml --raw_path data/raw/raw.jsonl <span><span>#</span> writes data/processed/{train,val,test}.jsonl</span></pre></div><p></p><h4>3) (Optional) Inject style/system rule</h4><a href="https://github.com#3-optional-inject-stylesystem-rule"></a><div><pre>python scripts/style_prompt.py --config configs/run_bnb.yaml \ --style <span><span>"</span>Answer in &le;2 concise sentences. No markdown.<span>"</span></span> \ --in data/processed/train.jsonl \ --out data/processed_with_style/train.jsonl <span><span>#</span> repeat for val/test if desired and update config data paths</span></pre></div><h4>4) Validate setup</h4><a href="https://github.com#4-validate-setup"></a><div><pre>make check <span><span>#</span> Comprehensive sanity check</span></pre></div><p></p><h4>5) Train (TensorBoard logs)</h4><a href="https://github.com#5-train-tensorboard-logs"></a><div><pre>python scripts/train.py --config configs/run_bnb.yaml tensorboard --logdir outputs/</pre></div><ul> <li>See <code>train/loss</code>, <code>eval/loss</code>, <code>train/lr</code>, and <code>eval/rougeL</code> live.</li> <li>Checkpoints saved every <code>save_steps</code> (adapters + trainer state only).</li> </ul> <h4>6) Evaluate</h4><a href="https://github.com#6-evaluate"></a><div><pre>python scripts/eval.py --config configs/run_bnb.yaml --split val <span><span>#</span> writes outputs/metrics.json and outputs/samples.jsonl</span></pre></div><p></p><h4>7) Inference</h4><a href="https://github.com#7-inference"></a><p>Interactive:</p><div><pre>python scripts/infer.py --config configs/run_bnb.yaml</pre></div><p>Batch:</p><div>demo_inputs.txt python scripts/infer.py --config configs/run_bnb.yaml --mode batch --input_file demo_inputs.txt --output_file outputs/preds.txt"&gt;<pre><span>echo</span> <span><span>"</span>Explain QLoRA in two lines.<span>"</span></span> <span>&gt;</span> demo_inputs.txt python scripts/infer.py --config configs/run_bnb.yaml --mode batch --input_file demo_inputs.txt --output_file outputs/preds.txt</pre></div><h5>Inference Quality Improvements</h5><a href="https://github.com#inference-quality-improvements"></a><p>The inference script includes several optimizations to ensure high-quality, single-turn responses:</p><p><strong>1. Proper Stopping Conditions</strong></p><ul> <li>Forces stop at EOS tokens using <code>eos_token_id=tokenizer.eos_token_id</code></li> <li>Adds custom stop tokens for chat template boundaries (<code>&lt;|user|&gt;</code>, <code>|assistant|&gt;</code>)</li> <li>Prevents multi-turn generation where the model continues beyond the assistant's response</li> </ul><p><strong>2. Template Boundary Parsing</strong></p><ul> <li>Extracts only the assistant's response from the full generation</li> <li>Strips everything after the first <code>&lt;|assistant|&gt;</code> &rarr; <code>&lt;|user|&gt;</code> boundary</li> <li>Handles both <code>|assistant|&gt;</code> end tags and natural conversation boundaries</li> </ul><p><strong>3. Template Consistency</strong></p><ul> <li>Loads the same Jinja chat template used during training</li> <li>Ensures inference format exactly matches training format</li> <li>Prevents template mismatches that can cause poor generation quality</li> </ul><p><strong>Example of clean output extraction:</strong></p><div>You are helpful|system|&gt;&lt;|user|&gt;Hello|user|&gt;&lt;|assistant|&gt;Hi there!&lt;|user|&gt;..." # Cleaned output extracts only: # "Hi there!""&gt;<pre><span># Raw generation might include:</span> <span># "&lt;|system|&gt;You are helpful|system|&gt;&lt;|user|&gt;Hello|user|&gt;&lt;|assistant|&gt;Hi there!&lt;|user|&gt;..."</span> <span># Cleaned output extracts only:</span> <span># "Hi there!"</span></pre></div><p>These improvements ensure that:</p><ul> <li>Models stop generating at appropriate conversation boundaries</li> <li>Output is clean and contains only the intended assistant response</li> <li>Template consistency is maintained between training and inference</li> <li>Multi-turn conversations don't bleed into single responses</li> </ul><p><strong>Inference in Action:</strong> <a href="https://github.com/Ashx098/sft-play/blob/main/results/Screenshot%20from%202025-08-10%2003-18-39.png"><img src="https://github.com/Ashx098/sft-play/raw/main/results/Screenshot%20from%202025-08-10%2003-18-39.png" alt="Interactive Inference"></a></p><h4>8) (Optional) Merge adapters &rarr; FP16 model</h4><a href="https://github.com#8-optional-merge-adapters--fp16-model"></a><div><pre>python scripts/merge_lora.py --config configs/run_bnb.yaml \ --adapters adapters/last \ --out outputs/merged_fp16 \ --dtype fp16</pre></div><hr><p></p><h2>&#128736;&#65039; Automation Commands</h2><a href="https://github.com#%EF%B8%8F-automation-commands"></a> <h3>Complete Makefile Reference</h3><a href="https://github.com#complete-makefile-reference"></a><div><pre><span><span>#</span> Setup</span> make install <span><span>#</span> Install dependencies (auto-detects uv/pip)</span> make setup-dirs <span><span>#</span> Create all necessary directories</span> <span><span>#</span> Data Pipeline</span> make process <span><span>#</span> Process raw data to structured chat</span> make style <span><span>#</span> Apply style prompts to all splits</span> make render <span><span>#</span> Render chat templates</span> make full-pipeline <span><span>#</span> Complete data processing pipeline</span> <span><span>#</span> Training &amp; Evaluation</span> make train <span><span>#</span> Start training with current config</span> make train-bnb <span><span>#</span> Start training with BitsAndBytes backend</span> make train-unsloth <span><span>#</span> Start training with Unsloth backend</span> make train-with-tb <span><span>#</span> Train with TensorBoard monitoring</span> make train-bnb-tb <span><span>#</span> BitsAndBytes training with TensorBoard</span> make train-unsloth-tb <span><span>#</span> Unsloth training with TensorBoard</span> make <span>eval</span> <span><span>#</span> Evaluate on validation set</span> make eval-test <span><span>#</span> Evaluate on test set</span> make eval-quick <span><span>#</span> Quick evaluation (200 samples)</span> make eval-full <span><span>#</span> Full evaluation (no limit)</span> <span><span>#</span> Inference</span> make infer <span><span>#</span> Interactive inference (chat mode)</span> make infer-batch <span><span>#</span> Batch inference from file</span> make infer-interactive <span><span>#</span> Interactive inference (explicit)</span> <span><span>#</span> Model Management</span> make download-model <span><span>#</span> Download a model from Hugging Face Hub</span> make merge <span><span>#</span> Merge LoRA adapters to FP16 model</span> make merge-bf16 <span><span>#</span> Merge LoRA adapters to BF16 model</span> make merge-test <span><span>#</span> Test merged model loading</span> <span><span>#</span> Monitoring</span> make tensorboard <span><span>#</span> Start TensorBoard on outputs/tb</span> make tb-stop <span><span>#</span> Kill any running TensorBoard</span> make tb-clean <span><span>#</span> Remove TB event files</span> make tb-open <span><span>#</span> Print exact path &amp; suggest URL</span> <span><span>#</span> Utilities</span> make check <span><span>#</span> Validate project setup</span> make clean <span><span>#</span> Clean generated files</span> make <span>help</span> <span><span>#</span> Show all commands</span></pre></div><p></p><h3>Workflow Scripts</h3><a href="https://github.com#workflow-scripts"></a><div><pre><span><span>#</span> Interactive setup with sample data</span> ./workflows/quick_start.sh <span><span>#</span> Batch processing for multiple datasets</span> ./workflows/batch_process.sh</pre></div><h3>Customization</h3><a href="https://github.com#customization"></a><div><pre><span><span>#</span> Custom style prompts</span> make style STYLE=<span><span>"</span>Answer in JSON format only<span>"</span></span> <span><span>#</span> Custom configuration</span> make train CONFIG=configs/my_config.yaml <span><span>#</span> Custom workflows</span> make process <span>&amp;&amp;</span> make style <span>&amp;&amp;</span> make train</pre></div><hr><p></p><h2>&#129504; Design Notes</h2><a href="https://github.com#-design-notes"></a> <ul> <li><p><strong>Memory-efficient checkpoints</strong>: we save <strong>only LoRA adapters</strong> + trainer state. Result: tiny checkpoints, fast resume. Merge at the end only if you need a single FP16 folder.</p></li> <li><p><strong>VRAM-aware</strong>: when <code>batch_size/grad_accum</code> are <code>auto</code>, training probes free VRAM and picks safe values (starts at <code>bs=1</code>, increases accumulation).</p></li> <li><p><strong>Template flexibility</strong>: training renders Jinja on-the-fly, so you can change <code>chat_templates/default.jinja</code> without reprocessing.</p><div><pre>{{ system }} User: {{ user }} Assistant: {{ assistant }}</pre></div></li> <li><p><strong>Backends</strong>: set <code>tuning.backend: unsloth</code> if installed; otherwise it auto-falls back to bnb with a warning.</p></li> <li><p><strong>Complete automation</strong>: Makefile provides 20+ commands for every aspect of the pipeline.</p></li> <li><p><strong>VRAM efficiency</strong>: Qwen2.5-3B + QLoRA + bnb &rarr; ~6.5 GB VRAM at seq_len=512</p></li> <li><p><strong>Modes</strong>:</p><ul> <li><code>qlora</code> &rarr; 4-bit base + LoRA (best for 8 GB on 7B/3B causal LMs)</li> <li><code>lora</code> &rarr; fp16/bf16 base + LoRA (fine for 1&ndash;3B, or enough VRAM)</li> <li><code>full</code> &rarr; full fine-tune (use for small seq2seq, e.g., FLAN-T5-base)</li> </ul> </li> </ul> <hr> <h2>&#129514; Troubleshooting</h2><a href="https://github.com#-troubleshooting"></a><p></p><h3>Configuration Issues</h3><a href="https://github.com#configuration-issues"></a> <ul> <li><p><strong>Training Arguments Mismatch Error</strong></p><div><pre><code>ValueError: --load_best_model_at_end requires the save and eval strategy to match </code></pre></div><p><strong>Solution</strong>: This was fixed in the training script. The issue occurred when <code>evaluation_strategy</code> and <code>save_strategy</code> didn't match. The script now properly handles both <code>evaluation_strategy</code> and <code>eval_strategy</code> keys from config files.</p></li> <li><p><strong>Backend Switching: BitsAndBytes &harr; Unsloth</strong></p><p><strong>Use the provided backend-specific configs:</strong></p><div><pre><span><span>#</span> For BitsAndBytes (stable, broad compatibility)</span> make train CONFIG=configs/run_bnb.yaml <span><span>#</span> For Unsloth (faster, requires compatible CUDA)</span> make train CONFIG=configs/run_unsloth.yaml</pre></div><p><strong>Or create custom config:</strong></p><div><pre><span><span>#</span> For Unsloth backend</span> <span>include</span>: <span>configs/config_base.yaml</span> <span>tuning</span>: <span>backend</span>: <span>unsloth</span> <span>train</span>: <span>bf16</span>: <span>true </span><span><span>#</span> Unsloth works better with bfloat16</span> <span>fp16</span>: <span>false</span> <span><span>#</span> For BitsAndBytes backend </span> <span>include</span>: <span>configs/config_base.yaml</span> <span>tuning</span>: <span>backend</span>: <span>bnb</span> <span>train</span>: <span>bf16</span>: <span>false </span><span><span>#</span> BitsAndBytes is more stable with float16</span> <span>fp16</span>: <span>true</span></pre></div><p><strong>Key differences:</strong></p><ul> <li><strong>Unsloth</strong>: Faster training, requires specific CUDA versions, works best with <code>bf16: true</code></li> <li><strong>BitsAndBytes</strong>: More stable, broader compatibility, works best with <code>fp16: true</code></li> <li><strong>Auto-fallback</strong>: If Unsloth fails to import, the system automatically falls back to BitsAndBytes</li> </ul> </li> </ul> <h3>Memory and Performance Issues</h3><a href="https://github.com#memory-and-performance-issues"></a> <ul> <li><p><strong>CUDA OOM</strong></p><ul> <li>Lower <code>model.max_seq_len</code> (e.g., 512 &rarr; 384).</li> <li>Keep <code>mode: qlora</code>, <code>backend: bnb</code>, <code>batch_size: auto</code>, <code>grad_accum: auto</code>.</li> <li>Ensure TensorBoard isn't eating VRAM on the same GPU (runs on CPU, but double-check).</li> </ul> </li> <li><p><strong>Unsloth import fails</strong></p><ul> <li>Use <code>backend: bnb</code> (default).</li> <li>If you insist on Unsloth, match its CUDA/PTX requirements.</li> </ul> </li> <li><p><strong>XFormers compatibility issues</strong></p><div><pre><code>NotImplementedError: No operator found for `memory_efficient_attention_backward` </code></pre></div><p><strong>Solution</strong>: This is a known compatibility issue with Unsloth + XFormers on certain GPU/CUDA configurations. The system now automatically falls back to BitsAndBytes when Unsloth is requested:</p><p><strong>Automatic Fallback</strong>: When you use <code>configs/run_unsloth.yaml</code>, the system detects XFormers issues and automatically switches to BitsAndBytes with a warning message.</p><p><strong>Recommended Approach</strong>: Use BitsAndBytes directly for maximum stability:</p><div><pre>make train-bnb <span><span>#</span> Direct BitsAndBytes training</span> make train-bnb-tb <span><span>#</span> BitsAndBytes with TensorBoard</span></pre></div><p><strong>Manual Configuration</strong>: If you want to force BitsAndBytes:</p><div><pre><span>tuning</span>: <span>backend</span>: <span>bnb</span> <span>train</span>: <span>bf16</span>: <span>false</span> <span>fp16</span>: <span>true</span></pre></div></li> </ul> <h3>Data and Training Issues</h3><a href="https://github.com#data-and-training-issues"></a> <ul> <li><p><strong>Weird formatting in generations</strong></p><ul> <li>Check <code>chat_templates/default.jinja</code> and your style prompt.</li> <li>Remember causal LMs may echo the prompt; <code>infer.py</code> strips assistant tags heuristically.</li> </ul> </li> <li><p><strong>Metrics too low</strong></p><ul> <li>Increase epochs to 3&ndash;5.</li> <li>Tune LoRA <code>r</code> (16&rarr;32) or LR (2e-4 &rarr; 1e-4).</li> <li>Ensure your processed data is clean and task-consistent.</li> </ul> </li> <li><p><strong>ROUGE metrics showing 0.0</strong></p><div><pre><code>[train] Warning: Could not compute ROUGE metrics: argument 'ids': 'list' object cannot be interpreted as an integer </code></pre></div><p><strong>Note</strong>: This is a known issue with the ROUGE evaluation library and doesn't affect training. The model is still learning (check the decreasing loss values).</p></li> </ul> <h3>Setup Issues</h3><a href="https://github.com#setup-issues"></a> <ul> <li><p><strong>Setup issues</strong></p><ul> <li>Run <code>make check</code> to validate your setup</li> <li>Use <code>./workflows/quick_start.sh</code> for guided setup</li> <li>Check <code>AUTOMATION_GUIDE.md</code> for detailed automation docs</li> </ul> </li> </ul> <h3>Configuration Validation</h3><a href="https://github.com#configuration-validation"></a> <ul> <li><p><strong>Before training, always validate your config:</strong></p><div><pre>make check <span><span>#</span> Comprehensive validation</span> python scripts/train.py --config configs/run_bnb.yaml --help <span><span>#</span> Check arguments</span></pre></div></li> <li><p><strong>Common config mistakes:</strong></p><ul> <li>Mismatched <code>evaluation_strategy</code> and <code>save_strategy</code> (now auto-fixed)</li> <li>Wrong precision settings for your backend (<code>bf16</code> vs <code>fp16</code>)</li> <li>Missing data files or incorrect paths</li> <li>Incompatible model settings with available VRAM</li> </ul> </li> </ul> <hr> <h2>&#9989; Definition of Done (v0.1)</h2><a href="https://github.com#-definition-of-done-v01"></a> <ul> <li>End-to-end run on <strong>Qwen2.5-3B (QLoRA+bnb)</strong> on an <strong>8 GB</strong> GPU without OOM.</li> <li>Live TensorBoard charts.</li> <li><code>outputs/metrics.json</code> with ROUGE-L (and others).</li> <li><code>infer.py</code> produces sensible answers.</li> <li>(Optional) <code>outputs/merged_fp16</code> exists and loads with HF.</li> <li><strong>Complete automation</strong> with Makefile and workflow scripts.</li> <li><strong>Sanity checking</strong> with <code>make check</code> validation.</li> </ul> <hr><p></p><h2>&#128218; Documentation</h2><a href="https://github.com#-documentation"></a> <ul> <li><strong>AUTOMATION_GUIDE.md</strong> - Detailed automation system documentation</li> <li><strong>SETUP_DOCUMENTATION.md</strong> - Complete project setup guide</li> <li><strong>LICENSE</strong> - MIT License for open source use</li> </ul> <hr> <h2>&#127919; Quick Examples</h2><a href="https://github.com#-quick-examples"></a><p><strong>Try fine-tuning on EduGen Small Q&amp;A (<a href="https://www.kaggle.com/datasets/avinashmynampati/edugen-small-qa">Kaggle link</a>). Takes ~10 min on an RTX 4060.</strong></p><h3>&#128640; Real Results: 10-Minute Fine-Tune Success</h3><a href="https://github.com#-real-results-10-minute-fine-tune-success"></a><p><strong>Tried a 10-minute QLoRA fine-tune on Qwen2.5-1.5B with 240 Q&amp;As (EduGen style):</strong></p><ul> <li><strong>Baseline ROUGE-L</strong>: 0.17 &rarr; <strong>After SFT</strong>: 0.33 (<strong>~95% improvement!</strong>)</li> <li><strong>SARI</strong>: 40 &rarr; 55 (<strong>+15 points</strong>)</li> </ul><p>That's ~95% growth in ROUGE-L and +15 points in SARI in just 10 minutes! Even with tiny data, the model became step-by-step and precise.</p><p><strong>Proof that fine-tuning isn't just for labs &mdash; you can try it in a few minutes on consumer GPUs.</strong></p><h4>&#128202; Metric Improvements (Visual Proof)</h4><a href="https://github.com#-metric-improvements-visual-proof"></a><p><strong>ROUGE-L Score Improvement:</strong> <a href="https://github.com/Ashx098/sft-play/blob/main/results/Rouge-L.png"><img src="https://github.com/Ashx098/sft-play/raw/main/results/Rouge-L.png" alt="ROUGE-L Results"></a></p><p><strong>SARI Score Improvement:</strong> <a href="https://github.com/Ashx098/sft-play/blob/main/results/SARI.png"><img src="https://github.com/Ashx098/sft-play/raw/main/results/SARI.png" alt="SARI Results"></a></p><h3>Usage Examples</h3><a href="https://github.com#usage-examples"></a><p><strong>Complete beginner workflow:</strong></p><div><pre>./workflows/quick_start.sh <span><span>#</span> One command setup</span> make check <span><span>#</span> Validate everything</span> make train-bnb-tb <span><span>#</span> Train with TensorBoard monitoring</span></pre></div><p><strong>Advanced user workflow:</strong></p><div><pre>make install <span>&amp;&amp;</span> make setup-dirs make process make style STYLE=<span><span>"</span>Be concise and professional<span>"</span></span> make check make train CONFIG=configs/my_qlora.yaml make eval-test make merge <span>&amp;&amp;</span> make merge-test</pre></div><p><strong>Development workflow:</strong></p><div><pre>make full-pipeline <span><span>#</span> Process all data</span> make eval-quick <span><span>#</span> Fast validation</span> make infer <span><span>#</span> Test interactively</span></pre></div><p>That's it! The automation system makes SFT-Play truly plug-and-play. Run <code>make help</code> to see all available commands, or start with <code>./workflows/quick_start.sh</code> for a guided experience.</p></article></div></section>]]></description><pubDate>Mon, 11 Aug 2025 14:52:43 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mmfn9b/make_llm_remember_menot_by_prompt_or_rag/</link><title>Make LLM remember me.not by prompt or Rag?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mmfn9b/make_llm_remember_menot_by_prompt_or_rag/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mmfn9b/make_llm_remember_menot_by_prompt_or_rag/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mmfn9b/make_llm_remember_menot_by_prompt_or_rag/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi, everyone. I m kinda excited to make a local LLM assistant, but how can i make the model remember my informations without any prompt or context informations. </p><p>Im curious about how llm really remember facts, tho i was told that LLM absorted facts mainly in Pretraining process. so, do i need to SFT LLM with my dataset or shoud i Continue Pretraining with unsupervised dataset first.</p></div><!-- SC_ON --></section>]]></description><pubDate>Sun, 10 Aug 2025 16:29:47 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mme7sm/the_curious_case_of_running_unsloth_glm41v9b_gguf/</link><title>the curious case of running unsloth GLM-4.1V-9B GGUF on llama.cpp: No mmproj files, Multi-modal CLI requires -mmproj, and doesn't support --jinja?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mme7sm/the_curious_case_of_running_unsloth_glm41v9b_gguf/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mme7sm/the_curious_case_of_running_unsloth_glm41v9b_gguf/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mme7sm/the_curious_case_of_running_unsloth_glm41v9b_gguf/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hello everyone,</p><p>I&#39;m trying to test the <strong>Unsloth GLM-4.1V-9B-Thinking VLM GGUF</strong> on a local <strong>llama.cpp</strong> build, but I&#39;m running into a confusing issue regarding the multi-modal projection file and chat templates.</p><hr/><h2>My Setup</h2><ul><li><strong>Model:</strong> <code>unsloth/GLM-4.1V-9B-Thinking-UD-Q4_K_XL.gguf</code></li><li><strong>Executables:</strong> <code>llama-cli.exe</code> and <code>llama-mtmd-cli.exe</code><br/>(both from a pre-built <strong>llama.cpp</strong> build <strong>b6103</strong>)</li></ul><hr/><h2>The Problem</h2><p>My goal is to use the model&#39;s <strong>VLM features</strong> by providing both a prompt <strong>and</strong> an image.<br/>However, this model <strong>doesn&#39;t come with an mmproj file</strong>.</p><ul><li><p><strong><code>llama-cli.exe</code></strong>:</p><ul><li>Recognizes the <code>--jinja</code> flag.</li><li>Does <strong>not</strong> support multi-modal flags like <code>--image</code> or <code>-i</code>.</li></ul></li><li><p><strong><code>llama-mtmd-cli.exe</code></strong>:</p><ul><li>Supports the <code>--image</code> flag.</li><li>Does <strong>not</strong> support the <code>--jinja</code> flag.</li><li>Appears to require a separate <code>-mmproj</code> file to function.</li></ul></li></ul><hr/><h2>What I Have Tried</h2><ol><li><p><strong>Text-only with <code>llama-cli.exe</code></strong>  </p><ul><li>Loads model and responds to text-only prompts.<br/></li><li>Confirms <code>--jinja</code> works correctly here.</li></ul></li><li><p><strong>VLM command with <code>llama-cli.exe</code></strong>  </p><ul><li>Failed ‚Äî <code>--image</code> flag is not available.</li></ul></li><li><p><strong>VLM command with <code>llama-mtmd-cli.exe</code></strong>  </p><ul><li>Using <code>--jinja</code> ‚Üí Error:<br/><code>error: invalid argument: --jinja</code></li><li>Using <code>--image</code> without <code>--jinja</code> ‚Üí Error:<br/><code>-mmproj flag is required</code>I assumed, based on similar models, that the <strong>GLM-4.1V-9B GGUF</strong> has the multi-modal projection layers <em>baked-in</em> and wouldn‚Äôt require a separate <code>mmproj</code> file.<br/>However, after checking the <strong>Unsloth Hugging Face</strong> page, I couldn‚Äôt find any dedicated <code>mmproj</code> file.</li></ul></li></ol><p>Has anyone successfully run this model on llama.cpp?Any guidance on how to get this model working would be greatly appreciated.<br/>Thank you!</p></div><!-- SC_ON --></section>]]></description><pubDate>Sun, 10 Aug 2025 15:02:28 +0530</pubDate></item><item><link>https://i.redd.it/mlikt0f7ythf1.png</link><title>gpt-oss Fine-tuning is here! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1ml480n/gptoss_finetuning_is_here/</guid><comments>https://www.reddit.com/r/unsloth/comments/1ml480n/gptoss_finetuning_is_here/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1ml480n/gptoss_finetuning_is_here/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys, we now support gpt-oss finetuning. We‚Äôve managed to make gpt-oss train on just 14GB of VRAM, making it possible to work on free Colab.</p><p>We also talk about our <strong>bugfixes</strong>, <strong>notebooks</strong> etc all in our guide: <a href="https://docs.unsloth.ai/basics/gpt-oss">https://docs.unsloth.ai/basics/gpt-oss</a></p><p>Unfortunately due to gpt-oss&#39; architecture, if you want to train the model without Unsloth, you‚Äôll need to upcast the weights to bf16 before training. This approach, significantly increases both VRAM usage and training time by as much as <strong>300% more memory usage</strong>!</p><p>gpt-oss-120b model fits on 65GB of VRAM with Unsloth.</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/mlikt0f7ythf1.png' /></section>]]></description><pubDate>Sat, 09 Aug 2025 00:43:19 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mkp7ua/how_you_could_boost_pp_rates_of_amd_mi50/</link><title>How you could boost P/P rates of AMD MI50</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mkp7ua/how_you_could_boost_pp_rates_of_amd_mi50/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mkp7ua/how_you_could_boost_pp_rates_of_amd_mi50/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mkp7ua/how_you_could_boost_pp_rates_of_amd_mi50/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Continue from my last post, and thanks for valuable comments!</p><p>(Localllama&#39;s Moderator blocked my post now, but I don&#39;t know what I violated)</p><p>In the beginning, I set up 4070ti(12GB VRAM) + MI50(32GB VRAM) on my gaming gear,</p><p>However, I only could access 12 +12 GB of vram in two GPUs - it was restricted by size of first gpu&#39;s VRAM(12G)</p><p>or, MI 32GB only by turn off using 4070ti on Win11 / Vulkan / LM studio environment.</p><p>Since last weekeens, I have been trying to access the rest portion of total 44G Vram(gpu0+gpu1) in Local LLM running.</p><p>(It wasn&#39;t fault of MI50, it is clearly related with incomplete vulkan/llama.cpp implementation of LM Studio)</p><p>Most easy solution may be put MI50 on &quot;first&quot; PCI 5.0 slot,  but the MI50 doesn&#39; supports screen output unless bios rom writing.</p><p>Finally, I found a simple way to exchange gpu0 and 1 postion in Windows. -</p><p>Just go right Control Panel =&gt; System =&gt; Display =&gt; Graphics</p><p>and Let RADEON VII(MI50) as a primary graphic card of LM Studio Apps</p><p>By this way, I got &quot;almost&quot; 32GB VRAMs (sorry it&#39;s not 32+12GB yet) in LM Studio </p><p>It not only gluing 32GB of HBM on your gpu, but also can steal prompt processing ability from old Nvidia GPU</p><p>Please show three results from favorite scenarios. Whole test have conducted Win11/Vulkan Envrionment.</p><p><strong>1. Legal Document Analysis(21,928 Input tokens)</strong></p><p>Model : ERNIE-4.5-21B-A3B (Q6_K, size: 18.08GB)  to check effects of GPU position between GPU 0 and 1</p><p>GPU Setting                     Token Generation  Total Output(Tokens)   Time to 1st Token</p><p>MI50(gpu0)+4070TI(gpu1)   23.27(token/s)          1303(tokens)             195.74sec</p><p>4070TI(gpu0)+MI50(gpu1)   24.00(token/s)          1425(tokens)             174.62sec</p><p><strong>2. Hard SF Novel Writing (929 Input tokens)</strong></p><p>Model : Qwen3-30B-A3B-Thinking-2507 (Q8_0, 32.48GB) - Max accessible memory test</p><p>GPU Setting                     Token Generation  Total Output(Tokens)   Time to 1st Token</p><p>MI50(main)+4070TI(sub)*       13.86(token/s)             6437(tokens)           13.08sec</p><p>MI50(32GB only)                 17.93(token/s)             5656(tokens)          17.75sec</p><ul><li>Whole model has landed on MI50(about 21GB) &amp; 4070(11GB) successfully.</li></ul><p><strong>3. Multilingual Novel Summerization(27,393 Input Tokens)</strong></p><p>Gemma-3-27b-QAT (Q4_0, 16.43GB, 4bit KV Cache)</p><p>GPU Setting                     Token Generation  Total Output(Tokens)   Time to 1st Token</p><p>MI50(main)+4070TI(sub)          4.19(tokens)               907(tokens)            10min 2sec</p><p>MI50(only)                            2.92(tokens)               1058(token)           33min** 41s </p><p>Many GPU poor including me always said that &quot;I&#39;m patient man&quot;, however, 33 minutes vs. 10 minutes is a good reason to think twice before ordering MI50 and adding Nvidia used Card instead. - P/P is really crawling on AMD but this disadvantage can be overcome by attaching Nvidia Card.</p><p>I still think the MI50 is a very cheap and appropriate investment for hobbiest even considering these drawbacks.</p><p>If anyone is familiar with the Linux environment and llama.cpp, I&#39;d appreciate it if you could share some insights and benchmark result on distributed inference using RPC. Setting it up that way might allow access to all VRAM, excluding any frameworks penalties from using multiple GPUs.</p></div><!-- SC_ON --></section>]]></description><pubDate>Fri, 08 Aug 2025 13:25:47 +0530</pubDate></item><item><link>https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF</link><title>Qwen3-4B-2507 Unsloth Dynamic GGUFs out now! (huggingface.co)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mjgsm9/qwen34b2507_unsloth_dynamic_ggufs_out_now/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mjgsm9/qwen34b2507_unsloth_dynamic_ggufs_out_now/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 9 min | <a href='https://www.reddit.com/r/unsloth/comments/1mjgsm9/qwen34b2507_unsloth_dynamic_ggufs_out_now/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey y&#39;all here they are for the new Qwen model including Thinking version: <a href="https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF">https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF</a></p><p>Let us know if there are any issues.</p><p>P.S. gpt-oss support coming tomorrow and I think you guys are gonna LOVE it. We did some cooking and made some magic work! ;)</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://cdn-thumbnails.huggingface.co/social-thumbnails/models/unsloth/Qwen3-4B-Instruct-2507-GGUF.png' /></section><section class='parsed-content'><div><ul> <li>Fine-tune Qwen3 (14B) for free using our Google <a href="https://docs.unsloth.ai/get-started/unsloth-notebooks">Colab notebook here</a>!</li> <li>Read our Blog about Qwen3 support: <a href="https://unsloth.ai/blog/qwen3">unsloth.ai/blog/qwen3</a></li> <li>View the rest of our notebooks in our <a href="https://docs.unsloth.ai/get-started/unsloth-notebooks">docs here</a>.</li> <li>Run &amp; export your fine-tuned model to Ollama, llama.cpp or HF.</li> </ul> <h2> <a href="https://huggingface.co#highlights"> </a> <span> Highlights </span> </h2><p>We introduce the updated version of the <strong>Qwen3-4B non-thinking mode</strong>, named <strong>Qwen3-4B-Instruct-2507</strong>, featuring the following key enhancements:</p><ul> <li><strong>Significant improvements</strong> in general capabilities, including <strong>instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage</strong>.</li> <li><strong>Substantial gains</strong> in long-tail knowledge coverage across <strong>multiple languages</strong>.</li> <li><strong>Markedly better alignment</strong> with user preferences in <strong>subjective and open-ended tasks</strong>, enabling more helpful responses and higher-quality text generation.</li> <li><strong>Enhanced capabilities</strong> in <strong>256K long-context understanding</strong>.</li> </ul> <h2> <a href="https://huggingface.co#model-overview"> </a> <span> Model Overview </span> </h2><p><strong>Qwen3-4B-Instruct-2507</strong> has the following features:</p><ul> <li>Type: Causal Language Models</li> <li>Training Stage: Pretraining &amp; Post-training</li> <li>Number of Parameters: 4.0B</li> <li>Number of Paramaters (Non-Embedding): 3.6B</li> <li>Number of Layers: 36</li> <li>Number of Attention Heads (GQA): 32 for Q and 8 for KV</li> <li>Context Length: <strong>262,144 natively</strong>.</li> </ul><p><strong>NOTE: This model supports only non-thinking mode and does not generate <code><think></think></code> blocks in its output. Meanwhile, specifying <code>enable_thinking=False</code> is no longer required.</strong></p><p>For more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our <a href="https://qwenlm.github.io/blog/qwen3/">blog</a>, <a href="https://github.com/QwenLM/Qwen3">GitHub</a>, and <a href="https://qwen.readthedocs.io/en/latest/">Documentation</a>.</p><h2> <a href="https://huggingface.co#performance"> </a> <span> Performance </span> </h2><div><table> <thead><tr> <th></th> <th>GPT-4.1-nano-2025-04-14</th> <th>Qwen3-30B-A3B Non-Thinking</th> <th>Qwen3-4B Non-Thinking</th> <th>Qwen3-4B-Instruct-2507</th> </tr> </thead><tbody><tr> <td><strong>Knowledge</strong></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>MMLU-Pro</td> <td>62.8</td> <td>69.1</td> <td>58.0</td> <td><strong>69.6</strong></td> </tr> <tr> <td>MMLU-Redux</td> <td>80.2</td> <td>84.1</td> <td>77.3</td> <td><strong>84.2</strong></td> </tr> <tr> <td>GPQA</td> <td>50.3</td> <td>54.8</td> <td>41.7</td> <td><strong>62.0</strong></td> </tr> <tr> <td>SuperGPQA</td> <td>32.2</td> <td>42.2</td> <td>32.0</td> <td><strong>42.8</strong></td> </tr> <tr> <td><strong>Reasoning</strong></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>AIME25</td> <td>22.7</td> <td>21.6</td> <td>19.1</td> <td><strong>47.4</strong></td> </tr> <tr> <td>HMMT25</td> <td>9.7</td> <td>12.0</td> <td>12.1</td> <td><strong>31.0</strong></td> </tr> <tr> <td>ZebraLogic</td> <td>14.8</td> <td>33.2</td> <td>35.2</td> <td><strong>80.2</strong></td> </tr> <tr> <td>LiveBench 20241125</td> <td>41.5</td> <td>59.4</td> <td>48.4</td> <td><strong>63.0</strong></td> </tr> <tr> <td><strong>Coding</strong></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>LiveCodeBench v6 (25.02-25.05)</td> <td>31.5</td> <td>29.0</td> <td>26.4</td> <td><strong>35.1</strong></td> </tr> <tr> <td>MultiPL-E</td> <td>76.3</td> <td>74.6</td> <td>66.6</td> <td><strong>76.8</strong></td> </tr> <tr> <td>Aider-Polyglot</td> <td>9.8</td> <td><strong>24.4</strong></td> <td>13.8</td> <td>12.9</td> </tr> <tr> <td><strong>Alignment</strong></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>IFEval</td> <td>74.5</td> <td><strong>83.7</strong></td> <td>81.2</td> <td>83.4</td> </tr> <tr> <td>Arena-Hard v2*</td> <td>15.9</td> <td>24.8</td> <td>9.5</td> <td><strong>43.4</strong></td> </tr> <tr> <td>Creative Writing v3</td> <td>72.7</td> <td>68.1</td> <td>53.6</td> <td><strong>83.5</strong></td> </tr> <tr> <td>WritingBench</td> <td>66.9</td> <td>72.2</td> <td>68.5</td> <td><strong>83.4</strong></td> </tr> <tr> <td><strong>Agent</strong></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>BFCL-v3</td> <td>53.0</td> <td>58.6</td> <td>57.6</td> <td><strong>61.9</strong></td> </tr> <tr> <td>TAU1-Retail</td> <td>23.5</td> <td>38.3</td> <td>24.3</td> <td><strong>48.7</strong></td> </tr> <tr> <td>TAU1-Airline</td> <td>14.0</td> <td>18.0</td> <td>16.0</td> <td><strong>32.0</strong></td> </tr> <tr> <td>TAU2-Retail</td> <td>-</td> <td>31.6</td> <td>28.1</td> <td><strong>40.4</strong></td> </tr> <tr> <td>TAU2-Airline</td> <td>-</td> <td>18.0</td> <td>12.0</td> <td><strong>24.0</strong></td> </tr> <tr> <td>TAU2-Telecom</td> <td>-</td> <td><strong>18.4</strong></td> <td>17.5</td> <td>13.2</td> </tr> <tr> <td><strong>Multilingualism</strong></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>MultiIF</td> <td>60.7</td> <td><strong>70.8</strong></td> <td>61.3</td> <td>69.0</td> </tr> <tr> <td>MMLU-ProX</td> <td>56.2</td> <td><strong>65.1</strong></td> <td>49.6</td> <td>61.6</td> </tr> <tr> <td>INCLUDE</td> <td>58.6</td> <td><strong>67.8</strong></td> <td>53.8</td> <td>60.1</td> </tr> <tr> <td>PolyMATH</td> <td>15.6</td> <td>23.3</td> <td>16.6</td> <td><strong>31.1</strong></td> </tr> </tbody> </table> </div><p>*: For reproducibility, we report the win rates evaluated by GPT-4.1.</p><h2> <a href="https://huggingface.co#quickstart"> </a> <span> Quickstart </span> </h2><p>The code of Qwen3 has been in the latest Hugging Face <code>transformers</code> and we advise you to use the latest version of <code>transformers</code>.</p><p>With <code>transformers&lt;4.51.0</code>, you will encounter the following error:</p><pre><code>KeyError: 'qwen3' </code></pre><p>The following contains a code snippet illustrating how to use the model generate content based on given inputs. </p><pre><code><span>from</span> transformers <span>import</span> AutoModelForCausalLM, AutoTokenizer model_name = <span>"Qwen/Qwen3-4B-Instruct-2507"</span> <span># load the tokenizer and the model</span> tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=<span>"auto"</span>, device_map=<span>"auto"</span> ) <span># prepare the model input</span> prompt = <span>"Give me a short introduction to large language model."</span> messages = [ {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: prompt} ] text = tokenizer.apply_chat_template( messages, tokenize=<span>False</span>, add_generation_prompt=<span>True</span>, ) model_inputs = tokenizer([text], return_tensors=<span>"pt"</span>).to(model.device) <span># conduct text completion</span> generated_ids = model.generate( **model_inputs, max_new_tokens=<span>16384</span> ) output_ids = generated_ids[][<span>len</span>(model_inputs.input_ids[]):].tolist() content = tokenizer.decode(output_ids, skip_special_tokens=<span>True</span>) <span>print</span>(<span>"content:"</span>, content) </code></pre><p>For deployment, you can use <code>sglang&gt;=0.4.6.post1</code> or <code>vllm&gt;=0.8.5</code> or to create an OpenAI-compatible API endpoint:</p><ul> <li>SGLang:<pre><code>python -m sglang.launch_server --model-path Qwen/Qwen3-4B-Instruct-2507 --context-length 262144 </code></pre> </li> <li>vLLM:<pre><code>vllm serve Qwen/Qwen3-4B-Instruct-2507 --max-model-len 262144 </code></pre> </li> </ul><p><strong>Note: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as <code>32,768</code>.</strong></p><p>For local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.</p><h2> <a href="https://huggingface.co#agentic-use"> </a> <span> Agentic Use </span> </h2><p>Qwen3 excels in tool calling capabilities. We recommend using <a href="https://github.com/QwenLM/Qwen-Agent">Qwen-Agent</a> to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.</p><p>To define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.</p><pre><code><span>from</span> qwen_agent.agents <span>import</span> Assistant <span># Define LLM</span> llm_cfg = { <span>'model'</span>: <span>'Qwen3-4B-Instruct-2507'</span>, <span># Use a custom endpoint compatible with OpenAI API:</span> <span>'model_server'</span>: <span>'http://localhost:8000/v1'</span>, <span># api_base</span> <span>'api_key'</span>: <span>'EMPTY'</span>, } <span># Define Tools</span> tools = [ {<span>'mcpServers'</span>: { <span># You can specify the MCP configuration file</span> <span>'time'</span>: { <span>'command'</span>: <span>'uvx'</span>, <span>'args'</span>: [<span>'mcp-server-time'</span>, <span>'--local-timezone=Asia/Shanghai'</span>] }, <span>"fetch"</span>: { <span>"command"</span>: <span>"uvx"</span>, <span>"args"</span>: [<span>"mcp-server-fetch"</span>] } } }, <span>'code_interpreter'</span>, <span># Built-in tools</span> ] <span># Define Agent</span> bot = Assistant(llm=llm_cfg, function_list=tools) <span># Streaming generation</span> messages = [{<span>'role'</span>: <span>'user'</span>, <span>'content'</span>: <span>'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'</span>}] <span>for</span> responses <span>in</span> bot.run(messages=messages): <span>pass</span> <span>print</span>(responses) </code></pre> <h2> <a href="https://huggingface.co#best-practices"> </a> <span> Best Practices </span> </h2><p>To achieve optimal performance, we recommend the following settings:</p><ol> <li><p><strong>Sampling Parameters</strong>:</p><ul> <li>We suggest using <code>Temperature=0.7</code>, <code>TopP=0.8</code>, <code>TopK=20</code>, and <code>MinP=0</code>.</li> <li>For supported frameworks, you can adjust the <code>presence_penalty</code> parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.</li> </ul> </li> <li><p><strong>Adequate Output Length</strong>: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.</p></li> <li><p><strong>Standardize Output Format</strong>: We recommend using prompts to standardize model outputs when benchmarking.</p><ul> <li><strong>Math Problems</strong>: Include "Please reason step by step, and put your final answer within \boxed{}." in the prompt.</li> <li><strong>Multiple-Choice Questions</strong>: Add the following JSON structure to the prompt to standardize responses: "Please show your choice in the <code>answer</code> field with only the choice letter, e.g., <code>"answer": "C"</code>."</li> </ul> </li> </ol> <h3> <a href="https://huggingface.co#citation"> </a> <span> Citation </span> </h3><p>If you find our work helpful, feel free to give us a cite.</p><pre><code>@misc{qwen3technicalreport, title={Qwen3 Technical Report}, author={Qwen Team}, year={2025}, eprint={2505.09388}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2505.09388}, } </code></pre> </div><div class="gallery"><p><img src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5"></p><p><img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-2507/Qwen3-4B-Instruct.001.jpeg"></p></div></section>]]></description><pubDate>Thu, 07 Aug 2025 02:39:42 +0530</pubDate></item><item><link>https://huggingface.co/blog/driaforall/towards-open-evolutionary-agents</link><title>Towards Open Evolutionary Agents (huggingface.co)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mj4ihy/towards_open_evolutionary_agents/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mj4ihy/towards_open_evolutionary_agents/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 28 min | <a href='https://www.reddit.com/r/unsloth/comments/1mj4ihy/towards_open_evolutionary_agents/'>Post permalink</a></p></section><section class='preview-image'><img src='https://cdn-thumbnails.huggingface.co/social-thumbnails/blog/driaforall/towards-open-evolutionary-agents.png' /></section><section class='parsed-content'><div><p><a href="https://huggingface.co/blog"> Back to Articles</a></p><h2> <a href="https://huggingface.co#executive-summary"> </a> <span> Executive Summary </span> </h2><p>We performed comprehensive analysis of 29 experiments using OpenEvolve, an open-source evolutionary coding agent, on the <a href="https://algotune.io/">AlgoTune</a> benchmark suite reveals that both proprietary and open models achieve significant performance gains. Notably, open models like Google's Gemma 3 27B and Alibaba's Qwen3-Coder 480B demonstrated impressive optimization capabilities, rivalling, and sometimes uniquely surpassing their closed-source counterparts. Gemini Flash 2.5 achieved the top speedup of 2.04x, with Gemma 3 27B and Qwen3-Coder reaching 1.63x and 1.41x, respectively. </p><p>AlgoTune represents a critical benchmark for evaluating algorithmic optimization capabilities, featuring diverse computational tasks spanning graph algorithms, linear algebra operations, and optimization problems. Unlike traditional coding benchmarks that focus on correctness, AlgoTune specifically measures performance improvement - challenging systems to not just solve problems, but to make existing solutions faster. Each task provides a baseline implementation and measures speedup through rigorous profiling, making it particularly well-suited for evolutionary agents that iteratively refine code. </p><p><strong>Dashboard Overview</strong>: This 2x2 visualization summarizes key experimental findings:</p><ul> <li><strong>A. Temperature Optimization Study</strong>: Shows pure temperature comparison (0.2, 0.4, 0.8) with clean performance curves</li> <li><strong>B. Extended Iterations</strong>: Demonstrates continued performance gains from 100 to 200 iterations</li> <li><strong>C. All Models Tested (Best Config)</strong>: Comprehensive comparison of all 10 unique models at their optimal configurations</li> <li><strong>D. Parallelism Impact</strong>: Shows critical performance degradation with serial evaluation</li> </ul> <h3> <a href="https://huggingface.co#key-findings"> </a> <span> Key Findings </span> </h3> <ol> <li><strong>More Iterations Yield Better Results</strong>: 200 iterations achieved 2.04x speedup vs 1.64x for 100 iterations (24% improvement)</li> <li><strong>Specialization Beats Size</strong>: Qwen3-Coder (480B MoE, coding-specialized) outperformed Qwen3-235B general model</li> <li><strong>Evolution Strategy is Model-Dependent</strong>: Strong coding models excel with diff-based evolution; weaker models need full rewrites</li> <li><strong>Temperature Optimization Matters</strong>: 0.4 proved optimal for Gemini models (1.29x vs 1.17x at 0.2)</li> <li><strong>Artifacts Boost Performance</strong>: Including debugging artifacts improved results by 17%</li> <li><strong>Parallelism is Critical</strong>: Serial evaluation catastrophically fails (50% worse performance, 14x slower)</li> </ol> <h3> <a href="https://huggingface.co#top-performers"> </a> <span> Top Performers </span> </h3> <table> <tbody><tr> <th>Model</th> <th>Config</th> <th>AlgoTune Score</th> <th>Best Task</th> </tr> </tbody><tbody> <tr> <td>Gemini Flash 2.5</td> <td>200 iter, diff, temp 0.4</td> <td>2.04x</td> <td>count_connected_components: 95.78x</td> </tr> <tr> <td>Gemini Flash 2.5</td> <td>100 iter, diff, temp 0.4</td> <td>1.64x</td> <td>psd_cone_projection: 32.7x</td> </tr> <tr> <td>Gemma 3 27B</td> <td>100 iter, diff</td> <td>1.63x</td> <td>psd_cone_projection: 41.1x</td> </tr> <tr> <td>Qwen3-Coder 480B</td> <td>100 iter, diff, temp 0.6</td> <td>1.41x</td> <td>psd_cone_projection: 41.9x</td> </tr> </tbody> </table> <h2> <a href="https://huggingface.co#experiment-statistics"> </a> <span> Experiment Statistics </span> </h2> <ul> <li><strong>Total Experiments</strong>: 29</li> <li><strong>Models Tested</strong>: 10 unique (Gemini family, Qwen family, Meta, Google Open, Ensembles)</li> <li><strong>Tasks Evaluated</strong>: 30 AlgoTune benchmarks</li> <li><strong>Best Overall Result</strong>: 2.04x speedup (Gemini Flash 2.5, 200 iterations)</li> <li><strong>Worst Result</strong>: 0.396x (Serial diff evaluation - 50% performance degradation)</li> <li><strong>Temperature Range Tested</strong>: 0.2, 0.4, 0.8</li> <li><strong>Evolution Strategies</strong>: Diff-based and full rewrite</li> <li><strong>Evaluation Approach</strong>: Parallel (16 workers) vs Serial comparison</li> </ul> <h2> <a href="https://huggingface.co#table-of-contents"> </a> <span> Table of Contents </span> </h2> <ol> <li>Experimental Overview</li> <li>Methodology</li> <li>Detailed Results</li> <li>Evolved Code Analysis</li> <li>Parameter Optimization Study</li> <li>Model Comparison</li> <li>Technical Implementation</li> <li>Conclusions</li> </ol> <h2> <a href="https://huggingface.co#detailed-experiment-analyses"> </a> <span> Detailed Experiment Analyses </span> </h2> <ul> <li><a href="https://dria.co/blog/baseline-experiments-analysis">Baseline Experiments</a> - Initial model testing and key discoveries</li> <li><a href="https://dria.co/blog/temperature-optimization-study">Temperature Study</a> - Optimal temperature analysis (0.2, 0.4, 0.8)</li> <li><a href="https://dria.co/blog/parameter-tuning-analysis">Parameter Tuning</a> - Token limits, artifacts, migration rates</li> <li><a href="https://dria.co/blog/evolution-strategies-analysis">Evolution Strategies</a> - Diff vs full rewrite, serial vs parallel</li> <li><a href="https://dria.co/blog/model-comparison-analysis">Model Comparison</a> - Comprehensive model performance analysis</li> <li><a href="https://dria.co/blog/iteration-impact-analysis:-100-vs-200-iterations">Iteration Impact</a> - 100 vs 200 iterations deep dive</li> <li><a href="https://dria.co/blog/ensemble-analysis:-why-more-models-better-results">Ensemble Analysis</a> - Why combining models failed</li> </ul> <h2> <a href="https://huggingface.co#experimental-overview"> </a> <span> Experimental Overview </span> </h2> <h3> <a href="https://huggingface.co#experiment-phases"> </a> <span> Experiment Phases </span> </h3> <ul> <li><strong>Phase 1</strong>: Initial experiments and parameter tuning</li> <li><strong>Phase 2</strong>: Extended iteration experiments and new models</li> </ul> <h3> <a href="https://huggingface.co#models-tested"> </a> <span> Models Tested </span> </h3> <ol> <li><p><strong>Gemini Family</strong></p><ul> <li>Flash 2.5: Google's efficient coding model</li> <li>Flash 2.5 Lite: Smaller variant for baseline testing</li> </ul> </li> <li><p><strong>Qwen Family</strong></p><ul> <li>Qwen3-Coder-480B-A35B-Instruct: 480B MoE with 35B active, coding-specialized</li> <li>Qwen3-235B-A22B: 235B general-purpose model</li> <li>Qwen3-32B: Smaller variant</li> </ul> </li> <li><p><strong>Other Models</strong></p><ul> <li>Gemma 3 27B: Google's open model</li> <li>Llama 3.3 70B: Meta's latest model</li> <li>Ensemble configurations</li> </ul> </li> </ol> <h3> <a href="https://huggingface.co#experiment-categories"> </a> <span> Experiment Categories </span> </h3> <ol> <li><strong>Baseline Testing</strong> (7 experiments): Establish model baselines</li> <li><strong>Temperature Study</strong> (3 experiments): Find optimal temperature</li> <li><strong>Parameter Tuning</strong> (8 experiments): Optimize various parameters</li> <li><strong>Model Comparison</strong> (6 experiments): Cross-model analysis</li> <li><strong>Extended Iterations</strong> (4 experiments): Impact of more evolution</li> </ol> <h2> <a href="https://huggingface.co#methodology"> </a> <span> Methodology </span> </h2> <h3> <a href="https://huggingface.co#algotune-integration"> </a> <span> AlgoTune Integration </span> </h3> <ul> <li>Converted 30 AlgoTune tasks to OpenEvolve format using <code>algotune_to_openevolve.py</code></li> <li>Tasks span various algorithm categories: graphs, linear algebra, optimization</li> <li>Used AlgoTune's standard evaluation: harmonic mean of speedups</li> </ul> <h3> <a href="https://huggingface.co#evolution-approach"> </a> <span> Evolution Approach </span> </h3> <ul> <li><strong>Island-based MAP-Elites</strong>: 4 islands with periodic migration</li> <li><strong>Cascade evaluation</strong>: Quick validation &rarr; performance testing &rarr; full evaluation</li> <li><strong>Two evolution strategies</strong>: <ul> <li>Diff-based: Incremental code changes</li> <li>Full rewrite: Complete function regeneration</li> </ul> </li> </ul> <h3> <a href="https://huggingface.co#performance-metric"> </a> <span> Performance Metric </span> </h3><p>AlgoTune uses a logarithmic performance scale that gets converted to speedup factors:</p><pre><code><span># AlgoTune score calculation</span> speedup = (<span>10</span> ** (performance * <span>3</span>)) - <span>1</span> <span># Convert from log scale</span> algotune_score = harmonic_mean(all_speedups) <span># Overall benchmark score</span> </code></pre><p>The harmonic mean emphasizes consistent performance across all tasks - a model must perform well on all 30 benchmarks to achieve a high score.</p><h2> <a href="https://huggingface.co#detailed-results"> </a> <span> Detailed Results </span> </h2> <h3> <a href="https://huggingface.co#performance-rankings---all-experiments"> </a> <span> Performance Rankings - All Experiments </span> </h3><p><strong>Chart Explanation</strong>: Horizontal bars show AlgoTune scores (harmonic mean of speedups) for all experiments. Green bars indicate good performance (&gt;1.5x), blue for moderate (1.2-1.5x), orange for modest (1.0-1.2x), and red for poor (&lt;1.0x). The dashed line at 1.0x represents baseline performance.</p><h3> <a href="https://huggingface.co#phase-1-baseline-experiments"> </a> <span> Phase 1: Baseline Experiments </span> </h3> <h4> <a href="https://huggingface.co#1-gemini-flash-25-lite-baseline"> </a> <span> 1. Gemini Flash 2.5 Lite Baseline </span> </h4> <ul> <li><strong>Config</strong>: Full rewrite, temp 0.8, 4k tokens</li> <li><strong>Result</strong>: 1.10x speedup</li> <li><strong>Key Learning</strong>: Established baseline for optimization</li> </ul> <h4> <a href="https://huggingface.co#2-evolution-strategy-discovery"> </a> <span> 2. Evolution Strategy Discovery </span> </h4><p>Comparing diff-based vs full rewrite on Flash Lite:</p><ul> <li><strong>Full rewrite</strong>: 1.10x &#10003;</li> <li><strong>Diff-based</strong>: 0.79x &#10007;</li> </ul><p><strong>Evidence of Struggle with Diffs</strong>:</p><pre><code><span># Attempted diff that failed</span> - <span>for</span> i <span>in</span> <span>range</span>(n): - <span>if</span> <span>not</span> visited[i]: - dfs(i) + <span>for</span> i <span>in</span> <span>range</span>(n): + <span>if</span> <span>not</span> visited[i]: + <span># Incomplete optimization</span> + dfs(i) <span># No actual improvement</span> </code></pre> <h3> <a href="https://huggingface.co#phase-2-temperature-optimization-study"> </a> <span> Phase 2: Temperature Optimization Study </span> </h3><p>Systematic testing on Gemini Flash 2.5 Lite with 16k tokens:</p><table> <tbody><tr> <th>Temperature</th> <th>AlgoTune Score</th> <th>Avg Performance</th> <th>Duration</th> <th>Key Finding</th> </tr> </tbody><tbody> <tr> <td>0.2</td> <td>1.17x</td> <td>0.162</td> <td>4074s</td> <td>Conservative but stable</td> </tr> <tr> <td><strong>0.4</strong></td> <td><strong>1.29x</strong></td> <td><strong>0.175</strong></td> <td><strong>3784s</strong></td> <td><strong>Best performance achieved</strong></td> </tr> <tr> <td>0.8</td> <td>1.02x</td> <td>0.159</td> <td>3309s</td> <td>Too random, performance drops</td> </tr> </tbody> </table><p><strong>Statistical Significance</strong>: 10.3% improvement from 0.2&rarr;0.4, 20.9% degradation from 0.4&rarr;0.8</p><h3> <a href="https://huggingface.co#phase-3-parameter-fine-tuning"> </a> <span> Phase 3: Parameter Fine-Tuning </span> </h3><p>All experiments at optimal temp 0.4:</p><h4> <a href="https://huggingface.co#token-limit-impact"> </a> <span> Token Limit Impact </span> </h4> <ul> <li>16k tokens: 1.29x (baseline)</li> <li>32k tokens: 1.28x (no improvement)</li> <li><strong>Conclusion</strong>: 16k sufficient, larger context doesn't help</li> </ul> <h4> <a href="https://huggingface.co#artifacts-impact"> </a> <span> Artifacts Impact </span> </h4> <ul> <li>With artifacts: 1.29x</li> <li>Without artifacts: 1.07x</li> <li><strong>Impact</strong>: 17% performance loss without debugging info</li> </ul><p><em>Note: "Artifacts" in OpenEvolve are debugging outputs that programs can return during evaluation, helping the LLM understand execution behavior and make better optimization decisions.</em></p><h4> <a href="https://huggingface.co#top-programs-for-inspiration"> </a> <span> Top Programs for Inspiration </span> </h4> <ul> <li>Top 3: 1.29x (baseline)</li> <li>Top 5: 1.28x (minimal difference)</li> <li><strong>Conclusion</strong>: 3 programs sufficient</li> </ul> <h4> <a href="https://huggingface.co#migration-rate"> </a> <span> Migration Rate </span> </h4> <ul> <li>0.1 rate: 1.29x (baseline)</li> <li>0.2 rate: 1.25x (slightly worse)</li> <li><strong>Conclusion</strong>: Less frequent migration preserves diversity</li> </ul> <h3> <a href="https://huggingface.co#phase-4-applying-learnings-to-strong-models"> </a> <span> Phase 4: Applying Learnings to Strong Models </span> </h3> <h4> <a href="https://huggingface.co#gemini-flash-25-100-iterations"> </a> <span> Gemini Flash 2.5 (100 iterations) </span> </h4> <ul> <li><strong>Config</strong>: Diff-based, temp 0.4, 16k tokens</li> <li><strong>Result</strong>: 1.64x speedup</li> <li><strong>Top achievement</strong>: count_connected_components at 48.1x</li> </ul> <h4> <a href="https://huggingface.co#qwen3-coder-480b"> </a> <span> Qwen3-Coder-480B </span> </h4> <ul> <li><strong>Config</strong>: Diff-based, temp 0.6, 4k tokens</li> <li><strong>Result</strong>: 1.41x speedup</li> <li><strong>Note</strong>: Only tested at temp 0.6, may not be optimal</li> </ul> <h4> <a href="https://huggingface.co#gemma-3-27b"> </a> <span> Gemma 3 27B </span> </h4> <ul> <li><strong>Config</strong>: Diff-based (inherited from successful configs)</li> <li><strong>Result</strong>: 1.63x speedup</li> <li><strong>Surprise</strong>: Matched Flash 2.5 performance</li> </ul> <h3> <a href="https://huggingface.co#phase-5-extended-iterations"> </a> <span> Phase 5: Extended Iterations </span> </h3> <h4> <a href="https://huggingface.co#gemini-flash-25-200-iterations"> </a> <span> Gemini Flash 2.5 (200 iterations) </span> </h4> <ul> <li><strong>Result</strong>: 2.04x speedup (24% improvement over 100 iter)</li> <li><strong>Breakthrough</strong>: count_connected_components reached 95.78x</li> <li><strong>Found optimal</strong>: In just iteration 2!</li> </ul><p>Progress over iterations:</p><ul> <li>Iteration 10: 1.15x</li> <li>Iteration 50: 1.48x </li> <li>Iteration 100: 1.64x</li> <li>Iteration 200: 2.04x</li> </ul> <h3> <a href="https://huggingface.co#phase-6-serial-vs-parallel-evaluation"> </a> <span> Phase 6: Serial vs Parallel Evaluation </span> </h3> <h4> <a href="https://huggingface.co#critical-finding-parallelism-is-essential"> </a> <span> Critical Finding: Parallelism is Essential </span> </h4><p>We tested serial evaluation (one task at a time) vs parallel evaluation:</p><table> <tbody><tr> <th>Configuration</th> <th>Evolution Type</th> <th>AlgoTune Score</th> <th>Duration</th> <th>vs Parallel</th> </tr> </tbody><tbody> <tr> <td><strong>Parallel</strong></td> <td>Diff-based</td> <td>0.793x</td> <td>0.9 hours</td> <td>Baseline</td> </tr> <tr> <td><strong>Serial</strong></td> <td>Diff-based</td> <td>0.396x</td> <td>13.0 hours</td> <td>50% worse, 14x slower</td> </tr> <tr> <td><strong>Parallel</strong></td> <td>Full rewrite</td> <td>1.10x</td> <td>0.9 hours</td> <td>Baseline</td> </tr> <tr> <td><strong>Serial</strong></td> <td>Full rewrite</td> <td>0.585x</td> <td>13.1 hours</td> <td>47% worse, 14x slower</td> </tr> </tbody> </table><p><strong>Key Insights</strong>:</p><ol> <li><strong>Performance Catastrophe</strong>: Serial evaluation leads to 47-50% worse results</li> <li><strong>Time Explosion</strong>: 14x slower execution (13 hours vs 0.9 hours)</li> <li><strong>Why It Fails</strong>: <ul> <li>No cross-task learning during evolution</li> <li>Timeout issues compound across serial tasks</li> <li>Evolution gets "stuck" on difficult tasks</li> <li>Lost opportunity for parallel discovery</li> </ul> </li> </ol><p><strong>Conclusion</strong>: Parallel evaluation is not just an optimization - it's essential for OpenEvolve to function properly</p><h2> <a href="https://huggingface.co#evolved-code-analysis"> </a> <span> Evolved Code Analysis </span> </h2> <h3> <a href="https://huggingface.co#example-1-count-connected-components---algorithm-discovery"> </a> <span> Example 1: Count Connected Components - Algorithm Discovery </span> </h3><p><strong>Original Implementation</strong> (All models start here):</p><pre><code><span>def</span> <span>solve</span>(<span>problem</span>): n = problem[<span>"num_nodes"</span>] edges = problem[<span>"edges"</span>] <span># Build adjacency list</span> adj = [[] <span>for</span> _ <span>in</span> <span>range</span>(n)] <span>for</span> u, v <span>in</span> edges: adj[u].append(v) adj[v].append(u) <span># DFS to count components</span> visited = [<span>False</span>] * n count = <span>def</span> <span>dfs</span>(<span>node</span>): visited[node] = <span>True</span> <span>for</span> neighbor <span>in</span> adj[node]: <span>if</span> <span>not</span> visited[neighbor]: dfs(neighbor) <span>for</span> i <span>in</span> <span>range</span>(n): <span>if</span> <span>not</span> visited[i]: dfs(i) count += <span>1</span> <span>return</span> {<span>"number_connected_components"</span>: count} </code></pre><p><strong>Performance</strong>: 1.0x (baseline)</p><p><strong>Evolved - Gemini Flash 2.5 (200 iter)</strong>:</p><pre><code><span>def</span> <span>solve</span>(<span>problem</span>): n = problem.get(<span>"num_nodes"</span>, ) edges = problem.get(<span>"edges"</span>, []) <span>if</span> n == : <span>return</span> {<span>"number_connected_components"</span>: } <span># Build adjacency list with deque for O(1) operations</span> adj = [[] <span>for</span> _ <span>in</span> <span>range</span>(n)] <span>for</span> u, v <span>in</span> edges: adj[u].append(v) adj[v].append(u) <span># BFS with early termination and efficient tracking</span> visited = [<span>False</span>] * n count = <span>from</span> collections <span>import</span> deque <span>for</span> start <span>in</span> <span>range</span>(n): <span>if</span> visited[start]: <span>continue</span> <span># BFS for this component</span> queue = deque([start]) visited[start] = <span>True</span> <span>while</span> queue: node = queue.popleft() <span>for</span> neighbor <span>in</span> adj[node]: <span>if</span> <span>not</span> visited[neighbor]: visited[neighbor] = <span>True</span> queue.append(neighbor) count += <span>1</span> <span>return</span> {<span>"number_connected_components"</span>: count} </code></pre><p><strong>Performance</strong>: 95.78x speedup <strong>Key Optimizations</strong>: </p><ul> <li>Switched from DFS to BFS</li> <li>Used deque for O(1) queue operations</li> <li>Added early termination checks</li> <li>Defensive programming with get()</li> </ul><p><strong>Evolved - Qwen3-Coder (100 iter)</strong>:</p><pre><code><span>def</span> <span>solve</span>(<span>problem</span>): n = problem[<span>"num_nodes"</span>] edges = problem[<span>"edges"</span>] <span># Union-Find approach - completely different algorithm!</span> parent = <span>list</span>(<span>range</span>(n)) <span>def</span> <span>find</span>(<span>x</span>): <span>if</span> parent[x] != x: parent[x] = find(parent[x]) <span># Path compression</span> <span>return</span> parent[x] <span>def</span> <span>union</span>(<span>x, y</span>): px, py = find(x), find(y) <span>if</span> px != py: parent[px] = py <span># Process edges</span> <span>for</span> u, v <span>in</span> edges: union(u, v) <span># Count unique roots</span> <span>return</span> {<span>"number_connected_components"</span>: <span>len</span>(<span>set</span>(find(i) <span>for</span> i <span>in</span> <span>range</span>(n)))} </code></pre><p><strong>Performance</strong>: ~25x speedup <strong>Key Insight</strong>: Different model found completely different algorithm!</p><ul> <li>Union-Find with path compression</li> <li>O(&alpha;(n)) per operation (near constant)</li> <li>More memory efficient</li> </ul> <h3> <a href="https://huggingface.co#example-2-psd-cone-projection---incremental-optimization"> </a> <span> Example 2: PSD Cone Projection - Incremental Optimization </span> </h3><p><strong>Original Implementation</strong>:</p><pre><code><span>def</span> <span>solve</span>(<span>problem</span>): <span>import</span> numpy <span>as</span> np A = np.array(problem[<span>"matrix"</span>]) <span># Eigenvalue decomposition</span> eigenvalues, eigenvectors = np.linalg.eigh(A) <span># Set negative eigenvalues to zero</span> eigenvalues[eigenvalues &lt; ] = <span>0</span> <span># Reconstruct matrix</span> A_psd = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T <span>return</span> {<span>"projected_matrix"</span>: A_psd.tolist()} </code></pre><p><strong>Performance</strong>: 1.0x (baseline)</p><p><strong>Evolution with Gemini Flash 2.5 Diffs</strong>:</p><p>Iteration 23:</p><pre><code><span>- eigenvalues[eigenvalues &lt; 0] = 0</span> <span>- A_psd = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T</span> <span>+ # Vectorized operation</span> <span>+ eigenvalues = np.maximum(eigenvalues, 0)</span> <span>+ A_psd = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T</span> </code></pre><p>Performance: 1.8x</p><p>Iteration 67:</p><pre><code><span>- A_psd = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T</span> <span>+ # Eliminate intermediate array</span> <span>+ A_psd = (eigenvectors * eigenvalues) @ eigenvectors.T</span> </code></pre><p>Performance: 15.2x</p><p>Final (Iteration 95):</p><pre><code><span>def</span> <span>solve</span>(<span>problem</span>): <span>import</span> numpy <span>as</span> np A = np.array(problem[<span>"matrix"</span>]) w, v = np.linalg.eigh(A) <span># One-line vectorized operation</span> A_psd = (v * np.maximum(w, )) @ v.T <span>return</span> {<span>"projected_matrix"</span>: A_psd.tolist()} </code></pre><p><strong>Performance</strong>: 32.7x speedup</p><h3> <a href="https://huggingface.co#example-3-dct-optimization---convergent-evolution"> </a> <span> Example 3: DCT Optimization - Convergent Evolution </span> </h3><p><strong>Original</strong>:</p><pre><code><span>def</span> <span>solve</span>(<span>problem</span>): <span>import</span> numpy <span>as</span> np <span>from</span> scipy.fftpack <span>import</span> dct signal = np.array(problem[<span>"signal"</span>]) <span>return</span> {<span>"dct"</span>: dct(signal, <span>type</span>=<span>1</span>).tolist()} </code></pre><p><strong>Multiple Models Converged to Same Solution</strong>:</p><p>Gemini Flash 2.5:</p><pre><code>signal = np.array(problem[<span>"signal"</span>], dtype=np.float64) result = dct(signal, <span>type</span>=<span>1</span>, norm=<span>None</span>, overwrite_x=<span>False</span>) </code></pre><p>Qwen3-Coder:</p><pre><code>signal = np.asarray(problem[<span>"signal"</span>], dtype=np.float64) <span>return</span> {<span>"dct"</span>: dct(signal, <span>type</span>=<span>1</span>).tolist()} </code></pre><p><strong>Key Finding</strong>: Both discovered dtype=np.float64 optimization independently!</p><ul> <li>6.48x speedup for both</li> <li>Shows optimal solution exists and is discoverable</li> </ul> <h3> <a href="https://huggingface.co#example-4-sha256---hardware-limitations"> </a> <span> Example 4: SHA256 - Hardware Limitations </span> </h3><p><strong>Original</strong>:</p><pre><code><span>def</span> <span>solve</span>(<span>problem</span>): <span>import</span> hashlib data = problem[<span>"data"</span>].encode(<span>'utf-8'</span>) hash_object = hashlib.sha256(data) <span>return</span> {<span>"hash"</span>: hash_object.hexdigest()} </code></pre><p><strong>Best Evolution</strong> (multiple models):</p><pre><code><span>def</span> <span>solve</span>(<span>problem</span>): <span>import</span> hashlib <span>return</span> {<span>"hash"</span>: hashlib.sha256(problem[<span>"data"</span>].encode()).hexdigest()} </code></pre><p><strong>Performance</strong>: Only 1.1x speedup <strong>Learning</strong>: Hardware-bound operations have limited optimization potential</p><h2> <a href="https://huggingface.co#model-comparison"> </a> <span> Model Comparison </span> </h2><p>For a comprehensive view of all 10 unique models tested, see Panel C in the Executive Dashboard. The complete 29-experiment results are shown in the Performance Rankings chart above.</p><h3> <a href="https://huggingface.co#task-specific-performance-heatmap"> </a> <span> Task-Specific Performance Heatmap </span> </h3><p>This heatmap shows speedup factors achieved by different models on specific AlgoTune tasks. Darker red indicates higher speedup. Notable patterns:</p><ul> <li>Count connected components shows extreme variation (95x for Flash 2.5 vs 15x for Gemma 3)</li> <li>PSD projection achieved consistent high performance across models (32-42x)</li> <li>Hardware-bound tasks like SHA256 show minimal improvement regardless of model</li> </ul> <h3> <a href="https://huggingface.co#specialization-vs-size-analysis"> </a> <span> Specialization vs Size Analysis </span> </h3><p><strong>Qwen3-Coder (480B MoE, 35B active) vs Qwen3-235B</strong>:</p><ul> <li>Despite being "larger" (480B total parameters), Qwen3-Coder has only 35B active</li> <li>Coding specialization led to 68% better performance (1.41x vs 0.84x)</li> <li>Evidence that training data and objectives matter more than size</li> </ul> <h3> <a href="https://huggingface.co#evolution-strategy-by-model-capability"> </a> <span> Evolution Strategy by Model Capability </span> </h3><div><table> <thead><tr> <th>Model Type</th> <th>Best Strategy</th> <th>Evidence</th> </tr> </thead><tbody><tr> <td>Small/Weak (Flash Lite)</td> <td>Full rewrite</td> <td>0.79x with diff vs 1.10x full</td> </tr> <tr> <td>Strong Coding (Flash 2.5, Qwen3-Coder)</td> <td>Diff-based</td> <td>1.64x vs lower with full</td> </tr> <tr> <td>General Purpose</td> <td>Varies</td> <td>Model-dependent</td> </tr> </tbody> </table> </div><h2> <a href="https://huggingface.co#ensemble-experiments-why-more-models-%E2%89%A0-better-results"> </a> <span> Ensemble Experiments: Why More Models &ne; Better Results </span> </h2><p>Despite combining our two best performers (Gemini Flash 2.5 at 1.64x and Qwen3-Coder at 1.41x), the ensemble achieved only <strong>1.23x speedup</strong> - worse than either model individually.</p><p><strong>Visualization Guide</strong>:</p><ul> <li><strong>Panel A</strong>: Shows individual model performance vs ensemble - ensemble underperforms both models</li> <li><strong>Panel B</strong>: All 30 Tasks Performance Comparison - complete performance profiles for Gemini, Qwen, and Ensemble</li> <li><strong>Panel C</strong>: Evolution progress comparison - ensemble shows irregular oscillation vs smooth single model progress</li> <li><strong>Panel D</strong>: Model Agreement by Task - 3x30 heatmap showing pairwise agreement for each task, revealing conflict zones</li> </ul> <h3> <a href="https://huggingface.co#key-finding-conflicting-optimization-strategies"> </a> <span> Key Finding: Conflicting Optimization Strategies </span> </h3><p>The ensemble failed because models pursued incompatible approaches:</p><ul> <li><strong>Count Connected Components</strong>: Gemini used BFS, Qwen used Union-Find &rarr; oscillation</li> <li><strong>DCT Optimization</strong>: Different dtype strategies &rarr; lost optimizations</li> <li><strong>Result</strong>: 19% underperformance vs expected 1.5-1.7x</li> </ul> <h3> <a href="https://huggingface.co#example-algorithm-conflict"> </a> <span> Example: Algorithm Conflict </span> </h3><pre><code><span># Iteration N: Gemini suggests BFS</span> queue = deque([start]) <span># BFS approach</span> <span># Iteration N+1: Qwen suggests Union-Find </span> parent = <span>list</span>(<span>range</span>(n)) <span># Different algorithm!</span> <span># Result: Hybrid mess that optimizes neither</span> </code></pre> <h3> <a href="https://huggingface.co#observed-ensemble-failure-modes"> </a> <span> Observed Ensemble Failure Modes </span> </h3> <ol> <li><strong>Algorithm conflicts</strong>: BFS vs Union-Find approaches in same task</li> <li><strong>Optimization conflicts</strong>: Memory optimization vs compute optimization</li> <li><strong>Style conflicts</strong>: Functional vs imperative code patterns</li> </ol><p><a href="https://dria.co/blog/ensemble-analysis:-why-more-models-better-results">&rarr; Detailed ensemble analysis</a></p><h2> <a href="https://huggingface.co#technical-implementation-details"> </a> <span> Technical Implementation Details </span> </h2> <h3> <a href="https://huggingface.co#openevolve-configuration"> </a> <span> OpenEvolve Configuration </span> </h3><pre><code><span># Optimal configuration discovered</span> <span>max_iterations:</span> <span>200</span> <span># More is better</span> <span>diff_based_evolution:</span> <span>true</span> <span># For capable models</span> <span>temperature:</span> <span>0.4</span> <span># For Gemini, 0.6 for others</span> <span>max_tokens:</span> <span>16000</span> <span># Sweet spot</span> <span>num_top_programs:</span> <span>3</span> <span>num_islands:</span> <span>4</span> <span>migration_interval:</span> <span>20</span> <span>migration_rate:</span> <span>0.1</span> <span>include_artifacts:</span> <span>true</span> <span># Critical for performance</span> </code></pre> <h3> <a href="https://huggingface.co#island-evolution-dynamics"> </a> <span> Island Evolution Dynamics </span> </h3> <ul> <li>4 islands maintain diversity</li> <li>Migration every 20 iterations prevents premature convergence</li> <li>Each island can discover different solutions</li> <li>Best program tracked globally</li> </ul> <h3> <a href="https://huggingface.co#cascade-evaluation-benefits"> </a> <span> Cascade Evaluation Benefits </span> </h3> <ol> <li><strong>Stage 1</strong>: Quick syntax/import validation (&lt; 1s)</li> <li><strong>Stage 2</strong>: Small test cases (&lt; 10s)</li> <li><strong>Stage 3</strong>: Full benchmark suite (&lt; 60s)</li> </ol><p>Saved ~70% of evaluation time by failing fast.</p><h2> <a href="https://huggingface.co#key-observations-from-experiments"> </a> <span> Key Observations from Experiments </span> </h2> <h3> <a href="https://huggingface.co#1-performance-improvements-by-type"> </a> <span> 1. Performance Improvements by Type </span> </h3> <ul> <li>Algorithmic changes (DFS &rarr; BFS): Up to 95x speedup observed</li> <li>Vectorization optimizations: 32x speedup achieved in matrix operations</li> <li>Minor code changes (variable renaming, loop adjustments): Typically &lt; 2x speedup</li> </ul> <h3> <a href="https://huggingface.co#2-model-solution-diversity"> </a> <span> 2. Model Solution Diversity </span> </h3> <ul> <li>Count connected components: Gemini found BFS approach, Qwen found Union-Find</li> <li>Matrix operations: Different models used different vectorization strategies</li> <li>Multiple valid optimization paths were discovered for most tasks</li> </ul> <h3> <a href="https://huggingface.co#3-evolution-strategy-performance"> </a> <span> 3. Evolution Strategy Performance </span> </h3> <ul> <li>Diff-based evolution with strong coding models: Up to 1.64x overall speedup</li> <li>Full rewrite with weaker models: 1.10x speedup (vs 0.79x with diffs)</li> <li>Model capability determined optimal strategy</li> </ul> <h3> <a href="https://huggingface.co#4-iteration-impact"> </a> <span> 4. Iteration Impact </span> </h3> <ul> <li>100 iterations: 1.64x average speedup achieved</li> <li>200 iterations: 2.04x average speedup (24% improvement)</li> <li>Performance continued improving through 200 iterations</li> </ul> <h3> <a href="https://huggingface.co#5-parameter-effects"> </a> <span> 5. Parameter Effects </span> </h3> <ul> <li>Temperature 0.4: Best individual run (1.291x) but high variance</li> <li>Including artifacts: ~17% performance improvement observed</li> <li>Token limit: 16k vs 32k showed no significant difference</li> <li>Migration rate: 0.1 outperformed 0.2 in experiments</li> </ul> <h3> <a href="https://huggingface.co#6-parallelism-impact"> </a> <span> 6. Parallelism Impact </span> </h3> <ul> <li>Serial evaluation: 0.396x-0.585x speedup (performance degradation)</li> <li>Parallel evaluation: 0.793x-1.10x speedup for same configurations</li> <li>Time difference: 13 hours (serial) vs 0.9 hours (parallel)</li> </ul> <h2> <a href="https://huggingface.co#conclusions"> </a> <span> Conclusions </span> </h2> <h3> <a href="https://huggingface.co#key-experimental-findings"> </a> <span> Key Experimental Findings </span> </h3> <ol> <li><strong>Iteration Count</strong>: 200 iterations achieved 2.04x speedup vs 1.64x for 100 iterations (24% improvement)</li> <li><strong>Model Specialization</strong>: Qwen3-Coder (coding-specialized) outperformed larger general models</li> <li><strong>Temperature Settings</strong>: Results varied - best individual run at 0.4, but high variance observed</li> <li><strong>Evolution Strategy</strong>: Diff-based worked better for strong coding models, full rewrite for weaker models</li> <li><strong>Parallelism</strong>: Serial evaluation degraded performance by 47-50% and increased time 14x</li> <li><strong>Ensemble Results</strong>: Combined models achieved 1.23x vs 1.64x and 1.41x individually</li> </ol> <h3> <a href="https://huggingface.co#observed-patterns"> </a> <span> Observed Patterns </span> </h3> <ul> <li>Models discovered different algorithmic approaches (BFS vs Union-Find for graph problems)</li> <li>Hardware-bound tasks (SHA256) showed minimal improvement across all configurations</li> <li>Artifacts inclusion improved performance by approximately 17%</li> <li>Migration rate of 0.1 performed better than 0.2 in tested configurations</li> </ul> </div><div class="gallery"><p><img src="https://cdn-avatars.huggingface.co/v1/production/uploads/633d5f988bbe861bdcd60a6e/KqyN3m4-TkYOVe1RPokK2.png"></p><p><img src="https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png"></p><p><img src="https://imagedelivery.net/kbUqkpOIvA4TJOyi-hNQfQ/f2a95b89-6fef-4378-c58a-88cde49ea200/public"></p><p><img src="https://dria.co/file/root/blog-attachment/6890ca17831d00ba4cab81a0/6890d57299b392485ab34362/performance_rankings.png"></p><p><img src="https://imagedelivery.net/kbUqkpOIvA4TJOyi-hNQfQ/eaae601f-4f84-4fb3-a4d8-032e9a46c300/public"></p><p><img src="https://imagedelivery.net/kbUqkpOIvA4TJOyi-hNQfQ/b6e37efd-b6ac-4ecc-36fe-3c235f438f00/public"></p></div></section>]]></description><pubDate>Wed, 06 Aug 2025 18:50:34 +0530</pubDate></item><item><link>https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF</link><title>Qwen3-Coder GGUFs with even more fixes esp. for tool calling! (huggingface.co)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mj1dwo/qwen3coder_ggufs_with_even_more_fixes_esp_for/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mj1dwo/qwen3coder_ggufs_with_even_more_fixes_esp_for/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 6 min | <a href='https://www.reddit.com/r/unsloth/comments/1mj1dwo/qwen3coder_ggufs_with_even_more_fixes_esp_for/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Recently we&#39;ve updated Qwen3-Coder and although we previously addressed tool calling issues, the fix only worked in certain setups, such as llama.cpp. With other configurations, tool functionality remained inconsistent.</p><p>This new update has undergone extensive testing, by us and others, and should significantly improve tool calling reliability and mostly resolve any strange behaviors.</p><p>You may still experience some issues though, however this is now out of our hands as we have already done the most fixes we could. Now we will need to wait for the amazing llama.cpp team to fix the rest.</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://cdn-thumbnails.huggingface.co/social-thumbnails/models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF.png' /></section><section class='parsed-content'><div><ul> <li>Fine-tune Qwen3 (14B) for free using our Google <a href="https://docs.unsloth.ai/get-started/unsloth-notebooks">Colab notebook</a>!</li> <li>Read our Blog about Qwen3 support: <a href="https://unsloth.ai/blog/qwen3">unsloth.ai/blog/qwen3</a></li> <li>View the rest of our notebooks in our <a href="https://docs.unsloth.ai/get-started/unsloth-notebooks">docs here</a>. </li> </ul> <h2> <a href="https://huggingface.co#highlights"> </a> <span> Highlights </span> </h2><p><strong>Qwen3-Coder</strong> is available in multiple sizes. Today, we're excited to introduce <strong>Qwen3-Coder-30B-A3B-Instruct</strong>. This streamlined model maintains impressive performance and efficiency, featuring the following key enhancements: </p><ul> <li><strong>Significant Performance</strong> among open models on <strong>Agentic Coding</strong>, <strong>Agentic Browser-Use</strong>, and other foundational coding tasks.</li> <li><strong>Long-context Capabilities</strong> with native support for <strong>256K</strong> tokens, extendable up to <strong>1M</strong> tokens using Yarn, optimized for repository-scale understanding.</li> <li><strong>Agentic Coding</strong> supporting for most platform such as <strong>Qwen Code</strong>, <strong>CLINE</strong>, featuring a specially designed function call format.</li> </ul> <h2> <a href="https://huggingface.co#model-overview"> </a> <span> Model Overview </span> </h2><p><strong>Qwen3-Coder-30B-A3B-Instruct</strong> has the following features:</p><ul> <li>Type: Causal Language Models</li> <li>Training Stage: Pretraining &amp; Post-training</li> <li>Number of Parameters: 30.5B in total and 3.3B activated</li> <li>Number of Layers: 48</li> <li>Number of Attention Heads (GQA): 32 for Q and 4 for KV</li> <li>Number of Experts: 128</li> <li>Number of Activated Experts: 8</li> <li>Context Length: <strong>262,144 natively</strong>.</li> </ul><p><strong>NOTE: This model supports only non-thinking mode and does not generate <code><think></think></code> blocks in its output. Meanwhile, specifying <code>enable_thinking=False</code> is no longer required.</strong></p><p>For more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our <a href="https://qwenlm.github.io/blog/qwen3-coder/">blog</a>, <a href="https://github.com/QwenLM/Qwen3-Coder">GitHub</a>, and <a href="https://qwen.readthedocs.io/en/latest/">Documentation</a>.</p><h2> <a href="https://huggingface.co#quickstart"> </a> <span> Quickstart </span> </h2><p>We advise you to use the latest version of <code>transformers</code>.</p><p>With <code>transformers&lt;4.51.0</code>, you will encounter the following error:</p><pre><code>KeyError: 'qwen3_moe' </code></pre><p>The following contains a code snippet illustrating how to use the model generate content based on given inputs. </p><pre><code><span>from</span> transformers <span>import</span> AutoModelForCausalLM, AutoTokenizer model_name = <span>"Qwen/Qwen3-Coder-30B-A3B-Instruct"</span> <span># load the tokenizer and the model</span> tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=<span>"auto"</span>, device_map=<span>"auto"</span> ) <span># prepare the model input</span> prompt = <span>"Write a quick sort algorithm."</span> messages = [ {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: prompt} ] text = tokenizer.apply_chat_template( messages, tokenize=<span>False</span>, add_generation_prompt=<span>True</span>, ) model_inputs = tokenizer([text], return_tensors=<span>"pt"</span>).to(model.device) <span># conduct text completion</span> generated_ids = model.generate( **model_inputs, max_new_tokens=<span>65536</span> ) output_ids = generated_ids[][<span>len</span>(model_inputs.input_ids[]):].tolist() content = tokenizer.decode(output_ids, skip_special_tokens=<span>True</span>) <span>print</span>(<span>"content:"</span>, content) </code></pre><p><strong>Note: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as <code>32,768</code>.</strong></p><p>For local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.</p><h2> <a href="https://huggingface.co#agentic-coding"> </a> <span> Agentic Coding </span> </h2><p>Qwen3-Coder excels in tool calling capabilities. </p><p>You can simply define or use any tools as following example.</p><pre><code><span># Your tool implementation</span> <span>def</span> <span>square_the_number</span>(<span>num: <span>float</span></span>) -&gt; <span>dict</span>: <span>return</span> num ** <span>2</span> <span># Define Tools</span> tools=[ { <span>"type"</span>:<span>"function"</span>, <span>"function"</span>:{ <span>"name"</span>: <span>"square_the_number"</span>, <span>"description"</span>: <span>"output the square of the number."</span>, <span>"parameters"</span>: { <span>"type"</span>: <span>"object"</span>, <span>"required"</span>: [<span>"input_num"</span>], <span>"properties"</span>: { <span>'input_num'</span>: { <span>'type'</span>: <span>'number'</span>, <span>'description'</span>: <span>'input_num is a number that will be squared'</span> } }, } } } ] <span>import</span> OpenAI <span># Define LLM</span> client = OpenAI( <span># Use a custom endpoint compatible with OpenAI API</span> base_url=<span>'http://localhost:8000/v1'</span>, <span># api_base</span> api_key=<span>"EMPTY"</span> ) messages = [{<span>'role'</span>: <span>'user'</span>, <span>'content'</span>: <span>'square the number 1024'</span>}] completion = client.chat.completions.create( messages=messages, model=<span>"Qwen3-Coder-30B-A3B-Instruct"</span>, max_tokens=<span>65536</span>, tools=tools, ) <span>print</span>(completion.choice[]) </code></pre> <h2> <a href="https://huggingface.co#best-practices"> </a> <span> Best Practices </span> </h2><p>To achieve optimal performance, we recommend the following settings:</p><ol> <li><p><strong>Sampling Parameters</strong>:</p><ul> <li>We suggest using <code>temperature=0.7</code>, <code>top_p=0.8</code>, <code>top_k=20</code>, <code>repetition_penalty=1.05</code>.</li> </ul> </li> <li><p><strong>Adequate Output Length</strong>: We recommend using an output length of 65,536 tokens for most queries, which is adequate for instruct models.</p></li> </ol> <h3> <a href="https://huggingface.co#citation"> </a> <span> Citation </span> </h3><p>If you find our work helpful, feel free to give us a cite.</p><pre><code>@misc{qwen3technicalreport, title={Qwen3 Technical Report}, author={Qwen Team}, year={2025}, eprint={2505.09388}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2505.09388}, } </code></pre> </div><div class="gallery"><p><img src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5"></p><p><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3-coder-30a3-main.jpg"></p></div></section>]]></description><pubDate>Wed, 06 Aug 2025 16:22:01 +0530</pubDate></item><item><link>https://huggingface.co/unsloth/gpt-oss-20b-GGUF</link><title>gpt-oss Unsloth GGUFs are here! (huggingface.co)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mik1qy/gptoss_unsloth_ggufs_are_here/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mik1qy/gptoss_unsloth_ggufs_are_here/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 8 min | <a href='https://www.reddit.com/r/unsloth/comments/1mik1qy/gptoss_unsloth_ggufs_are_here/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>You can now run OpenAI&#39;s gpt-oss-120b &amp; 20b open models locally with our GGUFs! ü¶•</p><p>Run the 120b model on 66GB RAM &amp; 20b model on 14GB RAM. Both in original precision.</p><p>20b GGUF: <a href="https://huggingface.co/unsloth/gpt-oss-20b-GGUF">https://huggingface.co/unsloth/gpt-oss-20b-GGUF</a></p><p><strong>Uploads includes our chat template fixes.</strong> Finetuning support coming soon!</p><p>Guide: <a href="https://docs.unsloth.ai/basics/gpt-oss">https://docs.unsloth.ai/basics/gpt-oss</a></p><p>120b GGUF: <a href="https://huggingface.co/unsloth/gpt-oss-120b-GGUF">https://huggingface.co/unsloth/gpt-oss-120b-GGUF</a></p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://cdn-thumbnails.huggingface.co/social-thumbnails/models/unsloth/gpt-oss-20b-GGUF.png' /></section><section class='parsed-content'><div><blockquote><p>GGUF uploads with our fixes. More details and <a href="https://docs.unsloth.ai/basics/gpt-oss">Read our guide here.</a></p></blockquote> <ul> <li>Fine-tune gpt-oss-20b for free using our <a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb">Google Colab notebook</a></li> <li>Read our Blog about gpt-oss support: <a href="https://unsloth.ai/blog/gpt-oss">unsloth.ai/blog/gpt-oss</a></li> <li>View the rest of our notebooks in our <a href="https://docs.unsloth.ai/get-started/unsloth-notebooks">docs here</a>.</li> <li>Thank you to the <a href="https://github.com/ggml-org/llama.cpp">llama.cpp</a> team for their work on supporting this model. We wouldn't be able to release quants without them!</li> </ul><p>The F32 quant is MXFP4 upcasted to BF16 for every single layer and is unquantized.</p><h2> <a href="https://huggingface.co#gpt-oss-20b-details"> </a> <span> gpt-oss-20b Details </span> </h2><p><a href="https://gpt-oss.com"><strong>Try gpt-oss</strong></a> &middot; <a href="https://cookbook.openai.com/topic/gpt-oss"><strong>Guides</strong></a> &middot; <a href="https://openai.com/index/gpt-oss-model-card"><strong>System card</strong></a> &middot; <a href="https://openai.com/index/introducing-gpt-oss/"><strong>OpenAI blog</strong></a> </p><p>Welcome to the gpt-oss series, <a href="https://openai.com/open-models">OpenAI&rsquo;s open-weight models</a> designed for powerful reasoning, agentic tasks, and versatile developer use cases.</p><p>We&rsquo;re releasing two flavors of the open models:</p><ul> <li><code>gpt-oss-120b</code> &mdash; for production, general purpose, high reasoning use cases that fits into a single H100 GPU (117B parameters with 5.1B active parameters)</li> <li><code>gpt-oss-20b</code> &mdash; for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)</li> </ul><p>Both models were trained on our <a href="https://github.com/openai/harmony">harmony response format</a> and should only be used with the harmony format as it will not work correctly otherwise.</p><blockquote><p>This model card is dedicated to the smaller <code>gpt-oss-20b</code> model. Check out <a href="https://huggingface.co/openai/gpt-oss-120b"><code>gpt-oss-120b</code></a> for the larger model.</p></blockquote> <h2> <a href="https://huggingface.co#highlights"> </a> <span> Highlights </span> </h2> <ul> <li><strong>Permissive Apache 2.0 license:</strong> Build freely without copyleft restrictions or patent risk&mdash;ideal for experimentation, customization, and commercial deployment. </li> <li><strong>Configurable reasoning effort:</strong> Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs. </li> <li><strong>Full chain-of-thought:</strong> Gain complete access to the model&rsquo;s reasoning process, facilitating easier debugging and increased trust in outputs. It&rsquo;s not intended to be shown to end users. </li> <li><strong>Fine-tunable:</strong> Fully customize models to your specific use case through parameter fine-tuning.</li> <li><strong>Agentic capabilities:</strong> Use the models&rsquo; native capabilities for function calling, <a href="https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#browser">web browsing</a>, <a href="https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#python">Python code execution</a>, and Structured Outputs.</li> <li><strong>Native MXFP4 quantization:</strong> The models are trained with native MXFP4 precision for the MoE layer, making <code>gpt-oss-120b</code> run on a single H100 GPU and the <code>gpt-oss-20b</code> model run within 16GB of memory.</li> </ul> <hr> <h2> <a href="https://huggingface.co#inference-examples"> </a> <span> Inference examples </span> </h2> <h2> <a href="https://huggingface.co#transformers"> </a> <span> Transformers </span> </h2><p>You can use <code>gpt-oss-120b</code> and <code>gpt-oss-20b</code> with Transformers. If you use the Transformers chat template, it will automatically apply the <a href="https://github.com/openai/harmony">harmony response format</a>. If you use <code>model.generate</code> directly, you need to apply the harmony format manually using the chat template or use our <a href="https://github.com/openai/harmony">openai-harmony</a> package.</p><p>To get started, install the necessary dependencies to setup your environment:</p><pre><code>pip install -U transformers kernels torch </code></pre><p>Once, setup you can proceed to run the model by running the snippet below:</p><pre><code><span>from</span> transformers <span>import</span> pipeline <span>import</span> torch model_id = <span>"openai/gpt-oss-20b"</span> pipe = pipeline( <span>"text-generation"</span>, model=model_id, torch_dtype=<span>"auto"</span>, device_map=<span>"auto"</span>, ) messages = [ {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"Explain quantum mechanics clearly and concisely."</span>}, ] outputs = pipe( messages, max_new_tokens=<span>256</span>, ) <span>print</span>(outputs[][<span>"generated_text"</span>][-<span>1</span>]) </code></pre><p>Alternatively, you can run the model via <a href="https://huggingface.co/docs/transformers/main/serving"><code>Transformers Serve</code></a> to spin up a OpenAI-compatible webserver:</p><pre><code>transformers serve transformers chat localhost:8000 --model-name-or-path openai/gpt-oss-20b </code></pre><p><a href="https://cookbook.openai.com/articles/gpt-oss/run-transformers">Learn more about how to use gpt-oss with Transformers.</a></p><h2> <a href="https://huggingface.co#vllm"> </a> <span> vLLM </span> </h2><p>vLLM recommends using <a href="https://docs.astral.sh/uv/">uv</a> for Python dependency management. You can use vLLM to spin up an OpenAI-compatible webserver. The following command will automatically download the model and start the server.</p><pre><code>uv pip install --pre vllm==0.10.1+gptoss \ --extra-index-url https://wheels.vllm.ai/gpt-oss/ \ --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \ --index-strategy unsafe-best-match vllm serve openai/gpt-oss-20b </code></pre><p><a href="https://cookbook.openai.com/articles/gpt-oss/run-vllm">Learn more about how to use gpt-oss with vLLM.</a></p><h2> <a href="https://huggingface.co#pytorch--triton"> </a> <span> PyTorch / Triton </span> </h2><p>To learn about how to use this model with PyTorch and Triton, check out our <a href="https://github.com/openai/gpt-oss?tab=readme-ov-file#reference-pytorch-implementation">reference implementations in the gpt-oss repository</a>.</p><h2> <a href="https://huggingface.co#ollama"> </a> <span> Ollama </span> </h2><p>If you are trying to run gpt-oss on consumer hardware, you can use Ollama by running the following commands after <a href="https://ollama.com/download">installing Ollama</a>.</p><pre><code><span># gpt-oss-20b</span> ollama pull gpt-oss:20b ollama run gpt-oss:20b </code></pre><p><a href="https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama">Learn more about how to use gpt-oss with Ollama.</a></p><h4> <a href="https://huggingface.co#lm-studio"> </a> <span> LM Studio </span> </h4><p>If you are using <a href="https://lmstudio.ai/">LM Studio</a> you can use the following commands to download.</p><pre><code><span># gpt-oss-20b</span> lms get openai/gpt-oss-20b </code></pre><p>Check out our <a href="https://github.com/openai/gpt-oss/blob/main/awesome-gpt-oss.md">awesome list</a> for a broader collection of gpt-oss resources and inference partners.</p><hr> <h2> <a href="https://huggingface.co#download-the-model"> </a> <span> Download the model </span> </h2><p>You can download the model weights from the <a href="https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4">Hugging Face Hub</a> directly from Hugging Face CLI:</p><pre><code><span># </span><span>gpt-oss-20b</span> huggingface-cli download openai/gpt-oss-20b --include "original/*" --local-dir gpt-oss-20b/ pip install gpt-oss python -m gpt_oss.chat model/ </code></pre> <h2> <a href="https://huggingface.co#reasoning-levels"> </a> <span> Reasoning levels </span> </h2><p>You can adjust the reasoning level that suits your task across three levels:</p><ul> <li><strong>Low:</strong> Fast responses for general dialogue. </li> <li><strong>Medium:</strong> Balanced speed and detail. </li> <li><strong>High:</strong> Deep and detailed analysis.</li> </ul><p>The reasoning level can be set in the system prompts, e.g., "Reasoning: high".</p><h2> <a href="https://huggingface.co#tool-use"> </a> <span> Tool use </span> </h2><p>The gpt-oss models are excellent for:</p><ul> <li>Web browsing (using built-in browsing tools)</li> <li>Function calling with defined schemas</li> <li>Agentic operations like browser tasks</li> </ul> <h2> <a href="https://huggingface.co#fine-tuning"> </a> <span> Fine-tuning </span> </h2><p>Both gpt-oss models can be fine-tuned for a variety of specialized use cases.</p><p>This smaller model <code>gpt-oss-20b</code> can be fine-tuned on consumer hardware, whereas the larger <a href="https://huggingface.co/openai/gpt-oss-120b"><code>gpt-oss-120b</code></a> can be fine-tuned on a single H100 node.</p></div><div class="gallery"><p><img src="https://raw.githubusercontent.com/openai/gpt-oss/main/docs/gpt-oss-20b.svg"></p></div></section>]]></description><pubDate>Wed, 06 Aug 2025 01:45:05 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mibxei/training_qwen3coder/</link><title>Training Qwen3-Coder</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mibxei/training_qwen3coder/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mibxei/training_qwen3coder/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mibxei/training_qwen3coder/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys,</p><p>Thanks for the lib, wanted to know if there is a way to train unsloth/Qwen3-Coder-30B-A3B-Instruct with vllm in a GRPO fashion, i see that its supported by vllm but as we need to use FastModel instead of FastModelLanguage It does not seem possible to have a vllm engine runnign for the training, is my understanding wrong?</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 05 Aug 2025 20:44:07 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mi5pzf/glm45_air_ud5_model_has_unused_tensor/</link><title>GLM4.5 AIR UD5. Model has unused tensor</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mi5pzf/glm45_air_ud5_model_has_unused_tensor/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mi5pzf/glm45_air_ud5_model_has_unused_tensor/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mi5pzf/glm45_air_ud5_model_has_unused_tensor/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>When i run the glm4.5 air q5 k xl with llama.cpp b6090 it says that </p><p>model has unused tensor 46 .... ignoring </p><p>model has unused tensor 46 .... ignoring </p><p>etc</p><p>Is this due to the model or llama.cpp is not ready yet?</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 05 Aug 2025 16:10:15 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mi3yis/qwen3coder30b_issues_with_tool_calls/</link><title>Qwen3-coder-30b issues with tool calls</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mi3yis/qwen3coder30b_issues_with_tool_calls/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mi3yis/qwen3coder30b_issues_with_tool_calls/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mi3yis/qwen3coder30b_issues_with_tool_calls/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I have been using the qwen3-30b series of models in LM studio server with Crush CLI and loving them but the coder variant always fails to call tools, somtimes it puts text in the response to the user, sometimes I get api errors about invalid messages in the payload.</p><p>I took the prompt template from qwen3-30b-2507-instruct and replaced the coders prompt template.</p><p>The coder model now calls tools correctly and I am no longer getting API errors but I dont actually know what it was I was changing exactly. Can swapping out the promp template this way cause other issues with the model or affect is coding abilities?</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 05 Aug 2025 14:21:57 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mi0dkw/modernbert_cant_be_trained_in_colab_anymore/</link><title>modernBERT can't be trained in colab anymore</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mi0dkw/modernbert_cant_be_trained_in_colab_anymore/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mi0dkw/modernbert_cant_be_trained_in_colab_anymore/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1mi0dkw/modernbert_cant_be_trained_in_colab_anymore/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>wondering if anyone knows how to fix this? </p><p><a href="https://github.com/unslothai/unsloth/issues/2902">https://github.com/unslothai/unsloth/issues/2902</a></p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 05 Aug 2025 10:37:30 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mhjjbj/cant_use_qwent3coder_30b/</link><title>can't use qwent3-coder 30b</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mhjjbj/cant_use_qwent3coder_30b/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mhjjbj/cant_use_qwent3coder_30b/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mhjjbj/cant_use_qwent3coder_30b/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Asking it for anything will work for a minute then it&#39;ll start repeating.</p><p>Verified it&#39;s not a context issue.</p><p>Fixed:</p><p>Updating llama.cpp fixed the issue.</p></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 04 Aug 2025 22:47:43 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mgsbx4/native_support_for_internvl3/</link><title>Native support for InternVL3?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mgsbx4/native_support_for_internvl3/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mgsbx4/native_support_for_internvl3/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mgsbx4/native_support_for_internvl3/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>It&#39;s a good vision-first model that should be really great for vision tasks especially when finetuned. Qwen2.5VL is actually better for less size out of the box and so being able to finetune the InternVL3 base model would realize a lot of the potential of this model.</p></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 04 Aug 2025 01:07:31 +0530</pubDate></item><item><link>https://www.reddit.com/r/LocalLLaMA/comments/1mgdur5/icmdpo_used_qwen3s_coherent_understanding_to/</link><title>üß† ICM+DPO: Used Qwen3's coherent understanding to improve Gemma3 at math - cross-model capability transfer with zero supervision</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mgeozo/icmdpo_used_qwen3s_coherent_understanding_to/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mgeozo/icmdpo_used_qwen3s_coherent_understanding_to/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 5 min | <a href='https://www.reddit.com/r/unsloth/comments/1mgeozo/icmdpo_used_qwen3s_coherent_understanding_to/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey <a href="https://www.reddit.com/r/LocalLLaMA">r/LocalLLaMA</a>! </p><p>Just released something that extends the recent <a href="https://arxiv.org/abs/2506.10139">ICM paper</a> in a big way - using one model&#39;s coherent understanding to improve a completely different model.</p><h1>Background: What is ICM?</h1><p>The original <a href="https://arxiv.org/abs/2506.10139">&quot;Unsupervised Elicitation of Language Models&quot;</a> paper showed something remarkable: <strong>models can generate their own training labels by finding internally coherent patterns</strong>.</p><p>Their key insight: pretrained models already understand concepts like mathematical correctness, but struggle to express this knowledge consistently. ICM finds label assignments that are &quot;mutually predictable&quot; - where each label can be predicted from all the others.</p><p><strong>Original ICM results</strong>: Matched performance of golden supervision without any external labels. Pretty amazing, but only improved the same model using its own labels.</p><h1>Our extension: Cross-model capability transfer</h1><p>We took ICM further - <strong>what if we use one model&#39;s coherent understanding to improve a completely different model?</strong></p><p><strong>Our process:</strong></p><ol><li>Used ICM on Qwen3 to extract its coherent math reasoning patterns</li><li>Generated DPO training data from Qwen3&#39;s coherent vs incoherent solutions</li><li>Trained Gemma3 on this data - <strong>Gemma3 learned from Qwen3&#39;s understanding</strong></li><li>Zero external supervision, pure model-to-model knowledge transfer</li></ol><h1>Results on local models</h1><p><strong>Qwen3-0.6B</strong>: 63.2 ‚Üí 66.0 MATH-500 (+4%) [original ICM self-improvement]<br/><strong>Gemma3-1B</strong>: 41.0 ‚Üí 45.6 MATH-500 (+11%) [<strong>novel: learned from Qwen3!</strong>]</p><p><strong>The breakthrough</strong>: Successfully transferred mathematical reasoning coherence from Qwen3 to improve Gemma3&#39;s abilities across different architectures.</p><h1>Why this matters beyond the original paper</h1><ul><li><strong>Cross-model knowledge transfer</strong> - use any strong model to improve your local models</li><li><strong>Democratizes capabilities</strong> - extract from closed/expensive models to improve open ones</li><li><strong>No training data needed</strong> - pure capability extraction and transfer</li><li><strong>Scales the ICM concept</strong> - from self-improvement to ecosystem-wide improvement</li></ul><h1>What&#39;s available</h1><ul><li><strong>Code</strong>: <a href="https://github.com/codelion/icm">https://github.com/codelion/icm</a></li><li><strong>Both models</strong>: Self-improved Qwen3 + Gemma3 (learned from Qwen3)</li><li><strong>Transfer pipeline</strong>: Extract from any model to improve another</li><li><strong>Full writeup</strong>: <a href="https://huggingface.co/blog/codelion/internal-coherence-maximization">https://huggingface.co/blog/codelion/internal-coherence-maximization</a></li></ul><h1>Quick start</h1><pre><code>git clone https://github.com/codelion/icm.git &amp;&amp; cd icm &amp;&amp; pip install -e .# Extract coherent patterns from a strong model (teacher)icm run --model Qwen/Qwen2.5-Math-7B-Instruct --dataset gsm8k --max-examples 500# Use those patterns to improve your local model (student)icm export --format dpo --output-path teacher_knowledge.jsonl# Train your model on teacher_knowledge.jsonl</code></pre><p>Anyone interested in trying capability transfer with their local models?</p></div><!-- SC_ON --></section><section class='separator separator-after-selftext'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><p>Hey <a href="https://www.reddit.com/r/LocalLLaMA/">r/LocalLLaMA</a>! </p><p>Just released something that extends the recent <a href="https://arxiv.org/abs/2506.10139">ICM paper</a> in a big way - using one model's coherent understanding to improve a completely different model. </p><h2>Background: What is ICM?</h2><p>The original <a href="https://arxiv.org/abs/2506.10139">"Unsupervised Elicitation of Language Models"</a> paper showed something remarkable: <strong>models can generate their own training labels by finding internally coherent patterns</strong>. </p><p>Their key insight: pretrained models already understand concepts like mathematical correctness, but struggle to express this knowledge consistently. ICM finds label assignments that are "mutually predictable" - where each label can be predicted from all the others. </p><p><strong>Original ICM results</strong>: Matched performance of golden supervision without any external labels. Pretty amazing, but only improved the same model using its own labels. </p><h2>Our extension: Cross-model capability transfer</h2><p>We took ICM further - <strong>what if we use one model's coherent understanding to improve a completely different model?</strong> </p><p><strong>Our process:</strong> </p><ol> <li><p>Used ICM on Qwen3 to extract its coherent math reasoning patterns </p></li><li><p>Generated DPO training data from Qwen3's coherent vs incoherent solutions </p></li><li><p>Trained Gemma3 on this data - <strong>Gemma3 learned from Qwen3's understanding</strong> </p></li><li><p>Zero external supervision, pure model-to-model knowledge transfer </p></li> </ol><h2>Results on local models</h2><p><strong>Qwen3-0.6B</strong>: 63.2 &rarr; 66.0 MATH-500 (+4%) [original ICM self-improvement]<br><strong>Gemma3-1B</strong>: 41.0 &rarr; 45.6 MATH-500 (+11%) [<strong>novel: learned from Qwen3!</strong>] </p><p><strong>The breakthrough</strong>: Successfully transferred mathematical reasoning coherence from Qwen3 to improve Gemma3's abilities across different architectures. </p><h2>Why this matters beyond the original paper</h2><ul> <li><p><strong>Cross-model knowledge transfer</strong> - use any strong model to improve your local models </p></li><li><p><strong>Democratizes capabilities</strong> - extract from closed/expensive models to improve open ones </p></li><li><p><strong>No training data needed</strong> - pure capability extraction and transfer </p></li><li><p><strong>Scales the ICM concept</strong> - from self-improvement to ecosystem-wide improvement </p></li> </ul><h2>What's available</h2><ul> <li><p><strong>Code</strong>: <a href="https://github.com/codelion/icm">https://github.com/codelion/icm</a> </p></li><li><p><strong>Both models</strong>: Self-improved Qwen3 + Gemma3 (learned from Qwen3) </p></li><li><p><strong>Transfer pipeline</strong>: Extract from any model to improve another </p></li><li><p><strong>Full writeup</strong>: <a href="https://huggingface.co/blog/codelion/internal-coherence-maximization">https://huggingface.co/blog/codelion/internal-coherence-maximization</a> </p></li> </ul><h2>Quick start</h2><pre> git clone https://github.com/codelion/icm.git &amp;&amp; cd icm &amp;&amp; pip install -e . # Extract coherent patterns from a strong model (teacher) icm run --model Qwen/Qwen2.5-Math-7B-Instruct --dataset gsm8k --max-examples 500 # Use those patterns to improve your local model (student) icm export --format dpo --output-path teacher_knowledge.jsonl # Train your model on teacher_knowledge.jsonl</pre><p>Anyone interested in trying capability transfer with their local models? </p></div></section>]]></description><pubDate>Sun, 03 Aug 2025 14:36:39 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mgcgxg/we_enabled_multigpu_training_in_unsloth_ai_a/</link><title>We enabled Multi-GPU training in Unsloth AI ‚Äî a feature that‚Äôs usually paid ‚Äî using just 2 Copilot prompts!</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mgcgxg/we_enabled_multigpu_training_in_unsloth_ai_a/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mgcgxg/we_enabled_multigpu_training_in_unsloth_ai_a/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1mgcgxg/we_enabled_multigpu_training_in_unsloth_ai_a/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p><a href="https://github.com/oevortex/unsloth">https://github.com/oevortex/unsloth</a></p></div><!-- SC_ON --></section>]]></description><pubDate>Sun, 03 Aug 2025 12:11:05 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mfmt7x/request_4bit_quant_of_unslothmedgemma27bit_to/</link><title>Request: 4bit quant of unsloth/medgemma-27b-it to make it finetunable for the GPU poor</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mfmt7x/request_4bit_quant_of_unslothmedgemma27bit_to/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mfmt7x/request_4bit_quant_of_unslothmedgemma27bit_to/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1mfmt7x/request_4bit_quant_of_unslothmedgemma27bit_to/'>Post permalink</a></p></section>]]></description><pubDate>Sat, 02 Aug 2025 15:28:57 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mezuw9/newbie_needs_help/</link><title>Newbie Needs Help</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mezuw9/newbie_needs_help/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mezuw9/newbie_needs_help/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mezuw9/newbie_needs_help/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey everyone. I hate to ask such a basic question, but I&#39;m kinda stuck and need some help.</p><p>I&#39;ve only recently started diving into the world of self-hosted LLM&#39;s and AI services. Having a ton of fun so far.</p><p>I&#39;m running Ollama and Open WebUI in docker locally. I&#39;ve used the models from Ollama which have been great so far. I recently started trying out new models from huggingface.co. The Unsloth team has released several models recently I&#39;m wanting to try out. Specifically the Qwen3-30B-A3B-2507 Thinking and Instruct models.</p><p>However I&#39;m running into some really odd behavior with these models. I downloaded the GGUF files for <code>Qwen3-30B-A3B-Instruct-2507-UD-Q4_K_XL.gguf</code> and <code>Qwen3-30B-A3B-Thinking-2507-UD-Q4_K_XL.gguf</code>. In Open WebUI I set the temperature, min_p, top_p, topk, max_tokens, and presence_penalty settings for the models according to the Unsloth Qwen3 documentation. I installed the GGUF model files by using the model management in Open WebUI and uploading the GGUF&#39;s.</p><p>Odd behavior I see:</p><ol><li>When I query the Thinking model, I don&#39;t get any &quot;Thinking&quot; indicator like I do with other Thinking models. It responds just like a reasoning model. Forcing the &quot;think&quot; parameter causes an error saying the model doesn&#39;t support thinking.</li><li>When I query either model sometimes it gives a very short accurate answer, other times it just goes on and on and on and on. Seemingly coming up with questions on topics I never asked about.</li></ol><p>I don&#39;t see anyone else complaining about these issues, so I assume it&#39;s because I&#39;ve done something wrong.</p><p>Any help would be appreciate.</p></div><!-- SC_ON --></section>]]></description><pubDate>Fri, 01 Aug 2025 21:11:34 +0530</pubDate></item><item><link>https://i.redd.it/2lhskdy9rdgf1.jpeg</link><title>OpenAI open-source model possible Analysis! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mesac9/openai_opensource_model_possible_analysis/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mesac9/openai_opensource_model_possible_analysis/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mesac9/openai_opensource_model_possible_analysis/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>See our tweet for a detailed breakdown: <a href="https://x.com/danielhanchen/status/1951212068583120958">https://x.com/danielhanchen/status/1951212068583120958</a></p><p>Will it get released today or very soon? Let&#39;s wait and see ü§© what do you guys think?</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/2lhskdy9rdgf1.jpeg' /></section>]]></description><pubDate>Fri, 01 Aug 2025 15:22:32 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mes61k/run_quantized_model_in_vllm/</link><title>Run Quantized Model in vLLM</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mes61k/run_quantized_model_in_vllm/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mes61k/run_quantized_model_in_vllm/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mes61k/run_quantized_model_in_vllm/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>So far I only hosted Models using vLLM from the creator, mostly qwen Models where I can just use &quot;vllm serve &lt;model_name&gt;&quot; and vllt does the rest (or I use vllm&#39;s docker image). This works if on the huggingface page there is only one quantized version, but in Unsloths Models there are usually plenty of different quantized versions, like Q4_1, Q4_0 etc. </p><p>Can I host them the same way with vllm (are they in the transformers package)? If not, how would I serve them with vllm? If yes, how do I specify the quantization type? </p><p>When I click on the quantization type and there on &quot;use this model&quot; -&gt; vllm, it will just tell me to use &quot;vllm serve &lt;model_name&gt;&quot;, it&#39;s the same command without any reference to the quantization type. </p><p>I could not find information for this anywhere online, can you help me with this? </p><p>Thank you! :)</p></div><!-- SC_ON --></section>]]></description><pubDate>Fri, 01 Aug 2025 15:15:06 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1me8jqz/qwen3_says_no_bullshit/</link><title>Qwen3 says No Bullshit</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1me8jqz/qwen3_says_no_bullshit/</guid><comments>https://www.reddit.com/r/unsloth/comments/1me8jqz/qwen3_says_no_bullshit/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1me8jqz/qwen3_says_no_bullshit/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Thinking model vs Instruct model such a difference...  </p><p>I just downloaded qwen3 thinking and instruct quantized models by <a href="https://www.reddit.com/u/unsloth">u/unsloth</a> . To test, I gave them the same query which is to plan my day. Instruct model gave crap reply. after explaining it again and again, it gave me 4 hours sleep schedule. and it says reduce your shift schedule so that you can sleep better.   </p><p>On the other hand, with just one query to &quot;thinking&quot; model, it gave me well-structured reply. So, other than technical explanations, use thinking model which gives you very apt reply.</p><p>Both are same model. Thinking model says this:</p><p><a href="https://preview.redd.it/z1j3swz009gf1.png?width=628&amp;format=png&amp;auto=webp&amp;s=539e130a47dadc1ac6b26f97cc962ec3c7eb9d77">https://preview.redd.it/z1j3swz009gf1.png?width=628&amp;format=png&amp;auto=webp&amp;s=539e130a47dadc1ac6b26f97cc962ec3c7eb9d77</a></p></div><!-- SC_ON --></section>]]></description><pubDate>Thu, 31 Jul 2025 23:26:00 +0530</pubDate></item><item><link>https://i.redd.it/q2vulitx78gf1.png</link><title>Run 'Qwen3-Coder-Flash' locally with Unsloth Dynamic GGUFs! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1me4bv7/run_qwen3coderflash_locally_with_unsloth_dynamic/</guid><comments>https://www.reddit.com/r/unsloth/comments/1me4bv7/run_qwen3coderflash_locally_with_unsloth_dynamic/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1me4bv7/run_qwen3coderflash_locally_with_unsloth_dynamic/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Qwen3-Coder-Flash is here! ‚ú® The 30B model excels in coding &amp; agentic tasks. Run locally with up to 1M context length. Full precision runs with just 33GB RAM.</p><p>GGUFs: <a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF">https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF</a></p><p>Hey friends, as usual, we always update our models and communicate with the model teams to ensure open-source models are of the highest quality they can be. We fixed tool-calling for Qwen3-Coder so now it should work properly. If you‚Äôre downloading our 30B-A3B quants, no need to worry as these already include our fixes. For the 480B-A35B model you need to redownload.</p><p>1M context GGUF: <a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF">https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF</a></p><p>Guide for Qwen3-Coder: <a href="https://docs.unsloth.ai/basics/qwen3-coder">https://docs.unsloth.ai/basics/qwen3-coder</a></p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/q2vulitx78gf1.png' /></section>]]></description><pubDate>Thu, 31 Jul 2025 20:47:00 +0530</pubDate></item><item><link>https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF</link><title>Fixes for: Qwen3-30B-A3B-Thinking-2507 GGUF. (huggingface.co)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mdxkty/fixes_for_qwen330ba3bthinking2507_gguf/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mdxkty/fixes_for_qwen330ba3bthinking2507_gguf/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 12 min | <a href='https://www.reddit.com/r/unsloth/comments/1mdxkty/fixes_for_qwen330ba3bthinking2507_gguf/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey everyone, we saw some of you having issues with using the latest Qwen3-30B Thinking model in tools other than llama.cpp. For example, some users experienced outputs which consistently doen&#39;t wrap reasoning tokens in¬†<code>&lt;think&gt;</code>¬†and¬†<code>&lt;/think&gt;.</code> </p><p>We re-uploaded the GGUFs and we verified that removing the &lt;think&gt; is fine, since the model&#39;s probability of producing the think token seems to be nearly 100% anyways.This should make LMStudio, Ollama etc. inference work rather than just llama.cpp.</p><p>Yes, you will need to redownload the weights.</p><p>Qwen3-30B-A3B-Thinking-2507: <a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF">https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF</a></p><p>Let us know if you&#39;re still having any issues. :)</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://cdn-thumbnails.huggingface.co/social-thumbnails/models/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF.png' /></section><section class='parsed-content'><div><ul> <li>Fine-tune Qwen3 (14B) for free using our Google <a href="https://docs.unsloth.ai/get-started/unsloth-notebooks">Colab notebook here</a>!</li> <li>Read our Blog about Qwen3 support: <a href="https://unsloth.ai/blog/qwen3">unsloth.ai/blog/qwen3</a></li> <li>View the rest of our notebooks in our <a href="https://docs.unsloth.ai/get-started/unsloth-notebooks">docs here</a>.</li> <li>Run &amp; export your fine-tuned model to Ollama, llama.cpp or HF.</li> </ul> <h2> <a href="https://huggingface.co#highlights"> </a> <span> Highlights </span> </h2><p>Over the past three months, we have continued to scale the <strong>thinking capability</strong> of Qwen3-30B-A3B, improving both the <strong>quality and depth</strong> of reasoning. We are pleased to introduce <strong>Qwen3-30B-A3B-Thinking-2507</strong>, featuring the following key enhancements:</p><ul> <li><strong>Significantly improved performance</strong> on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise.</li> <li><strong>Markedly better general capabilities</strong>, such as instruction following, tool usage, text generation, and alignment with human preferences.</li> <li><strong>Enhanced 256K long-context understanding</strong> capabilities.</li> </ul><p><strong>NOTE</strong>: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.</p><h2> <a href="https://huggingface.co#model-overview"> </a> <span> Model Overview </span> </h2><p><strong>Qwen3-30B-A3B-Thinking-2507</strong> has the following features:</p><ul> <li>Type: Causal Language Models</li> <li>Training Stage: Pretraining &amp; Post-training</li> <li>Number of Parameters: 30.5B in total and 3.3B activated</li> <li>Number of Paramaters (Non-Embedding): 29.9B</li> <li>Number of Layers: 48</li> <li>Number of Attention Heads (GQA): 32 for Q and 4 for KV</li> <li>Number of Experts: 128</li> <li>Number of Activated Experts: 8</li> <li>Context Length: <strong>262,144 natively</strong>.</li> </ul><p><strong>NOTE: This model supports only thinking mode. Meanwhile, specifying <code>enable_thinking=True</code> is no longer required.</strong></p><p>Additionally, to enforce model thinking, the default chat template automatically includes <code><think></think></code>. Therefore, it is normal for the model's output to contain only <code></code> without an explicit opening <code><think></think></code> tag.</p><p>For more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our <a href="https://qwenlm.github.io/blog/qwen3/">blog</a>, <a href="https://github.com/QwenLM/Qwen3">GitHub</a>, and <a href="https://qwen.readthedocs.io/en/latest/">Documentation</a>.</p><h2> <a href="https://huggingface.co#performance"> </a> <span> Performance </span> </h2><div><table> <thead><tr> <th></th> <th>Gemini2.5-Flash-Thinking</th> <th>Qwen3-235B-A22B Thinking</th> <th>Qwen3-30B-A3B Thinking</th> <th>Qwen3-30B-A3B-Thinking-2507</th> </tr> </thead><tbody><tr> <td><strong>Knowledge</strong></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>MMLU-Pro</td> <td>81.9</td> <td><strong>82.8</strong></td> <td>78.5</td> <td>80.9</td> </tr> <tr> <td>MMLU-Redux</td> <td>92.1</td> <td><strong>92.7</strong></td> <td>89.5</td> <td>91.4</td> </tr> <tr> <td>GPQA</td> <td><strong>82.8</strong></td> <td>71.1</td> <td>65.8</td> <td>73.4</td> </tr> <tr> <td>SuperGPQA</td> <td>57.8</td> <td><strong>60.7</strong></td> <td>51.8</td> <td>56.8</td> </tr> <tr> <td><strong>Reasoning</strong></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>AIME25</td> <td>72.0</td> <td>81.5</td> <td>70.9</td> <td><strong>85.0</strong></td> </tr> <tr> <td>HMMT25</td> <td>64.2</td> <td>62.5</td> <td>49.8</td> <td><strong>71.4</strong></td> </tr> <tr> <td>LiveBench 20241125</td> <td>74.3</td> <td><strong>77.1</strong></td> <td>74.3</td> <td>76.8</td> </tr> <tr> <td><strong>Coding</strong></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>LiveCodeBench v6 (25.02-25.05)</td> <td>61.2</td> <td>55.7</td> <td>57.4</td> <td><strong>66.0</strong></td> </tr> <tr> <td>CFEval</td> <td>1995</td> <td><strong>2056</strong></td> <td>1940</td> <td>2044</td> </tr> <tr> <td>OJBench</td> <td>23.5</td> <td><strong>25.6</strong></td> <td>20.7</td> <td>25.1</td> </tr> <tr> <td><strong>Alignment</strong></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>IFEval</td> <td><strong>89.8</strong></td> <td>83.4</td> <td>86.5</td> <td>88.9</td> </tr> <tr> <td>Arena-Hard v2$</td> <td>56.7</td> <td><strong>61.5</strong></td> <td>36.3</td> <td>56.0</td> </tr> <tr> <td>Creative Writing v3</td> <td><strong>85.0</strong></td> <td>84.6</td> <td>79.1</td> <td>84.4</td> </tr> <tr> <td>WritingBench</td> <td>83.9</td> <td>80.3</td> <td>77.0</td> <td><strong>85.0</strong></td> </tr> <tr> <td><strong>Agent</strong></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>BFCL-v3</td> <td>68.6</td> <td>70.8</td> <td>69.1</td> <td><strong>72.4</strong></td> </tr> <tr> <td>TAU1-Retail</td> <td>65.2</td> <td>54.8</td> <td>61.7</td> <td><strong>67.8</strong></td> </tr> <tr> <td>TAU1-Airline</td> <td><strong>54.0</strong></td> <td>26.0</td> <td>32.0</td> <td>48.0</td> </tr> <tr> <td>TAU2-Retail</td> <td><strong>66.7</strong></td> <td>40.4</td> <td>34.2</td> <td>58.8</td> </tr> <tr> <td>TAU2-Airline</td> <td>52.0</td> <td>30.0</td> <td>36.0</td> <td><strong>58.0</strong></td> </tr> <tr> <td>TAU2-Telecom</td> <td><strong>31.6</strong></td> <td>21.9</td> <td>22.8</td> <td>26.3</td> </tr> <tr> <td><strong>Multilingualism</strong></td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>MultiIF</td> <td>74.4</td> <td>71.9</td> <td>72.2</td> <td><strong>76.4</strong></td> </tr> <tr> <td>MMLU-ProX</td> <td><strong>80.2</strong></td> <td>80.0</td> <td>73.1</td> <td>76.4</td> </tr> <tr> <td>INCLUDE</td> <td><strong>83.9</strong></td> <td>78.7</td> <td>71.9</td> <td>74.4</td> </tr> <tr> <td>PolyMATH</td> <td>49.8</td> <td><strong>54.7</strong></td> <td>46.1</td> <td>52.6</td> </tr> </tbody> </table> </div><p>$ For reproducibility, we report the win rates evaluated by GPT-4.1.</p><p>&amp; For highly challenging tasks (including PolyMATH and all reasoning and coding tasks), we use an output length of 81,920 tokens. For all other tasks, we set the output length to 32,768.</p><h2> <a href="https://huggingface.co#quickstart"> </a> <span> Quickstart </span> </h2><p>The code of Qwen3-MoE has been in the latest Hugging Face <code>transformers</code> and we advise you to use the latest version of <code>transformers</code>.</p><p>With <code>transformers&lt;4.51.0</code>, you will encounter the following error:</p><pre><code>KeyError: 'qwen3_moe' </code></pre><p>The following contains a code snippet illustrating how to use the model generate content based on given inputs. </p><pre><code><span>from</span> transformers <span>import</span> AutoModelForCausalLM, AutoTokenizer model_name = <span>"Qwen/Qwen3-30B-A3B-Thinking-2507"</span> <span># load the tokenizer and the model</span> tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=<span>"auto"</span>, device_map=<span>"auto"</span> ) <span># prepare the model input</span> prompt = <span>"Give me a short introduction to large language model."</span> messages = [ {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: prompt} ] text = tokenizer.apply_chat_template( messages, tokenize=<span>False</span>, add_generation_prompt=<span>True</span>, ) model_inputs = tokenizer([text], return_tensors=<span>"pt"</span>).to(model.device) <span># conduct text completion</span> generated_ids = model.generate( **model_inputs, max_new_tokens=<span>32768</span> ) output_ids = generated_ids[][<span>len</span>(model_inputs.input_ids[]):].tolist() <span># parsing thinking content</span> <span>try</span>: <span># rindex finding 151668 ()</span> index = <span>len</span>(output_ids) - output_ids[::-<span>1</span>].index(<span>151668</span>) <span>except</span> ValueError: index = thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=<span>True</span>).strip(<span>"\n"</span>) content = tokenizer.decode(output_ids[index:], skip_special_tokens=<span>True</span>).strip(<span>"\n"</span>) <span>print</span>(<span>"thinking content:"</span>, thinking_content) <span># no opening <think> tag</think></span> <span>print</span>(<span>"content:"</span>, content) </code></pre><p>For deployment, you can use <code>sglang&gt;=0.4.6.post1</code> or <code>vllm&gt;=0.8.5</code> or to create an OpenAI-compatible API endpoint:</p><ul> <li>SGLang:<pre><code>python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-Thinking-2507 --context-length 262144 --reasoning-parser deepseek-r1 </code></pre> </li> <li>vLLM:<pre><code>vllm serve Qwen/Qwen3-30B-A3B-Thinking-2507 --max-model-len 262144 --enable-reasoning --reasoning-parser deepseek_r1 </code></pre> </li> </ul><p><strong>Note: If you encounter out-of-memory (OOM) issues, you may consider reducing the context length to a smaller value. However, since the model may require longer token sequences for reasoning, we strongly recommend using a context length greater than 131,072 when possible.</strong></p><p>For local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.</p><h2> <a href="https://huggingface.co#agentic-use"> </a> <span> Agentic Use </span> </h2><p>Qwen3 excels in tool calling capabilities. We recommend using <a href="https://github.com/QwenLM/Qwen-Agent">Qwen-Agent</a> to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.</p><p>To define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.</p><pre><code><span>from</span> qwen_agent.agents <span>import</span> Assistant <span># Define LLM</span> <span># Using Alibaba Cloud Model Studio</span> llm_cfg = { <span>'model'</span>: <span>'qwen3-30b-a3b-thinking-2507'</span>, <span>'model_type'</span>: <span>'qwen_dashscope'</span>, } <span># Using OpenAI-compatible API endpoint. It is recommended to disable the reasoning and the tool call parsing</span> <span># functionality of the deployment frameworks and let Qwen-Agent automate the related operations. For example, </span> <span># `VLLM_USE_MODELSCOPE=true vllm serve Qwen/Qwen3-30B-A3B-Thinking-2507 --served-model-name Qwen3-30B-A3B-Thinking-2507 --tensor-parallel-size 8 --max-model-len 262144`.</span> <span>#</span> <span># llm_cfg = {</span> <span># 'model': 'Qwen3-30B-A3B-Thinking-2507',</span> <span># </span> <span># # Use a custom endpoint compatible with OpenAI API:</span> <span># 'model_server': 'http://localhost:8000/v1', # api_base without reasoning and tool call parsing</span> <span># 'api_key': 'EMPTY',</span> <span># 'generate_cfg': {</span> <span># 'thought_in_content': True,</span> <span># },</span> <span># }</span> <span># Define Tools</span> tools = [ {<span>'mcpServers'</span>: { <span># You can specify the MCP configuration file</span> <span>'time'</span>: { <span>'command'</span>: <span>'uvx'</span>, <span>'args'</span>: [<span>'mcp-server-time'</span>, <span>'--local-timezone=Asia/Shanghai'</span>] }, <span>"fetch"</span>: { <span>"command"</span>: <span>"uvx"</span>, <span>"args"</span>: [<span>"mcp-server-fetch"</span>] } } }, <span>'code_interpreter'</span>, <span># Built-in tools</span> ] <span># Define Agent</span> bot = Assistant(llm=llm_cfg, function_list=tools) <span># Streaming generation</span> messages = [{<span>'role'</span>: <span>'user'</span>, <span>'content'</span>: <span>'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'</span>}] <span>for</span> responses <span>in</span> bot.run(messages=messages): <span>pass</span> <span>print</span>(responses) </code></pre> <h2> <a href="https://huggingface.co#best-practices"> </a> <span> Best Practices </span> </h2><p>To achieve optimal performance, we recommend the following settings:</p><ol> <li><p><strong>Sampling Parameters</strong>:</p><ul> <li>We suggest using <code>Temperature=0.6</code>, <code>TopP=0.95</code>, <code>TopK=20</code>, and <code>MinP=0</code>.</li> <li>For supported frameworks, you can adjust the <code>presence_penalty</code> parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.</li> </ul> </li> <li><p><strong>Adequate Output Length</strong>: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 81,920 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.</p></li> <li><p><strong>Standardize Output Format</strong>: We recommend using prompts to standardize model outputs when benchmarking.</p><ul> <li><strong>Math Problems</strong>: Include "Please reason step by step, and put your final answer within \boxed{}." in the prompt.</li> <li><strong>Multiple-Choice Questions</strong>: Add the following JSON structure to the prompt to standardize responses: "Please show your choice in the <code>answer</code> field with only the choice letter, e.g., <code>"answer": "C"</code>."</li> </ul> </li> <li><p><strong>No Thinking Content in History</strong>: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.</p></li> </ol> <h3> <a href="https://huggingface.co#citation"> </a> <span> Citation </span> </h3><p>If you find our work helpful, feel free to give us a cite.</p><pre><code>@misc{qwen3technicalreport, title={Qwen3 Technical Report}, author={Qwen Team}, year={2025}, eprint={2505.09388}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2505.09388}, } </code></pre> </div><div class="gallery"><p><img src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5"></p><p><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-2507/Qwen3-30B-A3B-Thinking-2507.jpeg"></p></div></section>]]></description><pubDate>Thu, 31 Jul 2025 15:37:01 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mdrupf/seeking_expert_guidance_in_tts_training/</link><title>Seeking Expert Guidance in TTS training</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mdrupf/seeking_expert_guidance_in_tts_training/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mdrupf/seeking_expert_guidance_in_tts_training/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mdrupf/seeking_expert_guidance_in_tts_training/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hello everyone. I‚Äôm new here and seeking concrete guidance on achieving low end‚Äëto‚Äë-end latency in TTS voice cloning through Orpheus or similar models. If you have direct experience with frameworks, model optimizations, or hardware strategies and are willing to assist, please reach out. </p></div><!-- SC_ON --></section>]]></description><pubDate>Thu, 31 Jul 2025 09:41:33 +0530</pubDate></item><item><link>https://i.redd.it/c4lmnkivb1gf1.png</link><title>Unsloth Dynamic 'Qwen3-30B-A3B-THINKING-2507' GGUFs out now! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mdakn5/unsloth_dynamic_qwen330ba3bthinking2507_ggufs_out/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mdakn5/unsloth_dynamic_qwen330ba3bthinking2507_ggufs_out/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mdakn5/unsloth_dynamic_qwen330ba3bthinking2507_ggufs_out/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Qwen releases Qwen3-30B-A3B-Thinking-2507! ‚ú® The 30B model runs locally in full precision with just 33GB RAM.</p><p>GGUFs: <a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF">https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF</a></p><p>Unsloth also supports Qwen3-2507 fine-tuning and RL!</p><p>Guide to run/fine-tune: <a href="https://docs.unsloth.ai/basics/qwen3-2507">https://docs.unsloth.ai/basics/qwen3-2507</a></p><p>Happy running guys!</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/c4lmnkivb1gf1.png' /></section>]]></description><pubDate>Wed, 30 Jul 2025 21:34:50 +0530</pubDate></item><item><link>https://i.redd.it/q92411zwv0gf1.jpeg</link><title>Google Gemma 3n Challenge ($150,000 in prizes) ends in 7 days! + New Gemma 3n notebooks (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1md88x2/google_gemma_3n_challenge_150000_in_prizes_ends/</guid><comments>https://www.reddit.com/r/unsloth/comments/1md88x2/google_gemma_3n_challenge_150000_in_prizes_ends/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1md88x2/google_gemma_3n_challenge_150000_in_prizes_ends/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys thought you should know the challenge ends in one week!</p><p>We also just made 2 new fine-tuning Gemma 3n Kaggle notebooks for Vision &amp; Audio to spark your creativity. Your fine-tuned model is eligible to be used to compete for any of the prizes on any track!</p><p>New notebooks + Challenge Details: <a href="https://www.kaggle.com/code/danielhanchen/gemma-3n-4b-multimodal-finetuning-inference">https://www.kaggle.com/code/danielhanchen/gemma-3n-4b-multimodal-finetuning-inference</a></p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/q92411zwv0gf1.jpeg' /></section>]]></description><pubDate>Wed, 30 Jul 2025 20:05:23 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1md09qz/discrepancy_between_merged_lora_model_vs_dynamic/</link><title>Discrepancy Between Merged LoRA Model vs. Dynamic Adapter Loading: Is This Expected?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1md09qz/discrepancy_between_merged_lora_model_vs_dynamic/</guid><comments>https://www.reddit.com/r/unsloth/comments/1md09qz/discrepancy_between_merged_lora_model_vs_dynamic/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1md09qz/discrepancy_between_merged_lora_model_vs_dynamic/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi everyone, I&#39;ve¬†been working on fine-tuning a model using Unsloth and LoRA, and¬†I&#39;ve encountered a  difference in behavior that I&#39;d like to understand better.</p><p>My core observation is that when I run inference using the base model with¬†the LoRA adapter loaded dynamically, the model&#39;s output is different‚Äîand often more consistent‚Äîthan when I use a pre-merged version of the same model and adapter.</p><p>Here‚Äôs my fine-tuning and inference workflow:</p><p>Setup¬†and Training:</p><ul><li><p>I load a base model (e.g.,¬†unsloth/Qwen3-4B) with¬†FastLanguageModel.</p></li><li><p>I add several new special tokens to the tokenizer ([action],¬†[/action], etc.).</p></li><li><p>I resize¬†the model&#39;s token embeddings to accommodate the new vocabulary (model.resize_token_embeddings).</p></li><li><p>I then fine-tune the model using LoRA and¬†save the adapter.</p></li></ul><p>Inference Methods:</p><ul><li><p>Method A (Dynamic Loading):¬†I load the original base model and then¬†attach the trained LoRA adapter using¬†PeftModel.from_pretrained(model, adapter_path).</p></li><li><p>Method B (Merged Model):¬†I¬†create a merged model using¬†model.save_pretrained_merged(&quot;./merged_model&quot;,¬†tokenizer, ...)¬†and then load this new standalone model for inference.</p></li></ul><p>The Discrepancy: When I give the same prompt to both models, their responses differ.¬†Method A (Dynamic Loading)¬†consistently produces outputs that strictly¬†follow the format taught during fine-tuning (e.g.,¬†[action]{...}[/action]). However,¬†Method B (Merged Model)¬†sometimes¬†generates slightly malformed or &quot;hallucinated&quot; structures (e.g., using unexpected keys like¬†actionDate¬†or breaking the JSON format).</p><p>This leads¬†me to my main questions:</p><ol><li>Is this difference in behavior expected?¬†Why would a merged model behave differently from a dynamically loaded one? Is there some subtle¬†information loss or change in the model&#39;s computational path that occurs during the merging¬†process?</li><li>Is my merging process correct?¬†I&#39;ve been¬†creating the merged model with the line below, passing in the modified tokenizer. Is¬†this the correct way to merge a model that has both a LoRA adapter and a¬†modified tokenizer, or is there a more robust method to ensure the merged model behaves identically to¬†the dynamically loaded version?</li></ol><p>&#8203;</p><pre><code>    model.save_pretrained_merged(        &quot;./merged_models/my-final-model&quot;,        modified_tokenizer,        save_method=&quot;merged_16bit&quot;,    )</code></pre><p>I&#39;m trying¬†to understand if this is a known trade-off or if I&#39;m missing a¬†step in my workflow to create a perfectly faithful merged model. Any insights or advice¬†on best practices would be greatly appreciated.Thank you!</p></div><!-- SC_ON --></section>]]></description><pubDate>Wed, 30 Jul 2025 13:03:50 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1md01ve/how_to_quantize_myself_docs_say_only_for/</link><title>How to quantize myself? Docs say only for fine-tuning?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1md01ve/how_to_quantize_myself_docs_say_only_for/</guid><comments>https://www.reddit.com/r/unsloth/comments/1md01ve/how_to_quantize_myself_docs_say_only_for/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1md01ve/how_to_quantize_myself_docs_say_only_for/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I want to quantize this LLM : <a href="https://huggingface.co/Tesslate/UIGEN-X-4B-0729">https://huggingface.co/Tesslate/UIGEN-X-4B-0729</a></p><p>but when reading through the unsloth docs, nothing is mentioned about quantizing by yourself, it only mentions fine-tuning</p><p>So my question is, is unsloth not made for doing quantization yourself? </p></div><!-- SC_ON --></section>]]></description><pubDate>Wed, 30 Jul 2025 12:49:31 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mcyr2i/which_is_better_to_improve_a_specific_domain_of/</link><title>Which is better to improve a specific domain of knowledge? Continued pretrain or supervised fine tuning?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mcyr2i/which_is_better_to_improve_a_specific_domain_of/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mcyr2i/which_is_better_to_improve_a_specific_domain_of/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mcyr2i/which_is_better_to_improve_a_specific_domain_of/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Eg let&#39;s say I want to improve domain knowledge got DeepSeek for my industry, which is sorely lacking, how do I do so other than rag?</p><p>Continued pretrain or supervised fine tune? Does anyone have any resources or experiences to share please.</p></div><!-- SC_ON --></section>]]></description><pubDate>Wed, 30 Jul 2025 11:28:31 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mcht2q/request_glm45air/</link><title>request: GLM-4.5-Air</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mcht2q/request_glm45air/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mcht2q/request_glm45air/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mcht2q/request_glm45air/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Would it be possible to create a unsloth gguf of the <a href="https://huggingface.co/zai-org/GLM-4.5-Air">new light GLM4.5 release</a>?</p><p>I remember these guys releasing SWE Dev 32B and it was the best coding model you could run on two 3090&#39;s up until now. Would love to try this new release, thanks guys üôè</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 29 Jul 2025 23:00:43 +0530</pubDate></item><item><link>https://i.redd.it/ymmlfv5yeuff1.png</link><title>Unsloth Dynamic 'Qwen3-30B-A3B-Instruct-2507' GGUFs out now! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mcgpeq/unsloth_dynamic_qwen330ba3binstruct2507_ggufs_out/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mcgpeq/unsloth_dynamic_qwen330ba3binstruct2507_ggufs_out/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mcgpeq/unsloth_dynamic_qwen330ba3binstruct2507_ggufs_out/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Qwen releases Qwen3-30B-A3B-Instruct-2507! ‚ú® The 30B model rivals GPT-4o&#39;s performance and runs locally in full precision with just 33GB RAM.</p><p>GGUFs: <a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF">https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF</a></p><p>Unsloth also supports Qwen3-2507 fine-tuning and RL!</p><p>Guide to run/fine-tune: <a href="https://docs.unsloth.ai/basics/qwen3-2507">https://docs.unsloth.ai/basics/qwen3-2507</a></p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/ymmlfv5yeuff1.png' /></section>]]></description><pubDate>Tue, 29 Jul 2025 22:21:05 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mc6i5z/trl_suddenly_update_to_0200_unsloth_have_to_fix/</link><title>trl suddenly update to 0.20.0, unsloth have to fix something  now.</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mc6i5z/trl_suddenly_update_to_0200_unsloth_have_to_fix/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mc6i5z/trl_suddenly_update_to_0200_unsloth_have_to_fix/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mc6i5z/trl_suddenly_update_to_0200_unsloth_have_to_fix/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys, when i was finetuning Qwen model in the morining today , everything works fine. but after i finish ed my lunch i started a notebook from kaggle and import unsloth, i meet some dependences issues with trl. so i check pypi and found that trl have a update today. so now it  will have error with import unsloth when you install unsloth from pip. </p><p>well, now i use the trl==0.19.1 to not raise error.</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 29 Jul 2025 14:34:14 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mbv3xn/attributeerror_module_unslothppotrainer_has_no/</link><title>AttributeError: module 'UnslothPPOTrainer' has no attribute 'UnslothPPOTrainer'</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mbv3xn/attributeerror_module_unslothppotrainer_has_no/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mbv3xn/attributeerror_module_unslothppotrainer_has_no/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 5 min | <a href='https://www.reddit.com/r/unsloth/comments/1mbv3xn/attributeerror_module_unslothppotrainer_has_no/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi</p><p>I am trying llm training using unsloth on multi gpus environment.My training code is as follows.When I run it with one gpu, It is working.</p><p><code>python train_grpo_multi.py</code></p><p>But when I trying it with accelerate, it causes errors</p><p><code>accelerate launch train_grpo_multi.py</code></p><p>AttributeError: module &#39;UnslothPPOTrainer&#39; has no attribute &#39;UnslothPPOTrainer&#39;</p><p>What did I wrong?</p><p>```from unsloth import FastLanguageModelfrom trl import SFTTrainer, SFTConfigfrom datasets import Datasetfrom datasets import load_datasetimport pandas as pdimport numpy as npfrom accelerate import Acceleratorimport torchimport osimport gc, torchfrom transformers import TrainingArguments, DataCollatorForSeq2Seqfrom unsloth.chat_templates import get_chat_template, train_on_responses_only</p><p>gc.collect()torch.cuda.empty_cache()</p><h1>os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0,1&quot; #Select Which devices to use. Or, comment if you want to use all GPUs.</h1><p>os.environ[&quot;UNSLOTH_RETURN_LOGITS&quot;] = &quot;1&quot;accelerator = Accelerator()</p><p>device = accelerator.devicemax_seq_length = 2048 # Can increase for longer reasoning traceslora_rank = 32 # Larger rank = smarter, but slower</p><p>def load_model(model_path):    max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!    device_index = Accelerator().process_index    device_map = {&quot;&quot;: device_index}    # device_map = &quot;auto&quot; # Use &quot;auto&quot; to use all available GPUs    print(&quot;device_map&quot;,device_map)    model, tokenizer = FastLanguageModel.from_pretrained(        model_name = model_path,        max_seq_length = max_seq_length,        load_in_4bit = False, # False for LoRA 16bit        fast_inference = False, # Enable vLLM fast inference        max_lora_rank = lora_rank,        # gpu_memory_utilization = 0.6, # Reduce if out of memory        # device_map=device_map,        device_map = &quot;balanced&quot;,        use_cache=False,    )</p><pre><code>return model, tokenizer</code></pre><p>def model_LoRA(base_model):    model = FastLanguageModel.get_peft_model(        base_model,        r = lora_rank, # Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128        target_modules = [            &quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,            &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;,        ],        lora_alpha = lora_rank*2, # *2 speeds up training        # use_gradient_checkpointing = &quot;unsloth&quot;, # Reduces memory usage        use_gradient_checkpointing = False,        random_state = 3407,        use_rslora= False, # Use RSLORA for better performance</p><pre><code>)return model</code></pre><p>model, tokenizer = load_model(model_path=&quot;/home/jovyan/llm-shared/next_bixby/models/qwen/Qwen3-4B&quot;)model = model_LoRA(base_model=model)</p><p>reasoning_start = &quot;&lt;start_working_out&gt;&quot; # Acts as &lt;think&gt;reasoning_end   = &quot;&lt;end_working_out&gt;&quot;   # Acts as &lt;/think&gt;solution_start  = &quot;&lt;SOLUTION&gt;&quot;solution_end    = &quot;&lt;/SOLUTION&gt;&quot;</p><p>system_prompt = \f&quot;&quot;&quot;You are given a problem.Think about the problem and provide your working out.Place it between {reasoning_start} and {reasoning_end}.Then, provide your solution between {solution_start}{solution_end}&quot;&quot;&quot;system_prompt</p><p>chat_template = \    &quot;{% if messages[0][&#39;role&#39;] == &#39;system&#39; %}&quot;\        &quot;{{ messages[0][&#39;content&#39;] + eos_token }}&quot;\        &quot;{% set loop_messages = messages[1:] %}&quot;\    &quot;{% else %}&quot;\        &quot;{{ &#39;{system_prompt}&#39; + eos_token }}&quot;\        &quot;{% set loop_messages = messages %}&quot;\    &quot;{% endif %}&quot;\    &quot;{% for message in loop_messages %}&quot;\        &quot;{% if message[&#39;role&#39;] == &#39;user&#39; %}&quot;\            &quot;{{ message[&#39;content&#39;] }}&quot;\        &quot;{% elif message[&#39;role&#39;] == &#39;assistant&#39; %}&quot;\            &quot;{{ message[&#39;content&#39;] + eos_token }}&quot;\        &quot;{% endif %}&quot;\    &quot;{% endfor %}&quot;\    &quot;{% if add_generation_prompt %}{{ &#39;{reasoning_start}&#39; }}&quot;\    &quot;{% endif %}&quot;</p><h1>Replace with out specific template:</h1><p>chat_template = chat_template\    .replace(&quot;&#39;{system_prompt}&#39;&quot;,   f&quot;&#39;{system_prompt}&#39;&quot;)\    .replace(&quot;&#39;{reasoning_start}&#39;&quot;, f&quot;&#39;{reasoning_start}&#39;&quot;)tokenizer.chat_template = chat_template</p><p>tokenizer.apply_chat_template([    {&quot;role&quot; : &quot;user&quot;, &quot;content&quot; : &quot;What is 1+1?&quot;},    {&quot;role&quot; : &quot;assistant&quot;, &quot;content&quot; : f&quot;{reasoning_start}I think it&#39;s 2.{reasoning_end}{solution_start}2{solution_end}&quot;},    {&quot;role&quot; : &quot;user&quot;, &quot;content&quot; : &quot;What is 2+2?&quot;},], tokenize = False, add_generation_prompt = True)</p><p>dataset = load_dataset(&quot;unsloth/OpenMathReasoning-mini&quot;, split = &quot;cot&quot;)dataset = dataset.to_pandas()[    [&quot;expected_answer&quot;, &quot;problem&quot;, &quot;generated_solution&quot;]]</p><h1>Try converting to number - if not, replace with NaN</h1><p>is_number = pd.to_numeric(pd.Series(dataset[&quot;expected_answer&quot;]), errors = &quot;coerce&quot;).notnull()</p><h1>Select only numbers</h1><p>dataset = dataset.iloc[np.where(is_number)[0]]</p><p>def format_dataset(x):    expected_answer = x[&quot;expected_answer&quot;]    problem = x[&quot;problem&quot;]</p><pre><code># Remove generated &lt;think&gt; and &lt;/think&gt;thoughts = x[&quot;generated_solution&quot;]thoughts = thoughts.replace(&quot;&lt;think&gt;&quot;, &quot;&quot;).replace(&quot;&lt;/think&gt;&quot;, &quot;&quot;)# Strip newlines on left and rightthoughts = thoughts.strip()# Add our custom formattingfinal_prompt = \    reasoning_start + thoughts + reasoning_end + \    solution_start + expected_answer + solution_endreturn [    {&quot;role&quot; : &quot;system&quot;,    &quot;content&quot; : system_prompt},    {&quot;role&quot; : &quot;user&quot;,      &quot;content&quot; : problem},    {&quot;role&quot; : &quot;assistant&quot;, &quot;content&quot; : final_prompt},]</code></pre><p>dataset[&quot;Messages&quot;] = dataset.apply(format_dataset, axis = 1)tokenizer.apply_chat_template(dataset[&quot;Messages&quot;][0], tokenize = False)</p><p>dataset[&quot;N&quot;] = dataset[&quot;Messages&quot;].apply(lambda x: len(tokenizer.apply_chat_template(x)))</p><p>dataset = dataset.loc[dataset[&quot;N&quot;] &lt;= max_seq_length/2].copy()dataset.shape</p><p>dataset[&quot;text&quot;] = tokenizer.apply_chat_template(dataset[&quot;Messages&quot;].values.tolist(), tokenize = False)dataset = Dataset.from_pandas(dataset)dataset</p><p>trainer = SFTTrainer(    model = model,    # tokenizer = tokenizer,    train_dataset = dataset,    args = SFTConfig(        ddp_find_unused_parameters= False, # Set to False for GRPO        dataset_text_field = &quot;text&quot;,        per_device_train_batch_size = 1,        gradient_accumulation_steps = 1, # Use GA to mimic batch size!        warmup_steps = 5,        num_train_epochs = 2, # Set this for 1 full training run.        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs        logging_steps = 5,        optim = &quot;adamw_8bit&quot;,        weight_decay = 0.01,        # lr_scheduler_type = &quot;linear&quot;,        seed = 3407,        report_to = &quot;none&quot;, # Use this for WandB etc        # data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),    ),)</p><h1>If the model is wrapped in DDP, access the underlying module:</h1><p>if hasattr(trainer.model, &quot;module&quot;) and hasattr(trainer.model.module, &quot;_set_static_graph&quot;):    trainer.model.module._set_static_graph()elif hasattr(trainer.model, &quot;_set_static_graph&quot;):    trainer.model._set_static_graph()</p><p>trainer_stats = trainer.train()```</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 29 Jul 2025 04:26:09 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mbcay4/unsloth_dynamic_ggufs_embedded_q4_k_vs_q8_0/</link><title>Unsloth Dynamic GGUFs embedded Q4_K vs Q8_0</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mbcay4/unsloth_dynamic_ggufs_embedded_q4_k_vs_q8_0/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mbcay4/unsloth_dynamic_ggufs_embedded_q4_k_vs_q8_0/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mbcay4/unsloth_dynamic_ggufs_embedded_q4_k_vs_q8_0/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Will there be any difference using Q8_0 weights for <code>token_embd.weight</code> layer?</p><p>I have noticed that bartowski models in Q4_K_L usually gives better results vs Q4_K_M/Q4_0, while having fast prompt processing.</p><p>I&#39;m interested if there will be any value to use Q8_0 instead of Q4_K for <code>token_embd.weight</code> layer for Q4_K_XL quantization or not?</p></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 28 Jul 2025 15:42:15 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mak4ec/finetunable_vlm_for_small_details/</link><title>finetunable VLM for small details?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mak4ec/finetunable_vlm_for_small_details/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mak4ec/finetunable_vlm_for_small_details/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mak4ec/finetunable_vlm_for_small_details/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi there, I&#39;m a medical doctor.For generating drafts of medical reports based on text input, I‚Äôve had good experiences fine-tuning Qwq32. For interpreting medical images, I‚Äôm currently fine-tuning LLaMA 3.2 11B Vision. Gemma 3 26B and Qwen-VL-2.5 32B also work, but they tend to miss small details. I am waiting for a DGX spark, until then my VRAM is limited to 24GB.</p><p>Here‚Äôs my question:Which vision-language model is well-suited for fine-tuning (ideally with QLoRA) and includes a visual encoder capable of capturing fine details in images?</p><p>The use case is ultrasound of the neck ‚Äì specifically, counting and measuring lymph nodes. This is for my own personal productivity and not for clinical deployment; I remain fully responsible for the interpretations. But the task is highly repetitive, so I‚Äôm simply looking for an effective VLM to assist with it.</p><p>Any recommendations are much appreciated.Thank you!</p></div><!-- SC_ON --></section>]]></description><pubDate>Sun, 27 Jul 2025 17:07:20 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mai68v/request_advice_voxtral_small_24b/</link><title>Request / advice: Voxtral (Small 24B)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mai68v/request_advice_voxtral_small_24b/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mai68v/request_advice_voxtral_small_24b/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mai68v/request_advice_voxtral_small_24b/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Recently MistralAI released new audio+text-to-text model, Voxtral-Mini and Voxtral-Small <a href="https://huggingface.co/mistralai/Voxtral-Small-24B-2507">Voxtral [Huggingface]</a>. They claim to outperform Whisper large-v3. </p><p>i have a NVIDIA RTX 6000 ADA to run local tests. The Voxtral-Small (24B) does not fit onto this card in full precision. Would it be possible to create Q4/Q5/Q6 quants to retain the audio capabilities? I would like to test the transcription capabilities for audio that includes frequent language switching.</p><p>If possible, what would be necessary to realize these quants (infrastructure and/or pricing)?</p><p>Thanks for any advice.</p></div><!-- SC_ON --></section>]]></description><pubDate>Sun, 27 Jul 2025 15:06:22 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1ma4gct/request_swedev/</link><title>Request: swe-dev</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1ma4gct/request_swedev/</guid><comments>https://www.reddit.com/r/unsloth/comments/1ma4gct/request_swedev/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1ma4gct/request_swedev/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Source: <a href="https://huggingface.co/THUDM/SWE-Dev-32B">https://huggingface.co/THUDM/SWE-Dev-32B</a></p></div><!-- SC_ON --></section>]]></description><pubDate>Sun, 27 Jul 2025 02:41:37 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1m9zkhc/running_bnb4bit_on_vllm/</link><title>Running bnb-4bit on vLLM</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1m9zkhc/running_bnb4bit_on_vllm/</guid><comments>https://www.reddit.com/r/unsloth/comments/1m9zkhc/running_bnb4bit_on_vllm/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1m9zkhc/running_bnb4bit_on_vllm/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey. I would like to run <a href="https://huggingface.co/unsloth/Qwen2.5-72B-Instruct-bnb-4bit">https://huggingface.co/unsloth/Qwen2.5-72B-Instruct-bnb-4bit</a> on vLLM, but naturally it does not seem to run.</p><pre><code>    s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig Value error, Invalid repository ID or local directory specified: &#39;unsloth/Qwen2.5-72B-Instruct-bnb-4bit&#39; Please verify the following requirements:1. Provide a valid Hugging Face repository ID.2. Specify a local directory that contains a recognized configuration file.- For Hugging Face models: ensure the presence of a &#39;config.json&#39;.- For Mistral models: ensure the presence of a &#39;params.json&#39;.3. For GGUF: pass the local path of the GGUF checkpoint.Loading GGUF from a remote repo directly is not yet supported[type=value_error, input_value=ArgsKwargs((), {&#39;model&#39;: ...attention_dtype&#39;: None}), input_type=ArgsKwargs]For further information visit https://errors.pydantic.dev/2.11/v/value_error</code></pre><p>Would appreciate some guide on this. If it&#39;s not possible, what would be the closts to bnb 4bit? AWQ?</p><p>my run command:</p><p><code>python3 -m vllm.entrypoints.openai.api_server --host</code> <a href="http://0.0.0.0"><code>0.0.0.0</code></a> <code>--port 8000 --model unsloth/Qwen2.5-72B-Instruct-bnb-4bit --gpu-memory-utilization 0.95 --api-key redacted --max-model-len 1000 --served-model-name test --enable-auto-tool-choice --tool-call-parser hermes --guided-decoding-backend auto</code></p></div><!-- SC_ON --></section>]]></description><pubDate>Sat, 26 Jul 2025 23:16:53 +0530</pubDate></item><item><link>https://huggingface.co/unsloth/Magistral-Small-2507-GGUF</link><title>Magistral-2507 Dynamic GGUFs out now! (huggingface.co)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1m9rm9n/magistral2507_dynamic_ggufs_out_now/</guid><comments>https://www.reddit.com/r/unsloth/comments/1m9rm9n/magistral2507_dynamic_ggufs_out_now/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 13 min | <a href='https://www.reddit.com/r/unsloth/comments/1m9rm9n/magistral2507_dynamic_ggufs_out_now/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Has the correct chat template too! Just thought we should update you guys incase you all werent aware! :)</p><p>Hope you guys have an amazing weekend and thanks for all the support this week! &lt;3</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://cdn-thumbnails.huggingface.co/social-thumbnails/models/unsloth/Magistral-Small-2507-GGUF.png' /></section><section class='parsed-content'><div><div><p>Building upon Mistral Small 3.1 (2503), <strong>with added reasoning capabilities</strong>, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters.</p><p>Magistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.</p><p>Learn more about Magistral in our <a href="https://mistral.ai/news/magistral/">blog post</a>.</p><p>The model was presented in the paper <a href="https://huggingface.co/papers/2506.10910">Magistral</a>.</p><h2> <a href="https://huggingface.co#updates-compared-with-magistral-small-10"> </a> <span> Updates compared with <a href="https://huggingface.co/mistralai/Magistral-Small-2506">Magistral Small 1.0</a> </span> </h2><p>Magistral Small 1.1 should give you about the same performance as Magistral Small 1.0 as seen in the <a href="https://huggingface.co#benchmark-results">benchmark results</a>.</p><p>The update involves the following features:</p><ul> <li>Better tone and model behaviour. You should experiment better LaTeX and Markdown formatting, and shorter answers on easy general prompts.</li> <li>The model is less likely to enter infinite generation loops.</li> <li><code>[THINK]</code> and <code>[/THINK]</code> special tokens encapsulate the reasoning content in a thinking chunk. This makes it easier to parse the reasoning trace and prevents confusion when the '[THINK]' token is given as a string in the prompt.</li> <li>The reasoning prompt is now given in the system prompt.</li> </ul> <h2> <a href="https://huggingface.co#key-features"> </a> <span> Key Features </span> </h2> <ul> <li><strong>Reasoning:</strong> Capable of long chains of reasoning traces before providing an answer.</li> <li><strong>Multilingual:</strong> Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi.</li> <li><strong>Apache 2.0 License:</strong> Open license allowing usage and modification for both commercial and non-commercial purposes.</li> <li><strong>Context Window:</strong> A 128k context window, <strong>but</strong> performance might degrade past <strong>40k</strong>. Hence we recommend setting the maximum model length to 40k.</li> </ul> <h2> <a href="https://huggingface.co#benchmark-results"> </a> <span> Benchmark Results </span> </h2><div><table> <thead><tr> <th>Model</th> <th>AIME24 pass@1</th> <th>AIME25 pass@1</th> <th>GPQA Diamond</th> <th>Livecodebench (v5)</th> </tr> </thead><tbody><tr> <td><strong>Magistral Medium 1.1</strong></td> <td>72.03%</td> <td>60.99%</td> <td>71.46%</td> <td>59.35%</td> </tr> <tr> <td>Magistral Medium 1.0</td> <td>73.59%</td> <td>64.95%</td> <td>70.83%</td> <td>59.36%</td> </tr> <tr> <td><strong>Magistral Small 1.1</strong></td> <td>70.52%</td> <td>62.03%</td> <td>65.78%</td> <td>59.17%</td> </tr> <tr> <td>Magistral Small 1.0</td> <td>70.68%</td> <td>62.76%</td> <td>68.18%</td> <td>55.84%</td> </tr> </tbody> </table> </div><h2> <a href="https://huggingface.co#sampling-parameters"> </a> <span> Sampling parameters </span> </h2><p>Please make sure to use: </p><ul> <li><code>top_p</code>: 0.95</li> <li><code>temperature</code>: 0.7</li> <li><code>max_tokens</code>: 40960</li> </ul> <h2> <a href="https://huggingface.co#basic-chat-template"> </a> <span> Basic Chat Template </span> </h2><p>We highly recommend including the following system prompt for the best results, you can edit and customise it if needed for your specific use case.</p><blockquote><p>First draft your thinking process (inner monologue) until you arrive at a response. Format your response using Markdown, and use LaTeX for any mathematical equations. Write both your thoughts and the response in the same language as the input.</p><p>Your thinking process must follow the template below:[THINK]Your thoughts or/and draft, like working through an exercise on scratch paper. Be as casual and as long as you want until you are confident to generate the response. Use the same language as the input.[/THINK]Here, provide a self-contained response.</p></blockquote><p>The <code>[THINK]</code> and <code>[/THINK]</code> are special tokens that <strong>must</strong> be encoded as such.</p><p><em><strong>Please make sure to use <a href="https://github.com/mistralai/mistral-common">mistral-common</a> as the source of truth</strong></em>. Find <a href="https://huggingface.co#usage">below</a> examples from libraries supporting <code>mistral-common</code>.</p><p>We invite you to choose, depending on your use case and requirements, between keeping reasoning traces during multi-turn interactions or keeping only the final assistant response.</p><h2> <a href="https://huggingface.co#usage"> </a> <span> Usage </span> </h2><p>The model can be used with the following frameworks;</p><h3> <a href="https://huggingface.co#inference"> </a> <span> Inference </span> </h3> <ul> <li><a href="https://github.com/vllm-project/vllm"><code>vllm (recommended)</code></a>: See <a href="https://huggingface.co#vllm-recommended">below</a></li> <li><a href="https://github.com/huggingface/transformers"><code>transformers</code></a>: See <a href="https://huggingface.co#transformers">below</a></li> </ul> <h3> <a href="https://huggingface.co#vllm-recommended"> </a> <span> vLLM (recommended) </span> </h3><p>We recommend using this model with the <a href="https://github.com/vllm-project/vllm">vLLM library</a> to implement production-ready inference pipelines.</p><p><strong><em>Installation</em></strong></p><p>Make sure you install the latest <a href="https://github.com/vllm-project/vllm/"><code>vLLM</code></a> code:</p><pre><code>pip install -U vllm \ --pre \ --extra-index-url https://wheels.vllm.ai/nightly </code></pre><p>Doing so should automatically install <a href="https://github.com/mistralai/mistral-common/releases/tag/v1.8.2"><code>mistral_common &gt;= 1.8.2</code></a>.</p><p>To check:</p><pre><code>python -c "import mistral_common; print(mistral_common.__version__)" </code></pre><p>You can also make use of a ready-to-go <a href="https://github.com/vllm-project/vllm/blob/main/Dockerfile">docker image</a> or on the <a href="https://hub.docker.com/layers/vllm/vllm-openai/latest/images/sha256-de9032a92ffea7b5c007dad80b38fd44aac11eddc31c435f8e52f3b7404bbf39">docker hub</a>.</p><p>Serve model as follows:</p><pre><code>vllm serve mistralai/Magistral-Small-2507 --reasoning-parser mistral --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --tensor-parallel-size 2 </code></pre><p>Ping model as follows:</p><details> <summary>Python snippet</summary><pre><code><span>from</span> typing <span>import</span> <span>Any</span> <span>from</span> openai <span>import</span> OpenAI <span>from</span> huggingface_hub <span>import</span> hf_hub_download <span># Modify OpenAI's API key and API base to use vLLM's API server.</span> openai_api_key = <span>"EMPTY"</span> openai_api_base = <span>"http://localhost:8000/v1"</span> TEMP = <span>0.7</span> TOP_P = <span>0.95</span> MAX_TOK = <span>40_960</span> client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.<span>list</span>() model = models.data[].<span>id</span> <span>def</span> <span>load_system_prompt</span>(<span>repo_id: <span>str</span>, filename: <span>str</span></span>) -&gt; <span>dict</span>[<span>str</span>, <span>Any</span>]: file_path = hf_hub_download(repo_id=repo_id, filename=filename) <span>with</span> <span>open</span>(file_path, <span>"r"</span>) <span>as</span> file: system_prompt = file.read() index_begin_think = system_prompt.find(<span>"[THINK]"</span>) index_end_think = system_prompt.find(<span>"[/THINK]"</span>) <span>return</span> { <span>"role"</span>: <span>"system"</span>, <span>"content"</span>: [ {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: system_prompt[:index_begin_think]}, { <span>"type"</span>: <span>"thinking"</span>, <span>"thinking"</span>: system_prompt[ index_begin_think + <span>len</span>(<span>"[THINK]"</span>) : index_end_think ], <span>"closed"</span>: <span>True</span>, }, { <span>"type"</span>: <span>"text"</span>, <span>"text"</span>: system_prompt[index_end_think + <span>len</span>(<span>"[/THINK]"</span>) :], }, ], } SYSTEM_PROMPT = load_system_prompt(model, <span>"SYSTEM_PROMPT.txt"</span>) query = <span>"Write 4 sentences, each with at least 8 words. Now make absolutely sure that every sentence has exactly one word less than the previous sentence."</span> <span># or try out other queries</span> <span># query = "Exactly how many days ago did the French Revolution start? Today is June 4th, 2025."</span> <span># query = "Think about 5 random numbers. Verify if you can combine them with addition, multiplication, subtraction or division to 133"</span> <span># query = "If it takes 30 minutes to dry 12 T-shirts in the sun, how long does it take to dry 33 T-shirts?"</span> messages = [ SYSTEM_PROMPT, {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: query} ] stream = client.chat.completions.create( model=model, messages=messages, stream=<span>True</span>, temperature=TEMP, top_p=TOP_P, max_tokens=MAX_TOK, ) <span>print</span>(<span>"client: Start streaming chat completions...:\n"</span>) printed_reasoning_content = <span>False</span> answer = [] <span>for</span> chunk <span>in</span> stream: reasoning_content = <span>None</span> content = <span>None</span> <span># Check the content is reasoning_content or content</span> <span>if</span> <span>hasattr</span>(chunk.choices[].delta, <span>"reasoning_content"</span>): reasoning_content = chunk.choices[].delta.reasoning_content <span>elif</span> <span>hasattr</span>(chunk.choices[].delta, <span>"content"</span>): content = chunk.choices[].delta.content <span>if</span> reasoning_content <span>is</span> <span>not</span> <span>None</span>: <span>if</span> <span>not</span> printed_reasoning_content: printed_reasoning_content = <span>True</span> <span>print</span>(<span>"Start reasoning:\n"</span>, end=<span>""</span>, flush=<span>True</span>) <span>print</span>(reasoning_content, end=<span>""</span>, flush=<span>True</span>) <span>elif</span> content <span>is</span> <span>not</span> <span>None</span>: <span># Extract and print the content</span> <span>if</span> <span>not</span> reasoning_content <span>and</span> printed_reasoning_content: answer.extend(content) <span>print</span>(content, end=<span>""</span>, flush=<span>True</span>) <span>if</span> answer: <span>print</span>(<span>"\n\n=============\nAnswer\n=============\n"</span>) <span>print</span>(<span>""</span>.join(answer)) <span>else</span>: <span>print</span>(<span>"\n\n=============\nNo Answer\n=============\n"</span>) <span>print</span>(<span>"No answer was generated by the model, probably because the maximum number of tokens was reached."</span>) <span># client: Start streaming chat completions...:</span> <span>#</span> <span># Start reasoning:</span> <span># First, I need to write ...</span> <span># ...</span> <span>#</span> <span>#</span> <span># =============</span> <span># Answer</span> <span># =============</span> <span># </span> <span># Here are four sentences where each has at least 8 words, and each subsequent sentence has exactly one word less than the previous one:</span> <span># 1. The quick brown fox jumps over the lazy dog and rests.</span> <span># 2. The lazy dog rests under the big shady tree peacefully.</span> <span># 3. The big shady tree provides ample shade during summer.</span> <span># 4. The tree's leaves are very lush and green.</span> </code></pre> </details> <h3> <a href="https://huggingface.co#transformers"> </a> <span> Transformers </span> </h3><p>Make sure you install the latest <a href="https://github.com/huggingface/transformers/"><code>Transformers</code></a> code:</p><pre><code>pip install git+https://github.com/huggingface/transformers </code></pre><p>Also make sure to install <a href="https://github.com/mistralai/mistral-common/releases/tag/v1.8.2"><code>mistral_common &gt;= 1.8.2</code></a>:</p><pre><code>pip install --upgrade mistral-common </code></pre><p>To check:</p><pre><code>python -c <span>"import mistral_common; print(mistral_common.__version__)"</span> </code></pre><p>Now you can use Transformers with Magistral:</p><details> <summary>Python snippet</summary><pre><code><span>from</span> typing <span>import</span> <span>Any</span> <span>import</span> torch <span>from</span> huggingface_hub <span>import</span> hf_hub_download <span>from</span> transformers <span>import</span> AutoModelForCausalLM, AutoTokenizer TEMP = <span>0.7</span> TOP_P = <span>0.95</span> MAX_TOK = <span>40_960</span> <span>def</span> <span>load_system_prompt</span>(<span>repo_id: <span>str</span>, filename: <span>str</span></span>) -&gt; <span>dict</span>[<span>str</span>, <span>Any</span>]: file_path = hf_hub_download(repo_id=repo_id, filename=filename) <span>with</span> <span>open</span>(file_path, <span>"r"</span>) <span>as</span> file: system_prompt = file.read() index_begin_think = system_prompt.find(<span>"[THINK]"</span>) index_end_think = system_prompt.find(<span>"[/THINK]"</span>) <span>return</span> { <span>"role"</span>: <span>"system"</span>, <span>"content"</span>: [ {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: system_prompt[:index_begin_think]}, { <span>"type"</span>: <span>"thinking"</span>, <span>"thinking"</span>: system_prompt[ index_begin_think + <span>len</span>(<span>"[THINK]"</span>) : index_end_think ], <span>"closed"</span>: <span>True</span>, }, { <span>"type"</span>: <span>"text"</span>, <span>"text"</span>: system_prompt[index_end_think + <span>len</span>(<span>"[/THINK]"</span>) :], }, ], } model_id = <span>"mistralai/Magistral-Small-2507"</span> SYSTEM_PROMPT = load_system_prompt(model_id, <span>"SYSTEM_PROMPT.txt"</span>) query = <span>"Think about 5 random numbers. Verify if you can combine them with addition, multiplication, subtraction or division to 133."</span> <span># or try out other queries</span> <span># query = "Exactly how many days ago did the French Revolution start? Today is June 4th, 2025."</span> <span># query = "Write 4 sentences, each with at least 8 words. Now make absolutely sure that every sentence has exactly one word less than the previous sentence."</span> <span># query = "If it takes 30 minutes to dry 12 T-shirts in the sun, how long does it take to dry 33 T-shirts?"</span> tokenizer = AutoTokenizer.from_pretrained(model_id, tokenizer_type=<span>"mistral"</span>, use_fast=<span>False</span>) model = AutoModelForCausalLM.from_pretrained( model_id, torch_dtype=torch.bfloat16, device_map=<span>"auto"</span> ) input_ids = tokenizer.apply_chat_template( [ SYSTEM_PROMPT, {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: query}, ], ) output = model.generate( input_ids=torch.tensor([input_ids], device=model.device), pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, temperature=TEMP, top_p=TOP_P, do_sample=<span>True</span>, max_new_tokens=MAX_TOK, )[] decoded_output = tokenizer.decode(output[<span>len</span>(input_ids) :]) <span>print</span>(decoded_output) <span># [THINK]Alright, I need to think of 5 random numbers first. Let's say I pick the numbers 5, 10, 2, 7, and 3.</span> <span># </span> <span># Now, I need to see if I can combine these numbers using addition, multiplication, subtraction, or division to get 133.</span> <span># ...</span> <span># ...</span> <span># ...</span> <span># But if we're to find any five numbers that can be combined to make 133, then yes, such sets exist, like the one demonstrated above.[/THINK]Yes, it is possible to combine some sets of five random numbers to make 133 using basic arithmetic operations. For example, the numbers 13, 10, 1, 2, and 3 can be combined as follows to make 133:</span> <span># </span> <span># \[ (13 \times 10) + (3 \times (2 - 1)) = 130 + 3 = 133 \]</span> <span># </span> <span># However, not all sets of five random numbers can be combined in this way to make 133. For instance, with the numbers 5, 10, 2, 7, and 3, it is not possible to combine them using the allowed operations to get exactly 133.</span> <span># </span> <span># Therefore, the ability to combine five random numbers to make 133 depends on the specific numbers chosen.</span> <span># </span> <span># $133 = (13 \times 10) + (3 \times (2 - 1))$</span> </code></pre> </details></div><section><div><dl><dt>Downloads last month</dt><dd>3,835 </dd></dl> </div><h2> Model tree for <span>unsloth/Magistral-Small-2507-GGUF</span> <a href="https://huggingface.co/docs/hub/model-cards#specifying-a-base-model"></a></h2> </section></div></section>]]></description><pubDate>Sat, 26 Jul 2025 17:32:46 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1m97y69/is_there_any_way_to_disable_vision_part_of_model/</link><title>Is there any way to disable vision part of model when finetuning on text only?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1m97y69/is_there_any_way_to_disable_vision_part_of_model/</guid><comments>https://www.reddit.com/r/unsloth/comments/1m97y69/is_there_any_way_to_disable_vision_part_of_model/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1m97y69/is_there_any_way_to_disable_vision_part_of_model/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>For models like gemma that work for multiple modalities </p><p>Since gemma finetuning takes more memory than qwen3, it would help with fiting model in memory</p></div><!-- SC_ON --></section>]]></description><pubDate>Sat, 26 Jul 2025 00:37:13 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1m96onb/magistralsmall2507_not_thinking_consistently/</link><title>Magistral-Small-2507 not thinking consistently?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1m96onb/magistralsmall2507_not_thinking_consistently/</guid><comments>https://www.reddit.com/r/unsloth/comments/1m96onb/magistralsmall2507_not_thinking_consistently/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1m96onb/magistralsmall2507_not_thinking_consistently/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I&#39;m not a big Magistral user so I decided to give it a try, and I&#39;m not seeing it think consistently, and if it does, I don&#39;t see it using thinking tags. I&#39;ve read through <a href="https://docs.unsloth.ai/basics/tutorials-how-to-fine-tune-and-run-llms/magistral-how-to-run-and-fine-tune">unsloth&#39;s guide</a>, and I tried the &quot;easy&quot; questions like the strawberry test and it got that wrong with no rumination.</p><p>Is this me or are others seeing this?</p><p>My llama-swap settings:</p><pre><code>  /root/llama-builds/llama.cpp/bin/llama-server  --port ${PORT}  --flash-attn  -sm none -mg 0  -ngl 99  -ctk q8_0 -ctv f16  --model /mnt/models/unsloth/Magistral-Small-2507-UD-Q4_K_XL.gguf  --jinja  --temp 0.7  --top-p 0.95  --min-p 0.01  --ctx-size 40960</code></pre></div><!-- SC_ON --></section>]]></description><pubDate>Fri, 25 Jul 2025 23:47:57 +0530</pubDate></item><item><link>https://i.redd.it/8rvt074j60ff1.jpeg</link><title>Qwen3-2507-Thinking Unsloth Dynamic GGUFs out now! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1m8wafe/qwen32507thinking_unsloth_dynamic_ggufs_out_now/</guid><comments>https://www.reddit.com/r/unsloth/comments/1m8wafe/qwen32507thinking_unsloth_dynamic_ggufs_out_now/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1m8wafe/qwen32507thinking_unsloth_dynamic_ggufs_out_now/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>You can now run Qwen3-235B-A22B-Thinking-2507 with our Dynamic GGUFs: <a href="https://huggingface.co/unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF">https://huggingface.co/unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF</a></p><p>The full 250GB model gets reduced to just 87GB (-65% size).</p><p>Achieve &gt;6 tokens/s on 88GB unified memory or 80GB RAM + 8GB VRAM.</p><p>Guide: <a href="https://docs.unsloth.ai/basics/qwen3-2507">https://docs.unsloth.ai/basics/qwen3-2507</a></p><p>Keep in mind the quants are dynamic yes, but iMatrix dynamic GGUFs are still converting and will be up in a few hours! Thanks guys! üíï</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/8rvt074j60ff1.jpeg' /></section>]]></description><pubDate>Fri, 25 Jul 2025 16:38:58 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1m8grem/open_source_finetuning_success_stories/</link><title>Open source fine-tuning success stories</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1m8grem/open_source_finetuning_success_stories/</guid><comments>https://www.reddit.com/r/unsloth/comments/1m8grem/open_source_finetuning_success_stories/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1m8grem/open_source_finetuning_success_stories/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey everyone,</p><p>I&#39;ve been trying a mix of unsloth powered approaches (SFT, GRPO) on fine tuning models towards small tasks with limited success.</p><p>I was wondering if there were any open source projects out there that finetune models to meaningful outcomes that I could learn from.</p><p>Interested in learning more about the sophistication of the setup, how they arrived at hyper-parameters, and what kind of success they had.</p><p>Thanks</p></div><!-- SC_ON --></section>]]></description><pubDate>Fri, 25 Jul 2025 02:53:46 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1m8bhfp/newbie_trying_to_load_qwen_3_30b_from_ssd_give_me/</link><title>[Newbie] Trying to load Qwen 3 30B from SSD, give me out of memory on RTX 3090</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1m8bhfp/newbie_trying_to_load_qwen_3_30b_from_ssd_give_me/</guid><comments>https://www.reddit.com/r/unsloth/comments/1m8bhfp/newbie_trying_to_load_qwen_3_30b_from_ssd_give_me/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1m8bhfp/newbie_trying_to_load_qwen_3_30b_from_ssd_give_me/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi,<br/>What mess am I doing?<br/>Can I fine-tune/train this model (safetensors version) to a Q8 GUFF in my machine?<br/>I&#39;m running unslot under WSL on a machine with 128 GB and a RTX 3090 Ti. About 85 GB are available to WSL. Relevant python script bellow:</p><p># Configure 4-bit quantization</p><p>bnb_config = BitsAndBytesConfig(<br/>load_in_4bit=True,<br/>bnb_4bit_quant_type=&quot;nf4&quot;,<br/>bnb_4bit_compute_dtype=torch.float16,<br/>bnb_4bit_use_double_quant=True,<br/>llm_int8_enable_fp32_cpu_offload=True,<br/>)</p><p>print(&quot;Loading with transformers + BitsAndBytesConfig...&quot;)<br/>tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)  </p><p>model = AutoModelForCausalLM.from_pretrained(<br/>model_path,<br/>quantization_config=bnb_config,<br/>device_map=&quot;auto&quot;,<br/>max_memory={0: &quot;24GB&quot;, &quot;cpu&quot;: &quot;80GB&quot;},<br/>trust_remote_code=True,<br/>torch_dtype=torch.float16,<br/>)</p><p>Thanks for any help.</p></div><!-- SC_ON --></section>]]></description><pubDate>Thu, 24 Jul 2025 23:29:17 +0530</pubDate></item><item><link>https://i.redd.it/egk8nelbhtef1.png</link><title>1-bit Qwen3-Coder &amp;amp; 1M Context Dynamic GGUFs out now! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1m838s6/1bit_qwen3coder_1m_context_dynamic_ggufs_out_now/</guid><comments>https://www.reddit.com/r/unsloth/comments/1m838s6/1bit_qwen3coder_1m_context_dynamic_ggufs_out_now/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1m838s6/1bit_qwen3coder_1m_context_dynamic_ggufs_out_now/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys we uploaded a 1-bit 150GB quant for Qwen3-Coder which is 30GB smaller Q2_K_XL: <a href="https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF">https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF</a></p><p>Also all the GGUFs for 1M context length are now uploaded: <a href="https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF">https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF</a>Remember more context = more RAM use.</p><p>Happy running &amp; don&#39;t forget to see our Qwen3-Coder on running the model with optimal settings &amp; setup for fast inference: <a href="https://docs.unsloth.ai/basics/qwen3-coder">https://docs.unsloth.ai/basics/qwen3-coder</a></p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/egk8nelbhtef1.png' /></section>]]></description><pubDate>Thu, 24 Jul 2025 18:06:58 +0530</pubDate></item><item><link>https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF</link><title>Kimi K2 GGUFs updated with fixed system prompts! (huggingface.co)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1m7ptgw/kimi_k2_ggufs_updated_with_fixed_system_prompts/</guid><comments>https://www.reddit.com/r/unsloth/comments/1m7ptgw/kimi_k2_ggufs_updated_with_fixed_system_prompts/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 17 min | <a href='https://www.reddit.com/r/unsloth/comments/1m7ptgw/kimi_k2_ggufs_updated_with_fixed_system_prompts/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys, we recently informed the Kimi team about the correct system prompts and they were quick to address the issue. Now we reuploaded all of the quants to use these new changes.</p><p>More info about the fixes: <a href="https://x.com/danielhanchen/status/1946163064665260486">https://x.com/danielhanchen/status/1946163064665260486</a></p><p>We also updated safetensor files too.</p></div><!-- SC_ON --></section><section class='separator separator-after-selftext'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='parsed-content'><div><ul> <li>You can now use the latest update of <a href="https://github.com/ggml-org/llama.cpp">llama.cpp</a> to run the model.</li> <li>For complete detailed instructions, see our guide: <a href="https://docs.unsloth.ai/basics/kimi-k2">docs.unsloth.ai/basics/kimi-k2</a></li> </ul><p>It is recommended to have at least 128GB unified RAM memory to run the small quants. With 16GB VRAM and 256 RAM, expect 5+ tokens/sec. For best results, use any 2-bit XL quant or above.</p><p>Set the temperature to 0.6 recommended) to reduce repetition and incoherence.</p><hr> <hr><p><a href="https://huggingface.co/moonshotai"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Moonshot%20AI-ffc107?color=ffc107&amp;logoColor=white" alt="Hugging Face"></a> <a href="https://twitter.com/kimi_moonshot"><img src="https://img.shields.io/badge/Twitter-Kimi.ai-white?logo=x&amp;logoColor=white" alt="Twitter Follow"></a> <a href="https://discord.gg/TYU2fdJykW"><img src="https://img.shields.io/badge/Discord-Kimi.ai-white?logo=discord&amp;logoColor=white" alt="Discord"></a> </p><p><b>&#128240;&nbsp;&nbsp;<a href="https://moonshotai.github.io/Kimi-K2/">Tech Blog</a></b> &nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp; <b>&#128196;&nbsp;&nbsp;Paper Link (coming soon)</b> </p><h2> <a href="https://huggingface.co#0-changelog"> </a> <span> 0. Changelog </span> </h2> <h3> <a href="https://huggingface.co#2025715"> </a> <span> 2025.7.15 </span> </h3> <ul> <li>We have updated our tokenizer implementation. Now special tokens like <code>[EOS]</code> can be encoded to their token ids.</li> <li>We fixed a bug in the chat template that was breaking multi-turn tool calls.</li> </ul> <h2> <a href="https://huggingface.co#1-model-introduction"> </a> <span> 1. Model Introduction </span> </h2><p>Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.</p><h3> <a href="https://huggingface.co#key-features"> </a> <span> Key Features </span> </h3> <ul> <li>Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability.</li> <li>MuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up.</li> <li>Agentic Intelligence: Specifically designed for tool use, reasoning, and autonomous problem-solving.</li> </ul> <h3> <a href="https://huggingface.co#model-variants"> </a> <span> Model Variants </span> </h3> <ul> <li><strong>Kimi-K2-Base</strong>: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions.</li> <li><strong>Kimi-K2-Instruct</strong>: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking.</li> </ul><div><picture> <img alt="Evaluation Results" width="80%" src="https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/figures/banner.png"> </picture> </div><h2> <a href="https://huggingface.co#2-model-summary"> </a> <span> 2. Model Summary </span> </h2><div><table> <thead><tr> <th></th> <th></th> </tr> </thead><tbody><tr> <td><strong>Architecture</strong></td> <td>Mixture-of-Experts (MoE)</td> </tr> <tr> <td><strong>Total Parameters</strong></td> <td>1T</td> </tr> <tr> <td><strong>Activated Parameters</strong></td> <td>32B</td> </tr> <tr> <td><strong>Number of Layers</strong> (Dense layer included)</td> <td>61</td> </tr> <tr> <td><strong>Number of Dense Layers</strong></td> <td>1</td> </tr> <tr> <td><strong>Attention Hidden Dimension</strong></td> <td>7168</td> </tr> <tr> <td><strong>MoE Hidden Dimension</strong> (per Expert)</td> <td>2048</td> </tr> <tr> <td><strong>Number of Attention Heads</strong></td> <td>64</td> </tr> <tr> <td><strong>Number of Experts</strong></td> <td>384</td> </tr> <tr> <td><strong>Selected Experts per Token</strong></td> <td>8</td> </tr> <tr> <td><strong>Number of Shared Experts</strong></td> <td>1</td> </tr> <tr> <td><strong>Vocabulary Size</strong></td> <td>160K</td> </tr> <tr> <td><strong>Context Length</strong></td> <td>128K</td> </tr> <tr> <td><strong>Attention Mechanism</strong></td> <td>MLA</td> </tr> <tr> <td><strong>Activation Function</strong></td> <td>SwiGLU</td> </tr> </tbody> </table> </div><h2> <a href="https://huggingface.co#3-evaluation-results"> </a> <span> 3. Evaluation Results </span> </h2> <h4> <a href="https://huggingface.co#instruction-model-evaluation-results"> </a> <span> Instruction model evaluation results </span> </h4><div><table> <thead> <tr> <th>Benchmark</th> <th>Metric</th> <th><sup>Kimi K2 Instruct</sup></th> <th><sup>DeepSeek-V3-0324</sup></th> <th><sup>Qwen3-235B-A22B <br><sup>(non-thinking)</sup></sup></th> <th><sup>Claude Sonnet 4 <br><sup>(w/o extended thinking)</sup></sup></th> <th><sup>Claude Opus 4 <br><sup>(w/o extended thinking)</sup></sup></th> <th><sup>GPT-4.1</sup></th> <th><sup>Gemini 2.5 Flash <br> Preview (05-20)</sup></th> </tr> </thead> <tbody> <tr> <td><strong>Coding Tasks</strong></td> </tr> <tr> <td>LiveCodeBench v6<br><sup>(Aug 24 - May 25)</sup></td> <td>Pass@1</td> <td><strong>53.7</strong></td> <td>46.9</td> <td>37.0</td> <td>48.5</td> <td>47.4</td> <td>44.7</td> <td>44.7</td> </tr> <tr> <td>OJBench</td> <td>Pass@1</td> <td><strong>27.1</strong></td> <td>24.0</td> <td>11.3</td> <td>15.3</td> <td>19.6</td> <td>19.5</td> <td>19.5</td> </tr> <tr> <td>MultiPL-E</td> <td>Pass@1</td> <td><ins><strong>85.7</strong></ins></td> <td>83.1</td> <td>78.2</td> <td>88.6</td> <td><strong>89.6</strong></td> <td>86.7</td> <td>85.6</td> </tr> <tr> <td>SWE-bench Verified <br><sup>(Agentless Coding)</sup></td> <td>Single Patch w/o Test (Acc)</td> <td><ins><strong>51.8</strong></ins></td> <td>36.6</td> <td>39.4</td> <td>50.2</td> <td><strong>53.0</strong></td> <td>40.8</td> <td>32.6</td> </tr> <tr> <td>SWE-bench Verified <br> <sup>(Agentic Coding)</sup></td> <td>Single Attempt (Acc)</td> <td><ins><strong>65.8</strong></ins></td> <td>38.8</td> <td>34.4</td> <td><strong>72.7</strong><sup>*</sup></td> <td>72.5<sup>*</sup></td> <td>54.6</td> <td>&mdash;</td> </tr> <tr> <td>Multiple Attempts (Acc)</td> <td><ins><strong>71.6</strong></ins></td> <td>&mdash;</td> <td>&mdash;</td> <td><strong>80.2</strong></td> <td>79.4<sup>*</sup></td> <td>&mdash;</td> <td>&mdash;</td> </tr> <tr> <td>SWE-bench Multilingual<br> <sup>(Agentic Coding)</sup></td> <td>Single Attempt (Acc)</td> <td><ins><strong>47.3</strong> </ins></td> <td>25.8</td> <td>20.9</td> <td><strong>51.0</strong></td> <td>&mdash;</td> <td>31.5</td> <td>&mdash;</td> </tr> <tr> <td>TerminalBench</td> <td>Inhouse Framework (Acc)</td> <td><ins><strong>30.0</strong></ins></td> <td>&mdash;</td> <td>&mdash;</td> <td>35.5</td> <td><strong>43.2</strong></td> <td>8.3</td> <td>&mdash;</td> </tr> <tr> <td>Terminus (Acc)</td> <td><ins><strong>25.0</strong> </ins></td> <td>16.3</td> <td>6.6</td> <td>&mdash;</td> <td>&mdash;</td> <td><strong>30.3</strong></td> <td>16.8</td> </tr> <tr> <td>Aider-Polyglot</td> <td>Acc</td> <td>60.0</td> <td>55.1</td> <td><ins><strong>61.8</strong></ins></td> <td>56.4</td> <td><strong>70.7</strong></td> <td>52.4</td> <td>44.0</td> </tr> <tr> <td><strong>Tool Use Tasks</strong></td> </tr> <tr> <td>Tau2 retail</td> <td>Avg@4</td> <td><ins><strong>70.6</strong></ins></td> <td>69.1</td> <td>57.0</td> <td>75.0</td> <td><strong>81.8</strong></td> <td>74.8</td> <td>64.3</td> </tr> <tr> <td>Tau2 airline</td> <td>Avg@4</td> <td><ins><strong>56.5</strong></ins></td> <td>39.0</td> <td>26.5</td> <td>55.5</td> <td><strong>60.0</strong></td> <td>54.5</td> <td>42.5</td> </tr> <tr> <td>Tau2 telecom</td> <td>Avg@4</td> <td><strong>65.8</strong></td> <td>32.5</td> <td>22.1</td> <td>45.2</td> <td>57.0</td> <td>38.6</td> <td>16.9</td> </tr> <tr> <td>AceBench</td> <td>Acc</td> <td><ins><strong>76.5</strong></ins></td> <td>72.7</td> <td>70.5</td> <td>76.2</td> <td>75.6</td> <td><strong>80.1</strong></td> <td>74.5</td> </tr> <tr> <td><strong>Math &amp; STEM Tasks</strong></td> </tr> <tr> <td>AIME 2024</td> <td>Avg@64</td> <td><strong>69.6</strong></td> <td>59.4<sup>*</sup></td> <td>40.1<sup>*</sup></td> <td>43.4</td> <td>48.2</td> <td>46.5</td> <td>61.3</td> </tr> <tr> <td>AIME 2025</td> <td>Avg@64</td> <td><strong>49.5</strong></td> <td>46.7</td> <td>24.7<sup>*</sup></td> <td>33.1<sup>*</sup></td> <td>33.9<sup>*</sup></td> <td>37.0</td> <td>46.6</td> </tr> <tr> <td>MATH-500</td> <td>Acc</td> <td><strong>97.4</strong></td> <td>94.0<sup>*</sup></td> <td>91.2<sup>*</sup></td> <td>94.0</td> <td>94.4</td> <td>92.4</td> <td>95.4</td> </tr> <tr> <td>HMMT 2025</td> <td>Avg@32</td> <td><strong>38.8</strong></td> <td>27.5</td> <td>11.9</td> <td>15.9</td> <td>15.9</td> <td>19.4</td> <td>34.7</td> </tr> <tr> <td>CNMO 2024</td> <td>Avg@16</td> <td>74.3</td> <td><ins><strong>74.7</strong></ins></td> <td>48.6</td> <td>60.4</td> <td>57.6</td> <td>56.6</td> <td><strong>75.0</strong></td> </tr> <tr> <td>PolyMath-en</td> <td>Avg@4</td> <td><strong>65.1</strong></td> <td>59.5</td> <td>51.9</td> <td>52.8</td> <td>49.8</td> <td>54.0</td> <td>49.9</td> </tr> <tr> <td>ZebraLogic</td> <td>Acc</td> <td><strong>89.0</strong></td> <td>84.0</td> <td>37.7<sup>*</sup></td> <td>73.7</td> <td>59.3</td> <td>58.5</td> <td>57.9</td> </tr> <tr> <td>AutoLogi</td> <td>Acc</td> <td><ins><strong>89.5</strong></ins></td> <td>88.9</td> <td>83.3</td> <td><strong>89.8</strong></td> <td>86.1</td> <td>88.2</td> <td>84.1</td> </tr> <tr> <td>GPQA-Diamond</td> <td>Avg@8</td> <td><strong>75.1</strong></td> <td>68.4<sup>*</sup></td> <td>62.9<sup>*</sup></td> <td>70.0<sup>*</sup></td> <td>74.9<sup>*</sup></td> <td>66.3</td> <td>68.2</td> </tr> <tr> <td>SuperGPQA</td> <td>Acc</td> <td><strong>57.2</strong></td> <td>53.7</td> <td>50.2</td> <td>55.7</td> <td>56.5</td> <td>50.8</td> <td>49.6</td> </tr> <tr> <td>Humanity's Last Exam<br><sup>(Text Only)</sup></td> <td>-</td> <td>4.7</td> <td>5.2</td> <td><ins><strong>5.7</strong></ins></td> <td>5.8</td> <td><strong>7.1</strong></td> <td>3.7</td> <td>5.6</td> </tr> <tr> <td><strong>General Tasks</strong></td> </tr> <tr> <td>MMLU</td> <td>EM</td> <td><ins><strong>89.5</strong></ins></td> <td>89.4</td> <td>87.0</td> <td>91.5</td> <td><strong>92.9</strong></td> <td>90.4</td> <td>90.1</td> </tr> <tr> <td>MMLU-Redux</td> <td>EM</td> <td><ins><strong>92.7</strong></ins></td> <td>90.5</td> <td>89.2</td> <td>93.6</td> <td><strong>94.2</strong></td> <td>92.4</td> <td>90.6</td> </tr> <tr> <td>MMLU-Pro</td> <td>EM</td> <td>81.1</td> <td><ins><strong>81.2</strong></ins><sup>*</sup></td> <td>77.3</td> <td>83.7</td> <td><strong>86.6</strong></td> <td>81.8</td> <td>79.4</td> </tr> <tr> <td>IFEval</td> <td>Prompt Strict</td> <td><strong>89.8</strong></td> <td>81.1</td> <td>83.2<sup>*</sup></td> <td>87.6</td> <td>87.4</td> <td>88.0</td> <td>84.3</td> </tr> <tr> <td>Multi-Challenge</td> <td>Acc</td> <td><strong>54.1</strong></td> <td>31.4</td> <td>34.0</td> <td>46.8</td> <td>49.0</td> <td>36.4</td> <td>39.5</td> </tr> <tr> <td>SimpleQA</td> <td>Correct</td> <td><ins><strong>31.0</strong></ins></td> <td>27.7</td> <td>13.2</td> <td>15.9</td> <td>22.8</td> <td><strong>42.3</strong></td> <td>23.3</td> </tr> <tr> <td>Livebench</td> <td>Pass@1</td> <td><strong>76.4</strong></td> <td>72.4</td> <td>67.6</td> <td>74.8</td> <td>74.6</td> <td>69.8</td> <td>67.8</td> </tr> </tbody> </table> </div><p><sup>&bull; Bold denotes global SOTA, and underlined denotes open-source SOTA. </sup><br><sup>&bull; Data points marked with * are taken directly from the model's tech report or blog. </sup><br><sup>&bull; All metrics, except for SWE-bench Verified (Agentless), are evaluated with an 8k output token length. SWE-bench Verified (Agentless) is limited to a 16k output token length. </sup><br><sup>&bull; Kimi K2 achieves 65.8% pass@1 on the SWE-bench Verified tests with bash/editor tools (single-attempt patches, no test-time compute). It also achieves a 47.3% pass@1 on the SWE-bench Multilingual tests under the same conditions. Additionally, we report results on SWE-bench Verified tests (71.6%) that leverage parallel test-time compute by sampling multiple sequences and selecting the single best via an internal scoring model. </sup><br><sup>&bull; To ensure the stability of the evaluation, we employed avg@k on the AIME, HMMT, CNMO, PolyMath-en, GPQA-Diamond, EvalPlus, Tau2. </sup><br><sup>&bull; Some data points have been omitted due to prohibitively expensive evaluation costs. </sup></p><hr> <h4> <a href="https://huggingface.co#base-model-evaluation-results"> </a> <span> Base model evaluation results </span> </h4><div><table> <thead> <tr> <th>Benchmark</th> <th>Metric</th> <th>Shot</th> <th>Kimi K2 Base</th> <th>Deepseek-V3-Base</th> <th>Qwen2.5-72B</th> <th>Llama 4 Maverick</th> </tr> </thead> <tbody> <tr> <td><strong>General Tasks</strong></td> </tr> <tr> <td>MMLU</td> <td>EM</td> <td>5-shot</td> <td><strong>87.8</strong></td> <td>87.1</td> <td>86.1</td> <td>84.9</td> </tr> <tr> <td>MMLU-pro</td> <td>EM</td> <td>5-shot</td> <td><strong>69.2</strong></td> <td>60.6</td> <td>62.8</td> <td>63.5</td> </tr> <tr> <td>MMLU-redux-2.0</td> <td>EM</td> <td>5-shot</td> <td><strong>90.2</strong></td> <td>89.5</td> <td>87.8</td> <td>88.2</td> </tr> <tr> <td>SimpleQA</td> <td>Correct</td> <td>5-shot</td> <td><strong>35.3</strong></td> <td>26.5</td> <td>10.3</td> <td>23.7</td> </tr> <tr> <td>TriviaQA</td> <td>EM</td> <td>5-shot</td> <td><strong>85.1</strong></td> <td>84.1</td> <td>76.0</td> <td>79.3</td> </tr> <tr> <td>GPQA-Diamond</td> <td>Avg@8</td> <td>5-shot</td> <td>48.1</td> <td><strong>50.5</strong></td> <td>40.8</td> <td>49.4</td> </tr> <tr> <td>SuperGPQA</td> <td>EM</td> <td>5-shot</td> <td><strong>44.7</strong></td> <td>39.2</td> <td>34.2</td> <td>38.8</td> </tr> <tr> <td><strong>Coding Tasks</strong></td> </tr> <tr> <td>LiveCodeBench v6</td> <td>Pass@1</td> <td>1-shot</td> <td><strong>26.3</strong></td> <td>22.9</td> <td>21.1</td> <td>25.1</td> </tr> <tr> <td>EvalPlus</td> <td>Pass@1</td> <td>-</td> <td><strong>80.3</strong></td> <td>65.6</td> <td>66.0</td> <td>65.5</td> </tr> <tr> <td><strong>Mathematics Tasks</strong></td> </tr> <tr> <td>MATH</td> <td>EM</td> <td>4-shot</td> <td><strong>70.2</strong></td> <td>60.1</td> <td>61.0</td> <td>63.0</td> </tr> <tr> <td>GSM8k</td> <td>EM</td> <td>8-shot</td> <td><strong>92.1</strong></td> <td>91.7</td> <td>90.4</td> <td>86.3</td> </tr> <tr> <td><strong>Chinese Tasks</strong></td> </tr> <tr> <td>C-Eval</td> <td>EM</td> <td>5-shot</td> <td><strong>92.5</strong></td> <td>90.0</td> <td>90.9</td> <td>80.9</td> </tr> <tr> <td>CSimpleQA</td> <td>Correct</td> <td>5-shot</td> <td><strong>77.6</strong></td> <td>72.1</td> <td>50.5</td> <td>53.5</td> </tr> </tbody> </table> </div><p><sup>&bull; We only evaluate open-source pretrained models in this work. We report results for Qwen2.5-72B because the base checkpoint for Qwen3-235B-A22B was not open-sourced at the time of our study. </sup><br><sup>&bull; All models are evaluated using the same evaluation protocol. </sup></p><h2> <a href="https://huggingface.co#4-deployment"> </a> <span> 4. Deployment </span> </h2> <blockquote><p>You can access Kimi K2's API on <a href="https://platform.moonshot.ai">https://platform.moonshot.ai</a> , we provide OpenAI/Anthropic-compatible API for you.</p><p>The Anthropic-compatible API maps temperature by <code>real_temperature = request_temperature * 0.6</code> for better compatible with existing applications.</p></blockquote><p>Our model checkpoints are stored in the block-fp8 format, you can find it on <a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct">Huggingface</a>.</p><p>Currently, Kimi-K2 is recommended to run on the following inference engines:</p><ul> <li>vLLM</li> <li>SGLang</li> <li>KTransformers</li> <li>TensorRT-LLM</li> </ul><p>Deployment examples for vLLM and SGLang can be found in the <a href="https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/blob/main/docs/deploy_guidance.md">Model Deployment Guide</a>.</p><hr> <h2> <a href="https://huggingface.co#5-model-usage"> </a> <span> 5. Model Usage </span> </h2> <h3> <a href="https://huggingface.co#chat-completion"> </a> <span> Chat Completion </span> </h3><p>Once the local inference service is up, you can interact with it through the chat endpoint:</p><pre><code><span>def</span> <span>simple_chat</span>(<span>client: OpenAI, model_name: <span>str</span></span>): messages = [ {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>"You are Kimi, an AI assistant created by Moonshot AI."</span>}, {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: [{<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"Please give a brief self-introduction."</span>}]}, ] response = client.chat.completions.create( model=model_name, messages=messages, stream=<span>False</span>, temperature=<span>0.6</span>, max_tokens=<span>256</span> ) <span>print</span>(response.choices[].message.content) </code></pre> <blockquote><p>The recommended temperature for Kimi-K2-Instruct is <code>temperature = 0.6</code>. If no special instructions are required, the system prompt above is a good default.</p></blockquote> <hr> <h3> <a href="https://huggingface.co#tool-calling"> </a> <span> Tool Calling </span> </h3><p>Kimi-K2-Instruct has strong tool-calling capabilities. To enable them, you need to pass the list of available tools in each request, then the model will autonomously decide when and how to invoke them.</p><p>The following example demonstrates calling a weather tool end-to-end:</p><pre><code><span># Your tool implementation</span> <span>def</span> <span>get_weather</span>(<span>city: <span>str</span></span>) -&gt; <span>dict</span>: <span>return</span> {<span>"weather"</span>: <span>"Sunny"</span>} <span># Tool schema definition</span> tools = [{ <span>"type"</span>: <span>"function"</span>, <span>"function"</span>: { <span>"name"</span>: <span>"get_weather"</span>, <span>"description"</span>: <span>"Retrieve current weather information. Call this when the user asks about the weather."</span>, <span>"parameters"</span>: { <span>"type"</span>: <span>"object"</span>, <span>"required"</span>: [<span>"city"</span>], <span>"properties"</span>: { <span>"city"</span>: { <span>"type"</span>: <span>"string"</span>, <span>"description"</span>: <span>"Name of the city"</span> } } } } }] <span># Map tool names to their implementations</span> tool_map = { <span>"get_weather"</span>: get_weather } <span>def</span> <span>tool_call_with_client</span>(<span>client: OpenAI, model_name: <span>str</span></span>): messages = [ {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>"You are Kimi, an AI assistant created by Moonshot AI."</span>}, {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"What's the weather like in Beijing today? Use the tool to check."</span>} ] finish_reason = <span>None</span> <span>while</span> finish_reason <span>is</span> <span>None</span> <span>or</span> finish_reason == <span>"tool_calls"</span>: completion = client.chat.completions.create( model=model_name, messages=messages, temperature=<span>0.6</span>, tools=tools, <span># tool list defined above</span> tool_choice=<span>"auto"</span> ) choice = completion.choices[] finish_reason = choice.finish_reason <span>if</span> finish_reason == <span>"tool_calls"</span>: messages.append(choice.message) <span>for</span> tool_call <span>in</span> choice.message.tool_calls: tool_call_name = tool_call.function.name tool_call_arguments = json.loads(tool_call.function.arguments) tool_function = tool_map[tool_call_name] tool_result = tool_function(**tool_call_arguments) <span>print</span>(<span>"tool_result:"</span>, tool_result) messages.append({ <span>"role"</span>: <span>"tool"</span>, <span>"tool_call_id"</span>: tool_call.<span>id</span>, <span>"name"</span>: tool_call_name, <span>"content"</span>: json.dumps(tool_result) }) <span>print</span>(<span>"-"</span> * <span>100</span>) <span>print</span>(choice.message.content) </code></pre><p>The <code>tool_call_with_client</code> function implements the pipeline from user query to tool execution. This pipeline requires the inference engine to support Kimi-K2&rsquo;s native tool-parsing logic. For streaming output and manual tool-parsing, see the <a href="https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/blob/main/docs/tool_call_guidance.md">Tool Calling Guide</a>.</p><hr> <h2> <a href="https://huggingface.co#6-license"> </a> <span> 6. License </span> </h2><p>Both the code repository and the model weights are released under the <a href="https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/blob/main/LICENSE">Modified MIT License</a>.</p><hr> <h2> <a href="https://huggingface.co#7-third-party-notices"> </a> <span> 7. Third Party Notices </span> </h2><p>See <a href="https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/blob/main/THIRD_PARTY_NOTICES.md">THIRD PARTY NOTICES</a></p><hr> <h2> <a href="https://huggingface.co#7-contact-us"> </a> <span> 7. Contact Us </span> </h2><p>If you have any questions, please reach out at <a href="https://huggingface.comailto:support@moonshot.cn">support@moonshot.cn</a>.</p></div><div class="gallery"><p><img src="https://raw.githubusercontent.com/MoonshotAI/Kimi-K2/main/figures/kimi-logo.png"></p><p><img src="https://img.shields.io/badge/Homepage-Moonshot%20AI-white?logo=Kimi&amp;logoColor=white"></p><p><img src="https://img.shields.io/badge/License-Modified_MIT-f5de53?&amp;color=f5de53"></p></div></section>]]></description><pubDate>Thu, 24 Jul 2025 05:48:21 +0530</pubDate></item><item><link>https://i.redd.it/s9cwrvwg1jef1.png</link><title>Unsloth Qwen3-Coder Dynamic 2-bit GGUFs out now! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1m6wtwd/unsloth_qwen3coder_dynamic_2bit_ggufs_out_now/</guid><comments>https://www.reddit.com/r/unsloth/comments/1m6wtwd/unsloth_qwen3coder_dynamic_2bit_ggufs_out_now/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1m6wtwd/unsloth_qwen3coder_dynamic_2bit_ggufs_out_now/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>We made dynamic 2bit to 8bit dynamic Unsloth quants for the 480B model! Dynamic 2bit needs 182GB of space (down from 512GB). Also, we&#39;re making <strong>1M context length variants</strong>!</p><p>You can achieve &gt;6 tokens/s on <strong>182GB unified memory or 158GB RAM + 24GB VRAM</strong> via MoE offloading. You do not need 182GB of VRAM, since llama.cpp can offload MoE layers to RAM via </p><pre><code>-ot &quot;.ffn_.*_exps.=CPU&quot;</code></pre><p>Unfortunately 1bit models cannot be made since there are some quantization issues (similar to Qwen 235B) - we&#39;re investigating why this happens.</p><p>You can also run the <strong>un-quantized 8bit / 16bit</strong> versions also using llama,cpp offloading! Use Q8_K_XL which will be completed in an hour or so.</p><p>To increase performance and context length, use KV cache quantization, especially the _1 variants (higher accuracy than _0 variants). More details <a href="https://docs.unsloth.ai/basics/qwen3-coder#how-to-fit-long-context-256k-to-1m">here</a>.</p><p><code>--cache-type-k q4_1</code></p><p>Enable flash attention as well and also try llama.cpp&#39;s NEW high throughput mode for multi user inference (similar to vLLM). Details on how to are <a href="https://docs.unsloth.ai/basics/qwen3-coder#improving-generation-speed">here</a>.</p><p>Qwen3-Coder-480B-A35B GGUFs (still ongoing) are at <a href="https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF">https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF</a></p><p>1 million context length variants will be up at <a href="https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF">https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF</a></p><p>Docs on how to run it are here: <a href="https://docs.unsloth.ai/basics/qwen3-coder">https://docs.unsloth.ai/basics/qwen3-coder</a></p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/s9cwrvwg1jef1.png' /></section>]]></description><pubDate>Wed, 23 Jul 2025 07:25:56 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1m6r1ug/sft_medgemma_requires_over_90gb_gpu_memory/</link><title>SFT Medgemma requires over 90GB GPU memory</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1m6r1ug/sft_medgemma_requires_over_90gb_gpu_memory/</guid><comments>https://www.reddit.com/r/unsloth/comments/1m6r1ug/sft_medgemma_requires_over_90gb_gpu_memory/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1m6r1ug/sft_medgemma_requires_over_90gb_gpu_memory/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I tried to full fine-tune &quot;unsloth/medgemma-27b-text-it-unsloth-bnb-4bit&quot; by setting full_finetuning=True when loading the pre-trained model. I set batch size = 1, and max_squence_length = 2048. I ran it on a 90GB h100, and it showed out of memory. I was quite surprised by it, even with a 27B model, I think 90GB should fit. I&#39;ve never used the full_finetuning mode before on other models. Did I do anything wrong? </p></div><!-- SC_ON --></section>]]></description><pubDate>Wed, 23 Jul 2025 03:10:57 +0530</pubDate></item><item><link>https://i.redd.it/7x99qkvs6fef1.png</link><title>Unsloth Dynamic Qwen3-235B-A22B-2507 GGUFs out now! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1m6cvi4/unsloth_dynamic_qwen3235ba22b2507_ggufs_out_now/</guid><comments>https://www.reddit.com/r/unsloth/comments/1m6cvi4/unsloth_dynamic_qwen3235ba22b2507_ggufs_out_now/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1m6cvi4/unsloth_dynamic_qwen3235ba22b2507_ggufs_out_now/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>You can now run <strong>Qwen3-235B-A22B-2507</strong> with our Dynamic <strong>2-bit</strong> GGUFs! <a href="https://t.co/PFpiHBcM4V">https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF</a>  </p><p>The full 250GB model gets reduced to just 88GB (-65% size).  </p><p>Achieve &gt;5 tokens/s on 89GB unified memory or 80GB RAM + 8GB VRAM.</p><p>And ofcourse our Qwen3 guide: <a href="https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune">https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune</a></p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/7x99qkvs6fef1.png' /></section>]]></description><pubDate>Tue, 22 Jul 2025 18:04:02 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1m6bwlw/ruler_looks_promising_does_anyone_have_experience/</link><title>RULER looks promising. Does anyone have experience with it</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1m6bwlw/ruler_looks_promising_does_anyone_have_experience/</guid><comments>https://www.reddit.com/r/unsloth/comments/1m6bwlw/ruler_looks_promising_does_anyone_have_experience/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1m6bwlw/ruler_looks_promising_does_anyone_have_experience/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p><a href="https://art.openpipe.ai/fundamentals/ruler#combining-ruler-with-independent-rewards">https://art.openpipe.ai/fundamentals/ruler#combining-ruler-with-independent-rewards</a></p><p>RULER promises to be a universal reward function. reading the docs, it seems legit to me.<br/>wanted to try to play around with this, but having difficulty understanding the Framework it uses (ART), if anyone has used it could they tell if there&#39;s anyway to use this along with Unsloth or any custom implementation notebook which can be looked at</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 22 Jul 2025 17:16:23 +0530</pubDate></item></channel></rss>
