<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><atom:link href="http://192.168.1.132/?platform=reddit&amp;subreddit=unsloth&amp;averagePostsPerDay=2&amp;content=0&amp;comments=7&amp;view=rss" rel="self" type="application/rss+xml"/><title>/r/unsloth</title><description>Hot posts in /r/unsloth (roughly 2 posts per day)</description><link>https://www.reddit.com/r/unsloth/</link><language>en-us</language><lastBuildDate>Sat, 20 Sep 2025 17:54:35 +0000</lastBuildDate><generator>Upvote RSS</generator><image><url>http://192.168.1.132//app/cache/images/styles-redditmedia-com-t5_aqipas-styles-communityIcon_86tg5vvk98lc1-144x400.png</url><title>/r/unsloth</title><link>https://www.reddit.com/r/unsloth/</link><width>144</width><height>144</height></image><item><link>https://www.reddit.com/r/unsloth/comments/1nlvohc/nvidianemotronnano9bv2gguf/</link><title>NVIDIA-Nemotron-Nano-9B-v2-GGUF</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1nlvohc/nvidianemotronnano9bv2gguf/</guid><comments>https://www.reddit.com/r/unsloth/comments/1nlvohc/nvidianemotronnano9bv2gguf/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1nlvohc/nvidianemotronnano9bv2gguf/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Team, just bringing this to your attention that <a href="https://huggingface.co/unsloth/NVIDIA-Nemotron-Nano-9B-v2-GGUF">GGUF files</a> are missing for this one. Take care</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Hello there yes, we realized the files were missing as there were some chat template issues with the model. We could fix it but unfortunately it will take a lot of our time, I think for now we shall make it private and revisit it another time.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nlvohc/nvidianemotronnano9bv2gguf/nf8cnjg/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 20 Sep 2025 16:55:35 +0530</pubDate></item><item><link>https://i.redd.it/9ugqxkxi1rpf1.png</link><title>Mistral - Magistral 1.2 out now! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1njhggy/mistral_magistral_12_out_now/</guid><comments>https://www.reddit.com/r/unsloth/comments/1njhggy/mistral_magistral_12_out_now/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1njhggy/mistral_magistral_12_out_now/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Mistral releases Magistral 1.2, their new reasoning + vision models! 🔥 Magistral-Small-2509 excels at coding + math, and is a major upgrade over 1.1.</p><p>Fine-tune Magistral 1.2 via our free notebook: <a href="https://docs.unsloth.ai/basics/magistral#fine-tuning-magistral-with-unsloth">https://docs.unsloth.ai/basics/magistral#fine-tuning-magistral-with-unsloth</a></p><p>Run the 24B model locally with 32GB RAM using our GGUFs: <a href="https://huggingface.co/unsloth/Magistral-Small-2509-GGUF">https://huggingface.co/unsloth/Magistral-Small-2509-GGUF</a></p><p>Thanks to the Mistral team for Day 0 access!</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/9ugqxkxi1rpf1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>tank u, tank u, tank u!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1njhggy/mistral_magistral_12_out_now/nesug21/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The balanced device map for Kaggle does not work with the Gemma 3 12B too. It gives a CUDA illegal memory access error.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1njhggy/mistral_magistral_12_out_now/nes9rvs/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Nice will try it out!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1njhggy/mistral_magistral_12_out_now/ner53y2/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Does not really work when I run the code as is in Kaggle, with the correct runtime selected: ---------------------------------------------------------------------------ValueError                                Traceback (most recent call last)/tmp/ipykernel_36/964778966.py in &lt;cell line: 0&gt;()     17 ] # More models at <a href="https://huggingface.co/unsloth">https://huggingface.co/unsloth</a>     18 ---&gt; 19 model, tokenizer = FastLanguageModel.from_pretrained(     20     model_name = &quot;unsloth/Magistral-Small-2509-unsloth-bnb-4bit&quot;,     21     max_seq_length = 2048,   # Context length - can be longer, but uses more memory</p><p>/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py in from_pretrained(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, <em>args, *</em>kwargs)    363         #     dispatch_model = FastGraniteModel    364         else:--&gt; 365             return FastModel.from_pretrained(    366                 model_name                 = old_model_name,    367                 max_seq_length             = max_seq_length,</p><p>/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py in from_pretrained(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, return_logits, fullgraph, use_exact_model_name, auto_model, whisper_language, whisper_task, unsloth_force_compile, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, <em>args, *</em>kwargs)    876             auto_model = AutoModelForVision2Seq if is_vlm else AutoModelForCausalLM    877 --&gt; 878         model, tokenizer = FastBaseModel.from_pretrained(    879             model_name        = model_name,    880             max_seq_length    = max_seq_length,</p><p>/usr/local/lib/python3.11/dist-packages/unsloth/models/vision.py in from_pretrained(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, trust_remote_code, model_types, tokenizer_name, auto_model, use_gradient_checkpointing, supports_sdpa, whisper_language, whisper_task, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, unsloth_vllm_standby, **kwargs)    478         raise_handler = RaiseUninitialized()    479         if not fast_inference:--&gt; 480             model = auto_model.from_pretrained(    481                 model_name,    482                 device_map              = device_map,</p><p>/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py in from_pretrained(cls, pretrained_model_name_or_path, <em>model_args, *</em>kwargs)    598             if model_class.config_class == config.sub_configs.get(&quot;text_config&quot;, None):    599                 config = config.get_text_config()--&gt; 600             return model_class.from_pretrained(    601                 pretrained_model_name_or_path, <em>model_args, config=config, *</em>hub_kwargs, **kwargs    602             )</p><p>/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py in _wrapper(<em>args, *</em>kwargs)    315         old_dtype = torch.get_default_dtype()    316         try:--&gt; 317             return func(<em>args, *</em>kwargs)    318         finally:    319             torch.set_default_dtype(old_dtype)</p><p>/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, <em>model_args, *</em>kwargs)   5072                 offload_index,   5073                 error_msgs,-&gt; 5074             ) = cls._load_pretrained_model(   5075                 model,   5076                 state_dict,</p><p>/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py in _load_pretrained_model(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)   5535    5536             for args in args_list:-&gt; 5537                 _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)   5538                 error_msgs += _error_msgs   5539 </p><p>/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py in load_shard_file(args)    973     # Skip it with fsdp on ranks other than 0    974     elif not (is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized):--&gt; 975         disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(    976             model_to_load,    977             state_dict,</p><p>/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py in decorate_context(<em>args, *</em>kwargs)    118     def decorate_context(<em>args, *</em>kwargs):    119         with ctx_factory():--&gt; 120             return func(<em>args, *</em>kwargs)    121     122     return decorate_context</p><p>/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py in _load_state_dict_into_meta_model(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)    881     882             else:--&gt; 883                 hf_quantizer.create_quantized_param(    884                     model, param, param_name, param_device, state_dict, unexpected_keys    885                 )</p><p>/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer<em>bnb_4bit.py in create_quantized_param(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)    217                 param_name + &quot;.quant_state.bitsandbytes</em>_nf4&quot; not in state_dict    218             ):--&gt; 219                 raise ValueError(    220                     f&quot;Supplied state dict for {param_name} does not contain <code>bitsandbytes__*</code> and possibly other <code>quantized_stats</code> components.&quot;    221                 )</p><p>ValueError: Supplied state dict for model.language_model.layers.9.mlp.up_proj.weight does not contain <code>bitsandbytes__*</code> and possibly other <code>quantized_stats</code> components.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1njhggy/mistral_magistral_12_out_now/nes75lx/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Wed, 17 Sep 2025 21:42:17 +0530</pubDate></item><item><link>https://i.redd.it/1gzrb9vftjpf1.png</link><title>Vision RL is now in Unsloth! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1nim8ce/vision_rl_is_now_in_unsloth/</guid><comments>https://www.reddit.com/r/unsloth/comments/1nim8ce/vision_rl_is_now_in_unsloth/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1nim8ce/vision_rl_is_now_in_unsloth/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>You can now train Vision LLMs with Reinforcement Learning via Unsloth!</p><ul><li>Qwen2.5-VL GSPO Colab notebook: <a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_5_7B_VL_GRPO.ipynb">https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_5_7B_VL_GRPO.ipynb</a></li><li>GSPO is also now supported! The notebook uses GSPO or GRPO</li><li>Unsloth VLM RL via GRPO is 1.5× faster, with 90% less VRAM, 15× longer context &amp; no accuracy loss.</li><li>Same optimizations from text RL should apply to vision LLMs as well.</li></ul><p>⭐Read our VLM RL blog: <a href="https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl">https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl</a></p><p>Happy RL everyone! :)</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/1gzrb9vftjpf1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Sounds really great!Any time estimates for Apple Silicon / MLX support?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nim8ce/vision_rl_is_now_in_unsloth/nejz8lh/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Another awesome release!</p><p>I love the team at Unsloth :)</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nim8ce/vision_rl_is_now_in_unsloth/nekvex3/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Any ideas on how well a finetune of the Qwen2.5-VL model would work as a text encoder for Qwen Image?</p><p>And what sort of dataset would be required to do that sort of thing?<br/>I&#39;m guessing image/text pairs, but I&#39;m not sure.</p><p>I know you all mostly just make the tools, but I&#39;m curious if anyone on your team has tried this sort of thing yet.<br/>Great stuff though! Keep up the good work. &lt;3</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nim8ce/vision_rl_is_now_in_unsloth/neki471/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Let’s gooooo</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nim8ce/vision_rl_is_now_in_unsloth/nek6pc5/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>qwen next.....</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nim8ce/vision_rl_is_now_in_unsloth/nek2ruk/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Wait GSPO and DR GRPO can be combined?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nim8ce/vision_rl_is_now_in_unsloth/nek7qqg/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>great job! i was wondering would vllm run vision model with lora finetuned/grpo converted to gguf the same way and methods untrained model would run?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nim8ce/vision_rl_is_now_in_unsloth/nekc3s0/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 16 Sep 2025 21:56:55 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1nigtoo/help_with_gemma3_270mipynb_example_notebook/</link><title>Help with Gemma3_(270M).ipynb example Notebook</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1nigtoo/help_with_gemma3_270mipynb_example_notebook/</guid><comments>https://www.reddit.com/r/unsloth/comments/1nigtoo/help_with_gemma3_270mipynb_example_notebook/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1nigtoo/help_with_gemma3_270mipynb_example_notebook/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>This notebook is referenced in the unsloth docs, but I keep getting stuck at one step with an exception.  I swear I have run all of the previous steps in order properly.  Please, help me get through this.  Thank you.</p><p>Error:<br/>&quot;Unsloth: Your model needs to call `.get_peft_model` first!&quot;</p><p>Step: &lt;- Have to change the False to True on this step</p><pre><code>if True:    from unsloth import FastLanguageModel    model, tokenizer = FastLanguageModel.from_pretrained(        model_name = &quot;gemma-3&quot;, # YOUR MODEL YOU USED FOR TRAINING        max_seq_length = 2048,        load_in_4bit = False,    )</code></pre><p>Notebook:</p><p><a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(270M">https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(270M).ipynb</a>.ipynb)</p><p>Document reference:</p><p><a href="https://docs.unsloth.ai/models/gemma-3-how-to-run-and-fine-tune">https://docs.unsloth.ai/models/gemma-3-how-to-run-and-fine-tune</a></p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Is there a better place to report an issue with these colab notebooks?  I consider them to be part of the official Unsloth documentation, so they really should work as is.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nigtoo/help_with_gemma3_270mipynb_example_notebook/neruwbc/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 16 Sep 2025 18:31:00 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1nfnzp2/qwen_next_gguf_when/</link><title>Qwen next gguf when?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1nfnzp2/qwen_next_gguf_when/</guid><comments>https://www.reddit.com/r/unsloth/comments/1nfnzp2/qwen_next_gguf_when/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1nfnzp2/qwen_next_gguf_when/'>Post permalink</a></p></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>There is currently no support for it in llama.cpp. I&#39;ve heard it said that it could be weeks or months before we see GGUF. If true, by the time support is added I&#39;m sure we will already have newer models with llama.cpp support that we will be going crazy about.</p><p>I think it&#39;s a lesson to the Qwen team: get technical infrastructure and third parties in place and onboard before releasing models, otherwise hype is immediately going to fizzle.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nfnzp2/qwen_next_gguf_when/ndxv99t/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Tabbyapi is also probably a good place to look.  I’m pro variable quants so explanation is good for my stuff</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nfnzp2/qwen_next_gguf_when/ne4aj5z/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 13 Sep 2025 09:44:13 +0530</pubDate></item><item><link>https://i.redd.it/ewtq2ax40dof1.png</link><title>Dynamic 3-bit DeepSeek V3.1 GGUF gets 75.6% on Aider Polyglot (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1ndiftz/dynamic_3bit_deepseek_v31_gguf_gets_756_on_aider/</guid><comments>https://www.reddit.com/r/unsloth/comments/1ndiftz/dynamic_3bit_deepseek_v31_gguf_gets_756_on_aider/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1ndiftz/dynamic_3bit_deepseek_v31_gguf_gets_756_on_aider/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey everyone, it&#39;s Michael from <a href="https://github.com/unslothai/unsloth">Unsloth</a> here! Ever since we released Dynamic GGUFs, we&#39;ve received so much love thanks to you all, but we know better benchmarking was a top request!</p><p>Previously, we already benchmarked Gemma 3 and Llama 4 on 5-shot MMLU and KL Divergence but as we&#39;re holding our first <a href="https://www.reddit.com/r/Localllama">r/Localllama</a> AMA in about an hour, we&#39;re happy to showcase Aider Polyglot benchmarks for our DeepSeek-V3.1 GGUFs and were quite surprised by the results! <a href="https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF">https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF</a></p><ul><li>In the first DeepSeek-V3.1 graph, we compare thinking with other thinking models. In the 2nd graph, we compare non-thinking vs a non-Unsloth Dynamic imatrix GGUF</li><li>Our <strong>1-bit</strong> Unsloth Dynamic GGUF shrinks DeepSeek-V3.1 from <strong>671GB → 192GB (-75% size)</strong> and no-thinking mode outperforms GPT-4.1 (Apr 2025), GPT-4.5, and DeepSeek-V3-0324.</li><li><strong>3-bit</strong> Unsloth DeepSeek-V3.1 (thinking) GGUF: Outperforms Claude-4-Opus (thinking).</li><li><strong>5-bit</strong> Unsloth DeepSeek-V3.1 (non-thinking) GGUF: Matches Claude-4-Opus (non-thinking) performance.</li><li>Our Dynamic GGUFs <strong>perform consistently better</strong> than other non-Unsloth Dynamic imatrix GGUFs</li><li>Other non-Unsloth 1-bit and 2-bit DeepSeek-V3.1 quantizations, as well as standard 1-bit quantization without selective layer quantization, either failed to load or produced gibberish and looping outputs.</li></ul><p>For our DeepSeek-V3.1 experiments, we compared different bits of <strong>Unsloth Dynamic GGUFs</strong> against:</p><ul><li><strong>Full-precision, unquantized LLMs</strong> including GPT 4.5, 4.1, Claude-4-Opus, DeepSeek-V3-0324 etc.</li><li><em>Other</em> dynamic imatrix V3.1 GGUFs</li><li><strong>Semi-dynamic</strong> (some selective layer quantization) imatrix V3.1 GGUFs for ablation purposes.</li></ul><p>Benchmark experiments were mainly conducted by David (neolithic5452 on Aider Disc), a trusted community contributor to Aider Polyglot evaluations. Tests were run ~3 times and averaged for a median score, and the Pass-2 accuracy is reported as by convention.</p><p>Wish we could attach another image for the non-thinking benchmarks but if you&#39;d like more details, you can read our blogpost: <a href="https://docs.unsloth.ai/basics/unsloth-dynamic-ggufs-on-aider-polyglot">https://docs.unsloth.ai/basics/unsloth-dynamic-ggufs-on-aider-polyglot</a></p><p>Thanks guys so much for the support!<br/>Michael</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/ewtq2ax40dof1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>:O <strong>A three bit model?!</strong> that&#39;s astonishing. You&#39;re literally 21st century wizards.</p><p>Genuine question - How is this possible?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1ndiftz/dynamic_3bit_deepseek_v31_gguf_gets_756_on_aider/ndh1gkv/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Man, I have a hell of a rig (190gb vram +128gb ram) and I&#39;m unable to run even the friggin&#39; q2. Who has the hw to run any of these &gt;5 tk/s??</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1ndiftz/dynamic_3bit_deepseek_v31_gguf_gets_756_on_aider/ndlbr6q/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>How much vram you need for this w 128k context</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1ndiftz/dynamic_3bit_deepseek_v31_gguf_gets_756_on_aider/ne1uunc/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Wed, 10 Sep 2025 21:38:42 +0530</pubDate></item><item><link>https://i.redd.it/7vh8enuu07of1.png</link><title>Unsloth AMA happening tomorrow! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1ncy85n/unsloth_ama_happening_tomorrow/</guid><comments>https://www.reddit.com/r/unsloth/comments/1ncy85n/unsloth_ama_happening_tomorrow/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1ncy85n/unsloth_ama_happening_tomorrow/'>Post permalink</a></p></section><section class='preview-image'><img src='https://i.redd.it/7vh8enuu07of1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Hey everyone excited for <a href="https://www.reddit.com/u/danielhanchen">u/danielhanchen</a> and I to be hosting tomorrow&#39;s AMA on Wednesday 10am-1pm PST :D </p><p>Ask us anything about Unsloth!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1ncy85n/unsloth_ama_happening_tomorrow/ndcsy05/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Wed, 10 Sep 2025 04:42:54 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1nbw53b/how_to_create_datasets_for_unsloth_fine_tuning/</link><title>How to create datasets for unsloth fine tuning</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1nbw53b/how_to_create_datasets_for_unsloth_fine_tuning/</guid><comments>https://www.reddit.com/r/unsloth/comments/1nbw53b/how_to_create_datasets_for_unsloth_fine_tuning/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1nbw53b/how_to_create_datasets_for_unsloth_fine_tuning/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Title </p><p>Essentially I wanna create a dataset for either personal files</p><p>Or chat to imitate how characters speak / write </p><p>Or imitate the way someone chats </p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>We have a general guide for datasets here:</p><p>We also talk slightly about synthetic data generation: <a href="https://docs.unsloth.ai/basics/datasets-guide">https://docs.unsloth.ai/basics/datasets-guide</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nbw53b/how_to_create_datasets_for_unsloth_fine_tuning/nd502zx/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 09 Sep 2025 00:14:34 +0530</pubDate></item><item><link>https://i.redd.it/biam2gcj2ynf1.png</link><title>You can now run Grok 2.5 locally (120GB RAM). (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1nbo4rj/you_can_now_run_grok_25_locally_120gb_ram/</guid><comments>https://www.reddit.com/r/unsloth/comments/1nbo4rj/you_can_now_run_grok_25_locally_120gb_ram/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1nbo4rj/you_can_now_run_grok_25_locally_120gb_ram/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>You can now run xAI&#39;s Grok 2.5 locally on just 120GB RAM! 🚀</p><p>The 270B parameter model runs at ~5 t/s on a 128GB Mac via our Dynamic 3-bit GGUF.</p><p>Run at full precision with 539GB or use dynamic GGUFs like 3-bit at 118GB (-80% size), where we selectively keep important layers in higher 8-bits.</p><p>📖 You must follow our guide instructions or install the specific Grok 2 llama.cpp PR: <a href="https://docs.unsloth.ai/basics/grok-2">https://docs.unsloth.ai/basics/grok-2</a></p><p>Grok 2 GGUF: <a href="https://huggingface.co/unsloth/grok-2-GGUF">https://huggingface.co/unsloth/grok-2-GGUF</a></p><p>Thanks guys! :)</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/biam2gcj2ynf1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Can we also get MLX version?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nbo4rj/you_can_now_run_grok_25_locally_120gb_ram/nd39fyt/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Your work is highly appreciated. :)</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nbo4rj/you_can_now_run_grok_25_locally_120gb_ram/nd3w5nr/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Thanks Unsloth Team ! Its pretty exciting that we can have commercial model like grok to run locally. Do you had any impression about model performance ?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nbo4rj/you_can_now_run_grok_25_locally_120gb_ram/nd38ghw/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Sounds interesting</p><p>Nice job done</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nbo4rj/you_can_now_run_grok_25_locally_120gb_ram/nd4tx3x/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p><a href="https://preview.redd.it/q9cusex53znf1.png?width=1855&amp;format=png&amp;auto=webp&amp;s=ca057e5574f25ead00d5dc658af0cd5fe6f8679b">https://preview.redd.it/q9cusex53znf1.png?width=1855&amp;format=png&amp;auto=webp&amp;s=ca057e5574f25ead00d5dc658af0cd5fe6f8679b</a></p><p>im getting empty replies . anyone else has this issue ?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nbo4rj/you_can_now_run_grok_25_locally_120gb_ram/nd451gf/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>What&#39;s the pp delta on your 3-bit?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nbo4rj/you_can_now_run_grok_25_locally_120gb_ram/ndcnwsz/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Has any person successfully run these model with any GPU layers offloaded?  5090 32GB and always get CUDA memory error even with the recommended -ot offload flag.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nbo4rj/you_can_now_run_grok_25_locally_120gb_ram/ne8zawo/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Mon, 08 Sep 2025 19:14:25 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1nbggty/is_finetuning_a_12b_model_on_16gb_vram_possible/</link><title>Is finetuning a 12b model on 16gb vram possible?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1nbggty/is_finetuning_a_12b_model_on_16gb_vram_possible/</guid><comments>https://www.reddit.com/r/unsloth/comments/1nbggty/is_finetuning_a_12b_model_on_16gb_vram_possible/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1nbggty/is_finetuning_a_12b_model_on_16gb_vram_possible/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Can I finetune Mistral Nemo 12b Instruct using a 4060 Ti 16gb vram? I can finetune Qwen3 4b with 2048 max tokens and llama3.1 8b with 1024 max tokens on Windows via WSL. However, I don&#39;t know if it is impossible to train 12b under 16gb vram or if it&#39;s just an issue with my settings or library. I encounter OOM with 1024 max tokens. But when I lower it to 500 max tokens, training works, but after some steps, the loss becomes NaN. Can anyone answer me?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I suspect you&#39;re using LoRA for fine tuning isn&#39;t it? If so, you can try QLoRA, which is a Quantized LoRA as the name suggests, maybe that&#39;d work for you without going OOM. Otherwise Kaggle gives out 30 hours of 2 Nvidia T4 GPUs weekly, tho the GPUs are pretty old, you&#39;re going to get 32 GBs of VRAM overall, which is going to be enough for the fine tuning task you&#39;re dealing with right now!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nbggty/is_finetuning_a_12b_model_on_16gb_vram_possible/nd1na5i/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The biggest model you can train on 15GB VRAM is a 22B parameter one but with QLoRA. For LoRA maybe 7B is the largest u can fit in 15GB VRAM. Llama 8B just fits in 16GB VRAM but...unfortunately 16GB is technically 15GB so it overfits</p><p>Remember when you use T4 16GB VRAM GPUs, it&#39;s technically 15GB VRAM since 1GB is reserved for other stuff.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nbggty/is_finetuning_a_12b_model_on_16gb_vram_possible/nd1pvs8/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p><a href="https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements">https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nbggty/is_finetuning_a_12b_model_on_16gb_vram_possible/nd287u1/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>It should work with qlora (the load_in_4bit=True). Look for issues in the max position embeddings, training embed_tokens and lm_head layers - skip training embed_tokens and lm_head if possible. Reduce batch size and increase gradient accumulation steps. How many tokens your dataset has max? Do you have packing enabled?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nbggty/is_finetuning_a_12b_model_on_16gb_vram_possible/nd2ffoj/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The short answer is yes. To start exploring, use the colab script on the Unsloth webpage. It is radically simple.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nbggty/is_finetuning_a_12b_model_on_16gb_vram_possible/nd2y6yq/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Depends what your doing really some stuff is easy</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1nbggty/is_finetuning_a_12b_model_on_16gb_vram_possible/ndlqamm/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Mon, 08 Sep 2025 12:06:38 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1na24vc/request_q4_k_xl_quantization_for_the_new/</link><title>Request: Q4_K_XL quantization for the new distilled Qwen3 30B models</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1na24vc/request_q4_k_xl_quantization_for_the_new/</guid><comments>https://www.reddit.com/r/unsloth/comments/1na24vc/request_q4_k_xl_quantization_for_the_new/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1na24vc/request_q4_k_xl_quantization_for_the_new/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey everyone,</p><p>I recently saw that someone released some new distilled models on Hugging Face and I&#39;ve been testing them out:</p><p>BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32</p><p>BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Fp32</p><p>They seem really promising, especially for coding tasks — in my initial experiments they perform quite well.</p><p>From my experience, however, Q4_K_XL quantization is noticeably faster and more efficient than the more common Q4_K_M quantizations.</p><p>Would it be possible for you to release Q4_K_XL versions of these distilled models? I think many people would benefit from the speed/efficiency gains.</p><p>Thank you very much in advance!</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>are there benchmarks for those models? are they somehow better than their original ones?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1na24vc/request_q4_k_xl_quantization_for_the_new/ncsb8wp/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I did livebench coding with qwen3-coder-30b-a3b-instruct-480b-distill-v2 Q5_K_M, did 54 points. Higher than normal 30B-A3B, and I assume livebenches leaderboard are all FP16?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1na24vc/request_q4_k_xl_quantization_for_the_new/ndkc98o/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 06 Sep 2025 20:47:29 +0530</pubDate></item><item><link>https://i.redd.it/24uff9ie8fnf1.png</link><title>Dynamic 'Kimi-K2-Instruct-0905' Unsloth GGUFs out now! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n9iwbh/dynamic_kimik2instruct0905_unsloth_ggufs_out_now/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n9iwbh/dynamic_kimik2instruct0905_unsloth_ggufs_out_now/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n9iwbh/dynamic_kimik2instruct0905_unsloth_ggufs_out_now/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Most of the important ones including 1, 2, 4, 8-bit (full precision) etc. should be up now! <a href="https://huggingface.co/unsloth/Kimi-K2-Instruct-0905-GGUF">https://huggingface.co/unsloth/Kimi-K2-Instruct-0905-GGUF</a></p><p>You can follow our guide for more info, just make to to change the Kimi-K2 model name to &#39;Kimi-K2-Instruct-0905&#39; and it should work: <a href="https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally">https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally</a> We recommend using Q2_K_XL or larger.</p><p>Thanks so much guys!</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/24uff9ie8fnf1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>need mlx version, Unsloth!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n9iwbh/dynamic_kimik2instruct0905_unsloth_ggufs_out_now/ncnjdqk/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>How does the 1 bit quant fare? Is it coherent/competent?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n9iwbh/dynamic_kimik2instruct0905_unsloth_ggufs_out_now/ncpt5ny/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>To clarify - this runs on 128GB unified DDR + 24GB VRAM? It’s impressive to get that much performance with ~100GB still on an SSD</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n9iwbh/dynamic_kimik2instruct0905_unsloth_ggufs_out_now/ncr0xxj/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Wonder how much hardware it takes to get 20, or perhaps 30 t/s out of this. Such a cool model, seriously wanna play more with it =)</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n9iwbh/dynamic_kimik2instruct0905_unsloth_ggufs_out_now/ncsf67u/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Will test today, the 1 bit one, just barely will fit into my ram 😅</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n9iwbh/dynamic_kimik2instruct0905_unsloth_ggufs_out_now/ncpflad/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Are these Benchmarks with the quants?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n9iwbh/dynamic_kimik2instruct0905_unsloth_ggufs_out_now/ncqctxm/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>8 bit is full precision? When did f32 stop being the standard for full?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n9iwbh/dynamic_kimik2instruct0905_unsloth_ggufs_out_now/ncsafap/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 06 Sep 2025 03:51:57 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n95oh9/is_it_possible_to_create_my_own_unsloth_dynamic/</link><title>Is it possible to create my own unsloth dynamic quants?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n95oh9/is_it_possible_to_create_my_own_unsloth_dynamic/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n95oh9/is_it_possible_to_create_my_own_unsloth_dynamic/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n95oh9/is_it_possible_to_create_my_own_unsloth_dynamic/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I can&#39;t find any documentation about how to replicate unsloth dynamic quants,for exemple, if I finetune my own model using unsloth, and then want to create quantized GGUFs to run it, could I do it the same way unsloth does with the dynamic GGUFs?</p><p>I know I can quantize each layer with a different quant using llama-quantize, but unsloth has a method to find the right quantization for each layer, and I am wondering if it&#39;s documented anywhere how to do it alongside the code necessary.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p><a href="https://github.com/ggml-org/llama.cpp/blob/master/tools/quantize/README.md">https://github.com/ggml-org/llama.cpp/blob/master/tools/quantize/README.md</a></p><p>This is the llama.cpp tool for quantizing to the GGUF format.</p><blockquote><p>I know I can quantize each layer with a different quant using llama-quantize, but unsloth has a method to find the right quantization for each layer, and I am wondering if it&#39;s documented anywhere how to do it alongside the code necessary.</p></blockquote><p>It&#39;s likely trial-and-error. Try to look at their models to see which layers get higher-quantization types, and mimic that.</p><p>Create a calibration dataset for your model, and create your own imatrix.</p><p>Make sure you compare the perplexity of your quantized model versus the unquantized model using this: <a href="https://github.com/ggml-org/llama.cpp/blob/master/tools/perplexity/README.md">https://github.com/ggml-org/llama.cpp/blob/master/tools/perplexity/README.md</a></p><p>If perplexity increases by less than 7%, you did a good job.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n95oh9/is_it_possible_to_create_my_own_unsloth_dynamic/nckrjvf/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Yes - <a href="https://github.com/Thireus/GGUF-Tool-Suite/">https://github.com/Thireus/GGUF-Tool-Suite/</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n95oh9/is_it_possible_to_create_my_own_unsloth_dynamic/ncm3zzr/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Feel free to have a look at <a href="https://www.reddit.com/r/LocalLLaMA/comments/1nj8hee/release_daslab_gguf_nonuniform_quantization">https://www.reddit.com/r/LocalLLaMA/comments/1nj8hee/release_daslab_gguf_nonuniform_quantization</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n95oh9/is_it_possible_to_create_my_own_unsloth_dynamic/neqsjfv/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 05 Sep 2025 19:14:38 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n93onw/finetuning_deepseek_v31/</link><title>Finetuning Deepseek V3.1</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n93onw/finetuning_deepseek_v31/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n93onw/finetuning_deepseek_v31/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1n93onw/finetuning_deepseek_v31/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Is it possible to finetune Deepseek V3.1(not distill versions) using unsloth on a multi gpu setup?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Technically it works via pipeline parallelism but it might not be optimized enough and be very slow.</p><p>But unsure if it fits. We are working on f8 training which should help a lot</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n93onw/finetuning_deepseek_v31/ncjmtzz/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 05 Sep 2025 17:47:56 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n92ut3/how_to_change_a_subtle_behavior_of_model_by_fine/</link><title>How to change a subtle behavior of model by fine tuning?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n92ut3/how_to_change_a_subtle_behavior_of_model_by_fine/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n92ut3/how_to_change_a_subtle_behavior_of_model_by_fine/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n92ut3/how_to_change_a_subtle_behavior_of_model_by_fine/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Situation</p><p>A model I&#39;m using keeps having two quirks, 1) it keeps providing citations when I pressed for it to quote (sources) and when it does start citing, it throws up hallucinated sources. 2) it keeps thinking that a concept is X when that concept is actually Y</p><p>Otherwise the model is perfect. Today after first fine tuning with 400 rows of data the model completely broken and became lowish IQ. The verbosity of the model became super brief as well to match the fine tune dataset.</p><p>Because I just need to shape the 2 small behaviors above, are there any advice for me?</p><p>Should I limit my dataset to even small and focus on these 2 points only and then lower the LR?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>During fine-tuning, there are two main things you should do:</p><ol><li><p>Combine your dataset and mix it with a higher quality open source conversational day set like sharegpt, ultrachat etc. You can edit the weightings of each dataset.</p></li><li><p>You should augment your dataset with question answer pairs, summarizations of your dataset, etc - ie you need to expand your dataset with synthetic data generation - we have a synthetic data generation notebook on our repo.</p></li></ol></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n92ut3/how_to_change_a_subtle_behavior_of_model_by_fine/ncjkxli/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Try PEFT with very low ranks (try setting r in your LoRA configs to 8 and 16) and low alpha to avoid interfering too much with the original model.</p><p>Failing that, you might need to do RL. Something like GRPO can help steer model behavior without modifying too much too many of the underlying weights.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n92ut3/how_to_change_a_subtle_behavior_of_model_by_fine/ncqrc1y/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I don&#39;t think fine-tuning will help, you need grounding since the model is not able to answer factual queries well. Did you try with web search or RAG?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n92ut3/how_to_change_a_subtle_behavior_of_model_by_fine/ncjg5fs/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 05 Sep 2025 17:07:26 +0530</pubDate></item><item><link>https://i.redd.it/cqmnotmh96nf1.png</link><title>Unsloth Memory Efficient Reinforcement Learning (RL) is here! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n8efil/unsloth_memory_efficient_reinforcement_learning/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n8efil/unsloth_memory_efficient_reinforcement_learning/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n8efil/unsloth_memory_efficient_reinforcement_learning/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys, as you know RL used to be memory hungry, but we&#39;ve made lots of advancements this year to make it work on consumer hardware. Now, it&#39;s even more efficient! :)</p><p>We&#39;re introducing Unsloth&#39;s new kernels &amp; algorithms that allows faster RL training with 50% less VRAM, 10× more context length &amp; no accuracy loss.</p><p>Our main feature includes Unsloth Standby. Before, RL requires GPU splitting between training &amp; inference. With Unsloth Standby, you no longer have to.</p><p>⭐Read our educational blog for details, functionality and more: <a href="https://docs.unsloth.ai/basics/memory-efficient-rl">https://docs.unsloth.ai/basics/memory-efficient-rl</a></p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/cqmnotmh96nf1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Thank you so much for your continued hard work when producing my own reinforcement learning algorithms backed by unsloth the main cost by far was the need to use high-end GPU for high context. Should be able to switch back to local now what I do wouldn’t be possible without you guys and I’m sure many other feel the same way!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n8efil/unsloth_memory_efficient_reinforcement_learning/ncec64q/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Also VLM GRPO should be out next week guys hopefully!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n8efil/unsloth_memory_efficient_reinforcement_learning/ncehkol/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Unsloth you’ve taught me more than any other resource. Tysm I’m going to fill a boat with cocaine and ballerinas thanks to you.</p><p>Edit - no cocaine, Pink Molly is the new new</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n8efil/unsloth_memory_efficient_reinforcement_learning/ncf0b06/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Congrats Daniel and the Unsloth team! Great work.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n8efil/unsloth_memory_efficient_reinforcement_learning/ncegon2/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Great news! Thanks for the hard work. Looking forward to heating up a H100! ⚡️</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n8efil/unsloth_memory_efficient_reinforcement_learning/ncen4xz/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I understood nothing except it&#39;s cool</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n8efil/unsloth_memory_efficient_reinforcement_learning/ncf1e6c/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>It can run in RTX 5090?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n8efil/unsloth_memory_efficient_reinforcement_learning/ncejd9k/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 04 Sep 2025 21:39:56 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n80cno/updated_dynamic_deepseekv31_ggufs_upgraded/</link><title>Updated Dynamic DeepSeek-V3.1 GGUFs - upgraded performance! 🐋</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n80cno/updated_dynamic_deepseekv31_ggufs_upgraded/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n80cno/updated_dynamic_deepseekv31_ggufs_upgraded/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n80cno/updated_dynamic_deepseekv31_ggufs_upgraded/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys, we reuploaded the DeepSeek-V3.1 quants and according to 3rd party Aider polyglot benchmarks, they&#39;re even better than before: <a href="https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF">https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF</a></p><p>We&#39;ll announce the amazing benchmark results likely next week, yes you will need to redownload.</p><p>The benchmarks are 90% done already and we compared them other quants and our previous quants and the results are clearly an improvement.</p><p>We converted DeepSeek-V3.1 using our normal conversion, however we needed to update it as we didn&#39;t know llama.cpp overrode some of our layer quantization for conversion so we needed to change reupload them. The quants should only be a few MB bigger but the increase in accuracy is very large.</p><p>Guide to run should remain the same: <a href="https://docs.unsloth.ai/basics/deepseek-v3.1-how-to-run-locally">https://docs.unsloth.ai/basics/deepseek-v3.1-how-to-run-locally</a></p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Awesome!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n80cno/updated_dynamic_deepseekv31_ggufs_upgraded/ncbgq8n/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Which benchmarks should I look for coding and natural language?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n80cno/updated_dynamic_deepseekv31_ggufs_upgraded/ncbwoqv/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Ah what a disappointment it won&#39;t fit in 128GB hardware </p><p>Anyway thanks for you work</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n80cno/updated_dynamic_deepseekv31_ggufs_upgraded/ncc8ty0/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p><a href="https://www.reddit.com/u/yoracale">u/yoracale</a> does the llama.cpp &#39;override&#39; issue apply to any other unsloth quants (of other models)?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n80cno/updated_dynamic_deepseekv31_ggufs_upgraded/ncrmonf/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>How do you know if the benchmarks are &quot;amazing&quot; if you haven&#39;t completed them yet? Getting tired of hype without substance.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n80cno/updated_dynamic_deepseekv31_ggufs_upgraded/ncbm37b/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 04 Sep 2025 09:40:14 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n7sbak/new_to_llm_finetuning_and_trying_to_find_the_best/</link><title>New to LLM Fine-tuning and trying to find the best training method for my personal application.</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n7sbak/new_to_llm_finetuning_and_trying_to_find_the_best/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n7sbak/new_to_llm_finetuning_and_trying_to_find_the_best/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n7sbak/new_to_llm_finetuning_and_trying_to_find_the_best/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hello! I&#39;m looking to create an AI assistant for my personal planner app that has both canvas and g-cal integration, displays assignments, my daily schedule, and an organized calendar. I have already completed most of the UI for my app and the backend is nearly finished as well. I&#39;m currently looking to add an AI agent that I can use to control functionality on my app by running some methods I&#39;ve created that will edit the UI and also push assignments/events onto g-cal. Basically, I want to have the AI assistant both engage in conversation with me, and generate a formulaic reply that runs some of my methods and is readable by my application. Originally, I thought the best method to get this to work would be fine-tuning an existing LLM with a dataset that I created which replicated the functionality I needed. I also considered the option of simply feeding the API for my app to an LLM and instructing it with how to generate responses. What would you guys recommend in terms of the exact use case I&#39;m trying to fill? Any help is much appreciated, thanks in advance for your time.</p></div><!-- SC_ON --></section>]]></description><pubDate>Thu, 04 Sep 2025 03:28:49 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n6bfnf/how_to_run_unsloth_on_hpc/</link><title>How to run unsloth on HPC</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n6bfnf/how_to_run_unsloth_on_hpc/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n6bfnf/how_to_run_unsloth_on_hpc/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n6bfnf/how_to_run_unsloth_on_hpc/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey, I&#39;m a newbie to unsloth and AI in general, I&#39;ve gotten unsloth working on a local PC but need more firepower so hoping to run it on my university&#39;s HPC. I can give whatever details are needed about the system but not sure what&#39;s relevant that I can provide here so please tell me what I need to provide. </p><p>I tried writing and running the python code from the notebook on the HPC and it failed since unsloth wasn&#39;t installed in the python environment. Then I tried creating a singularity container as per HPC documentation and containering everything I thought was needed and that failed cuz the container couldn&#39;t access the GPU (needs Nvidia container toolkit or sthg and admins refused to install it for me). </p><p>Now I&#39;m lost. Idk what I should be doing to run unsloth and finetune my models on the HPC. Are there any other methods I have missed ? Or is there no other choice but to get the admins to help out ?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Unsloth multi gpu is not ready yet - try this modifications</p><p><a href="https://github.com/thad0ctor/unsloth-5090-multiple">https://github.com/thad0ctor/unsloth-5090-multiple</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n6bfnf/how_to_run_unsloth_on_hpc/nbz8x12/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>You may want to try huggingface trl if you have multiple gpus, from my understanding, they’re slower and less efficient, but wall clock time is most important if you have a bunch of gpus</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n6bfnf/how_to_run_unsloth_on_hpc/ncel6p7/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>What HPC scheduler is your university running? Is it some kind of Slurm+Enroot with Pyxis?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n6bfnf/how_to_run_unsloth_on_hpc/ncfsli5/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 02 Sep 2025 11:13:29 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n3olwr/does_unsloth_support_mamba_architecture/</link><title>Does Unsloth support mamba architecture?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n3olwr/does_unsloth_support_mamba_architecture/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n3olwr/does_unsloth_support_mamba_architecture/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n3olwr/does_unsloth_support_mamba_architecture/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I&#39;m quite interested in the new Nvidia Nano models and Falcon H1 series. I&#39;m wondering if Unsloth support finetuning these models?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Yes we do, Unsloth is the only framework that supports all transformer based models including TTS, BERT, etc. and this including state space/mamba models</p><p>Notebooks: <a href="https://github.com/unslothai/notebooks?tab=readme-ov-file#linear-attention-notebooks">https://github.com/unslothai/notebooks?tab=readme-ov-file#linear-attention-notebooks</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n3olwr/does_unsloth_support_mamba_architecture/nbfnblm/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 30 Aug 2025 06:34:03 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n36aqg/can_someone_explain_to_me_why_the_number_of/</link><title>Can someone explain to me why the number of parameters are different in an unsloth quant?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n36aqg/can_someone_explain_to_me_why_the_number_of/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n36aqg/can_someone_explain_to_me_why_the_number_of/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n36aqg/can_someone_explain_to_me_why_the_number_of/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I thought quants were not supposed to change norms/biases/other parameters in a model. </p><p>However, when i look at the original Kimi K2, i see a lot of small tensors like size [5, 56]</p><p><a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct/blob/main/model-1-of-61.safetensors">https://huggingface.co/moonshotai/Kimi-K2-Instruct/blob/main/model-1-of-61.safetensors</a></p><p>These are missing in the unsloth quant: </p><p><a href="https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/blob/main/UD-Q4_K_XL/Kimi-K2-Instruct-UD-Q4_K_XL-00001-of-00013.gguf">https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/blob/main/UD-Q4_K_XL/Kimi-K2-Instruct-UD-Q4_K_XL-00001-of-00013.gguf</a></p><p>What&#39;s happening here? Why do these tensors disappear?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Because that&#39;s float8. We converted f8 to bf16 those are scaling factors so they disappear when they get converted and they aren&#39;t necessary anymore</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n36aqg/can_someone_explain_to_me_why_the_number_of/nbe14va/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 29 Aug 2025 18:08:36 +0530</pubDate></item><item><link>https://i.redd.it/5q7trkshislf1.png</link><title>OpenAI gpt-oss Ultra Long Context is here! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n2hl06/openai_gptoss_ultra_long_context_is_here/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n2hl06/openai_gptoss_ultra_long_context_is_here/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1n2hl06/openai_gptoss_ultra_long_context_is_here/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys we&#39;ve got LOTS of updates for gpt-oss training today! We’re excited to introduce Unsloth Flex Attention support for OpenAI gpt-oss training that enables <strong>&gt;8× longer context lengths</strong>, <strong>&gt;50% less VRAM usage</strong> and <strong>&gt;1.5× faster training</strong> vs. all implementations including those using Flash Attention 3 (FA3). Unsloth Flex Attention makes it possible to train with a <strong>60K context length</strong> on just 80GB of VRAM for BF16 LoRA. Also:</p><ul><li>You can now export/save your QLoRA fine-tuned gpt-oss model to llama.cpp, vLLM, Ollama or HF</li><li>We <strong>fixed gpt-oss training losses going to infinity</strong> on float16 GPUs (like T4 Colab)</li><li>We fixed gpt-oss implementation issues irrelevant to Unsloth, most notably ensuring that <code>swiglu_limit = 7.0</code> is properly applied during MXFP4 inference in transformers</li><li>Unsloth Flex Attention scales with context, longer sequences yield bigger savings in both VRAM and training time</li></ul><p>🦥 Would highly recommend you guys to read our blog which has all the bug fixes, guides, details, explanations, findings etc. and it&#39;ll be really educational: <a href="https://docs.unsloth.ai/basics/long-context-gpt-oss-training">https://docs.unsloth.ai/basics/long-context-gpt-oss-training</a></p><p>We&#39;ll likely release our gpt-oss training notebook with direct saving capabilities to GGUF, llama.cpp next week.<br/>And we&#39;ll be releasing third-party Aider polygot benchmarks for DeepSeek-V3.1 next week. You guys will be amazed at how well IQ1_M performs!<br/>And next week we&#39;ll have another great update for RL! 😉<br/>And you can support our announcement tweet here: <a href="https://x.com/UnslothAI/status/1961108732361994248">https://x.com/UnslothAI/status/1961108732361994248</a></p><p>Thanks guys for reading and hope you all have a lovely Friday and long weekend,<br/>Mike! 🦥</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/5q7trkshislf1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Is there any framework available to train gpt-oss on a mac?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n2hl06/openai_gptoss_ultra_long_context_is_here/nb7fkyz/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Finally I can stop fintuning Mistral 7b and switch to gpt 20b.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n2hl06/openai_gptoss_ultra_long_context_is_here/nb6t3t8/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Noob here</p><p>So does it mean i have more context length if keep adding vram</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n2hl06/openai_gptoss_ultra_long_context_is_here/nb61fke/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>This is awesome, thanks guys!!!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n2hl06/openai_gptoss_ultra_long_context_is_here/nb7m9ai/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Yessssss thank you</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n2hl06/openai_gptoss_ultra_long_context_is_here/nb7q5xp/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Still not support GRPO?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n2hl06/openai_gptoss_ultra_long_context_is_here/nba2r62/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>So what would be the required VRAM now?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n2hl06/openai_gptoss_ultra_long_context_is_here/nblpbgu/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 28 Aug 2025 22:21:16 +0530</pubDate></item><item><link>https://www.reddit.com/gallery/1n1jq1l</link><title>Q5_K_XL and Q6_K_XL on 5-shot MMLU graph (Gallery)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n1jq1l/q5_k_xl_and_q6_k_xl_on_5shot_mmlu_graph/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n1jq1l/q5_k_xl_and_q6_k_xl_on_5shot_mmlu_graph/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1n1jq1l/q5_k_xl_and_q6_k_xl_on_5shot_mmlu_graph/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>In the 5-shot MMLU graph on this page: <a href="https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs">https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs</a></p><p>Where do Q5_K_XL and Q6_K_XL fall? Curious how they compare to the other quants.</p><p>neolithic has been running the various unsloth quants of DeepSeek V3.1 in non-thinking mode under llama.cpp against the Aider Polyglot Benchmark and posting the results in Discord. So far the results seem to loosely match the MMLU graph (Q3 is a little weird), but we don&#39;t have MMLU graph data for these two quants.</p><p>Disclaimers: I&#39;m not an expert graph maker. The axis don&#39;t really line up and while the graph with pass_rate_1 and pass_rate_2  shows a good comparison between those two passes, I feel like it loses the plot if the goal is to compare against MMLU. I also don&#39;t know what MMLU means. lol. Further, I guessed the MMLU numbers because I didn&#39;t see a data table. I may have guessed wrong.</p></div><!-- SC_ON --></section><section class='embedded-media'><p><img src="https://preview.redd.it/hq9tejr6vklf1.png?width=1970&amp;format=png&amp;auto=webp&amp;s=f2c72ef87744ead5fd2154158175a3cf56b53a39" height="896" width="1970" /></p><p><img src="https://preview.redd.it/8giygjr6vklf1.png?width=1972&amp;format=png&amp;auto=webp&amp;s=4fddff2b4e5ee7b474bed7594ce8a3502c425409" height="902" width="1972" /></p></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Aider polyglot is a very different benchmark from 5shot mmlu so you definitely can&#39;t compare the 2.</p><p>Q5_k_xl should be in between the benchmarks for Q5 and Q6 and Q6_k_xl should be equal to full precision already as Q6 already reaches it</p><p>Also we&#39;re exciting to be posting about aider polyglot scores for DeepSeek v3.1 soon thanks to neonilithic!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n1jq1l/q5_k_xl_and_q6_k_xl_on_5shot_mmlu_graph/naytvog/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>UD doesn&#39;t have much advantage after Q4.   Seems to be reasonable conclusion.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n1jq1l/q5_k_xl_and_q6_k_xl_on_5shot_mmlu_graph/nb0dwfn/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Q3 k xl the best bang for the buck?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n1jq1l/q5_k_xl_and_q6_k_xl_on_5shot_mmlu_graph/nb1wy2f/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I do not get it.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n1jq1l/q5_k_xl_and_q6_k_xl_on_5shot_mmlu_graph/nb38z91/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>If possible release more data about Q5_k_xl UD performance in models. I bet is very very close to FP16 performance.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n1jq1l/q5_k_xl_and_q6_k_xl_on_5shot_mmlu_graph/nb49w23/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Wed, 27 Aug 2025 20:41:36 +0530</pubDate></item><item><link>https://www.reddit.com/gallery/1n0lt4r</link><title>[Experiment] 10-min QLoRA Fine-Tuning on 240 QAs (ROUGE-L doubled, SARI +15) (Gallery)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n0lt4r/experiment_10min_qlora_finetuning_on_240_qas/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n0lt4r/experiment_10min_qlora_finetuning_on_240_qas/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1n0lt4r/experiment_10min_qlora_finetuning_on_240_qas/'>Post permalink</a></p></section><section class='embedded-media'><p><img src="https://preview.redd.it/xop6it558dlf1.png?width=1484&amp;format=png&amp;auto=webp&amp;s=655070f4ed341f8cddbf3e31a6464996909c3ed6" height="1068" width="1484" /></p><p><img src="https://preview.redd.it/qd0iou558dlf1.png?width=1420&amp;format=png&amp;auto=webp&amp;s=f519646d1f9de66e77d168392465e7642e13ad11" height="1068" width="1420" /></p></section>]]></description><pubDate>Tue, 26 Aug 2025 18:56:00 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1n0ehqf/thank_you_for_the_5090_support/</link><title>Thank you for the 5090 support!</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1n0ehqf/thank_you_for_the_5090_support/</guid><comments>https://www.reddit.com/r/unsloth/comments/1n0ehqf/thank_you_for_the_5090_support/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1n0ehqf/thank_you_for_the_5090_support/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I was sooo happy tonight to have PyTorch and Unsloth do their magic on my 5090; it&#39;s amazing.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Oh we haven&#39;t officially announced it yet but are going to soon with NVIDIA. Thank you so much for trying it out! :D</p><p>Does &#39;pip install unsloth&#39; directly work on your device btw?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n0ehqf/thank_you_for_the_5090_support/naq47ao/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Hey that&#39;s great! What happened?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n0ehqf/thank_you_for_the_5090_support/naq273m/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>!Remind me 3 days</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n0ehqf/thank_you_for_the_5090_support/narykpf/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>hows it running? did you have a 4090 before to compare?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1n0ehqf/thank_you_for_the_5090_support/nas071g/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 26 Aug 2025 12:01:19 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mzraar/facing_runtimeerror_unsloth_vllm_process_failed/</link><title>Facing "RuntimeError: Unsloth: vllm_process failed to load!"</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mzraar/facing_runtimeerror_unsloth_vllm_process_failed/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mzraar/facing_runtimeerror_unsloth_vllm_process_failed/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mzraar/facing_runtimeerror_unsloth_vllm_process_failed/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi, Can anyone help me to solve the below error while trying to use the predefined colab notebook of Unsloth for the synthetic data kit. I&#39;m even using an A100 GPU from Colab: </p><pre><code>🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.INFO 08-25 13:54:40 [__init__.py:241] Automatically detected platform cuda.🦥 Unsloth Zoo will now patch everything to make training faster!Unsloth: Patching vLLM v1 graph captureUnsloth: Patching vLLM v0 graph captureUnsloth: Using dtype = torch.bfloat16 for vLLM.Unsloth: vLLM loading unsloth/Llama-3.2-3B-Instruct with actual GPU utilization = 89.06%Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 39.56 GB.Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 320.Unsloth: vLLM&#39;s KV Cache can use up to 29.25 GB. Also swap space = 6 GB.Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.vLLM STDOUT: INFO 08-25 13:55:04 [__init__.py:241] Automatically detected platform cuda.Stdout stream ended before readiness message detected.---------------------------------------------------------------------------RuntimeError                              Traceback (most recent call last) in &lt;cell line: 0&gt;()      1 from unsloth.dataprep import SyntheticDataKit      2 ----&gt; 3 generator = SyntheticDataKit.from_pretrained(      4     # Choose any model from       5     model_name = &quot;unsloth/Llama-3.2-3B-Instruct&quot;,/tmp/ipython-input-2164116524.pyhttps://huggingface.co/unsloth in __init__(self, model_name, max_seq_length, gpu_memory_utilization, float8_kv_cache, conservativeness, token, **kwargs)    147         while not self.check_vllm_status():    148             if trial &gt;= 100:--&gt; 149                 raise RuntimeError(&quot;Unsloth: vllm_process failed to load!&quot;)    150             trial += 1    151             time.sleep(1)/usr/local/lib/python3.12/dist-packages/unsloth/dataprep/synthetic.pyRuntimeError: Unsloth: vllm_process failed to load!</code></pre></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Hi we&#39;re going to investigate, unfortunately the synthetic data notebook is not maintained from our side as much as it is using Meta&#39;s library.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mzraar/facing_runtimeerror_unsloth_vllm_process_failed/nawcc55/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Mon, 25 Aug 2025 19:30:29 +0530</pubDate></item><item><link>https://huggingface.co/unsloth/Seed-OSS-36B-Instruct-GGUF</link><title>ByteDance Seed-OSS Dynamic GGUFs out now! (huggingface.co)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mzcpx4/bytedance_seedoss_dynamic_ggufs_out_now/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mzcpx4/bytedance_seedoss_dynamic_ggufs_out_now/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mzcpx4/bytedance_seedoss_dynamic_ggufs_out_now/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys due to high demand, we&#39;ve released Dynamic imatrix quantized GGUFs for seed-oss. Currently only works in llama.cpp or tools which support the latest version of llama.cpp.</p><p>Thanks and let us know how they are! :)</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Thank you again 😊</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mzcpx4/bytedance_seedoss_dynamic_ggufs_out_now/naih946/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Is dataset for Imatrix generation is private? Curious will it be big difference then original quants, thanks!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mzcpx4/bytedance_seedoss_dynamic_ggufs_out_now/nam4ao5/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>So no Ollama support yet I’m guessing?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mzcpx4/bytedance_seedoss_dynamic_ggufs_out_now/namtrcx/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I&#39;m curious people&#39;s use-cases for this model!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mzcpx4/bytedance_seedoss_dynamic_ggufs_out_now/nb6tapg/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Mon, 25 Aug 2025 06:20:09 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1myspiq/gemma3_unsloth_template_error/</link><title>Gemma-3 Unsloth template error</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1myspiq/gemma3_unsloth_template_error/</guid><comments>https://www.reddit.com/r/unsloth/comments/1myspiq/gemma3_unsloth_template_error/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1myspiq/gemma3_unsloth_template_error/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi guys... I try to make fintune of Gemma-3-270M but always get this error when i try to save it like gguf... Any ideas what is wrong with unsloth google collab template?</p><p>i</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Hi there apologies for the issue we just fixed it: <a href="https://github.com/unslothai/notebooks/pull/88">https://github.com/unslothai/notebooks/pull/88</a></p><p>Can you rerun and restart the notebooks and see if it works?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1myspiq/gemma3_unsloth_template_error/nahhcgv/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sun, 24 Aug 2025 16:34:35 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1myrk9o/making_some_silly_mistake_while_saving_to_gguf/</link><title>Making some silly mistake while saving to GGUF from Lora?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1myrk9o/making_some_silly_mistake_while_saving_to_gguf/</guid><comments>https://www.reddit.com/r/unsloth/comments/1myrk9o/making_some_silly_mistake_while_saving_to_gguf/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 3 min | <a href='https://www.reddit.com/r/unsloth/comments/1myrk9o/making_some_silly_mistake_while_saving_to_gguf/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi</p><p>I ran a training run earlier on gemma3-270m and created a lora, which I saved in my google drive. I did not at that point save a gguf.</p><p>So now when I use colab and download the Lora and attempt to create a gguf, I&#39;m getting an error. I haven&#39;t done a save to gguf ever earlier, so I am not sure if I am making some silly mistake. Basically just copied the code from the official notebook and ran it, but not working. Can someone take a look.</p><p>My code:```</p><p>from google.colab import drive</p><p>drive.mount(&#39;/content/drive&#39;)</p><p>!cp -r /content/drive/MyDrive/stuff/lora_model .</p><p>from transformers import TextStreamer</p><p>from unsloth import FastModel</p><p>import torch</p><p>from unsloth import FastLanguageModel</p><p>from peft import PeftModel</p><p>max_seq_length = 3072</p><p>model, tokenizer = FastLanguageModel.from_pretrained(</p><pre><code>model_name = &quot;unsloth/gemma-3-270m-it&quot;, # YOUR MODELmax_seq_length = max_seq_length,load_in_4bit = False,  # 4 bit quantization to reduce memoryload_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memoryfull_finetuning = False, # [NEW!] We have full finetuning now!</code></pre><p>)</p><p>model = PeftModel.from_pretrained(model, &quot;lora_model&quot;)</p><pre><code>text = \[MY TESTING SAMPLE HERE\]</code></pre><p>_ = model.generate(</p><pre><code>**tokenizer(text, return_tensors = &quot;pt&quot;).to(&quot;cuda&quot;),max_new_tokens = 125,temperature = 1, top_p = 0.95, top_k = 64,streamer = TextStreamer(tokenizer, skip_prompt = True),</code></pre><p>)</p><p>print(&#39;\n+++++++++++++++++++++++++++++\n&#39;)</p><p>model.save_pretrained_merged(&quot;model&quot;, tokenizer, save_method = &quot;merged_16bit&quot;)</p><p>model.save_pretrained_gguf(&quot;model&quot;, tokenizer, quantization_method = &quot;q8_0&quot;)</p><p>```The load and inference run fine. Inference is in the finetuned format as expected. But when the GGUF part starts up, get this error.</p><p>If I run just the GGUF saving, then it says input folder not found, I guess because there is no model folder?</p><pre><code>/usr/local/lib/python3.12/dist-packages/unsloth\_zoo/saving\_utils.py:632: UserWarning: Model is not a PeftModel (no Lora adapters detected). Skipping Merge. Please use save\_pretrained() or push\_to\_hub() instead!warnings.warn(&quot;Model is not a PeftModel (no Lora adapters detected). Skipping Merge. Please use save\_pretrained() or push\_to\_hub() instead!&quot;)\---------------------------------------------------------------------------RuntimeError                              Traceback (most recent call last)/tmp/ipython-input-1119511992.py in &lt;cell line: 0&gt;()1 model.save\_pretrained\_merged(&quot;model&quot;, tokenizer, save\_method = &quot;merged\_16bit&quot;)\----&gt; 2 model.save\_pretrained\_gguf(&quot;model&quot;, tokenizer, quantization\_method = &quot;q8\_0&quot;)2 frames/usr/local/lib/python3.12/dist-packages/unsloth\_zoo/llama\_cpp.py in convert\_to\_gguf(input\_folder, output\_filename, quantization\_type, max\_shard\_size, print\_output, print\_outputs)654655     if not os.path.exists(input\_folder):\--&gt; 656         raise RuntimeError(f&quot;Unsloth: \`{input\_folder}\` does not exist?&quot;)657658     config\_file = os.path.join(input\_folder, &quot;config.json&quot;)RuntimeError: Unsloth: \`model\` does not exist?</code></pre><p>I also tried loading just the lora and then running inference.```    model, tokenizer = FastLanguageModel.from_pretrained(</p><pre><code>model_name = &quot;lora_model&quot;, # YOUR MODELmax_seq_length = max_seq_length,load_in_4bit = False,  # 4 bit quantization to reduce memoryload_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memoryfull_finetuning = False, # [NEW!] We have full finetuning now!)</code></pre><p>```</p><p>In such cases, the inference is the same as the vanilla untuned model and my  finetuning does not take effect.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Hi there apologies for the issue we just fixed it: <a href="https://github.com/unslothai/notebooks/pull/88">https://github.com/unslothai/notebooks/pull/88</a></p><p>Can you rerun and restart the notebooks and see if it works?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1myrk9o/making_some_silly_mistake_while_saving_to_gguf/nahhd50/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sun, 24 Aug 2025 15:26:45 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1myqw99/fine_tuned_qwen_model_following_grpo_notebook/</link><title>Fine tuned Qwen model following GRPO notebook sometimes infinitely repeats lines</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1myqw99/fine_tuned_qwen_model_following_grpo_notebook/</guid><comments>https://www.reddit.com/r/unsloth/comments/1myqw99/fine_tuned_qwen_model_following_grpo_notebook/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1myqw99/fine_tuned_qwen_model_following_grpo_notebook/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi all, </p><p>Getting into fine tuning LLMs and have currently been following the Qwen 4 GRPO notebook (<a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb">https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb</a> ) that shows how to train a model to have deepseek style reasoning traces. However, after training and when testing the model (exported model and run on llama.cpp), I notice that the model will more often than not end up repeating a sentence or two endlessly (e.g. in the reasoning CoT, model gets “stuck” and endlessly repeats a line, for example “step 10: {some math calculation}\nstep 10: {some math calculation}\n… “, or something like sentence1\nsentence2\nsentence1… etc.) on a prompt. It sometimes produces the correct answer in the expected format, but more often than not it does the above, even when on the right track.</p><p>I’ve tried training from the qwen3 4b base model and the 2507 instruct variant (thinking that maybe since the instruct is trained for instruction following and already “understands” the chat template but to no avail). I’ve also rented an a100 for a bit to see if a larger model (qwen3-30b) would have same issue, but seems like I run into the same problem.</p><p>I’ve currently been using a custom synthetically generated dataset with 665 rows, with approx. 30pct of them being general conversational text and the other 70% being domain specific questions (in this case mostly math and code related questions), in the same format as the unsloth/openmathreasoning-mini dataset used as a primer dataset. Settings for that part is left basically default (num epoch set to 2, etc). The GRPO trainer after uses dataset with both code and mathematical questions, with similar reward functions to the original notebook, with mathematical questions graded on correctness and code based on how much testcases passed (I’ve also added a reward function to penalize constant repeat of lines), and I’ve trained for about 500 steps.</p><p>I’ve noticed a few issues similar to this, but the mentioned fixes seem to always be related to chat template issues, whereas my fine tuned model will have this issue sometimes but not always. I have been experimenting with using the qwen3 chat template with tool call support, but the issue is present on the base chatML style chat template used during finetuning as well.</p><p>I’m curious on any ideas how I can solve this issue. I’ve tried presence/repeat/frequency penalty, but it doesn’t really work out and ultimately is only a bandaid fix. Is the “primer” dataset too large or overfitting the model? Do I need to run the GRPO trainer for more steps? I’m running it for “only” about 500 steps, is this too little/not enough? Should the dataset for my GRPO trainer be more diverse? </p><p>I’m only a traditional programmer and have only dabbled in computer vision before, a bit lost in LLM training lol, any suggestions and help would be extremely appreciated. Thanks!</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I had the same issue with Qwen, so i moved to gemma 3. Similar things were happening (it would write a good solution and then just babble endlessly) and i eventually realised i needed to pass the stop token ids in my gen params. I havent gone back to check whether this might fix the qwen issues, though maybe not, since the endless repetitions werent happening on even generation, though it was a sizeable portion. I lost weeks trying to get qwen to work 😭 gemma3 is working great though now.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1myqw99/fine_tuned_qwen_model_following_grpo_notebook/naef2kx/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I&#39;ve tried some months ago, I think in march to use unsloth for training qwen with GRPO, and I didn&#39;t get any error. I don&#39;t like the rewards given by default in the notebook. Let&#39;s check if the rewards are the issues or try to move toward an earlier version of unsloth. What I have found is that, in the version that I was using, setting ratio&gt;1 leads toward a freezed training. The train never starts.</p><p>However if you like you can use GTPO that is an algorithm inspired from GRPO that leads toward better results for qwen in math tests. It is based on unsloth and  here there is the <a href="https://colab.research.google.com/github/winstonsmith1897/GTPO/blob/main/colab/GTPO_training_example.ipynb">Colab notebook</a> and the <a href="https://github.com/winstonsmith1897/GTPO">GitHub page</a>.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1myqw99/fine_tuned_qwen_model_following_grpo_notebook/naeh39d/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Whats your hyperparameters during inference, e.g., temperature?</p><p>Also what&#39;s the chat format used in your unsloth notebook? Is it the one qwen recommends or some other version?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1myqw99/fine_tuned_qwen_model_following_grpo_notebook/nahj3n2/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sun, 24 Aug 2025 14:44:43 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mxz9bg/ampere_issue/</link><title>Ampere Issue</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mxz9bg/ampere_issue/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mxz9bg/ampere_issue/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mxz9bg/ampere_issue/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I am trying to use unsloth for fine tuning. Unfortunately, I have trouble satisfying dependencies for a couple of days now. There is a conflict<br/>The Base Package (unsloth) requires xformers &gt;= 0.0.27.post2 while The GPU-Specific Package (unsloth[cu121-ampere]) requires xformers == 0.0.22.post7. Can anyone help? I have a paper submission deadline by end of month and without this, we will not be able to submit.</p><pre><code>+-----------------------------------------------------------------------------------------+| NVIDIA-SMI 570.169                Driver Version: 570.169        CUDA Version: 12.8     ||-----------------------------------------+------------------------+----------------------+| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC || Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. ||                                         |                        |               MIG M. ||=========================================+========================+======================||   0  NVIDIA RTX A6000               Off |   00000000:3B:00.0 Off |                  Off || 30%   28C    P8              9W /  300W |       4MiB /  49140MiB |      0%      Default ||                                         |                        |                  N/A |+-----------------------------------------+------------------------+----------------------++-----------------------------------------------------------------------------------------+| Processes:                                                                              ||  GPU   GI   CI              PID   Type   Process name                        GPU Memory ||        ID   ID                                                               Usage      ||=========================================================================================||  No running processes found                                                             |+-----------------------------------------------------------------------------------------+[project]# pyproject.toml[project]name = &quot;unsloth fine tuning&quot;version = &quot;0.1.0&quot;description = &quot;Local tools&quot;requires-python = &quot;&gt;=3.11&quot;dependencies = [    # --- Core Dependencies ---    &quot;pandas&quot;, &quot;sacrebleu&quot;, &quot;unbabel-comet&quot;, &quot;rouge-score&quot;,    &quot;sentence-transformers&quot;, &quot;openpyxl&quot;, &quot;nltk&gt;=3.9.1&quot;, &quot;httpx&quot;,    &quot;requests&quot;, &quot;pydantic&quot;, &quot;pydantic-settings&quot;,    &quot;unsloth[cu121-ampere]&quot;,    &quot;transformers&gt;=4.41&quot;, &quot;datasets&quot;, &quot;peft&quot;, &quot;bitsandbytes&quot;,    &quot;trl&quot;, &quot;accelerate&quot;, &quot;optuna&quot;,]</code></pre><p>This is my dockerfile  </p><pre><code>FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04ENV DEBIAN_FRONTEND=noninteractiveRUN apt-get update &amp;&amp; apt-get install -y \    python3.11 \    python3.11-venv \    python3-pip \    git \    curl \    gnupg \    lsb-release \    cmake \    &amp;&amp; rm -rf /var/lib/apt/lists/*# Install Docker CLIRUN curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg &amp;&amp; \    echo &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(. /etc/os-release &amp;&amp; echo $VERSION_CODENAME) stable&quot; | tee /etc/apt/sources.list.d/docker.list &gt; /dev/null &amp;&amp; \    apt-get update &amp;&amp; \    apt-get install -y docker-ce-cli &amp;&amp; \    rm -rf /var/lib/apt/lists/*# Install Ollama CLIRUN curl -fsSL https://ollama.com/install.sh | shWORKDIR /installCOPY pyproject.toml ./RUN python3.11 -m pip install --upgrade pip uvRUN uv venv /opt/venv --clearENV PATH=&quot;/opt/venv/bin:$PATH&quot;RUN uv sync --extra-index-url https://download.pytorch.org/whl/cu121 --index-strategy unsafe-best-match --prerelease=allowWORKDIR /workspaceRUN useradd --create-home --shell /bin/bash unslothRUN chown -R unsloth:unsloth /workspaceUSER unslothENV SHELL=/bin/bash</code></pre></div><!-- SC_ON --></section>]]></description><pubDate>Sat, 23 Aug 2025 17:14:04 +0530</pubDate></item><item><link>https://i.redd.it/2x5q1odxkmkf1.png</link><title>Run DeepSeek-V3.1 locally with Dynamic 1-bit GGUFs! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mxgjg8/run_deepseekv31_locally_with_dynamic_1bit_ggufs/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mxgjg8/run_deepseekv31_locally_with_dynamic_1bit_ggufs/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mxgjg8/run_deepseekv31_locally_with_dynamic_1bit_ggufs/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guy - you can now run DeepSeek-V3.1 locally on 170GB RAM with our Dynamic 1-bit GGUFs.🐋</p><p>The most popular GGUF sizes are now all i-matrix quantized! GGUFs: <a href="https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF">https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF</a>  </p><p>The 715GB model gets reduced to 170GB (-80% size) by smartly quantizing layers. This 162GB works for Ollama so you can run the command:</p><pre><code>OLLAMA_MODELS=unsloth_downloaded_models ollama serve &amp;ollama run hf.co/unsloth/DeepSeek-V3.1-GGUF:TQ1_0</code></pre><p>We also fixed the chat template for llama.cpp supported tools. The 1-bit IQ1_M GGUF passes all our coding tests, however 2-bit Q2_K_XL is recommended.</p><p>Guide + info: <a href="https://docs.unsloth.ai/basics/deepseek-v3.1">https://docs.unsloth.ai/basics/deepseek-v3.1</a></p><p>Thank you everyone and please let us know how it goes! :)</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/2x5q1odxkmkf1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>0.1 quant when</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mxgjg8/run_deepseekv31_locally_with_dynamic_1bit_ggufs/na4qhsd/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Cool, now I just need to get 2x sticks of 96Gb RAM (192Gb total) so I can reasonably load it on my Ryzen + 5090 (192+32).(2x instead of 4x because Ryzen memory controller gets stressed hard trying to run 4 sticks at high speed)</p><p>Best right now is 2x64 which comes up short. Going to be a while.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mxgjg8/run_deepseekv31_locally_with_dynamic_1bit_ggufs/na4umfn/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>fire</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mxgjg8/run_deepseekv31_locally_with_dynamic_1bit_ggufs/na4r9z9/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Is there anywhere i can see performance if this models after quantization? Because i feel like a smaller model would perform better than this</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mxgjg8/run_deepseekv31_locally_with_dynamic_1bit_ggufs/na63fv9/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Got to run the IQ_1_S here with 9950X3D / 192GB DDR6000 RAM / RTX 4090. It&#39;s tight with full CPU MoE I have about 4GB free when running the OS with a web browser for the chat client and the model loaded :) Using GPU KV offload (with Q4_1 quant on K and V + flash attn) as the actual offload of the model itself is about 12GB or so. Got around 8.2t/s on inference itself (with 128K context) and around 42t/s on eval. Slower than GTP OSS 120B but the model is bigger...</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mxgjg8/run_deepseekv31_locally_with_dynamic_1bit_ggufs/na9ca0p/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Is this actually any good going down to 1 bit? I know they have a dynamic quantization approach where they aren’t quantizing every single layer to 1 bit, but certainly they’d have to quantize most weights pretty aggressively to get a model of this size to fit in 24GB of VRAM. </p><p>At that point, would this still be better than just using a smaller model with less aggressive quantization? I mean, generally 1 bit models are incoherent babbling machines.</p><p>Pretty cool they were able to do it, but I’d be quite surprised if this actually performs well enough to be worthwhile for real use compared to other options.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mxgjg8/run_deepseekv31_locally_with_dynamic_1bit_ggufs/naa6k1t/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Perplexity numbers for different quants ?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mxgjg8/run_deepseekv31_locally_with_dynamic_1bit_ggufs/nabmuon/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 23 Aug 2025 01:21:12 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mxfnl3/can_someone_explain_whats_load_in_4bit/</link><title>Can someone explain what's "load_in_4bit"</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mxfnl3/can_someone_explain_whats_load_in_4bit/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mxfnl3/can_someone_explain_whats_load_in_4bit/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mxfnl3/can_someone_explain_whats_load_in_4bit/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>When do I use it, and when do I not?</p><p>I know it enables 4-bit quantization, but does it quantize a model by loading it into CPU memory first and then loading the quantized version into VRAM?</p><p>Does it decrease the quality of the LoRA?</p><p>Does it make the LoRA only compatible with the 4-bit quantized version of the model? o</p><p>I’m going to try fine-tuning qwen3-235b-a22b, and then during inference either serve it as Q4, Q8 or FP8, whichever has the best speed:quality ration I’m still not quite sure whether I should set this or load_in_8bit to True or False.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>In unsloth, generally if you use `load_in_4bit`, it will change the name of the model that you are loading into the `bnb-4bit` version (this is already quantized). Hence, it immediately loads it into VRAM.</p><p>Does it decrease the quality of the LoRA? In some sense yes but not that much. You quantize the main model, the LoRA is still in 16-bit</p><p>Since the LoRA is 16-bit, it&#39;s also compatible for non 4-bit quantized</p><p>If you have the GPU, you can try `load_in_8bit` or `full_finetuning`. But the best tradeoff for speed and quality imo is still the 4-bit version.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mxfnl3/can_someone_explain_whats_load_in_4bit/na4mly6/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I have a question related to this topic as well. I’m wondering If I used a 4-bit quantized model for training, which merge option should I use for? Merge to 4bit or 16bit?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mxfnl3/can_someone_explain_whats_load_in_4bit/na9r0kd/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 23 Aug 2025 00:46:57 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mxbkjq/gguf_request_for_interns1mini8b/</link><title>GGUF Request for InternS1-Mini-8B !</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mxbkjq/gguf_request_for_interns1mini8b/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mxbkjq/gguf_request_for_interns1mini8b/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 3 min | <a href='https://www.reddit.com/r/unsloth/comments/1mxbkjq/gguf_request_for_interns1mini8b/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hello Unsloth community, <a href="https://www.reddit.com/u/danielhanchen">u/danielhanchen</a>, and <a href="https://www.reddit.com/u/yoracale">u/yoracale</a>,</p><p>I&#39;m a big fan of the amazing work you do in making powerful models accessible to everyone with your incredible quantization and training optimizations. The speed and memory savings you&#39;ve achieved for so many models are a game-changer for local inference. And with active collaborations, you have been able to bring zero-day ggufs for many latest models.</p><p>I&#39;m writing to request that you consider creating a GGUF quantization of a fascinating new model that was just released: <strong>InternS1-Mini-8B</strong> (<a href="https://huggingface.co/internlm/Intern-S1-mini">https://huggingface.co/internlm/Intern-S1-mini</a>) that may have gone under your radar. </p><p><strong><em>Edit-</em></strong> <a href="https://www.reddit.com/u/mortyspace">u/mortyspace</a> kindly made the quants for the model and they work great. Anyone interested can find them at <a href="https://huggingface.co/yarikdevcom/Intern-S1-mini-GGUF">https://huggingface.co/yarikdevcom/Intern-S1-mini-GGUF</a></p><h3><strong>What is InternS1-Mini-8B?</strong></h3><p>InternS1-Mini-8B is a new multimodal model from the same team behind the popular InternVL and InternLM models. While it&#39;s a smaller, more accessible version of their larger InternS1 model, it has a unique and powerful specialization.</p><ul><li><strong>Multimodal:</strong> It can process both text and images, which is essential for its primary use case.</li><li><strong>Built for Science:</strong> Unlike general-purpose multimodal models, InternS1-Mini-8B has been continuously pre-trained on a massive, 5 trillion token dataset, with over half of that data being scientific literature, diagrams, chemical formulas, and protein sequences. This deep domain expertise makes it a dedicated &quot;scientific research assistant.&quot;</li><li><strong>Efficient Architecture:</strong> The model uses a dense 8B-parameter language model (Qwen3-8B) and a 0.3B vision encoder, making it much more lightweight than its larger counterpart.</li></ul><h3><strong>Why is this model so interesting and important?</strong></h3><p>InternS1-Mini-8B isn&#39;t just another multimodal model—it&#39;s a specialized tool that could revolutionize local scientific research.</p><ul><li><strong>Interprets Complex Scientific Data:</strong> It can natively understand and reason about chemical structures, synthesis routes, protein sequences, and intricate diagrams. This goes beyond simple image captioning and allows for genuine scientific dialogue. It would also be fantastic in augmenting scientific RAG applications. </li><li><strong>Scientific Problem-Solving:</strong> Imagine a local model that can help you interpret a complex graph from a research paper, analyze a chemical structure from a picture, or even assist in brainstorming new experimental pathways. This is exactly what InternS1-Mini-8B is designed to do.</li><li><strong>Accessibility for Researchers:</strong> Having a locally runnable, quantized version of this model would make cutting-edge AI a reality for countless people working in chemistry, biology, materials science, and other fields.</li></ul><h3><strong>The Request:</strong></h3><p>I&#39;m aware that the Intern team has already released some GGUF quants, specifically <code>Q8_0</code> and <code>F16</code>. While this is a great start, these quants are still very large and can be challenging to run on typical consumer laptops with 8GB of VRAM.</p><p>This is where your work shines. The U-D quants you&#39;ve created are known to be far more memory-efficient and performant without a significant loss in quality. They would make InternS1-Mini-8B truly accessible to a much broader audience, including researchers and students who rely on more modest hardware.</p><p>We would be incredibly grateful if you could work your Unsloth magic on InternS1-Mini-8B. The efficiency and performance gains from your U-D quantizations would make this powerful scientific tool accessible on consumer hardware, democratizing AI for scientific research.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Please do Intern-S1 241B as well</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mxbkjq/gguf_request_for_interns1mini8b/na3roop/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Here is Q5_M + Q4_0 mmproj <a href="https://huggingface.co/yarikdevcom/Intern-S1-mini-GGUF">https://huggingface.co/yarikdevcom/Intern-S1-mini-GGUF</a>  <a href="https://www.reddit.com/u/PaceZealousideal6091">u/PaceZealousideal6091</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mxbkjq/gguf_request_for_interns1mini8b/na98kua/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Hi there we&#39;ll see what we can do</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mxbkjq/gguf_request_for_interns1mini8b/nahimi7/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Did you tried this one <a href="https://huggingface.co/internlm/Intern-S1-mini-GGUF">https://huggingface.co/internlm/Intern-S1-mini-GGUF</a> ? they already released with GGUF. Interesting model, will try as well tmrw.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mxbkjq/gguf_request_for_interns1mini8b/na5oack/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 22 Aug 2025 22:12:22 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mxaasb/question_about_rl/</link><title>Question about RL</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mxaasb/question_about_rl/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mxaasb/question_about_rl/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mxaasb/question_about_rl/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>So I was reading about RL and PPO and GRPO and their difference in Unsloth docs, and from my understanding, it works for tasks that are verifiable or closely verifiable or have a deterministic answer. What if I want the model to just generate better PDF outputs and layouts? I do have hand picked examples but in this case I assume RL would not work for me cuz there is no way really to have a reward function.</p><p>I have also noticed that it talks about thinking tokens coming up while training with GRPO, but lets say I wanna train a non thinking model instruction only, I should ditch this method?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>RL works when you can score how good is the aswer</p><p>If you want push model for tour preferences look up DPO</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mxaasb/question_about_rl/na3xd22/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>RL is useful if you don&#39;t have a dataset, but you have a way to say that an answer is better than another one. It can also be useful if you want better <a href="https://arxiv.org/pdf/2508.03772">out of distribution capabilities</a> with respect traditional SFT.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mxaasb/question_about_rl/naej4xh/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 22 Aug 2025 21:24:43 +0530</pubDate></item><item><link>https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF</link><title>Run Preliminary DeepSeek-V3.1 Unsloth Dynamic GGUFs (huggingface.co)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mwo3zu/run_preliminary_deepseekv31_unsloth_dynamic_ggufs/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mwo3zu/run_preliminary_deepseekv31_unsloth_dynamic_ggufs/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mwo3zu/run_preliminary_deepseekv31_unsloth_dynamic_ggufs/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys we uploaded preliminary non-imatrix quants for those who want to run it. They&#39;re all still dynamic and run very well - just not i-matrix quantized: <a href="https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF">https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF</a></p><ul><li>UD-Q2_K_XL (247GB) is recommended</li><li>Read our guide on how to run it: <a href="https://docs.unsloth.ai/basics/deepseek-v3.1">https://docs.unsloth.ai/basics/deepseek-v3.1</a></li></ul><p>There&#39;s some issues we have to resolve for imatrix and we will likely announce the imatrix quants in like 15 hours or so.</p><p>Happy running and let us know how these preliminary quants perform :)</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>can we get MLX versions too please</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mwo3zu/run_preliminary_deepseekv31_unsloth_dynamic_ggufs/na1j3wr/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Was wondering are you going to add SingLoRA or it doesn&#39;t make much sense?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mwo3zu/run_preliminary_deepseekv31_unsloth_dynamic_ggufs/na2bwik/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 22 Aug 2025 03:09:26 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mw7zkb/google_colab_crashing_when_finetuning_qwen3_4b/</link><title>google colab crashing when finetuning qwen3 4b instruct</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mw7zkb/google_colab_crashing_when_finetuning_qwen3_4b/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mw7zkb/google_colab_crashing_when_finetuning_qwen3_4b/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mw7zkb/google_colab_crashing_when_finetuning_qwen3_4b/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I&#39;ve used the default settings and a custom dataset, trained for 60 steps (to test) and when I tried to push to hub as a merged model, it crashed and said &quot;Your session crashed after using all available RAM.&quot; Is there any fix for this?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>kaggle has couple 16gb gpu free options, whereas colab i believe crashes when only 15gb are used.</p><p>You can simply also change your code. While traditionally, even with qlora, the advice was to reduce batch-size/max-length, somehow with a different unsloth script i managed to quadruple actual batch size (not the gradient one, i never choose more than =2 GA) without sacrificing max length. </p><p>in fact, i will go ahead and copy my kaggle code to fine tune qwen3 4b to paste it below.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mw7zkb/google_colab_crashing_when_finetuning_qwen3_4b/n9xg97d/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 21 Aug 2025 16:43:24 +0530</pubDate></item><item><link>https://www.reddit.com/r/ollama/comments/1mv7sc0/qwen34binstruct2507gguf_template_fixed/</link><title>Qwen3-4B-Instruct-2507-GGUF template fixed</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mvco9u/qwen34binstruct2507gguf_template_fixed/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mvco9u/qwen34binstruct2507gguf_template_fixed/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mvco9u/qwen34binstruct2507gguf_template_fixed/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>The Unsloth team uploaded templates to: <a href="https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF">https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF</a></p><p>And how the model works out of box. Same should happen to the Thinking variant soon.</p><p>This model is amazing and having a drop-in working version is great.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Note that this is just us adding the Ollama chat template to Ollama so people don&#39;t have to manually add it in.</p><p>If you were using llama.cpp, LM studio or any backend that uses llama.cpp, you shouldn&#39;t have had any problems. So this fix was specifically if you used our GGUF in Ollama and didn&#39;t add the chat template</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mvco9u/qwen34binstruct2507gguf_template_fixed/n9pqvn4/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Wed, 20 Aug 2025 17:24:40 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mv1ugc/valueerror_the_following_model_kwargs_are_not/</link><title>ValueError: The following `model_kwargs` are not used by the model: ['num_logits_to_keep'] (note: typos in the generate arguments will also show up in this list)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mv1ugc/valueerror_the_following_model_kwargs_are_not/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mv1ugc/valueerror_the_following_model_kwargs_are_not/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mv1ugc/valueerror_the_following_model_kwargs_are_not/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><pre><code>messages = [    {&quot;role&quot; : &quot;user&quot;, &quot;content&quot; : &quot;Continue the sequence: 1, 1, 2, 3, 5, 8,&quot;}]text = tokenizer.apply_chat_template(    messages,    tokenize = False,    add_generation_prompt = True, # Must add for generation)from transformers import TextStreamer_ = model.generate(    **tokenizer(text, return_tensors = &quot;pt&quot;).to(&quot;cuda&quot;),    max_new_tokens = 1000, # Increase for longer outputs!    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking    streamer = TextStreamer(tokenizer, skip_prompt = True),)</code></pre><p>this is the error</p><p>---------------------------------------------------------------------------</p><p>ValueError                                Traceback (most recent call last)</p><p>/tmp/ipython-input-3930286668.py in &lt;cell line: 0&gt;()</p><p>10</p><p>11 from transformers import TextStreamer</p><p>---&gt; 12 _ = model.generate(</p><p>13     **tokenizer(text, return_tensors = &quot;pt&quot;).to(&quot;cuda&quot;),</p><p>14     max_new_tokens = 1000, # Increase for longer outputs!</p><p>4 frames</p><p>/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py in _validate_model_kwargs(self, model_kwargs)</p><p>1600</p><p>1601         if unused_model_args:</p><p>-&gt; 1602             raise ValueError(</p><p>1603                 f&quot;The following `model_kwargs` are not used by the model: {unused_model_args} (note: typos in the&quot;</p><p>1604                 &quot; generate arguments will also show up in this list)&quot;</p><p>ValueError: The following `model_kwargs` are not used by the model: [&#39;num_logits_to_keep&#39;] (note: typos in the generate arguments will also show up in this list)</p><p>I tried debugging with gemini 2.5 pro and gpt5 but they did not help at all and I have no idea what the issue could be because I literally kept almost all the nodes except the &quot;loading finetuned model&quot;  which I updated to this</p><pre><code>if True:    from unsloth import FastLanguageModel    base_model, tokenizer = FastLanguageModel.from_pretrained(        model_name = &quot;unsloth/Qwen3-4B-Instruct-2507&quot;,        max_seq_length = 2048,        load_in_4bit = True,    )    from peft import PeftModel    model = PeftModel.from_pretrained(base_model, &quot;lora_model&quot;)    FastLanguageModel.for_inference(model)</code></pre><p>because when I tried to run the default node I got this error</p><p>```</p><p>==((====))==  Unsloth 2025.8.8: Fast Qwen3 patching. Transformers: 4.55.2.</p><p>\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.</p><p>O^O/ \_/ \    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0</p><p>\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]</p><p>&quot;-____-&quot;     Free license: <a href="http://github.com/unslothai/unsloth">http://github.com/unslothai/unsloth</a></p><p>Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!</p><p>model.safetensors: 100%</p><p> 3.55G/3.55G [00:25&lt;00:00, 78.2MB/s]</p><p>generation_config.json: 100%</p><p> 237/237 [00:00&lt;00:00, 28.3kB/s]</p><p>---------------------------------------------------------------------------</p><p>TypeError                                 Traceback (most recent call last)</p><p>/tmp/ipython-input-3850167755.py in &lt;cell line: 0&gt;()</p><p>1 if True:</p><p>2     from unsloth import FastLanguageModel</p><p>----&gt; 3     model, tokenizer = FastLanguageModel.from_pretrained(</p><p>4         model_name = &quot;lora_model&quot;, # YOUR MODEL YOU USED FOR TRAINING</p><p>5         max_seq_length = 2048,</p><p>1 frames</p><p>/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py in patch_peft_model(model, use_gradient_checkpointing)</p><p>2751         pass</p><p>2752         if not isinstance(model, PeftModelForCausalLM) and not isinstance(model, PeftModelForSequenceClassification):</p><p>-&gt; 2753             raise TypeError(</p><p>2754                 &quot;Unsloth: Your model needs to call `.get_peft_model` first!&quot;</p><p>2755             )</p><p>TypeError: Unsloth: Your model needs to call `.get_peft_model` first!</p><p>```</p></div><!-- SC_ON --></section>]]></description><pubDate>Wed, 20 Aug 2025 07:18:52 +0530</pubDate></item><item><link>https://i.redd.it/rsivghb2nzjf1.jpeg</link><title>Vision Tutorials failing (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mukcfg/vision_tutorials_failing/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mukcfg/vision_tutorials_failing/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mukcfg/vision_tutorials_failing/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi, </p><p>I am trying to eun the vision Tutorials at <a href="https://docs.unsloth.ai/basics/vision-fine-tuning">https://docs.unsloth.ai/basics/vision-fine-tuning</a> on Collab, specifically the one for Llama3.2 and I am getting memory issues on the T4. I last ran this tutorial a month ago and it ran fine, but now its getting OOM issues. Any reason why it&#39;s not working now? What can I do to overcome the OOM errors (besides paying for A100s).</p><p>Thanks for your help</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/rsivghb2nzjf1.jpeg' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Reduce batch_size 2 =&gt; 1,Load in 4bit Reduce maximum context length !!!!!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mukcfg/vision_tutorials_failing/n9jxf28/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Hi <a href="https://www.reddit.com/u/halien69">u/halien69</a> we did a new update to unsloth, can you restart evverything including your notebooks from scratch and see if it got fixed?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mukcfg/vision_tutorials_failing/n9uy5ct/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 19 Aug 2025 20:10:33 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mucie8/please_allow_me_to_unquantizeunfreeze_base_model/</link><title>Please allow me to unquantize/unfreeze base model params during LoRA tuning</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mucie8/please_allow_me_to_unquantizeunfreeze_base_model/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mucie8/please_allow_me_to_unquantizeunfreeze_base_model/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mucie8/please_allow_me_to_unquantizeunfreeze_base_model/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>This is something I am currently doing using HuggingFace code, and it works great, but VRAM is super tight.</p><p>I&#39;d sure love to free up some VRAM!! I noticed unsloth dropping my VRAM from 19-&gt;11 GB which is amazing, but also my setup just doesn&#39;t work though. I am really hoping some of those VRAM savings could be become possible in my hybrid setup!</p><p>Here is a summary of what I do:</p><ul><li>Load &quot;mistralai/Mistral-7B-Instruct-v0.3&quot;, 4bit quantized. Note that while much of the model is quantized, some parts of the model are still not quantized. e.g. Layernorm/embeddings/lm_head/modelnorm. HuggingFace customers can easily simply &#39;unfreeze&#39; these if they want, as long as they remember to save them to disk with torch.save afterwards (or merge). Unsloth, it appears... cannot, because it flat refuses to even train a &quot;fully quantized&quot; model (even though it is not really fully quantized...)</li><li>Add a Peft Model over the base model</li><li>Tune LoRA + embeddings + lm_head+modelnorm for 4 initial epochs.</li><li>After several initial epochs, I begin unquantizing and unfreezing layers (specifically just v_proj, o_proj, mlp), eventually layers 10-31 are tuned</li><li>Touch final layers/DPO at the end</li></ul><p>Anyway, when I tried it, I discovered unsloth will not update any modelnorm/layernorm in the base model for some reason. I filed a bug about this. <a href="https://github.com/unslothai/unsloth/issues/3178">https://github.com/unslothai/unsloth/issues/3178</a> But I wanted to confirm that there aren&#39;t other/bigger limitations relevant.</p><p>Is what I&#39;m asking technically feasible for unsloth? Would fully supporting this &#39;bloat&#39; unsloth too much, negating the savings? I hope it wouldn&#39;t, I suspect VRAM will increase but I am hopeful that HuggingFace can still be outperformed. I&#39;d love to see it if it can be done. I might even be able to help somewhat, but first I&#39;d like to know if what I&#39;m suggesting even makes sense when considering the internals unsloth&#39;s perf magic! Can it be done?</p><p>edit: I also tried to load Mistral with full_finetuning=True. but it seems it doesn&#39;t work even in the most basic case for Mistral. Also filed a bug about that.<a href="https://github.com/unslothai/unsloth/issues/3184">https://github.com/unslothai/unsloth/issues/3184</a>I don&#39;t actually want the model fully expanded anyway, but I suppose I could manually quantize some of the model as an alternative path?</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 19 Aug 2025 13:47:47 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mu5n8q/finetuning_a_code_generation_llm_on_bengali/</link><title>Fine-tuning a Code Generation LLM on Bengali Dataset - Need Model  Resource Recommendations</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mu5n8q/finetuning_a_code_generation_llm_on_bengali/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mu5n8q/finetuning_a_code_generation_llm_on_bengali/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mu5n8q/finetuning_a_code_generation_llm_on_bengali/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I want to fine-tune a code generation LLM on a dataset I created that looks like this:</p><p><code>csvid,instruction,response,test_list1,প্রথম n সংখ্যার ক্ষুদ্রতম গুণিতক খুঁজে বের করার জন্য একটি ফাংশন লিখুন।,&quot;def smallest_multiple(n):    if (n&lt;=2):      return n    i = n * 2    factors = [number  for number in range(n, 1, -1) if number * 2 &gt; n]    while True:        for a in factors:            if i % a != 0:                i += n                break            if (a == factors[-1] and i % a == 0):                return i&quot;,&quot;&quot;&quot;[&#39;assert smallest_multiple(13)==360360&#39;, &#39;assert smallest_multiple(2)==2&#39;, &#39;assert smallest_multiple(1)==1&#39;]&quot;&quot;&quot;2,সাধারণ কীগুলির জন্য মান যোগ করে দুটি অভিধানকে একত্রিত করার জন্য একটি ফাংশন লিখুন।,&quot;from collections import Counterdef add_dict(d1,d2):   add_dict = Counter(d1) + Counter(d2)   return add_dict&quot;,&quot;&quot;&quot;[&quot;assert add_dict({&#39;a&#39;: 100, &#39;b&#39;: 200, &#39;c&#39;:300},{&#39;a&#39;: 300, &#39;b&#39;: 200, &#39;d&#39;:400})==({&#39;b&#39;: 400, &#39;d&#39;: 400, &#39;a&#39;: 400, &#39;c&#39;: 300}) &quot;, &quot;assert add_dict({&#39;a&#39;: 500, &#39;b&#39;: 700, &#39;c&#39;:900},{&#39;a&#39;: 500, &#39;b&#39;: 600, &#39;d&#39;:900})==({&#39;b&#39;: 1300, &#39;d&#39;: 900, &#39;a&#39;: 1000, &#39;c&#39;: 900}) &quot;, &quot;assert add_dict({&#39;a&#39;:900,&#39;b&#39;:900,&#39;d&#39;:900},{&#39;a&#39;:900,&#39;b&#39;:900,&#39;d&#39;:900})==({&#39;b&#39;: 1800, &#39;d&#39;: 1800, &#39;a&#39;: 1800})&quot;]&quot;&quot;&quot;</code></p><p><strong>Dataset Structure:</strong>- <code>instruction</code> → coding task (in Bengali)- <code>response</code> → Python function solution<br/>- <code>test_list</code> → asserts to validate</p><p><strong>⚡ Setup:</strong> I only plan to use Kaggle free GPU for training.</p><p><strong>👉 Questions:</strong></p><ol><li>Which small/efficient model is best for this? (Qwen2.5-Coder, StarCoder, CodeLlama?)</li><li>Any good Kaggle notebook / resource for LoRA/QLoRA style finetuning on code datasets?</li></ol><p>Looking for something lightweight but useful for Bengali + code generation tasks. Any recommendations or experiences would be greatly appreciated!</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>try using gemma 3 270m, it&#39;s more light weight <a href="https://docs.unsloth.ai/basics/gemma-3-how-to-run-and-fine-tune">https://docs.unsloth.ai/basics/gemma-3-how-to-run-and-fine-tune</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mu5n8q/finetuning_a_code_generation_llm_on_bengali/n9uu8hu/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I have had good experience with Qwen2.5-coder in fine-tuning for coding related tasks you can check out the 2 notebooks in ellora that use it - <a href="https://github.com/codelion/ellora">https://github.com/codelion/ellora</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mu5n8q/finetuning_a_code_generation_llm_on_bengali/n9h1cea/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 19 Aug 2025 07:32:50 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mtyucz/promptcompletion_instruction_tuning_issue/</link><title>Prompt-Completion Instruction Tuning Issue</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mtyucz/promptcompletion_instruction_tuning_issue/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mtyucz/promptcompletion_instruction_tuning_issue/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mtyucz/promptcompletion_instruction_tuning_issue/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>There&#39;s a particular Instruction-finetuned model of &quot;Qwen2.5-Coder-7b-Instruct&quot; on Huggingface (unsloth model for which is not available) that I would like to instruction-finetune on my prompt-completion dataset</p><pre><code>train_dict={&quot;prompt&quot;: prompts, &quot;completion&quot;: completions}train_data = Dataset.from_dict(train_dict)</code></pre><p>I am passing in a Dataset object as above.</p><p>I load the model as</p><pre><code>model, tokenizer = FastLanguageModel.from_pretrained(.....model = FastLanguageModel.get_peft_model(......</code></pre><p>The training script is:</p><pre><code>from trl import SFTConfig, SFTTrainertrainer = SFTTrainer(    model = model,    tokenizer = tokenizer,    train_dataset = train_data,    max_seq_length = max_seq_length,    packing = False, # Can make training 5x faster for short sequences.    args = SFTConfig(        per_device_train_batch_size = BATCH_SIZE,        gradient_accumulation_steps = GRAD_ACCU, #4        # warmup_steps = 5,        # num_train_epochs = 1, # Set this for 1 full training run.        max_steps =2, #10,        learning_rate = 2e-4,        logging_steps = 1,        optim = &quot;adamw_8bit&quot;,        weight_decay = 0.01,        lr_scheduler_type = &quot;linear&quot;,        seed = 3407,        output_dir = OUTPUT_DIR,        report_to = &quot;wandb&quot; if USE_WANDB else &quot;none&quot;,        save_strategy=&quot;no&quot;,        completion_only_loss=True,    ),)trainer_stats = trainer.train()</code></pre><p>But, it is throwing in an error:</p><pre><code>RuntimeError: Unsloth: You must specify a `formatting_func`</code></pre><p>Note:  <strong>prompt and completion already contain chat template special tokens added using</strong></p><pre><code>tokenizer.apply_chat_template(..</code></pre><p>Could anyone please  suggest a way around how to train the model on completion only?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Try this, it worked for alapaca fomat after changing scripts</p><p>from unsloth.chat_templates import train_on_responses_onlytrainer = train_on_responses_only(    trainer,    instruction_part = &quot;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n&quot;,    response_part = &quot;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n&quot;,)</p><p>BTW For many applications traing or response only does not change real results significantly</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mtyucz/promptcompletion_instruction_tuning_issue/n9l8os7/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 19 Aug 2025 02:47:02 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mtph9h/first_time_training_need_advice_about_optimizing/</link><title>First time training need advice about optimizing for humble rtx 4060</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mtph9h/first_time_training_need_advice_about_optimizing/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mtph9h/first_time_training_need_advice_about_optimizing/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mtph9h/first_time_training_need_advice_about_optimizing/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I know this gpu is not much, but I want to fine tune the gemma 270m.</p><p>Any optimizing tips? I used the offical notebook for gemma3 270b, but had to disable torch compile.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>For Gemma 270m the model is so small it&#39;ll work on any setup. In general just use our notebooks as the model is the most optimized it can already be with unsloth vs any other setup/library</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mtph9h/first_time_training_need_advice_about_optimizing/n9grew8/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I find myself in the same position as you but with an Rtx 4070 🤭</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mtph9h/first_time_training_need_advice_about_optimizing/n9da2xw/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Mon, 18 Aug 2025 21:07:03 +0530</pubDate></item><item><link>https://i.redd.it/f303p8l0csjf1.png</link><title>New gpt-oss Fine-tuning Guide! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mtn4yw/new_gptoss_finetuning_guide/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mtn4yw/new_gptoss_finetuning_guide/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mtn4yw/new_gptoss_finetuning_guide/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hello everyone! We made a new step-by-step guide for fine-tuning gpt-oss! 🦥</p><p>You&#39;ll learn about:</p><ul><li>Locally training gpt-oss + inference FAQ &amp; tips</li><li>Reasoning effort &amp; Data prep </li><li>Evaluation, hyperparameters &amp; overfitting</li><li>Running &amp; saving your LLM to llama.cpp GGUF, HF etc.</li></ul><p>🔗Guide: <a href="https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune/">https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune/</a></p><p>Just a reminder we improved our fine-tuning and inference notebooks so if previously something wasn&#39;t working it should now!</p><p>Thank you for reading and let us know how we can improve guides in the future! :)</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/f303p8l0csjf1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Does it support native mxfp4 for training?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mtn4yw/new_gptoss_finetuning_guide/n9cy5oz/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Top K of 0.0 really hurts performance. Like 2x. Have you looked at accuracy with something like top k 96?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mtn4yw/new_gptoss_finetuning_guide/n9ctra8/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>great guide :)</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mtn4yw/new_gptoss_finetuning_guide/n9i1wmt/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Thanks for the updated guide! I finetuned <strong>gpt-oss-20B-bf16</strong> (using Unsloth’s BF16 weights – <a href="https://huggingface.co/unsloth/gpt-oss-20b-BF16">https://huggingface.co/unsloth/gpt-oss-20b-BF16</a>) with LoRA following the instructions. However, I wasn’t able to serve the LoRA-finetuned model with vLLM due to this error: KeyError: &#39;model.layers.10.mlp.experts.w2_bias&#39;.<br/>Is there a way to export the finetuned model for use in frameworks other than llama.cpp?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mtn4yw/new_gptoss_finetuning_guide/na16n07/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>is it possible to train an llm to not be so censored?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mtn4yw/new_gptoss_finetuning_guide/n9kg2hp/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Thx for your work, your models are the best and the fastest!</p><p>In future if you will have some % GPU usage left.. will be chance to convert this model to Unsloth? To reduce RAM..</p><p><a href="https://huggingface.co/speakleash/Bielik-4.5B-v3.0-Instruct">https://huggingface.co/speakleash/Bielik-4.5B-v3.0-Instruct</a></p><p>Because now on my phone Q4_K_M is very slow.. :(</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mtn4yw/new_gptoss_finetuning_guide/n9seg54/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Thank you for your amazing work on the gpt-oss-20b-GGUF model and the optimizations for low VRAM usage! I noticed that the current GGUF version on Hugging Face (<a href="https://huggingface.co/unsloth/gpt-oss-20b-GGUF">https://huggingface.co/unsloth/gpt-oss-20b-GGUF</a>) retains the original 128k token context length.Would it be possible for you to upload a fine-tuned version of gpt-oss-20b with an extended context length. This would be incredibly helpful for those of us working on tasks requiring larger context windows.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mtn4yw/new_gptoss_finetuning_guide/nbuwvpm/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Mon, 18 Aug 2025 19:40:30 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mthg2b/fine_tuning_gemma3_270m/</link><title>Fine Tuning Gemma3 270m</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mthg2b/fine_tuning_gemma3_270m/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mthg2b/fine_tuning_gemma3_270m/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mthg2b/fine_tuning_gemma3_270m/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi Greetings,</p><p>I want to fine tune gemma3 270m</p><p>I saw there is a google colab available</p><p>I cannot use it, I dont know how to use cloab notebooks</p><p>I would like simple python code to prepare data from normal text files</p><p>I would also like simple python code to train the model</p><p>And how to use the model once it is trained</p><p>I saw usecases where gemma could be trained to play chess</p><p>Can I give input of text files in text format and derived from books</p><p>So it would answer questions based on the book or information from text files</p><p>I am also interested in training gemma for games</p><p>Can I try a free approach, I have poor hardware , a GTX 1060</p><p>or I have to pay to get the fine tuning and training done</p><p>Regards.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>you can download a colab notebook as a py file. that should help you. i think it is in file-&gt; download as...</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mthg2b/fine_tuning_gemma3_270m/n9bpagw/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>You cannot simply use corpus of text to train it. You need to do instruction tuning(SFT) with Q/A pairs of the text. Else it would just do text completion (does not follow instructions) You can create synthetic q/a pairs using a bigger LLM from the book text. I don&#39;t usually suggest but If you want the LLM to have actual knowledge you might need continued pretraining from text corpus</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mthg2b/fine_tuning_gemma3_270m/n9chgmz/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Hi,I am doing Domain-Adapitve Pretraining (DAPT) on Gemma-3 270M to have better knowledge of Hungarian.It is going OK, but I am having problem with converting the merged model to .task format as I want to use it in Android app with mediapipe (model.safetensor-&gt;tflitle-&gt; .task)There are so many changes from time to time  in packages to do so...Anyone know a stable solution?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mthg2b/fine_tuning_gemma3_270m/nccgn1z/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Mon, 18 Aug 2025 15:11:56 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mtfu6p/gptoss_export_to_vllm_in_mxfp4/</link><title>GPT-OSS export to vLLM in MXFP4</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mtfu6p/gptoss_export_to_vllm_in_mxfp4/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mtfu6p/gptoss_export_to_vllm_in_mxfp4/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mtfu6p/gptoss_export_to_vllm_in_mxfp4/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Dear Unsloth,</p><p>Thanks for all of the hard work incorporating GPT-OSS into unsloth. I was wondering, is there an estimated date as to when we would be able to export the weights in MXFP4 format?   </p><p>Thank you,</p><p>Cihan </p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>We&#39;re working on it but to use your fine-tuned gpt-oss models in other frameworks (e.g. Hugging Face, llama.cpp with GGUF), you must train with <strong>LoRA</strong> on our <strong>BF16 model</strong> (So you <strong><em>MUST</em></strong> set <code>model_name = &quot;unsloth/gpt-oss-20b-BF16&quot;</code>) Keep in mind this process requires &gt;43GB VRAM. This produces a BF16 fine-tuned model that can be exported and converted as needed. </p><p>Currently vllm doesn&#39;t load bf16 models so you will need to convert that into a useable format for vllm. it will work in llama.cpp and Hugging Face however.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mtfu6p/gptoss_export_to_vllm_in_mxfp4/n9bzwaa/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Mon, 18 Aug 2025 13:31:22 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mspb6l/looking_for_advice_finetuning_gemma_270m_for_chat/</link><title>Looking for advice finetuning Gemma 270m for chat titles</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mspb6l/looking_for_advice_finetuning_gemma_270m_for_chat/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mspb6l/looking_for_advice_finetuning_gemma_270m_for_chat/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mspb6l/looking_for_advice_finetuning_gemma_270m_for_chat/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi,</p><p>What sort of hyper params are suggested for this task?</p><p>I have a dataset of about 6000 examples. </p><p>I&#39;ve tried the default params (set epoch = 1) but somehow the title generation of the finetuned model is quite bad. I get spelling mistakes too here and there.</p><p>My loss curve kind of just flattens within about 0.3 epochs and then nothing much changes.</p><p>Should I up the learning rate. Currently it is 2e-5. </p><p>And drop the r and alpha to like 8 and 16 maybe?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Use the instruct variant as your base model. Your LR is just too low, bump it to around 1e-4 (even 2e-4 works fine with LoRA), keep r at 16–32 with alpha about the same or double, and add a tiny bit of dropout (0.05–0.1). Train for 2–4 epochs with early stopping, short sequence length (~256), and a decent effective batch size via grad accumulation. </p><p>Make sure your data is clean and formatted consistently with the chat template, and keep decoding tight (low temp like 0.2–0.5, top_p 0.9, short max_new_tokens). Most of the “spelling mistakes” you saw are from underfitting plus noisy decoding, not because the model can’t do it, with a slightly higher LR, cleaner data, and controlled decoding, quality should improve a lot.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mspb6l/looking_for_advice_finetuning_gemma_270m_for_chat/n9c4rlq/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>It&#39;s usually all trial and error especially depending on model, but we do have a specific guide just for hyperparameters which you should read here: <a href="https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide">https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mspb6l/looking_for_advice_finetuning_gemma_270m_for_chat/n9a4rfq/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sun, 17 Aug 2025 17:43:38 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1ms6b17/how_are_you_running_kimi_k1/</link><title>How are you running Kimi K1?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1ms6b17/how_are_you_running_kimi_k1/</guid><comments>https://www.reddit.com/r/unsloth/comments/1ms6b17/how_are_you_running_kimi_k1/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1ms6b17/how_are_you_running_kimi_k1/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>It spawned, it got hyped and then... I am not reading anything about it since. Claude still seems to dominate the tool-using-models.</p><p>I got in touch with a vendor to order 2 Intel Pro B60s for my homelab and I am currently &quot;model shopping&quot;. And this reminded me that, hey, Kimi <em>does</em> exist, and Unsloth even made quant&#39;ed GGUFs.</p><p>But jeebus, it is impossible to fit into anything less than an entire shelf of servers. A 1T model is just... massive. So I am sure that offloading is basically required.</p><p>But how are you running Kimi K2? How is it? What&#39;s your t/s? It&#39;s capabilities, on plain paper, would make an absurdly amazing model to use for &quot;everything&quot; that isn&#39;t highly specialized. So it&#39;d be fun to run that. Originally I thought of using Deepseek R1 - but Kimi&#39;s MCP support seems to be much better. o.o</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>It&#39;s a great model, as is Qwen Coder. But it&#39;s too sparse according to people who have looked at the density of information it contains. Ideally they keep training it, and it becomes a really great model to run locally in a couple years when memory bandwidth and size doubles again.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1ms6b17/how_are_you_running_kimi_k1/n94nk1e/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Imagine a future with 100T token LLM hosted in shards across continents decentralised. All the Es in MoE distributed but acting as one single unified model.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1ms6b17/how_are_you_running_kimi_k1/n95y87n/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Open router?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1ms6b17/how_are_you_running_kimi_k1/n92de6e/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sun, 17 Aug 2025 01:41:50 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mr5al2/so_about_finetuning/</link><title>So, about finetuning...</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mr5al2/so_about_finetuning/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mr5al2/so_about_finetuning/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mr5al2/so_about_finetuning/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I was getting (a little too...?) curious about the AI VTuber Neuro-sama - and in a spur of randomness, I dug into a rabbithole. Part of the result is here: <a href="https://www.reddit.com/r/LocalLLaMA/comments/1mq5cwq/so_what_is_neurosama_ai_vtuber_built_with/">https://www.reddit.com/r/LocalLLaMA/comments/1mq5cwq/so_what_is_neurosama_ai_vtuber_built_with/</a></p><p>But as someone there mentioned, there is a possibility that she is being continiously refined to include memory. Well that or RAG.</p><p>Either way; I never looked into actually finetuning. How do you do that - basically? I am planning to purchase the Intel Pro B60 and two of those - so I would have a pretty decent amount of VRAM at my disposal. How&#39;d I run finetune on that and what would I need? o.o</p><p>I am a complete noob in that and still have ways to go outside of inference and a few things involved in that (platform, api, ...).</p><p>Thanks in advance!</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Fine tuning is the gateway from AI Engineer to ML Engineer.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mr5al2/so_about_finetuning/n8vrdm8/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Currently Intel are working with us on making unsloth work on Intel GPUs. It should work already.</p><p>As for finetuning itself, we have a complete step by step guide here: <a href="https://docs.unsloth.ai/get-started/fine-tuning-llms-guide">https://docs.unsloth.ai/get-started/fine-tuning-llms-guide</a></p><p>The guide has every single little bit of info you&#39;ll need and even more than necessary!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mr5al2/so_about_finetuning/n8wpami/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p><strong>TL;DR:</strong> Don’t buy GPUs just for finetuning. Rent. Start with a 7–13B model + QLoRA; use RAG/graph for “memory.”</p><ol><li><strong>Intel/XPU:</strong> Support seems to be coming (per dev posts), but today CUDA/ROCm is the safe path.</li><li>For VRAM, it depends on model size (from unsloth page):<ol><li>7–8B → 1×24 GB ✅</li><li>13B → 1×24–48 GB ✅</li><li>30–34B → 1×48–80 GB or 2×24 GB (with checkpointing) ✅</li><li>70B → usually multi-GPU (e.g., 4×80 GB)</li><li>400B-class → datacenter territory; not newbie-friendly</li></ol></li></ol><p><a href="https://preview.redd.it/easj60scwejf1.png?width=801&amp;format=png&amp;auto=webp&amp;s=5932ce7485c4961540268ab5dfdc3625b8fcb259">https://preview.redd.it/easj60scwejf1.png?width=801&amp;format=png&amp;auto=webp&amp;s=5932ce7485c4961540268ab5dfdc3625b8fcb259</a></p><p><strong>Why rent?</strong></p><p>Vast/RunPod/Lambda, etc. are cheap and flexible; a 7–13B QLoRA run on ~1–2M tokens finishes in <strong>hours</strong>, not weeks. No driver drama, and you’re not stuck with $2k+ of idle hardware.</p><p><strong>Regarding memory</strong></p><p>I suggest to use neo4j graph DB + <a href="https://github.com/getzep/graphiti">graphiti</a> to ingest memories in near-real time. It&#39;s very scalable, very fast, can ingest messages from chat and &quot;remember&quot; it 5-10 seconds later. I believe there was also an open-source package on top of it that is tuned specifically for chatbots. And if you grow out of chat and want to start ingesting any data, it supports arbitrary data as well. E.g. you can even ingest PDFs if you chunk contents into smaller paragraphs.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mr5al2/so_about_finetuning/n91df7r/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>To my knowledge each game is supported by seperate model, that is guided by main neuro model</p><p>There is definitely a memory module as it was upgraded but how it works? No idea , he talks about it as memories without specifics</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mr5al2/so_about_finetuning/n9ld1z1/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 15 Aug 2025 23:28:22 +0530</pubDate></item><item><link>https://i.redd.it/f667h5b9h0jf1.png</link><title>Google - Gemma 3 270M out now! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mq5hbb/google_gemma_3_270m_out_now/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mq5hbb/google_gemma_3_270m_out_now/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mq5hbb/google_gemma_3_270m_out_now/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Google releases Gemma 3 270M, a new model that runs locally on just 0.5 GB RAM. ✨</p><p>GGUF to run: <a href="https://huggingface.co/unsloth/gemma-3-270m-it-GGUF">https://huggingface.co/unsloth/gemma-3-270m-it-GGUF</a></p><p>Trained on 6T tokens, it runs fast on phones &amp; handles chat, coding &amp; math tasks.</p><p>Run at ~50 t/s with our Dynamic GGUF, or fine-tune in a few mins via Unsloth &amp; export to your phone.</p><p>Our notebooks makes the 270M prameter model very smart at playing chess and can predict the next chess move.</p><p>Fine-tuning notebook: <a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(270M">https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(270M).ipynb</a>.ipynb)</p><p>Guide: <a href="https://docs.unsloth.ai/basics/gemma-3">https://docs.unsloth.ai/basics/gemma-3</a></p><p>Thanks to the Gemma team for providing Unsloth with Day Zero support! :)</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/f667h5b9h0jf1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p><a href="https://giphy.com/gifs/tpVKvAabWt3G5csMkT" target="_blank"><img src="https://external-preview.redd.it/AlTN2QperrqFZt3LbKJtWoBaMVFwHnSvnK3LudGqEhg.gif?width=200&height=200&s=fee9bc60f8ce43945621530ae2a3a9732c529874" width="200" height="200"></a></p><p><strong><em>I LOVE the &quot;run locally on just 0.5 giga of ram</em></strong></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mq5hbb/google_gemma_3_270m_out_now/n8odolf/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Guys please release 0.5bit quant, I’m struggling to run it</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mq5hbb/google_gemma_3_270m_out_now/n8of3wf/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I can&#39;t even read these numbers. 270M? wait 500MB ram?? What is this? I can&#39;t run this, it&#39;s TOO MUCH!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mq5hbb/google_gemma_3_270m_out_now/n8r94fm/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Yess this is something ill definitely load up on my phone.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mq5hbb/google_gemma_3_270m_out_now/n8qc4wh/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>If I try to run it in the app I get an error. The app tells me I can only run .task files. I got the app from GitHub from your guide.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mq5hbb/google_gemma_3_270m_out_now/n8ph8bb/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Is it able to analyze images?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mq5hbb/google_gemma_3_270m_out_now/n8ol6wb/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>How can I incorporate it to a web app?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mq5hbb/google_gemma_3_270m_out_now/n8r8rjb/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 14 Aug 2025 21:57:40 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mpl382/gptoss_fixesupdates_for_finetuning_inference/</link><title>Gpt-oss Fixes/Updates for Fine-tuning  Inference</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mpl382/gptoss_fixesupdates_for_finetuning_inference/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mpl382/gptoss_fixesupdates_for_finetuning_inference/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mpl382/gptoss_fixesupdates_for_finetuning_inference/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys we noticed some of you having issues with the gpt-oss notebooks for fine-tuning &amp; inference. We did a large update to fix some issues and so you should see more stable runs.</p><p>Update Unsloth or Use our new updated finetuning notebook: <a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb">https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb</a>Or inference notebook: <a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb">https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb</a></p><p>And see instructions below to use the new update if local.</p><p>Keep in mind inference is still a bit iffy but it should work for the most part. We&#39;re still working on it.</p><p>As for saving and using the model to GGUF etc we&#39;re also working on that so stay tuned!</p><p>Use our new installation cell:<code>!pip install --upgrade -qqq uvtry: import numpy; install_numpy = f&quot;numpy=={numpy.__version__}&quot;except: install_numpy = &quot;numpy&quot;!uv pip install -qqq \    &quot;torch&gt;=2.8.0&quot; &quot;triton&gt;=3.4.0&quot; {install_numpy} \    &quot;unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo&quot; \    &quot;unsloth[base] @ git+https://github.com/unslothai/unsloth&quot; \    torchvision bitsandbytes \    git+https://github.com/huggingface/transformers \    git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels</code></p><p>Previous errors you might&#39;ve been getting included: GptOssTopKRouter or cuda error</p><p>Let us know if you&#39;re still having any issues! 🤗</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Can we train 20B with GRPO in blackwell MXFP4?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mpl382/gptoss_fixesupdates_for_finetuning_inference/n8mcwqb/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Does the 120B model work as well?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mpl382/gptoss_fixesupdates_for_finetuning_inference/n8l070g/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 14 Aug 2025 05:34:40 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1moylrq/need_help_torch_dynamoexcbackendcompilerfailed/</link><title>Need help: torch._dynamo.exc.BackendCompilerFailed</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1moylrq/need_help_torch_dynamoexcbackendcompilerfailed/</guid><comments>https://www.reddit.com/r/unsloth/comments/1moylrq/need_help_torch_dynamoexcbackendcompilerfailed/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1moylrq/need_help_torch_dynamoexcbackendcompilerfailed/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I ran into a very strange issue. The environment and the unsloth version are the same, the data is the same, and the model is also the same (gemma3). The code that could run last week can’t run this week. The error message is: torch._dynamo.exc.BackendCompilerFailed RuntimeError: Detected that you are using FX to symbolically trace a dynamo-optimized function. This is not supported at the moment.</p><p>Then, after I set the following, it can run normally: os.environ[&quot;UNSLOTH_COMPILE_DISABLE&quot;] = &quot;1&quot;</p><p>However, there’s a big difference in the start training loss: one is 10+, and the other is 1.9. The code is the same.</p><p>{&#39;loss&#39;: 15.0507, &#39;grad_norm&#39;: 26.66766929626465, &#39;learning_rate&#39;: 0.0, &#39;epoch&#39;: 0.0}</p><p>{&#39;loss&#39;: 1.8776, &#39;grad_norm&#39;: 5.469211101531982, &#39;learning_rate&#39;: 0.0, &#39;epoch&#39;: 0.0}</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I figure it out, unsloth-zoo is not match the unsloth. When I manually set it to the same version 2025.8.1, it will be fine. I don‘t know what has been changed. but The problem of loss is still there.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1moylrq/need_help_torch_dynamoexcbackendcompilerfailed/n8logiz/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Did you update any other packages like trl?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1moylrq/need_help_torch_dynamoexcbackendcompilerfailed/n8g6xdx/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Wed, 13 Aug 2025 13:44:32 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1moc90n/some_grpo_questions/</link><title>Some GRPO questions</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1moc90n/some_grpo_questions/</guid><comments>https://www.reddit.com/r/unsloth/comments/1moc90n/some_grpo_questions/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1moc90n/some_grpo_questions/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Thank so much for the great fine-tuning tool, especially for memory saving.</p><p>I have been testing GRPO with qwen3.  I have a question.</p><p>Reward score gets improved. Yes, it seems working. I run it for 10 epochs. My question is about loss.  Loss is almost zero for first 1 epoch.  Then, it goes higher while reward goes up.</p><p>Is it normal that Loss = 0 for long time?</p><p>And, how multi gpu is going for GRPO?  I heard multi gpu is possible in unsloth except GRPO.  GRPO will be even better with multi gpu support.  Thanks again.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Lower your learning rate.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1moc90n/some_grpo_questions/n8bf1c2/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>What do you mean by memory saving. Can you explain your experience further on GRPO ?  I&#39;m just curious I&#39;m also learning fine tuning but just couldn&#39;t find a good use case yet for GRPO personally</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1moc90n/some_grpo_questions/n8bwibb/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 12 Aug 2025 21:14:31 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mo8nt7/bug_support_needed_on_mistral_small_32/</link><title>BUG / Support needed on mistral small 3.2</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mo8nt7/bug_support_needed_on_mistral_small_32/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mo8nt7/bug_support_needed_on_mistral_small_32/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mo8nt7/bug_support_needed_on_mistral_small_32/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><pre><code>from unsloth import FastLanguageModelmax_seq_length = 2048   dtype = None  # or torch.float16 / torch.bfloat16 as your GPU supportsload_in_4bit = Truemodel, tokenizer = FastLanguageModel.from_pretrained(    model_name = &quot;mistralai/Mistral-Small-3.2-24B-Instruct-2506&quot;,    max_seq_length = max_seq_length,    dtype = dtype,    load_in_4bit = load_in_4bit,)</code></pre><p>i only loaded the model :  </p><pre><code>from unsloth.chat_templates import get_chat_template# Test promptmessages = [    {        &quot;role&quot;: &quot;system&quot;,        &quot;content&quot;: &quot;you area helpful assistant that can generate anagrams of words.&quot;    },    {        &quot;role&quot;: &quot;user&quot;,        &quot;content&quot;: &quot;make anagram of &#39;hello&#39;&quot;    }]tools = [    {        &quot;type&quot;: &quot;function&quot;,        &quot;function&quot;: {            &quot;name&quot;: &quot;generate_anagram&quot;,            &quot;description&quot;: &quot;Generate an anagram of a given word&quot;,            &quot;parameters&quot;: {                &quot;type&quot;: &quot;object&quot;,                &quot;properties&quot;: {                    &quot;word&quot;: {                        &quot;type&quot;: &quot;string&quot;,                        &quot;description&quot;: &quot;The word to generate an anagram of&quot;                    }                },                &quot;required&quot;: [&quot;word&quot;]            }        }    }]inputs = tokenizer.apply_chat_template(    messages,    tokenize=True,    padding=True,    add_generation_prompt=True,    return_tensors=&quot;pt&quot;,    return_attention_mask=True,    tools=tools,).to(&quot;cuda&quot;)outputs = model.generate(input_ids=inputs, max_new_tokens = 128, use_cache=True)decoded = tokenizer.batch_decode(outputs)print(decoded[0])</code></pre><p>thentried infenrece :  </p><p>and this error shows up:<br/>---------------------------------------------------------------------------</p><p>TypeError                                 Traceback (most recent call last)</p><p>Cell In[2], line 35</p><p>4 messages = [</p><p>5     {</p><p>6         &quot;role&quot;: &quot;system&quot;,</p><p>(...)     12     }</p><p>13 ]</p><p>15 tools = [</p><p>16     {</p><p>17         &quot;type&quot;: &quot;function&quot;,</p><p>(...)     32     }</p><p>33 ]</p><p>---&gt; 35 inputs = tokenizer.apply_chat_template(</p><p>36     messages,</p><p>37     tokenize=True,</p><p>38     padding=True,</p><p>39     add_generation_prompt=True,</p><p>40     return_tensors=&quot;pt&quot;,</p><p>41     return_attention_mask=True,</p><p>42     tools=tools,</p><p>43 ).to(&quot;cuda&quot;)</p><p>45 outputs = model.generate(input_ids=inputs, max_new_tokens = 128, use_cache=True)</p><p>47 decoded = tokenizer.batch_decode(outputs)</p><p>File ~/finetuning/venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172, in deprecate_kwarg.&lt;locals&gt;.wrapper.&lt;locals&gt;.wrapped_func(*args, **kwargs)</p><p>168 elif minimum_action in (Action.NOTIFY, Action.NOTIFY_ALWAYS) and not is_torchdynamo_compiling():</p><p>169     # DeprecationWarning is ignored by default, so we use FutureWarning instead</p><p>170     warnings.warn(message, FutureWarning, stacklevel=2)</p><p>--&gt; 172 return func(*args, **kwargs)</p><p>File ~/finetuning/venv/lib/python3.12/site-packages/transformers/processing_utils.py:1531, in ProcessorMixin.apply_chat_template(self, conversation, chat_template, **kwargs)</p><p>1529 video_metadata = []</p><p>1530 for message in conversation:</p><p>-&gt; 1531     visuals = [content for content in message[&quot;content&quot;] if content[&quot;type&quot;] in [&quot;image&quot;, &quot;video&quot;]]</p><p>1532     audio_fnames = [</p><p>1533         content[key]</p><p>1534         for content in message[&quot;content&quot;]</p><p>1535         for key in [&quot;audio&quot;, &quot;url&quot;, &quot;path&quot;]</p><p>1536         if key in content and content[&quot;type&quot;] == &quot;audio&quot;</p><p>1537     ]</p><p>1538     image_fnames = [</p><p>1539         vision_info[key]</p><p>1540         for vision_info in visuals</p><p>1541         for key in [&quot;image&quot;, &quot;url&quot;, &quot;path&quot;, &quot;base64&quot;]</p><p>1542         if key in vision_info and vision_info[&quot;type&quot;] == &quot;image&quot;</p><p>1543     ]</p><p>TypeError: string indices must be integers, not &#39;str&#39;</p><p>Is this a problem i have or in the unsloth library</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>I just tried the notebook here by changing the name: <a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_%2822B%29-Alpaca.ipynb">https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_%2822B%29-Alpaca.ipynb</a></p><p>It works fine for me so it might be your setup</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mo8nt7/bug_support_needed_on_mistral_small_32/n8ejm87/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 12 Aug 2025 18:54:49 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mo86wf/error_in_the_latest_unslothgptoss_finetuning/</link><title>Error in the latest unsloth/gpt-oss finetuning script! How to fix?: NotImplementedError: Unsloth: Logits are empty from 2024.11 onwards. To get raw logits again, please set the environment variable `UNSLOTH_RETURN_LOGITS` to `"1" BEFORE starting to train ie before `trainer.train()`.</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mo86wf/error_in_the_latest_unslothgptoss_finetuning/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mo86wf/error_in_the_latest_unslothgptoss_finetuning/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 3 min | <a href='https://www.reddit.com/r/unsloth/comments/1mo86wf/error_in_the_latest_unslothgptoss_finetuning/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Complete Error:<br/>(.venv) wstf@gen-ai:~/finetune-gpt-oss-20b$ python finetune_with_unsloth.py<br/>/home/wstf/finetune-gpt-oss-20b/finetune_with_unsloth.py:19: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.  </p><p>Please restructure your imports with &#39;import unsloth&#39; at the top of your file.<br/>  from unsloth import FastLanguageModel, is_bfloat16_supported<br/>🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.<br/>🦥 Unsloth Zoo will now patch everything to make training faster!<br/>Loading GPT-OSS 20B model with Unsloth...<br/>==((====))==  Unsloth 2025.8.4: Fast Gpt_Oss patching. Transformers: 4.55.0.<br/>   \\   /|    NVIDIA RTX 6000 Ada Generation. Num GPUs = 1. Max memory: 47.363 GB. Platform: Linux.<br/>O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1<br/>\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]<br/> &quot;-____-&quot;     Free license: <a href="http://github.com/unslothai/unsloth">http://github.com/unslothai/unsloth</a><br/>Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!<br/>Loading checkpoint shards: 100%|███████| 4/4 [00:01&lt;00:00,  2.07it/s\]  Adding LoRA adapters...  Unsloth: Making \`model.base\_model.model.model\` require gradients  Loading dataset...  Formatting dataset...  tokenizer eos token: &lt;|return|&gt;<br/>##################################<br/>tokenizer pad token: &lt;|reserved_200017|&gt;<br/>Setting up training configuration...<br/>GPU = NVIDIA RTX 6000 Ada Generation. Max memory = 47.363 GB.<br/>19.354 GB of memory reserved.<br/>Starting training...<br/>==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1<br/>   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 60<br/>O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4<br/>\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8<br/> &quot;-____-&quot;     Trainable parameters = 0 of 20,918,738,496 (0.00% trained)</p><p>wandb: Tracking run with wandb version 0.21.1<br/>wandb: Run data is saved locally in /home/wstf/finetune-gpt-oss-20b/wandb/run-20250812_155445-ksb3gy7i<br/>wandb: Run `wandb offline` to turn off syncing.  0%|                                         | 0/60 [00:00&lt;?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.<br/>Traceback (most recent call last):<br/>  File &quot;/home/wstf/finetune-gpt-oss-20b/finetune_with_unsloth.py&quot;, line 212, in &lt;module&gt;<br/>main()<br/>  File &quot;/home/wstf/finetune-gpt-oss-20b/finetune_with_unsloth.py&quot;, line 119, in main<br/>trainer_stats = trainer.train()<br/>^^^^^^^^^^^^^^^<br/>  File &quot;/home/wstf/finetune-gpt-oss-20b/.venv/lib/python3.12/site-packages/transformers/trainer.py&quot;, line 2238, in train<br/>return inner_training_loop(<br/>^^^^^^^^^^^^^^^^^^^^<br/>  File &quot;&lt;string&gt;&quot;, line 323, in _fast_inner_training_loop<br/>  File &quot;/home/wstf/finetune-gpt-oss-20b/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py&quot;, line 907, in training_step<br/>return super().training_step(*args, **kwargs)<br/>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br/>  File &quot;&lt;string&gt;&quot;, line 34, in _unsloth_training_step<br/>  File &quot;/home/wstf/finetune-gpt-oss-20b/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py&quot;, line 879, in compute_loss<br/>shift_logits = outputs.logits[..., :-1, :].contiguous()<br/>~~~~~~~~~~~~~~^^^^^^^^^^^^^<br/>  File &quot;/home/wstf/finetune-gpt-oss-20b/unsloth_compiled_cache/unsloth_compiled_module_gpt_oss.py&quot;, line 131, in raise_logits_error<br/>def raise_logits_error(*args, **kwargs): raise NotImplementedError(LOGITS_ERROR_STRING)<br/>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br/>NotImplementedError: Unsloth: Logits are empty from 2024.11 onwards. To get raw logits again, please set the environment variable `UNSLOTH_RETURN_LOGITS` to `&quot;1&quot; BEFORE starting to train ie before `trainer.train()`. For example:<br/>```<br/>import os<br/>os.environ[&#39;UNSLOTH_RETURN_LOGITS&#39;] = &#39;1&#39;<br/>trainer.train()<br/>```<br/>No need to restart your console - just add `os.environ[&#39;UNSLOTH_RETURN_LOGITS&#39;] = &#39;1&#39;` before trainer.train() and re-run the cell!  </p><p>Added &quot;os.environ[&#39;UNSLOTH_RETURN_LOGITS&#39;] = &#39;1&#39;&quot; before trainer.train() also called imports after &quot;os.environ[&#39;UNSLOTH_RETURN_LOGITS&#39;] = &#39;1&#39;&quot; but still getting the same error!<br/>Any solutions?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>It may be related to this bug: <a href="https://github.com/unslothai/unsloth/issues/3071">https://github.com/unslothai/unsloth/issues/3071</a></p><p>Not sure if the workaround they suggest (rolling back to the unsloth-zoo 2025-7-1 version) works for you.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mo86wf/error_in_the_latest_unslothgptoss_finetuning/n8iriwo/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 12 Aug 2025 18:34:56 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mnocsx/how_to_fix_this_attributeerror_gptosstopkrouter/</link><title>How to fix this? AttributeError: 'GptOssTopKRouter' object has no attribute 'weight'</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mnocsx/how_to_fix_this_attributeerror_gptosstopkrouter/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mnocsx/how_to_fix_this_attributeerror_gptosstopkrouter/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mnocsx/how_to_fix_this_attributeerror_gptosstopkrouter/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><pre><code>from unsloth import FastLanguageModelimport torchmax_seq_length = 1024dtype = None# 4bit pre quantized models we support for 4x faster downloading + no OOMs.fourbit_models = [    &quot;unsloth/gpt-oss-20b-unsloth-bnb-4bit&quot;, # 20B model using bitsandbytes 4bit quantization    &quot;unsloth/gpt-oss-120b-unsloth-bnb-4bit&quot;,    &quot;unsloth/gpt-oss-20b&quot;, # 20B model using MXFP4 format    &quot;unsloth/gpt-oss-120b&quot;,] # More models at https://huggingface.co/unslothmodel, tokenizer = FastLanguageModel.from_pretrained(    model_name = &quot;Guilherme34/GPT-OSS-UNCENSORED-20B&quot;,    dtype = dtype, # None for auto detection    max_seq_length = max_seq_length, # Choose any for long context!# 4 bit quantization to reduce memory    full_finetuning = False, # [NEW!] We have full finetuning now!    # token = &quot;hf_...&quot;, # use one if using gated models)==((====))==  Unsloth 2025.8.4: Fast Gpt_Oss patching. Transformers: 4.56.0.dev0.   \\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.4.0\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False] &quot;-____-&quot;     Free license: Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!http://github.com/unslothai/unsloth---------------------------------------------------------------------------AttributeError                            Traceback (most recent call last) in &lt;cell line: 0&gt;()     13 ] # More models at      14 ---&gt; 15 model, tokenizer = FastLanguageModel.from_pretrained(     16     model_name = &quot;Guilherme34/GPT-OSS-UNCENSORED-20B&quot;,     17     dtype = dtype, # None for auto detection/tmp/ipython-input-1559322843.pyhttps://huggingface.co/unsloth in __getattr__(self, name)   1960             if name in modules:   1961                 return modules[name]-&gt; 1962         raise AttributeError(   1963             f&quot;&#39;{type(self).__name__}&#39; object has no attribute &#39;{name}&#39;&quot;   1964         )/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.pyAttributeError: &#39;GptOssTopKRouter&#39; object has no attribute &#39;weight&#39;</code></pre></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>We&#39;re working on it</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mnocsx/how_to_fix_this_attributeerror_gptosstopkrouter/n86p4qu/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The strange thing is that there is no error when using unsloth_bnb.<br/>Perhaps the gpt-oss source code has been modified once.<br/>There is already an issue about it on GitHub.</p><p><a href="https://github.com/unslothai/unsloth/issues/3119">https://github.com/unslothai/unsloth/issues/3119</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mnocsx/how_to_fix_this_attributeerror_gptosstopkrouter/n88md7x/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>i&#39;ve finetuned a model and merged and when loading i get this</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mnocsx/how_to_fix_this_attributeerror_gptosstopkrouter/n869kml/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p><a href="https://www.reddit.com/u/Front">u/Front</a>_Thought9364 we just fixed it. Please update the installation cell and also reuse our notebook. You unfortunately need to redo the whole thing: <a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B">https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb</a>-Fine-tuning.ipynb)</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mnocsx/how_to_fix_this_attributeerror_gptosstopkrouter/n8k16ev/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 12 Aug 2025 02:03:59 +0530</pubDate></item><item><link>https://github.com/Ashx098/sft-play</link><title>From Data to Inference: Fully Automated QLoRA/LORA/Full Tuning for Local LLMs (github.com)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mn89jr/from_data_to_inference_fully_automated/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mn89jr/from_data_to_inference_fully_automated/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1mn89jr/from_data_to_inference_fully_automated/'>Post permalink</a></p></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Looks great, maybe you can see if you can implement some of the more advanced capability enhancing LoRAs like ellora - <a href="https://github.com/codelion/ellora">https://github.com/codelion/ellora</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mn89jr/from_data_to_inference_fully_automated/n82zdiy/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Mon, 11 Aug 2025 14:52:43 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mmfn9b/make_llm_remember_menot_by_prompt_or_rag/</link><title>Make LLM remember me.not by prompt or Rag?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mmfn9b/make_llm_remember_menot_by_prompt_or_rag/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mmfn9b/make_llm_remember_menot_by_prompt_or_rag/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mmfn9b/make_llm_remember_menot_by_prompt_or_rag/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi, everyone. I m kinda excited to make a local LLM assistant, but how can i make the model remember my informations without any prompt or context informations. </p><p>Im curious about how llm really remember facts, tho i was told that LLM absorted facts mainly in Pretraining process. so, do i need to SFT LLM with my dataset or shoud i Continue Pretraining with unsupervised dataset first.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>&gt; without any prompt or context informations.</p><p>This can only be done via fine-tuning since the information has to be baked into the weights if it has to work without any prompt or context. The only other thing I can think of is some sort of tool calling that can look up a memory or database to fetch profile information based on your user name or something like that.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mmfn9b/make_llm_remember_menot_by_prompt_or_rag/n7x9gnq/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>First begin by gathering the largest corpus of personal information you possibly can then do a continued pre-training run using this new base model. You could then run it through supervised fine-tuning to teach it how to perfectly use its new acquired data if you skip adding the used context via continued pre-training you’re teaching the models weights to almost never align something within its pre-trained context which will rapidly degrade intelligence and cause hallucinations</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mmfn9b/make_llm_remember_menot_by_prompt_or_rag/n7y4uhy/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>You wanna finetune or talking about langmem or similar?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mmfn9b/make_llm_remember_menot_by_prompt_or_rag/n7y9vxt/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sun, 10 Aug 2025 16:29:47 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mme7sm/the_curious_case_of_running_unsloth_glm41v9b_gguf/</link><title>the curious case of running unsloth GLM-4.1V-9B GGUF on llama.cpp: No mmproj files, Multi-modal CLI requires -mmproj, and doesn't support --jinja?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mme7sm/the_curious_case_of_running_unsloth_glm41v9b_gguf/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mme7sm/the_curious_case_of_running_unsloth_glm41v9b_gguf/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mme7sm/the_curious_case_of_running_unsloth_glm41v9b_gguf/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hello everyone,</p><p>I&#39;m trying to test the <strong>Unsloth GLM-4.1V-9B-Thinking VLM GGUF</strong> on a local <strong>llama.cpp</strong> build, but I&#39;m running into a confusing issue regarding the multi-modal projection file and chat templates.</p><hr/><h2>My Setup</h2><ul><li><strong>Model:</strong> <code>unsloth/GLM-4.1V-9B-Thinking-UD-Q4_K_XL.gguf</code></li><li><strong>Executables:</strong> <code>llama-cli.exe</code> and <code>llama-mtmd-cli.exe</code><br/>(both from a pre-built <strong>llama.cpp</strong> build <strong>b6103</strong>)</li></ul><hr/><h2>The Problem</h2><p>My goal is to use the model&#39;s <strong>VLM features</strong> by providing both a prompt <strong>and</strong> an image.<br/>However, this model <strong>doesn&#39;t come with an mmproj file</strong>.</p><ul><li><p><strong><code>llama-cli.exe</code></strong>:</p><ul><li>Recognizes the <code>--jinja</code> flag.</li><li>Does <strong>not</strong> support multi-modal flags like <code>--image</code> or <code>-i</code>.</li></ul></li><li><p><strong><code>llama-mtmd-cli.exe</code></strong>:</p><ul><li>Supports the <code>--image</code> flag.</li><li>Does <strong>not</strong> support the <code>--jinja</code> flag.</li><li>Appears to require a separate <code>-mmproj</code> file to function.</li></ul></li></ul><hr/><h2>What I Have Tried</h2><ol><li><p><strong>Text-only with <code>llama-cli.exe</code></strong>  </p><ul><li>Loads model and responds to text-only prompts.<br/></li><li>Confirms <code>--jinja</code> works correctly here.</li></ul></li><li><p><strong>VLM command with <code>llama-cli.exe</code></strong>  </p><ul><li>Failed — <code>--image</code> flag is not available.</li></ul></li><li><p><strong>VLM command with <code>llama-mtmd-cli.exe</code></strong>  </p><ul><li>Using <code>--jinja</code> → Error:<br/><code>error: invalid argument: --jinja</code></li><li>Using <code>--image</code> without <code>--jinja</code> → Error:<br/><code>-mmproj flag is required</code>I assumed, based on similar models, that the <strong>GLM-4.1V-9B GGUF</strong> has the multi-modal projection layers <em>baked-in</em> and wouldn’t require a separate <code>mmproj</code> file.<br/>However, after checking the <strong>Unsloth Hugging Face</strong> page, I couldn’t find any dedicated <code>mmproj</code> file.</li></ul></li></ol><p>Has anyone successfully run this model on llama.cpp?Any guidance on how to get this model working would be greatly appreciated.<br/>Thank you!</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>@ <a href="https://www.reddit.com/u/danielhanchen">u/danielhanchen</a> <a href="https://www.reddit.com/u/yoracale">u/yoracale</a>.  Please help if you have any idea.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mme7sm/the_curious_case_of_running_unsloth_glm41v9b_gguf/n83gkew/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sun, 10 Aug 2025 15:02:28 +0530</pubDate></item><item><link>https://i.redd.it/mlikt0f7ythf1.png</link><title>gpt-oss Fine-tuning is here! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1ml480n/gptoss_finetuning_is_here/</guid><comments>https://www.reddit.com/r/unsloth/comments/1ml480n/gptoss_finetuning_is_here/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1ml480n/gptoss_finetuning_is_here/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys, we now support gpt-oss finetuning. We’ve managed to make gpt-oss train on just 14GB of VRAM, making it possible to work on free Colab.</p><p>We also talk about our <strong>bugfixes</strong>, <strong>notebooks</strong> etc all in our guide: <a href="https://docs.unsloth.ai/basics/gpt-oss">https://docs.unsloth.ai/basics/gpt-oss</a></p><p>Unfortunately due to gpt-oss&#39; architecture, if you want to train the model without Unsloth, you’ll need to upcast the weights to bf16 before training. This approach, significantly increases both VRAM usage and training time by as much as <strong>300% more memory usage</strong>!</p><p>gpt-oss-120b model fits on 65GB of VRAM with Unsloth.</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/mlikt0f7ythf1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>In lm studio original version have reasoninf level selector. Unsloth modal doesnt have reasoning mode selectoe</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1ml480n/gptoss_finetuning_is_here/n7nkmd6/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Did you manage to fix the gpt-oss guffs to run on ollama? It was giving an error when running</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1ml480n/gptoss_finetuning_is_here/n7o7i3n/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I got stuck, but then was able to upgrade vllm? and it started working for some reason.<br/>Then I merged the lora and created a safetensors</p><p>I tried to run it with vllm, and got an error. I looked and the release is old. I tried with pip install from github vllm, but that failed. Do we need to wait for vllm release for support to run this model?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1ml480n/gptoss_finetuning_is_here/n7pbl5v/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I got it working over the weekend, thanks for this!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1ml480n/gptoss_finetuning_is_here/n84f9zq/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>It&#39;s not working in Colab GPT_OSS_MXFP4_(20B)-Inference.ipynb with T4 GPU - doesn&#39;t seem to like parameter &#39;reasoning_effort&#39; - throwing: AcceleratorError: CUDA error: device-side assert triggered - Uncommenting this parameter works but then give error when trying to train:</p><pre><code>AttributeError: &#39;PeftModel&#39; object has no attribute &#39;_flag_for_generation&#39;</code></pre><p>Tried a clean install - I&#39;m assuming it&#39;s using an older version of unsloth but I am simply running a copy of the provided colab .</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1ml480n/gptoss_finetuning_is_here/n836c2f/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>how to convert to gguf after fine tuning gpt-oss-20b?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1ml480n/gptoss_finetuning_is_here/n85jbru/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>why don&#39;t guys post some instructions to avoid overfitting the small LLMs and VLMs</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1ml480n/gptoss_finetuning_is_here/n88gzsw/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 09 Aug 2025 00:43:19 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mkp7ua/how_you_could_boost_pp_rates_of_amd_mi50/</link><title>How you could boost P/P rates of AMD MI50</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mkp7ua/how_you_could_boost_pp_rates_of_amd_mi50/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mkp7ua/how_you_could_boost_pp_rates_of_amd_mi50/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mkp7ua/how_you_could_boost_pp_rates_of_amd_mi50/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Continue from my last post, and thanks for valuable comments!</p><p>(Localllama&#39;s Moderator blocked my post now, but I don&#39;t know what I violated)</p><p>In the beginning, I set up 4070ti(12GB VRAM) + MI50(32GB VRAM) on my gaming gear,</p><p>However, I only could access 12 +12 GB of vram in two GPUs - it was restricted by size of first gpu&#39;s VRAM(12G)</p><p>or, MI 32GB only by turn off using 4070ti on Win11 / Vulkan / LM studio environment.</p><p>Since last weekeens, I have been trying to access the rest portion of total 44G Vram(gpu0+gpu1) in Local LLM running.</p><p>(It wasn&#39;t fault of MI50, it is clearly related with incomplete vulkan/llama.cpp implementation of LM Studio)</p><p>Most easy solution may be put MI50 on &quot;first&quot; PCI 5.0 slot,  but the MI50 doesn&#39; supports screen output unless bios rom writing.</p><p>Finally, I found a simple way to exchange gpu0 and 1 postion in Windows. -</p><p>Just go right Control Panel =&gt; System =&gt; Display =&gt; Graphics</p><p>and Let RADEON VII(MI50) as a primary graphic card of LM Studio Apps</p><p>By this way, I got &quot;almost&quot; 32GB VRAMs (sorry it&#39;s not 32+12GB yet) in LM Studio </p><p>It not only gluing 32GB of HBM on your gpu, but also can steal prompt processing ability from old Nvidia GPU</p><p>Please show three results from favorite scenarios. Whole test have conducted Win11/Vulkan Envrionment.</p><p><strong>1. Legal Document Analysis(21,928 Input tokens)</strong></p><p>Model : ERNIE-4.5-21B-A3B (Q6_K, size: 18.08GB)  to check effects of GPU position between GPU 0 and 1</p><p>GPU Setting                     Token Generation  Total Output(Tokens)   Time to 1st Token</p><p>MI50(gpu0)+4070TI(gpu1)   23.27(token/s)          1303(tokens)             195.74sec</p><p>4070TI(gpu0)+MI50(gpu1)   24.00(token/s)          1425(tokens)             174.62sec</p><p><strong>2. Hard SF Novel Writing (929 Input tokens)</strong></p><p>Model : Qwen3-30B-A3B-Thinking-2507 (Q8_0, 32.48GB) - Max accessible memory test</p><p>GPU Setting                     Token Generation  Total Output(Tokens)   Time to 1st Token</p><p>MI50(main)+4070TI(sub)*       13.86(token/s)             6437(tokens)           13.08sec</p><p>MI50(32GB only)                 17.93(token/s)             5656(tokens)          17.75sec</p><ul><li>Whole model has landed on MI50(about 21GB) &amp; 4070(11GB) successfully.</li></ul><p><strong>3. Multilingual Novel Summerization(27,393 Input Tokens)</strong></p><p>Gemma-3-27b-QAT (Q4_0, 16.43GB, 4bit KV Cache)</p><p>GPU Setting                     Token Generation  Total Output(Tokens)   Time to 1st Token</p><p>MI50(main)+4070TI(sub)          4.19(tokens)               907(tokens)            10min 2sec</p><p>MI50(only)                            2.92(tokens)               1058(token)           33min** 41s </p><p>Many GPU poor including me always said that &quot;I&#39;m patient man&quot;, however, 33 minutes vs. 10 minutes is a good reason to think twice before ordering MI50 and adding Nvidia used Card instead. - P/P is really crawling on AMD but this disadvantage can be overcome by attaching Nvidia Card.</p><p>I still think the MI50 is a very cheap and appropriate investment for hobbiest even considering these drawbacks.</p><p>If anyone is familiar with the Linux environment and llama.cpp, I&#39;d appreciate it if you could share some insights and benchmark result on distributed inference using RPC. Setting it up that way might allow access to all VRAM, excluding any frameworks penalties from using multiple GPUs.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Why are you sticking with LM Studio despite all these hurdles? Why not try vanilla llama.cpp with vanilla vulkan backend?</p><p>Those prompt processing numbers for the novel summerizarion in Gemma 3 are very very low, like single core inference low. Even with the 4070, something is very very wrong. I don&#39;t remember the numbers off the top of my head, by a single P40 with Gemma 3 27B QAT was at least 30x faster.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mkp7ua/how_you_could_boost_pp_rates_of_amd_mi50/n7kexzt/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 08 Aug 2025 13:25:47 +0530</pubDate></item><item><link>https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF</link><title>Qwen3-4B-2507 Unsloth Dynamic GGUFs out now! (huggingface.co)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mjgsm9/qwen34b2507_unsloth_dynamic_ggufs_out_now/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mjgsm9/qwen34b2507_unsloth_dynamic_ggufs_out_now/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mjgsm9/qwen34b2507_unsloth_dynamic_ggufs_out_now/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey y&#39;all here they are for the new Qwen model including Thinking version: <a href="https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF">https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF</a></p><p>Let us know if there are any issues.</p><p>P.S. gpt-oss support coming tomorrow and I think you guys are gonna LOVE it. We did some cooking and made some magic work! ;)</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Unsloth FTW !</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mjgsm9/qwen34b2507_unsloth_dynamic_ggufs_out_now/n7eajgb/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Great work!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mjgsm9/qwen34b2507_unsloth_dynamic_ggufs_out_now/n7b4m34/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>No 4B bnb-4bit for 2507 version for finetuning?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mjgsm9/qwen34b2507_unsloth_dynamic_ggufs_out_now/n7l8757/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 07 Aug 2025 02:39:42 +0530</pubDate></item><item><link>https://huggingface.co/blog/driaforall/towards-open-evolutionary-agents</link><title>Towards Open Evolutionary Agents (huggingface.co)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mj4ihy/towards_open_evolutionary_agents/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mj4ihy/towards_open_evolutionary_agents/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1mj4ihy/towards_open_evolutionary_agents/'>Post permalink</a></p></section>]]></description><pubDate>Wed, 06 Aug 2025 18:50:34 +0530</pubDate></item><item><link>https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF</link><title>Qwen3-Coder GGUFs with even more fixes esp. for tool calling! (huggingface.co)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mj1dwo/qwen3coder_ggufs_with_even_more_fixes_esp_for/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mj1dwo/qwen3coder_ggufs_with_even_more_fixes_esp_for/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mj1dwo/qwen3coder_ggufs_with_even_more_fixes_esp_for/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Recently we&#39;ve updated Qwen3-Coder and although we previously addressed tool calling issues, the fix only worked in certain setups, such as llama.cpp. With other configurations, tool functionality remained inconsistent.</p><p>This new update has undergone extensive testing, by us and others, and should significantly improve tool calling reliability and mostly resolve any strange behaviors.</p><p>You may still experience some issues though, however this is now out of our hands as we have already done the most fixes we could. Now we will need to wait for the amazing llama.cpp team to fix the rest.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Amazing, you guys are pillars of community.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mj1dwo/qwen3coder_ggufs_with_even_more_fixes_esp_for/n77r5s4/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Is 480B also updated?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mj1dwo/qwen3coder_ggufs_with_even_more_fixes_esp_for/n7801sr/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p><a href="https://preview.redd.it/nptxv1zkvjhf1.png?width=1018&amp;format=png&amp;auto=webp&amp;s=94eab845062efc1182390d17f5731fdafbb6e133">https://preview.redd.it/nptxv1zkvjhf1.png?width=1018&amp;format=png&amp;auto=webp&amp;s=94eab845062efc1182390d17f5731fdafbb6e133</a></p><p>I am not able to download to ollama or LMstudio. is this normal.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mj1dwo/qwen3coder_ggufs_with_even_more_fixes_esp_for/n7dmc7o/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Last updated shown as <strong>1 day ago.</strong><br/>Is there fixes coming or fixed and updated ?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mj1dwo/qwen3coder_ggufs_with_even_more_fixes_esp_for/n78abgf/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>What got fixed? Because I just downloaded new quants yesterday and the issue was still present. Is the update in the prompt template and nothing to do with the quants themselves?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mj1dwo/qwen3coder_ggufs_with_even_more_fixes_esp_for/n7944u2/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The new prompt template totally breaks integration with Continue. I had to use the old one which works for about 3-4 tool calls in a row before adding some xml closing tags breaking functionality... Anyways using new prompt it complains about the fact that it cannot convert null to string on lmstudio side... It s easily reproducible by just trying one or 2 prompts using continue</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mj1dwo/qwen3coder_ggufs_with_even_more_fixes_esp_for/n7o5u2u/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Vjv</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mj1dwo/qwen3coder_ggufs_with_even_more_fixes_esp_for/n7q2x1b/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Wed, 06 Aug 2025 16:22:01 +0530</pubDate></item><item><link>https://huggingface.co/unsloth/gpt-oss-20b-GGUF</link><title>gpt-oss Unsloth GGUFs are here! (huggingface.co)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mik1qy/gptoss_unsloth_ggufs_are_here/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mik1qy/gptoss_unsloth_ggufs_are_here/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mik1qy/gptoss_unsloth_ggufs_are_here/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>You can now run OpenAI&#39;s gpt-oss-120b &amp; 20b open models locally with our GGUFs! 🦥</p><p>Run the 120b model on 66GB RAM &amp; 20b model on 14GB RAM. Both in original precision.</p><p>20b GGUF: <a href="https://huggingface.co/unsloth/gpt-oss-20b-GGUF">https://huggingface.co/unsloth/gpt-oss-20b-GGUF</a></p><p><strong>Uploads includes our chat template fixes.</strong> Finetuning support coming soon!</p><p>Guide: <a href="https://docs.unsloth.ai/basics/gpt-oss">https://docs.unsloth.ai/basics/gpt-oss</a></p><p>120b GGUF: <a href="https://huggingface.co/unsloth/gpt-oss-120b-GGUF">https://huggingface.co/unsloth/gpt-oss-120b-GGUF</a></p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>We need a &quot;less safe&quot; version))</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mik1qy/gptoss_unsloth_ggufs_are_here/n741sru/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Is there no 4bit ones available? I see only bf16 options</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mik1qy/gptoss_unsloth_ggufs_are_here/n742bgy/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Finally 😸</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mik1qy/gptoss_unsloth_ggufs_are_here/n748usf/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Curious where your guide got openai&#39;s recommended settings from? the defaults the model ships with are way off from this, but these settings seem to make it rip and roar in LM Studio. but I can&#39;t find them anywhere but your guide</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mik1qy/gptoss_unsloth_ggufs_are_here/n7685oc/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p><a href="https://www.reddit.com/u/yoracale">u/yoracale</a> I am getting the following error with a freshly pulled llama.cpp:</p><p><code>gguf_init_from_file_impl: tensor &#39;blk.25.ffn_down_exps.weight&#39; has invalid ggml type 39 (NONE)</code></p><p><code>gguf_init_from_file_impl: failed to read tensor info</code></p><p><code>llama_model_load: error loading model: llama_model_loader: failed to load model from /media/rgilbreth/T9/Models/gpt-oss-120b-F16.gguf</code></p><p><code>llama_model_load_from_file_impl: failed to load model</code></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mik1qy/gptoss_unsloth_ggufs_are_here/n74brkq/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I’m using lm studio beta version on a Mac with the latest beta versions of runtimes. I noticed that the reasoning high prompt works with the smaller 20b model using the open ai version, but reasoning high as a system prompt doesn’t work with the unsloth f16 120b version - any ideas how I can set the reasoning to high using lm studio?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mik1qy/gptoss_unsloth_ggufs_are_here/n75ycdi/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>no support for GGUFs on Ollama for now?</p><p>my logs below:</p><p>root@userone:/home/user# ollama --versionollama version is 0.11.2</p><p>root@userone:/home/user# ollama listNAME                                      ID              SIZE     MODIFIED<br/>hf.co/unsloth/gpt-oss-20b-GGUF:Q8_K_XL    643ca1be12ac    13 GB    51 minutes ago    </p><p>root@userone:/home/user# ollama run hf.co/unsloth/gpt-oss-20b-GGUF:Q8_K_XLError: 500 Internal Server Error: unable to load model: /usr/share/ollama/.ollama/models/blobs/sha256-41f115a077c854eefe01dff3b3148df4511cbee3cd3f72a5ed288ee631539de0</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mik1qy/gptoss_unsloth_ggufs_are_here/n789ghz/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Wed, 06 Aug 2025 01:45:05 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mibxei/training_qwen3coder/</link><title>Training Qwen3-Coder</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mibxei/training_qwen3coder/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mibxei/training_qwen3coder/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mibxei/training_qwen3coder/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys,</p><p>Thanks for the lib, wanted to know if there is a way to train unsloth/Qwen3-Coder-30B-A3B-Instruct with vllm in a GRPO fashion, i see that its supported by vllm but as we need to use FastModel instead of FastModelLanguage It does not seem possible to have a vllm engine runnign for the training, is my understanding wrong?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I’m curious, why do we need to train a coder model? Isn’t parsing the documentation of libraries and components we intend to use sufficient?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mibxei/training_qwen3coder/n72k89a/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>MoE varaints of Qwen3 train 5-10x slower in unsloth in my expierence </p><p>example14B 3h vs 30A3B 35h</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mibxei/training_qwen3coder/n72kfa6/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 05 Aug 2025 20:44:07 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mi5pzf/glm45_air_ud5_model_has_unused_tensor/</link><title>GLM4.5 AIR UD5. Model has unused tensor</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mi5pzf/glm45_air_ud5_model_has_unused_tensor/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mi5pzf/glm45_air_ud5_model_has_unused_tensor/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mi5pzf/glm45_air_ud5_model_has_unused_tensor/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>When i run the glm4.5 air q5 k xl with llama.cpp b6090 it says that </p><p>model has unused tensor 46 .... ignoring </p><p>model has unused tensor 46 .... ignoring </p><p>etc</p><p>Is this due to the model or llama.cpp is not ready yet?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>MTP.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mi5pzf/glm45_air_ud5_model_has_unused_tensor/n715t9w/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 05 Aug 2025 16:10:15 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mi3yis/qwen3coder30b_issues_with_tool_calls/</link><title>Qwen3-coder-30b issues with tool calls</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mi3yis/qwen3coder30b_issues_with_tool_calls/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mi3yis/qwen3coder30b_issues_with_tool_calls/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mi3yis/qwen3coder30b_issues_with_tool_calls/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I have been using the qwen3-30b series of models in LM studio server with Crush CLI and loving them but the coder variant always fails to call tools, somtimes it puts text in the response to the user, sometimes I get api errors about invalid messages in the payload.</p><p>I took the prompt template from qwen3-30b-2507-instruct and replaced the coders prompt template.</p><p>The coder model now calls tools correctly and I am no longer getting API errors but I dont actually know what it was I was changing exactly. Can swapping out the promp template this way cause other issues with the model or affect is coding abilities?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>The qwen3-coder uses a new format to structure the tool calls. The qwen team included a python script on the huggingface repo for parsing the tool calls (I think it was JSON vs XML or something). The original tool call parser in your inference engine (like llama.cpp) may not work with the new format yet, and by replacing the template you&#39;re essentially forcing the model to use the old format, which the inference engine already supports.</p><p>I&#39;m no expert in LLM fine-tuning, but I think replacing the template might lead to the model not producing correct tool calls occasionally, since these weren&#39;t the format they were trained on. On the other hand, it&#39;s also possible that the model is smart enough to recognise the alternative format and works just fine.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mi3yis/qwen3coder30b_issues_with_tool_calls/n70vbbb/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 05 Aug 2025 14:21:57 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mi0dkw/modernbert_cant_be_trained_in_colab_anymore/</link><title>modernBERT can't be trained in colab anymore</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mi0dkw/modernbert_cant_be_trained_in_colab_anymore/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mi0dkw/modernbert_cant_be_trained_in_colab_anymore/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1mi0dkw/modernbert_cant_be_trained_in_colab_anymore/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>wondering if anyone knows how to fix this? </p><p><a href="https://github.com/unslothai/unsloth/issues/2902">https://github.com/unslothai/unsloth/issues/2902</a></p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Seems to be comptability issues with the new updates to transformers, we&#39;ll take a look</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mi0dkw/modernbert_cant_be_trained_in_colab_anymore/n70or8i/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Hi, if you want to use full param training please set full_finetuning=True, in FastModel.from_pretrained<br/>instead of<br/>for param in model.parameters():<br/>param.requires_grad = True</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mi0dkw/modernbert_cant_be_trained_in_colab_anymore/n71pr95/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Can Unsloth handle T5?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mi0dkw/modernbert_cant_be_trained_in_colab_anymore/n71qkxw/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 05 Aug 2025 10:37:30 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mhjjbj/cant_use_qwent3coder_30b/</link><title>can't use qwent3-coder 30b</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mhjjbj/cant_use_qwent3coder_30b/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mhjjbj/cant_use_qwent3coder_30b/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mhjjbj/cant_use_qwent3coder_30b/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Asking it for anything will work for a minute then it&#39;ll start repeating.</p><p>Verified it&#39;s not a context issue.</p><p>Fixed:</p><p>Updating llama.cpp fixed the issue.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>which quant and what is your question?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mhjjbj/cant_use_qwent3coder_30b/n6wme73/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Also post your hardware</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mhjjbj/cant_use_qwent3coder_30b/n6wqacc/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Runs well on Mac with LMStudio.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mhjjbj/cant_use_qwent3coder_30b/n6xu64m/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Choppy for me too. Unsloth q5-m. Downgraded to q4-m. Macminim4 with 32gb ram in ollama.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mhjjbj/cant_use_qwent3coder_30b/n6xca6j/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Mon, 04 Aug 2025 22:47:43 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mgsbx4/native_support_for_internvl3/</link><title>Native support for InternVL3?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mgsbx4/native_support_for_internvl3/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mgsbx4/native_support_for_internvl3/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mgsbx4/native_support_for_internvl3/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>It&#39;s a good vision-first model that should be really great for vision tasks especially when finetuned. Qwen2.5VL is actually better for less size out of the box and so being able to finetune the InternVL3 base model would realize a lot of the potential of this model.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Hi there it should work out of the box. Just change the model name</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mgsbx4/native_support_for_internvl3/n6ub5bl/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Mon, 04 Aug 2025 01:07:31 +0530</pubDate></item><item><link>https://www.reddit.com/r/LocalLLaMA/comments/1mgdur5/icmdpo_used_qwen3s_coherent_understanding_to/</link><title>🧠 ICM+DPO: Used Qwen3's coherent understanding to improve Gemma3 at math - cross-model capability transfer with zero supervision</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mgeozo/icmdpo_used_qwen3s_coherent_understanding_to/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mgeozo/icmdpo_used_qwen3s_coherent_understanding_to/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mgeozo/icmdpo_used_qwen3s_coherent_understanding_to/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey <a href="https://www.reddit.com/r/LocalLLaMA">r/LocalLLaMA</a>! </p><p>Just released something that extends the recent <a href="https://arxiv.org/abs/2506.10139">ICM paper</a> in a big way - using one model&#39;s coherent understanding to improve a completely different model.</p><h1>Background: What is ICM?</h1><p>The original <a href="https://arxiv.org/abs/2506.10139">&quot;Unsupervised Elicitation of Language Models&quot;</a> paper showed something remarkable: <strong>models can generate their own training labels by finding internally coherent patterns</strong>.</p><p>Their key insight: pretrained models already understand concepts like mathematical correctness, but struggle to express this knowledge consistently. ICM finds label assignments that are &quot;mutually predictable&quot; - where each label can be predicted from all the others.</p><p><strong>Original ICM results</strong>: Matched performance of golden supervision without any external labels. Pretty amazing, but only improved the same model using its own labels.</p><h1>Our extension: Cross-model capability transfer</h1><p>We took ICM further - <strong>what if we use one model&#39;s coherent understanding to improve a completely different model?</strong></p><p><strong>Our process:</strong></p><ol><li>Used ICM on Qwen3 to extract its coherent math reasoning patterns</li><li>Generated DPO training data from Qwen3&#39;s coherent vs incoherent solutions</li><li>Trained Gemma3 on this data - <strong>Gemma3 learned from Qwen3&#39;s understanding</strong></li><li>Zero external supervision, pure model-to-model knowledge transfer</li></ol><h1>Results on local models</h1><p><strong>Qwen3-0.6B</strong>: 63.2 → 66.0 MATH-500 (+4%) [original ICM self-improvement]<br/><strong>Gemma3-1B</strong>: 41.0 → 45.6 MATH-500 (+11%) [<strong>novel: learned from Qwen3!</strong>]</p><p><strong>The breakthrough</strong>: Successfully transferred mathematical reasoning coherence from Qwen3 to improve Gemma3&#39;s abilities across different architectures.</p><h1>Why this matters beyond the original paper</h1><ul><li><strong>Cross-model knowledge transfer</strong> - use any strong model to improve your local models</li><li><strong>Democratizes capabilities</strong> - extract from closed/expensive models to improve open ones</li><li><strong>No training data needed</strong> - pure capability extraction and transfer</li><li><strong>Scales the ICM concept</strong> - from self-improvement to ecosystem-wide improvement</li></ul><h1>What&#39;s available</h1><ul><li><strong>Code</strong>: <a href="https://github.com/codelion/icm">https://github.com/codelion/icm</a></li><li><strong>Both models</strong>: Self-improved Qwen3 + Gemma3 (learned from Qwen3)</li><li><strong>Transfer pipeline</strong>: Extract from any model to improve another</li><li><strong>Full writeup</strong>: <a href="https://huggingface.co/blog/codelion/internal-coherence-maximization">https://huggingface.co/blog/codelion/internal-coherence-maximization</a></li></ul><h1>Quick start</h1><pre><code>git clone https://github.com/codelion/icm.git &amp;&amp; cd icm &amp;&amp; pip install -e .# Extract coherent patterns from a strong model (teacher)icm run --model Qwen/Qwen2.5-Math-7B-Instruct --dataset gsm8k --max-examples 500# Use those patterns to improve your local model (student)icm export --format dpo --output-path teacher_knowledge.jsonl# Train your model on teacher_knowledge.jsonl</code></pre><p>Anyone interested in trying capability transfer with their local models?</p></div><!-- SC_ON --></section>]]></description><pubDate>Sun, 03 Aug 2025 14:36:39 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mgcgxg/we_enabled_multigpu_training_in_unsloth_ai_a/</link><title>We enabled Multi-GPU training in Unsloth AI - a feature that's usually paid - using just 2 Copilot prompts!</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mgcgxg/we_enabled_multigpu_training_in_unsloth_ai_a/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mgcgxg/we_enabled_multigpu_training_in_unsloth_ai_a/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1mgcgxg/we_enabled_multigpu_training_in_unsloth_ai_a/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p><a href="https://github.com/oevortex/unsloth">https://github.com/oevortex/unsloth</a></p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Hey thanks so much for creating this! The feature is actually not paid as it already works inside of Unsloth! FYI there are many other repos which did this which we mentioned in our multigpu article: <a href="https://docs.unsloth.ai/basics/multi-gpu-training-with-unsloth">https://docs.unsloth.ai/basics/multi-gpu-training-with-unsloth</a></p><p>It&#39;s just that we are working on a much much better version and easier version of it which we hope to release soon and thus haven&#39;t officially announced it.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mgcgxg/we_enabled_multigpu_training_in_unsloth_ai_a/n6nv96y/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>OP, how to actually use multi GPU in your repo? All the doc links point to Unsloth.ai</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mgcgxg/we_enabled_multigpu_training_in_unsloth_ai_a/n6ovjer/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sun, 03 Aug 2025 12:11:05 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mfmt7x/request_4bit_quant_of_unslothmedgemma27bit_to/</link><title>Request: 4bit quant of unsloth/medgemma-27b-it to make it finetunable for the GPU poor</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mfmt7x/request_4bit_quant_of_unslothmedgemma27bit_to/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mfmt7x/request_4bit_quant_of_unslothmedgemma27bit_to/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1mfmt7x/request_4bit_quant_of_unslothmedgemma27bit_to/'>Post permalink</a></p></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>We uploaded the text one but I&#39;m guessing you&#39;re specifically looking for the vision one: <a href="https://huggingface.co/unsloth/medgemma-27b-text-it-unsloth-bnb-4bit">https://huggingface.co/unsloth/medgemma-27b-text-it-unsloth-bnb-4bit</a></p><p>When you fine-tune using QLORA using Unsloth, we convert it to 4bit on the fly for you</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mfmt7x/request_4bit_quant_of_unslothmedgemma27bit_to/n6jn5t0/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 02 Aug 2025 15:28:57 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mezuw9/newbie_needs_help/</link><title>Newbie Needs Help</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mezuw9/newbie_needs_help/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mezuw9/newbie_needs_help/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1mezuw9/newbie_needs_help/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey everyone. I hate to ask such a basic question, but I&#39;m kinda stuck and need some help.</p><p>I&#39;ve only recently started diving into the world of self-hosted LLM&#39;s and AI services. Having a ton of fun so far.</p><p>I&#39;m running Ollama and Open WebUI in docker locally. I&#39;ve used the models from Ollama which have been great so far. I recently started trying out new models from huggingface.co. The Unsloth team has released several models recently I&#39;m wanting to try out. Specifically the Qwen3-30B-A3B-2507 Thinking and Instruct models.</p><p>However I&#39;m running into some really odd behavior with these models. I downloaded the GGUF files for <code>Qwen3-30B-A3B-Instruct-2507-UD-Q4_K_XL.gguf</code> and <code>Qwen3-30B-A3B-Thinking-2507-UD-Q4_K_XL.gguf</code>. In Open WebUI I set the temperature, min_p, top_p, topk, max_tokens, and presence_penalty settings for the models according to the Unsloth Qwen3 documentation. I installed the GGUF model files by using the model management in Open WebUI and uploading the GGUF&#39;s.</p><p>Odd behavior I see:</p><ol><li>When I query the Thinking model, I don&#39;t get any &quot;Thinking&quot; indicator like I do with other Thinking models. It responds just like a reasoning model. Forcing the &quot;think&quot; parameter causes an error saying the model doesn&#39;t support thinking.</li><li>When I query either model sometimes it gives a very short accurate answer, other times it just goes on and on and on and on. Seemingly coming up with questions on topics I never asked about.</li></ol><p>I don&#39;t see anyone else complaining about these issues, so I assume it&#39;s because I&#39;ve done something wrong.</p><p>Any help would be appreciate.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Yes this issue with the thinking model was stated and fixed a whole ago, see here: <a href="https://www.reddit.com/r/unsloth/s/hiBpHlaqzN">https://www.reddit.com/r/unsloth/s/hiBpHlaqzN</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mezuw9/newbie_needs_help/n6dkdqe/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 01 Aug 2025 21:11:34 +0530</pubDate></item><item><link>https://i.redd.it/2lhskdy9rdgf1.jpeg</link><title>OpenAI open-source model possible Analysis! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mesac9/openai_opensource_model_possible_analysis/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mesac9/openai_opensource_model_possible_analysis/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mesac9/openai_opensource_model_possible_analysis/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>See our tweet for a detailed breakdown: <a href="https://x.com/danielhanchen/status/1951212068583120958">https://x.com/danielhanchen/status/1951212068583120958</a></p><p>Will it get released today or very soon? Let&#39;s wait and see 🤩 what do you guys think?</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/2lhskdy9rdgf1.jpeg' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Merged llamacpp support?  :)</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mesac9/openai_opensource_model_possible_analysis/n6cncpb/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>???</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mesac9/openai_opensource_model_possible_analysis/n6d1b2n/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Still 128K in Q2 2025 for new releases...</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mesac9/openai_opensource_model_possible_analysis/n6j79jt/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 01 Aug 2025 15:22:32 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mes61k/run_quantized_model_in_vllm/</link><title>Run Quantized Model in vLLM</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mes61k/run_quantized_model_in_vllm/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mes61k/run_quantized_model_in_vllm/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mes61k/run_quantized_model_in_vllm/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>So far I only hosted Models using vLLM from the creator, mostly qwen Models where I can just use &quot;vllm serve &lt;model_name&gt;&quot; and vllt does the rest (or I use vllm&#39;s docker image). This works if on the huggingface page there is only one quantized version, but in Unsloths Models there are usually plenty of different quantized versions, like Q4_1, Q4_0 etc. </p><p>Can I host them the same way with vllm (are they in the transformers package)? If not, how would I serve them with vllm? If yes, how do I specify the quantization type? </p><p>When I click on the quantization type and there on &quot;use this model&quot; -&gt; vllm, it will just tell me to use &quot;vllm serve &lt;model_name&gt;&quot;, it&#39;s the same command without any reference to the quantization type. </p><p>I could not find information for this anywhere online, can you help me with this? </p><p>Thank you! :)</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>It would be helpful to know what HW you’re running on. Nevertheless you can read more about supported quants in vLLM docs: <a href="https://docs.vllm.ai/en/latest/features/quantization/index.html">https://docs.vllm.ai/en/latest/features/quantization/index.html</a></p><p>If running CUDA, you can use pretty much any quant. If you’re running ROCm your best bet is AWQ.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mes61k/run_quantized_model_in_vllm/n6bsmy0/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>In general vLLM is designed for quants other than GGUFs. They do support GGUFs yes, but their support for it isn&#39;t as updated as other quant methodologies. I would recommend using llama.cpp for now</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mes61k/run_quantized_model_in_vllm/n6da68y/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 01 Aug 2025 15:15:06 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1me8jqz/qwen3_says_no_bullshit/</link><title>Qwen3 says No Bullshit</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1me8jqz/qwen3_says_no_bullshit/</guid><comments>https://www.reddit.com/r/unsloth/comments/1me8jqz/qwen3_says_no_bullshit/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1me8jqz/qwen3_says_no_bullshit/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Thinking model vs Instruct model such a difference...  </p><p>I just downloaded qwen3 thinking and instruct quantized models by <a href="https://www.reddit.com/u/unsloth">u/unsloth</a> . To test, I gave them the same query which is to plan my day. Instruct model gave crap reply. after explaining it again and again, it gave me 4 hours sleep schedule. and it says reduce your shift schedule so that you can sleep better.   </p><p>On the other hand, with just one query to &quot;thinking&quot; model, it gave me well-structured reply. So, other than technical explanations, use thinking model which gives you very apt reply.</p><p>Both are same model. Thinking model says this:</p><p><a href="https://preview.redd.it/z1j3swz009gf1.png?width=628&amp;format=png&amp;auto=webp&amp;s=539e130a47dadc1ac6b26f97cc962ec3c7eb9d77">https://preview.redd.it/z1j3swz009gf1.png?width=628&amp;format=png&amp;auto=webp&amp;s=539e130a47dadc1ac6b26f97cc962ec3c7eb9d77</a></p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Instruct = Execute. Thinking = Plan</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1me8jqz/qwen3_says_no_bullshit/n67i67p/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>This is the best real-world explanation of COT.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1me8jqz/qwen3_says_no_bullshit/n68ztnk/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Which models on Huggingface did you compare specifically?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1me8jqz/qwen3_says_no_bullshit/n69i7iv/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Sleep from 5.30am to 1.30am? Reverse time? Damn.. These models are really highly futuristic.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1me8jqz/qwen3_says_no_bullshit/n6juzbc/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 31 Jul 2025 23:26:00 +0530</pubDate></item><item><link>https://i.redd.it/q2vulitx78gf1.png</link><title>Run 'Qwen3-Coder-Flash' locally with Unsloth Dynamic GGUFs! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1me4bv7/run_qwen3coderflash_locally_with_unsloth_dynamic/</guid><comments>https://www.reddit.com/r/unsloth/comments/1me4bv7/run_qwen3coderflash_locally_with_unsloth_dynamic/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1me4bv7/run_qwen3coderflash_locally_with_unsloth_dynamic/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Qwen3-Coder-Flash is here! ✨ The 30B model excels in coding &amp; agentic tasks. Run locally with up to 1M context length. Full precision runs with just 33GB RAM.</p><p>GGUFs: <a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF">https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF</a></p><p>Hey friends, as usual, we always update our models and communicate with the model teams to ensure open-source models are of the highest quality they can be. We fixed tool-calling for Qwen3-Coder so now it should work properly. If you’re downloading our 30B-A3B quants, no need to worry as these already include our fixes. For the 480B-A35B model you need to redownload.</p><p>1M context GGUF: <a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF">https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF</a></p><p>Guide for Qwen3-Coder: <a href="https://docs.unsloth.ai/basics/qwen3-coder">https://docs.unsloth.ai/basics/qwen3-coder</a></p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/q2vulitx78gf1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Awesome ! Thx ! ❤️</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1me4bv7/run_qwen3coderflash_locally_with_unsloth_dynamic/n66stc6/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Fantastic!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1me4bv7/run_qwen3coderflash_locally_with_unsloth_dynamic/n66v5yb/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>There is no &quot;thinking&quot; model for qwen3-coder? for coding, it should &quot;think&quot; a lot right?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1me4bv7/run_qwen3coderflash_locally_with_unsloth_dynamic/n67fh5o/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Are there issues with running these models on AMD GPU’s me and my friend tried running this same weights same settings same prompt. AMD GPU hits constant loops the Nvidia (his) worked perfectly until he filled the context window</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1me4bv7/run_qwen3coderflash_locally_with_unsloth_dynamic/n6bgxiy/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>How am I supposed to understand the 1m context being able to run on 33gb VRAM? I can barely load it with 128k context on 32gb (5090)?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1me4bv7/run_qwen3coderflash_locally_with_unsloth_dynamic/n6frq2y/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I seem to be unable to run the 30B instruct model with OpenCode or QwenCode. Both result in the following error </p><p>AI_RetryError: Failed after 4 attempts. Last error: Value is not callable: null at row 62, column 114:</p><p>Any ideas as to what I might be doing wrong? <a href="https://www.reddit.com/u/yoracale">/u/yoracale</a>?</p><p>Running it with llama.cpp with —jinga on my m3 max 36 GB</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1me4bv7/run_qwen3coderflash_locally_with_unsloth_dynamic/n6kfag2/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 31 Jul 2025 20:47:00 +0530</pubDate></item><item><link>https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF</link><title>Fixes for: Qwen3-30B-A3B-Thinking-2507 GGUF. (huggingface.co)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mdxkty/fixes_for_qwen330ba3bthinking2507_gguf/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mdxkty/fixes_for_qwen330ba3bthinking2507_gguf/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mdxkty/fixes_for_qwen330ba3bthinking2507_gguf/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey everyone, we saw some of you having issues with using the latest Qwen3-30B Thinking model in tools other than llama.cpp. For example, some users experienced outputs which consistently doen&#39;t wrap reasoning tokens in <code>&lt;think&gt;</code> and <code>&lt;/think&gt;.</code> </p><p>We re-uploaded the GGUFs and we verified that removing the &lt;think&gt; is fine, since the model&#39;s probability of producing the think token seems to be nearly 100% anyways.This should make LMStudio, Ollama etc. inference work rather than just llama.cpp.</p><p>Yes, you will need to redownload the weights.</p><p>Qwen3-30B-A3B-Thinking-2507: <a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF">https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF</a></p><p>Let us know if you&#39;re still having any issues. :)</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I belive the 2507 chat template contains &lt;think&gt;. </p><p>Cite from their model card: &quot;Additionally, to enforce model thinking, the default chat template automatically includes &lt;think&gt;. Therefore, it is normal for the model&#39;s output to contain only &lt;/think&gt; without an explicit opening &lt;think&gt; tag.&quot;</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mdxkty/fixes_for_qwen330ba3bthinking2507_gguf/n652q6q/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>qwen3-30b-a3b-instruct-2507<br/>Failed to generate AI response<br/>Failed to parse tool call: Expected &quot;&quot;arguments&quot;&quot;, but got &quot;&quot;descriptio&quot; at index 37.<br/>Not working with tools (MCP)</p><p>Qwen3-30B-A3B-Thinking-2507 works well</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mdxkty/fixes_for_qwen330ba3bthinking2507_gguf/n66vx6s/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 31 Jul 2025 15:37:01 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mdrupf/seeking_expert_guidance_in_tts_training/</link><title>Seeking Expert Guidance in TTS training</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mdrupf/seeking_expert_guidance_in_tts_training/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mdrupf/seeking_expert_guidance_in_tts_training/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mdrupf/seeking_expert_guidance_in_tts_training/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hello everyone. I’m new here and seeking concrete guidance on achieving low end‑to‑-end latency in TTS voice cloning through Orpheus or similar models. If you have direct experience with frameworks, model optimizations, or hardware strategies and are willing to assist, please reach out. </p></div><!-- SC_ON --></section>]]></description><pubDate>Thu, 31 Jul 2025 09:41:33 +0530</pubDate></item><item><link>https://i.redd.it/c4lmnkivb1gf1.png</link><title>Unsloth Dynamic 'Qwen3-30B-A3B-THINKING-2507' GGUFs out now! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mdakn5/unsloth_dynamic_qwen330ba3bthinking2507_ggufs_out/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mdakn5/unsloth_dynamic_qwen330ba3bthinking2507_ggufs_out/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mdakn5/unsloth_dynamic_qwen330ba3bthinking2507_ggufs_out/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Qwen releases Qwen3-30B-A3B-Thinking-2507! ✨ The 30B model runs locally in full precision with just 33GB RAM.</p><p>GGUFs: <a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF">https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF</a></p><p>Unsloth also supports Qwen3-2507 fine-tuning and RL!</p><p>Guide to run/fine-tune: <a href="https://docs.unsloth.ai/basics/qwen3-2507">https://docs.unsloth.ai/basics/qwen3-2507</a></p><p>Happy running guys!</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/c4lmnkivb1gf1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>A speedy beast like this is great for thinking models, so I was looking forward to this more than the instruct one! I get ~108 tok/s on llama.cpp with RTX 3090 on q5_K_M.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mdakn5/unsloth_dynamic_qwen330ba3bthinking2507_ggufs_out/n620551/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>This model in BF16 (60GB) is anxiety-ridden on my Macbook Pro M4 Max:</p><p>Me: &quot;hi&quot;</p><p>Qwen3-30b-a3b-2507: &quot;<em>&lt;think&gt;Okay, the user said &quot;hi&quot;. I need to respond politely. Let me check if there&#39;s any specific context or previous messages. Hmm, no, it&#39;s a new conversation.</em></p><p><em>First, greet them back. Maybe say something friendly like &quot;Hello! How can I assist you today?&quot; Wait, but maybe they just want to say hi. Should I keep it simple? Yeah, probably best to be warm and open-ended so they feel comfortable asking for help.</em></p><p><em>Wait, the user might not have a specific question yet. So my job is to invite them to ask anything. Let me make sure my response isn&#39;t too long. Just a friendly hello and an offer to help. Avoid using jargon or complex words. Keep it natural.</em></p><p><em>Also, check if there&#39;s any cultural consideration. &quot;Hi&quot; is pretty universal, so no issues there. Maybe add a smiley emoji to keep it friendly? But maybe some people find emojis unprofessional. Hmm, the user didn&#39;t specify. Since they started with a casual &quot;hi&quot;, probably okay to use a light touch.</em></p><p><em>Wait, the example response from before used &quot;Hello! How can I assist you today?&quot; No emoji. Maybe better to stick without for now unless sure. Let&#39;s go with that.&lt;/think&gt;</em></p><p><em>Hello! How can I assist you today?</em> 😊&quot;</p><p>&lt;4.84tok/s, 2.35s to first token&gt;</p><p>Edit: I downloaded the BF16 original and converted it to MLX at FP16. Very similar quality, but the token speed was 10x faster @ 57.54 tok/sec. I&#39;m blown away at the difference of mlx_lm.convert on the original .safetensors. This is more than &quot;usable&quot; for me. I&#39;m really impressed.</p><p>I was wondering why so few people seemed to bother with 16-bit MLX conversions of .safetensors files. It turns out my Mac can convert .safetensors to .mlx in seconds once it&#39;s downloaded. Learned something new today!  </p><p><code>python -m mlx_lm.convert \</code><br/><code>--hf-path Qwen/Qwen3-30B-A3B-Thinking-2507 \</code><br/><code>--mlx-path ~/models/Qwen3-30B-A3B-Thinking-2507-fp16.mlx</code></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mdakn5/unsloth_dynamic_qwen330ba3bthinking2507_ggufs_out/n6706jq/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I&#39;m having a bit of a rough time with this in RooCode with the Unsloth Dynamic quants.</p><p>Very frequently I&#39;m getting to a point where the model says it&#39;s about to write code, and it just gets stuck in an infinite loops where nothing happens. This is happening way, way, way too often.</p><p>I&#39;m also getting one off errors like:</p><blockquote><p>Roo tried to use write_to_file without value for required parameter &#39;path&#39;. Retrying...</p></blockquote><p>or</p><blockquote><p>Roo tried to use apply_diff without value for required parameter &#39;path&#39;. Retrying...</p></blockquote><p>It&#39;s actually happening way more often than what I was getting with the 30B Thinking and Non Thinking models that recently came out as well. In fact, I don&#39;t think I ever got an error with the Thinking &amp; Non Thinking models for Q4 - Q6 UD quants in RooCode which had me super pumped for this! This Coder model is the only one giving errors for me.</p><p>I&#39;ve tried the Q4 UD and Q5 UD quants and both have these issues. Downloading the Q6 UD to see if that changes anything.</p><p>But yeah, not going as smoothly as I&#39;d hope in RooCode. But just more surprising that I&#39;m having way worse performance in RooCode with this than the other two 30B models released this week. :(</p><p>My settings for llama-swap &amp; llama.cpp (I&#39;m running a 4090):</p><p><code>  &quot;Qwen3-Coder-30B-A3B-Instruct-UD-Q5KXL&quot;:    cmd: |      llama-server      -m /mnt/big/AI/models/llamacpp/Qwen3-Coder-30B-A3B-Instruct-UD-Q5_K_XL.gguf      --port ${PORT}      --flash-attn      --threads 16      --gpu-layers 30      --ctx-size 196608      --temp 0.7      --top-k 20      --top-p 0.8      --min-p 0.0      --repeat-penalty 1.05      --cache-type-k q8_0      --cache-type-v q8_0      --jinja</code></p><p>Debating if I should maybe try some other quants (like the non UD ones) to see if that helps? Maybe my settings above suck (although I did stick to the recommendation on the Unsloth site).</p><p>Anyone else having similar challenges with RooCode?</p><p><strong>UPDATE</strong>: Looks like there&#39;s an actual issue and Unsloth folks are looking at it: <a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/discussions/4">https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/discussions/4</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mdakn5/unsloth_dynamic_qwen330ba3bthinking2507_ggufs_out/n68xhh2/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>So what you recommend for codingOn 40ram 16 vram</p><p>This one or glm 4.5 int4 bit?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mdakn5/unsloth_dynamic_qwen330ba3bthinking2507_ggufs_out/n60oicc/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Great work team. I have done a video on how to run your Unsloth version with Ollama:  <a href="https://youtu.be/_KvpVHD_AkQ?si=-TTtbzBZfBwjudbQ">https://youtu.be/_KvpVHD_AkQ?si=-TTtbzBZfBwjudbQ</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mdakn5/unsloth_dynamic_qwen330ba3bthinking2507_ggufs_out/n68ktdq/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Wed, 30 Jul 2025 21:34:50 +0530</pubDate></item><item><link>https://i.redd.it/q92411zwv0gf1.jpeg</link><title>Google Gemma 3n Challenge ($150,000 in prizes) ends in 7 days! + New Gemma 3n notebooks (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1md88x2/google_gemma_3n_challenge_150000_in_prizes_ends/</guid><comments>https://www.reddit.com/r/unsloth/comments/1md88x2/google_gemma_3n_challenge_150000_in_prizes_ends/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1md88x2/google_gemma_3n_challenge_150000_in_prizes_ends/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys thought you should know the challenge ends in one week!</p><p>We also just made 2 new fine-tuning Gemma 3n Kaggle notebooks for Vision &amp; Audio to spark your creativity. Your fine-tuned model is eligible to be used to compete for any of the prizes on any track!</p><p>New notebooks + Challenge Details: <a href="https://www.kaggle.com/code/danielhanchen/gemma-3n-4b-multimodal-finetuning-inference">https://www.kaggle.com/code/danielhanchen/gemma-3n-4b-multimodal-finetuning-inference</a></p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/q92411zwv0gf1.jpeg' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Thanks for the update! Guess I know what I’ll be doing this weekend lol.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1md88x2/google_gemma_3n_challenge_150000_in_prizes_ends/n608x35/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Wed, 30 Jul 2025 20:05:23 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1md09qz/discrepancy_between_merged_lora_model_vs_dynamic/</link><title>Discrepancy Between Merged LoRA Model vs. Dynamic Adapter Loading: Is This Expected?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1md09qz/discrepancy_between_merged_lora_model_vs_dynamic/</guid><comments>https://www.reddit.com/r/unsloth/comments/1md09qz/discrepancy_between_merged_lora_model_vs_dynamic/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/unsloth/comments/1md09qz/discrepancy_between_merged_lora_model_vs_dynamic/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi everyone, I&#39;ve been working on fine-tuning a model using Unsloth and LoRA, and I&#39;ve encountered a  difference in behavior that I&#39;d like to understand better.</p><p>My core observation is that when I run inference using the base model with the LoRA adapter loaded dynamically, the model&#39;s output is different—and often more consistent—than when I use a pre-merged version of the same model and adapter.</p><p>Here’s my fine-tuning and inference workflow:</p><p>Setup and Training:</p><ul><li><p>I load a base model (e.g., unsloth/Qwen3-4B) with FastLanguageModel.</p></li><li><p>I add several new special tokens to the tokenizer ([action], [/action], etc.).</p></li><li><p>I resize the model&#39;s token embeddings to accommodate the new vocabulary (model.resize_token_embeddings).</p></li><li><p>I then fine-tune the model using LoRA and save the adapter.</p></li></ul><p>Inference Methods:</p><ul><li><p>Method A (Dynamic Loading): I load the original base model and then attach the trained LoRA adapter using PeftModel.from_pretrained(model, adapter_path).</p></li><li><p>Method B (Merged Model): I create a merged model using model.save_pretrained_merged(&quot;./merged_model&quot;, tokenizer, ...) and then load this new standalone model for inference.</p></li></ul><p>The Discrepancy: When I give the same prompt to both models, their responses differ. Method A (Dynamic Loading) consistently produces outputs that strictly follow the format taught during fine-tuning (e.g., [action]{...}[/action]). However, Method B (Merged Model) sometimes generates slightly malformed or &quot;hallucinated&quot; structures (e.g., using unexpected keys like actionDate or breaking the JSON format).</p><p>This leads me to my main questions:</p><ol><li>Is this difference in behavior expected? Why would a merged model behave differently from a dynamically loaded one? Is there some subtle information loss or change in the model&#39;s computational path that occurs during the merging process?</li><li>Is my merging process correct? I&#39;ve been creating the merged model with the line below, passing in the modified tokenizer. Is this the correct way to merge a model that has both a LoRA adapter and a modified tokenizer, or is there a more robust method to ensure the merged model behaves identically to the dynamically loaded version?</li></ol><p>&#8203;</p><pre><code>    model.save_pretrained_merged(        &quot;./merged_models/my-final-model&quot;,        modified_tokenizer,        save_method=&quot;merged_16bit&quot;,    )</code></pre><p>I&#39;m trying to understand if this is a known trade-off or if I&#39;m missing a step in my workflow to create a perfectly faithful merged model. Any insights or advice on best practices would be greatly appreciated.Thank you!</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I’ve been struggling with exactly the same problem too in recent days - it’s been driving me crazy. Specifically I’ve been trying to go from CPT -&gt; Finetuning -&gt; GRPO. However if I merge and save the models at any point they become a babbling mess.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1md09qz/discrepancy_between_merged_lora_model_vs_dynamic/n5y9ix9/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Hi guys don’t know exact reason but I am guessing the weight factor of Lora when it get merged. As I see you can set Lora weight when adding adapter to base model in llama.cpp, that provides the impact of Lora on base model. </p><p>I assume default merge should have weight factor of 1 but something is different or happening some change would may cause this issue. </p><p>Looking forward to someone’s expertise response.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1md09qz/discrepancy_between_merged_lora_model_vs_dynamic/n67ohkk/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I was having a similar issue with the model outputting absolute gibberish, but it turns the chat template wasn&#39;t applied when I ran it with ollama. Not sure if this is the kind of problem you&#39;re running into, but after downloading the model, you can check if the chat template is applied with:</p><p><code>ollama show {{your-model-name}} --template</code><br/>And if it just says &quot;{{ .Prompt }}%&quot; then this could be the reason the model is outputting nonsense from the training data you fine tuned it on.                                                                                                       </p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1md09qz/discrepancy_between_merged_lora_model_vs_dynamic/n67q8g6/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The discrepancy in quality is caused by when it’s natively loaded after a fine-tune it’s loaded at a 32f resolution and then when you merge the model, it’s downloaded to 16f cutting the total resolution of the adapter weights i.e. causing the discrepancy in their behavior quality, to my knowledge there’s zero work around with this as base models are at 16F and the weights have to be equivalent in formatting</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1md09qz/discrepancy_between_merged_lora_model_vs_dynamic/n68zijn/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I am facing the same issue here with CPT modelling on Mistral-7b-v0.3 model. Please can someone from the unsloth team can help us here?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1md09qz/discrepancy_between_merged_lora_model_vs_dynamic/n6oyfpk/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I&#39;ve had this issue in my current project. What happens if you merge and save in <code>f32</code>?</p><p>Even though it&#39;s less ideal, I&#39;ve found saving in <code>f32</code> helps reduce this issue. If you look into the code for <code>peft</code> and such, LoRA adapters are often loaded and trained in 32-bit because training in <code>bf16</code> can be unstable.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1md09qz/discrepancy_between_merged_lora_model_vs_dynamic/n62rhal/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Wed, 30 Jul 2025 13:03:50 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1md01ve/how_to_quantize_myself_docs_say_only_for/</link><title>How to quantize myself? Docs say only for fine-tuning?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1md01ve/how_to_quantize_myself_docs_say_only_for/</guid><comments>https://www.reddit.com/r/unsloth/comments/1md01ve/how_to_quantize_myself_docs_say_only_for/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1md01ve/how_to_quantize_myself_docs_say_only_for/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I want to quantize this LLM : <a href="https://huggingface.co/Tesslate/UIGEN-X-4B-0729">https://huggingface.co/Tesslate/UIGEN-X-4B-0729</a></p><p>but when reading through the unsloth docs, nothing is mentioned about quantizing by yourself, it only mentions fine-tuning</p><p>So my question is, is unsloth not made for doing quantization yourself? </p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Unsloth uses this project too</p><p><a href="https://github.com/bitsandbytes-foundation/bitsandbytes">https://github.com/bitsandbytes-foundation/bitsandbytes</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1md01ve/how_to_quantize_myself_docs_say_only_for/n5xvnzj/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I read this as you asking &quot;how do I quantize <em>myself</em>?&quot;</p><p>Like what do you want to become slightly dumber but faster</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1md01ve/how_to_quantize_myself_docs_say_only_for/n60zrq8/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>We quantize utizling bitsandbytes: <a href="https://github.com/bitsandbytes-foundation/bitsandbytes">https://github.com/bitsandbytes-foundation/bitsandbytes</a></p><p>And llama.cpp:<a href="https://github.com/ggml-org/llama.cpp">https://github.com/ggml-org/llama.cpp</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1md01ve/how_to_quantize_myself_docs_say_only_for/n5y2j0t/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>```from transformers import AutoTokenizerfrom unsloth import load_model_unfused</p><p>model_name = &quot;Tesslate/UIGEN-X-4B-0729&quot;tokenizer = AutoTokenizer.from_pretrained(model_name)model, _ = load_model_unfused(model_name, load_in_4bit=True, quantization_method=&quot;q4_k_m&quot;)model.save_pretrained_gguf(&quot;uigen-x-4b-q4&quot;, tokenizer, quantization_method=&quot;q4_k_m&quot;)</p><p>```</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1md01ve/how_to_quantize_myself_docs_say_only_for/n662dbg/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Wed, 30 Jul 2025 12:49:31 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mcyr2i/which_is_better_to_improve_a_specific_domain_of/</link><title>Which is better to improve a specific domain of knowledge? Continued pretrain or supervised fine tuning?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mcyr2i/which_is_better_to_improve_a_specific_domain_of/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mcyr2i/which_is_better_to_improve_a_specific_domain_of/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mcyr2i/which_is_better_to_improve_a_specific_domain_of/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Eg let&#39;s say I want to improve domain knowledge got DeepSeek for my industry, which is sorely lacking, how do I do so other than rag?</p><p>Continued pretrain or supervised fine tune? Does anyone have any resources or experiences to share please.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Continued pretrain is harder to do right, because of catastrophic forgetting.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mcyr2i/which_is_better_to_improve_a_specific_domain_of/n5xmpnm/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Continued pre-training assuming you have enough prepared data is superior for having the model naturally utilize the given data in responses , fine tuning on the other hand cannot add new information to the model only affecting how they use their given data , if you fine tune to respond on with data it doesn’t have in its base pool your quality will fall far and hallucination rate will skyrocket</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mcyr2i/which_is_better_to_improve_a_specific_domain_of/n5zivuc/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Wed, 30 Jul 2025 11:28:31 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mcht2q/request_glm45air/</link><title>request: GLM-4.5-Air</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mcht2q/request_glm45air/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mcht2q/request_glm45air/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mcht2q/request_glm45air/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Would it be possible to create a unsloth gguf of the <a href="https://huggingface.co/zai-org/GLM-4.5-Air">new light GLM4.5 release</a>?</p><p>I remember these guys releasing SWE Dev 32B and it was the best coding model you could run on two 3090&#39;s up until now. Would love to try this new release, thanks guys 🙏</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>needs llama.cpp <a href="https://github.com/ggml-org/llama.cpp/pull/14939">support</a> first</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mcht2q/request_glm45air/n5tzxud/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Yes we are excited to support it but most of the work will depend on the incredible llama.cpp team!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mcht2q/request_glm45air/n5wawhn/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p><a href="https://z.ai/blog/glm-4.5">https://z.ai/blog/glm-4.5</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mcht2q/request_glm45air/n6ozc78/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>GLM-4.5 : Capacités de raisonnement, de codage et d’agentivité</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mcht2q/request_glm45air/n6ozg7i/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 29 Jul 2025 23:00:43 +0530</pubDate></item><item><link>https://i.redd.it/ymmlfv5yeuff1.png</link><title>Unsloth Dynamic 'Qwen3-30B-A3B-Instruct-2507' GGUFs out now! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mcgpeq/unsloth_dynamic_qwen330ba3binstruct2507_ggufs_out/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mcgpeq/unsloth_dynamic_qwen330ba3binstruct2507_ggufs_out/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mcgpeq/unsloth_dynamic_qwen330ba3binstruct2507_ggufs_out/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Qwen releases Qwen3-30B-A3B-Instruct-2507! ✨ The 30B model rivals GPT-4o&#39;s performance and runs locally in full precision with just 33GB RAM.</p><p>GGUFs: <a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF">https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF</a></p><p>Unsloth also supports Qwen3-2507 fine-tuning and RL!</p><p>Guide to run/fine-tune: <a href="https://docs.unsloth.ai/basics/qwen3-2507">https://docs.unsloth.ai/basics/qwen3-2507</a></p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/ymmlfv5yeuff1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Wait, so we have almost the same performance as GPT-4o, with only 30b?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mcgpeq/unsloth_dynamic_qwen330ba3binstruct2507_ggufs_out/n5v3rxz/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>First thank you as always. Second which one for 33gb?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mcgpeq/unsloth_dynamic_qwen330ba3binstruct2507_ggufs_out/n5tobdj/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Thanks man as always Just playing with q6k_xl , it’s amazing It’s seems like this is finetuned on qwen3-coder Generating amazing code out of the box</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mcgpeq/unsloth_dynamic_qwen330ba3binstruct2507_ggufs_out/n5ul8jt/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Thank you! Is it already tuned for M1</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mcgpeq/unsloth_dynamic_qwen330ba3binstruct2507_ggufs_out/n5x16be/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Thank you Unsloth</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mcgpeq/unsloth_dynamic_qwen330ba3binstruct2507_ggufs_out/n5x8l9b/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Does Unsloth support “continued pretraining” with this model?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mcgpeq/unsloth_dynamic_qwen330ba3binstruct2507_ggufs_out/n5xct0j/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Is this mainly for general knowledge, reasoning or coding?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mcgpeq/unsloth_dynamic_qwen330ba3binstruct2507_ggufs_out/n5xd6ba/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 29 Jul 2025 22:21:05 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mc6i5z/trl_suddenly_update_to_0200_unsloth_have_to_fix/</link><title>trl suddenly update to 0.20.0, unsloth have to fix something now.</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mc6i5z/trl_suddenly_update_to_0200_unsloth_have_to_fix/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mc6i5z/trl_suddenly_update_to_0200_unsloth_have_to_fix/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mc6i5z/trl_suddenly_update_to_0200_unsloth_have_to_fix/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey guys, when i was finetuning Qwen model in the morining today , everything works fine. but after i finish ed my lunch i started a notebook from kaggle and import unsloth, i meet some dependences issues with trl. so i check pypi and found that trl have a update today. so now it  will have error with import unsloth when you install unsloth from pip. </p><p>well, now i use the trl==0.19.1 to not raise error.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Should be fixed, if you are on Colab / Kaggle, please delete runtime, and refresh the notebook. If you are on a local machine, do:</p><p><code>pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo</code></p><p>Please also update <code>huggingface_hub</code> via <code>pip install --upgrade --no-deps huggingface_hub</code></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mc6i5z/trl_suddenly_update_to_0200_unsloth_have_to_fix/n5rguw5/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>yeah, just downgrade trl. versioning in unsloth sucks btw. one day it works, the other nop, some dep has breaking changes. pin the ffing deps</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mc6i5z/trl_suddenly_update_to_0200_unsloth_have_to_fix/n5sl793/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Tue, 29 Jul 2025 14:34:14 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mbv3xn/attributeerror_module_unslothppotrainer_has_no/</link><title>AttributeError: module 'UnslothPPOTrainer' has no attribute 'UnslothPPOTrainer'</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mbv3xn/attributeerror_module_unslothppotrainer_has_no/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mbv3xn/attributeerror_module_unslothppotrainer_has_no/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 5 min | <a href='https://www.reddit.com/r/unsloth/comments/1mbv3xn/attributeerror_module_unslothppotrainer_has_no/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi</p><p>I am trying llm training using unsloth on multi gpus environment.My training code is as follows.When I run it with one gpu, It is working.</p><p><code>python train_grpo_multi.py</code></p><p>But when I trying it with accelerate, it causes errors</p><p><code>accelerate launch train_grpo_multi.py</code></p><p>AttributeError: module &#39;UnslothPPOTrainer&#39; has no attribute &#39;UnslothPPOTrainer&#39;</p><p>What did I wrong?</p><p>```from unsloth import FastLanguageModelfrom trl import SFTTrainer, SFTConfigfrom datasets import Datasetfrom datasets import load_datasetimport pandas as pdimport numpy as npfrom accelerate import Acceleratorimport torchimport osimport gc, torchfrom transformers import TrainingArguments, DataCollatorForSeq2Seqfrom unsloth.chat_templates import get_chat_template, train_on_responses_only</p><p>gc.collect()torch.cuda.empty_cache()</p><h1>os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0,1&quot; #Select Which devices to use. Or, comment if you want to use all GPUs.</h1><p>os.environ[&quot;UNSLOTH_RETURN_LOGITS&quot;] = &quot;1&quot;accelerator = Accelerator()</p><p>device = accelerator.devicemax_seq_length = 2048 # Can increase for longer reasoning traceslora_rank = 32 # Larger rank = smarter, but slower</p><p>def load_model(model_path):    max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!    device_index = Accelerator().process_index    device_map = {&quot;&quot;: device_index}    # device_map = &quot;auto&quot; # Use &quot;auto&quot; to use all available GPUs    print(&quot;device_map&quot;,device_map)    model, tokenizer = FastLanguageModel.from_pretrained(        model_name = model_path,        max_seq_length = max_seq_length,        load_in_4bit = False, # False for LoRA 16bit        fast_inference = False, # Enable vLLM fast inference        max_lora_rank = lora_rank,        # gpu_memory_utilization = 0.6, # Reduce if out of memory        # device_map=device_map,        device_map = &quot;balanced&quot;,        use_cache=False,    )</p><pre><code>return model, tokenizer</code></pre><p>def model_LoRA(base_model):    model = FastLanguageModel.get_peft_model(        base_model,        r = lora_rank, # Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128        target_modules = [            &quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,            &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;,        ],        lora_alpha = lora_rank*2, # *2 speeds up training        # use_gradient_checkpointing = &quot;unsloth&quot;, # Reduces memory usage        use_gradient_checkpointing = False,        random_state = 3407,        use_rslora= False, # Use RSLORA for better performance</p><pre><code>)return model</code></pre><p>model, tokenizer = load_model(model_path=&quot;/home/jovyan/llm-shared/next_bixby/models/qwen/Qwen3-4B&quot;)model = model_LoRA(base_model=model)</p><p>reasoning_start = &quot;&lt;start_working_out&gt;&quot; # Acts as &lt;think&gt;reasoning_end   = &quot;&lt;end_working_out&gt;&quot;   # Acts as &lt;/think&gt;solution_start  = &quot;&lt;SOLUTION&gt;&quot;solution_end    = &quot;&lt;/SOLUTION&gt;&quot;</p><p>system_prompt = \f&quot;&quot;&quot;You are given a problem.Think about the problem and provide your working out.Place it between {reasoning_start} and {reasoning_end}.Then, provide your solution between {solution_start}{solution_end}&quot;&quot;&quot;system_prompt</p><p>chat_template = \    &quot;{% if messages[0][&#39;role&#39;] == &#39;system&#39; %}&quot;\        &quot;{{ messages[0][&#39;content&#39;] + eos_token }}&quot;\        &quot;{% set loop_messages = messages[1:] %}&quot;\    &quot;{% else %}&quot;\        &quot;{{ &#39;{system_prompt}&#39; + eos_token }}&quot;\        &quot;{% set loop_messages = messages %}&quot;\    &quot;{% endif %}&quot;\    &quot;{% for message in loop_messages %}&quot;\        &quot;{% if message[&#39;role&#39;] == &#39;user&#39; %}&quot;\            &quot;{{ message[&#39;content&#39;] }}&quot;\        &quot;{% elif message[&#39;role&#39;] == &#39;assistant&#39; %}&quot;\            &quot;{{ message[&#39;content&#39;] + eos_token }}&quot;\        &quot;{% endif %}&quot;\    &quot;{% endfor %}&quot;\    &quot;{% if add_generation_prompt %}{{ &#39;{reasoning_start}&#39; }}&quot;\    &quot;{% endif %}&quot;</p><h1>Replace with out specific template:</h1><p>chat_template = chat_template\    .replace(&quot;&#39;{system_prompt}&#39;&quot;,   f&quot;&#39;{system_prompt}&#39;&quot;)\    .replace(&quot;&#39;{reasoning_start}&#39;&quot;, f&quot;&#39;{reasoning_start}&#39;&quot;)tokenizer.chat_template = chat_template</p><p>tokenizer.apply_chat_template([    {&quot;role&quot; : &quot;user&quot;, &quot;content&quot; : &quot;What is 1+1?&quot;},    {&quot;role&quot; : &quot;assistant&quot;, &quot;content&quot; : f&quot;{reasoning_start}I think it&#39;s 2.{reasoning_end}{solution_start}2{solution_end}&quot;},    {&quot;role&quot; : &quot;user&quot;, &quot;content&quot; : &quot;What is 2+2?&quot;},], tokenize = False, add_generation_prompt = True)</p><p>dataset = load_dataset(&quot;unsloth/OpenMathReasoning-mini&quot;, split = &quot;cot&quot;)dataset = dataset.to_pandas()[    [&quot;expected_answer&quot;, &quot;problem&quot;, &quot;generated_solution&quot;]]</p><h1>Try converting to number - if not, replace with NaN</h1><p>is_number = pd.to_numeric(pd.Series(dataset[&quot;expected_answer&quot;]), errors = &quot;coerce&quot;).notnull()</p><h1>Select only numbers</h1><p>dataset = dataset.iloc[np.where(is_number)[0]]</p><p>def format_dataset(x):    expected_answer = x[&quot;expected_answer&quot;]    problem = x[&quot;problem&quot;]</p><pre><code># Remove generated &lt;think&gt; and &lt;/think&gt;thoughts = x[&quot;generated_solution&quot;]thoughts = thoughts.replace(&quot;&lt;think&gt;&quot;, &quot;&quot;).replace(&quot;&lt;/think&gt;&quot;, &quot;&quot;)# Strip newlines on left and rightthoughts = thoughts.strip()# Add our custom formattingfinal_prompt = \    reasoning_start + thoughts + reasoning_end + \    solution_start + expected_answer + solution_endreturn [    {&quot;role&quot; : &quot;system&quot;,    &quot;content&quot; : system_prompt},    {&quot;role&quot; : &quot;user&quot;,      &quot;content&quot; : problem},    {&quot;role&quot; : &quot;assistant&quot;, &quot;content&quot; : final_prompt},]</code></pre><p>dataset[&quot;Messages&quot;] = dataset.apply(format_dataset, axis = 1)tokenizer.apply_chat_template(dataset[&quot;Messages&quot;][0], tokenize = False)</p><p>dataset[&quot;N&quot;] = dataset[&quot;Messages&quot;].apply(lambda x: len(tokenizer.apply_chat_template(x)))</p><p>dataset = dataset.loc[dataset[&quot;N&quot;] &lt;= max_seq_length/2].copy()dataset.shape</p><p>dataset[&quot;text&quot;] = tokenizer.apply_chat_template(dataset[&quot;Messages&quot;].values.tolist(), tokenize = False)dataset = Dataset.from_pandas(dataset)dataset</p><p>trainer = SFTTrainer(    model = model,    # tokenizer = tokenizer,    train_dataset = dataset,    args = SFTConfig(        ddp_find_unused_parameters= False, # Set to False for GRPO        dataset_text_field = &quot;text&quot;,        per_device_train_batch_size = 1,        gradient_accumulation_steps = 1, # Use GA to mimic batch size!        warmup_steps = 5,        num_train_epochs = 2, # Set this for 1 full training run.        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs        logging_steps = 5,        optim = &quot;adamw_8bit&quot;,        weight_decay = 0.01,        # lr_scheduler_type = &quot;linear&quot;,        seed = 3407,        report_to = &quot;none&quot;, # Use this for WandB etc        # data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),    ),)</p><h1>If the model is wrapped in DDP, access the underlying module:</h1><p>if hasattr(trainer.model, &quot;module&quot;) and hasattr(trainer.model.module, &quot;_set_static_graph&quot;):    trainer.model.module._set_static_graph()elif hasattr(trainer.model, &quot;_set_static_graph&quot;):    trainer.model._set_static_graph()</p><p>trainer_stats = trainer.train()```</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 29 Jul 2025 04:26:09 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mbcay4/unsloth_dynamic_ggufs_embedded_q4_k_vs_q8_0/</link><title>Unsloth Dynamic GGUFs embedded Q4_K vs Q8_0</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mbcay4/unsloth_dynamic_ggufs_embedded_q4_k_vs_q8_0/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mbcay4/unsloth_dynamic_ggufs_embedded_q4_k_vs_q8_0/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mbcay4/unsloth_dynamic_ggufs_embedded_q4_k_vs_q8_0/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Will there be any difference using Q8_0 weights for <code>token_embd.weight</code> layer?</p><p>I have noticed that bartowski models in Q4_K_L usually gives better results vs Q4_K_M/Q4_0, while having fast prompt processing.</p><p>I&#39;m interested if there will be any value to use Q8_0 instead of Q4_K for <code>token_embd.weight</code> layer for Q4_K_XL quantization or not?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>It could be possible but it will also use much more memory. We could do it in the future</p><p>You&#39;re better off just using a quant with higher precision</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mbcay4/unsloth_dynamic_ggufs_embedded_q4_k_vs_q8_0/n5l22bn/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Mon, 28 Jul 2025 15:42:15 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mak4ec/finetunable_vlm_for_small_details/</link><title>finetunable VLM for small details?</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mak4ec/finetunable_vlm_for_small_details/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mak4ec/finetunable_vlm_for_small_details/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mak4ec/finetunable_vlm_for_small_details/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi there, I&#39;m a medical doctor.For generating drafts of medical reports based on text input, I’ve had good experiences fine-tuning Qwq32. For interpreting medical images, I’m currently fine-tuning LLaMA 3.2 11B Vision. Gemma 3 26B and Qwen-VL-2.5 32B also work, but they tend to miss small details. I am waiting for a DGX spark, until then my VRAM is limited to 24GB.</p><p>Here’s my question:Which vision-language model is well-suited for fine-tuning (ideally with QLoRA) and includes a visual encoder capable of capturing fine details in images?</p><p>The use case is ultrasound of the neck – specifically, counting and measuring lymph nodes. This is for my own personal productivity and not for clinical deployment; I remain fully responsible for the interpretations. But the task is highly repetitive, so I’m simply looking for an effective VLM to assist with it.</p><p>Any recommendations are much appreciated.Thank you!</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Hi, not a pro but I&#39;m also working on VLM finetuing with unsloth (plant classification with gemma3). From my experience, the quality of vision-encoder and the model can make a very big difference. In my case, I had tried gemma3 and gemma3n and found that gemma3 is much easier to finetune to classify the plant from the same dataset.</p><p>So for your case I think you should start with MedGemma model, because it had pre-training on medical image already. Good luck !<br/> <a href="https://developers.google.com/health-ai-developer-foundations/medgemma/model-card">https://developers.google.com/health-ai-developer-foundations/medgemma/model-card</a></p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mak4ec/finetunable_vlm_for_small_details/n5kzh9y/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Hi everyone,</p><p>Thanks for your comments.</p><p>The fine-tuning of MedGemma-3-4B worked well, and I&#39;m pleased with the responses for my use case. It&#39;s not perfect yet, of course, but I can imagine it will get significantly better with more training data. It&#39;s definitely my favorite so far.</p><p>Google Gemini 2.5 also suggested unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit. The results here are also much better as compared to Llama3.2-11B-Vision, but MedGemma-3 seems to be superior. I will not be pursuing Llama-3.2-11B-Vision any further.</p><p>Now, I would like to fine-tune MedGemma-27B-IT (multimodal), but I can&#39;t find a 4-bit quantized version in Hugging Face format.</p><p>Is the Unsloth team reading this? If you have the time, could you please provide one?</p><p>Many thanks.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mak4ec/finetunable_vlm_for_small_details/n6i3elz/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sun, 27 Jul 2025 17:07:20 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1mai68v/request_advice_voxtral_small_24b/</link><title>Request / advice: Voxtral (Small 24B)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1mai68v/request_advice_voxtral_small_24b/</guid><comments>https://www.reddit.com/r/unsloth/comments/1mai68v/request_advice_voxtral_small_24b/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1mai68v/request_advice_voxtral_small_24b/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Recently MistralAI released new audio+text-to-text model, Voxtral-Mini and Voxtral-Small <a href="https://huggingface.co/mistralai/Voxtral-Small-24B-2507">Voxtral [Huggingface]</a>. They claim to outperform Whisper large-v3. </p><p>i have a NVIDIA RTX 6000 ADA to run local tests. The Voxtral-Small (24B) does not fit onto this card in full precision. Would it be possible to create Q4/Q5/Q6 quants to retain the audio capabilities? I would like to test the transcription capabilities for audio that includes frequent language switching.</p><p>If possible, what would be necessary to realize these quants (infrastructure and/or pricing)?</p><p>Thanks for any advice.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Hi there I don&#39;t think it&#39;s supported in llama.cpp as of yet so we may need to wait for that to happen .</p><p>As for finetuning we are working on making it in work in Unsloth</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1mai68v/request_advice_voxtral_small_24b/n5f43zn/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sun, 27 Jul 2025 15:06:22 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1ma4gct/request_swedev/</link><title>Request: swe-dev</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1ma4gct/request_swedev/</guid><comments>https://www.reddit.com/r/unsloth/comments/1ma4gct/request_swedev/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/unsloth/comments/1ma4gct/request_swedev/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Source: <a href="https://huggingface.co/THUDM/SWE-Dev-32B">https://huggingface.co/THUDM/SWE-Dev-32B</a></p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>We&#39;ll see what we can do, we heard they&#39;re releasing a new model soon so maybe we&#39;ll wait to convert that one</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1ma4gct/request_swedev/n5c2eez/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sun, 27 Jul 2025 02:41:37 +0530</pubDate></item><item><link>https://www.reddit.com/r/unsloth/comments/1m9zkhc/running_bnb4bit_on_vllm/</link><title>Running bnb-4bit on vLLM</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1m9zkhc/running_bnb4bit_on_vllm/</guid><comments>https://www.reddit.com/r/unsloth/comments/1m9zkhc/running_bnb4bit_on_vllm/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1m9zkhc/running_bnb4bit_on_vllm/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey. I would like to run <a href="https://huggingface.co/unsloth/Qwen2.5-72B-Instruct-bnb-4bit">https://huggingface.co/unsloth/Qwen2.5-72B-Instruct-bnb-4bit</a> on vLLM, but naturally it does not seem to run.</p><pre><code>    s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig Value error, Invalid repository ID or local directory specified: &#39;unsloth/Qwen2.5-72B-Instruct-bnb-4bit&#39; Please verify the following requirements:1. Provide a valid Hugging Face repository ID.2. Specify a local directory that contains a recognized configuration file.- For Hugging Face models: ensure the presence of a &#39;config.json&#39;.- For Mistral models: ensure the presence of a &#39;params.json&#39;.3. For GGUF: pass the local path of the GGUF checkpoint.Loading GGUF from a remote repo directly is not yet supported[type=value_error, input_value=ArgsKwargs((), {&#39;model&#39;: ...attention_dtype&#39;: None}), input_type=ArgsKwargs]For further information visit https://errors.pydantic.dev/2.11/v/value_error</code></pre><p>Would appreciate some guide on this. If it&#39;s not possible, what would be the closts to bnb 4bit? AWQ?</p><p>my run command:</p><p><code>python3 -m vllm.entrypoints.openai.api_server --host</code> <a href="http://0.0.0.0"><code>0.0.0.0</code></a> <code>--port 8000 --model unsloth/Qwen2.5-72B-Instruct-bnb-4bit --gpu-memory-utilization 0.95 --api-key redacted --max-model-len 1000 --served-model-name test --enable-auto-tool-choice --tool-call-parser hermes --guided-decoding-backend auto</code></p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>No advice but may I ask how you plan to use it?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1m9zkhc/running_bnb4bit_on_vllm/n5crmnm/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 26 Jul 2025 23:16:53 +0530</pubDate></item><item><link>https://huggingface.co/unsloth/Magistral-Small-2507-GGUF</link><title>Magistral-2507 Dynamic GGUFs out now! (huggingface.co)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1m9rm9n/magistral2507_dynamic_ggufs_out_now/</guid><comments>https://www.reddit.com/r/unsloth/comments/1m9rm9n/magistral2507_dynamic_ggufs_out_now/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1m9rm9n/magistral2507_dynamic_ggufs_out_now/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Has the correct chat template too! Just thought we should update you guys incase you all werent aware! :)</p><p>Hope you guys have an amazing weekend and thanks for all the support this week! &lt;3</p></div><!-- SC_ON --></section>]]></description><pubDate>Sat, 26 Jul 2025 17:32:46 +0530</pubDate></item><item><link>https://i.redd.it/8rvt074j60ff1.jpeg</link><title>Qwen3-2507-Thinking Unsloth Dynamic GGUFs out now! (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/unsloth/comments/1m8wafe/qwen32507thinking_unsloth_dynamic_ggufs_out_now/</guid><comments>https://www.reddit.com/r/unsloth/comments/1m8wafe/qwen32507thinking_unsloth_dynamic_ggufs_out_now/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/unsloth/comments/1m8wafe/qwen32507thinking_unsloth_dynamic_ggufs_out_now/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>You can now run Qwen3-235B-A22B-Thinking-2507 with our Dynamic GGUFs: <a href="https://huggingface.co/unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF">https://huggingface.co/unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF</a></p><p>The full 250GB model gets reduced to just 87GB (-65% size).</p><p>Achieve &gt;6 tokens/s on 88GB unified memory or 80GB RAM + 8GB VRAM.</p><p>Guide: <a href="https://docs.unsloth.ai/basics/qwen3-2507">https://docs.unsloth.ai/basics/qwen3-2507</a></p><p>Keep in mind the quants are dynamic yes, but iMatrix dynamic GGUFs are still converting and will be up in a few hours! Thanks guys! 💕</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/8rvt074j60ff1.jpeg' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Better than Gemini 2.5 Pro? This can be a game changer. Now if I could just run this bitch myself.</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1m8wafe/qwen32507thinking_unsloth_dynamic_ggufs_out_now/n52fxsy/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Is the graph for full model or 2bit qwant?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1m8wafe/qwen32507thinking_unsloth_dynamic_ggufs_out_now/n53m8at/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Update: The imatrix ggufs should be up now. Also top_p should be 0.95, not 20!</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1m8wafe/qwen32507thinking_unsloth_dynamic_ggufs_out_now/n54yeux/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Why didn’t compare it with Opus-4?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1m8wafe/qwen32507thinking_unsloth_dynamic_ggufs_out_now/n5btq66/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>GLM4.5?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1m8wafe/qwen32507thinking_unsloth_dynamic_ggufs_out_now/n5pmof4/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>First time using such heavier quants! There&#39;s two parts to it! Can lm studio use both the ggufs?</p></div><p><a href='https://www.reddit.com/r/unsloth/comments/1m8wafe/qwen32507thinking_unsloth_dynamic_ggufs_out_now/n6a9v0l/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 25 Jul 2025 16:38:58 +0530</pubDate></item></channel></rss>
