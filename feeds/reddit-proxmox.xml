<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><atom:link href="http://192.168.1.132/?platform=reddit&amp;subreddit=Proxmox&amp;averagePostsPerDay=9&amp;content=0&amp;comments=7&amp;filterPinnedComments&amp;filterOldPosts=3&amp;view=rss" rel="self" type="application/rss+xml"/><title>/r/Proxmox</title><description>Hot posts in /r/Proxmox (roughly 9 posts per day)</description><link>https://www.reddit.com/r/Proxmox/</link><language>en-us</language><lastBuildDate>Sat, 20 Sep 2025 18:12:21 +0000</lastBuildDate><generator>Upvote RSS</generator><image><url>http://192.168.1.132//app/cache/images/styles-redditmedia-com-t5_2w0wn-styles-communityIcon_l9fx4v8n3cw71-144x400.png</url><title>/r/Proxmox</title><link>https://www.reddit.com/r/Proxmox/</link><width>144</width><height>144</height></image><item><link>https://www.reddit.com/r/Proxmox/comments/1nloiqw/backups_from_pve_nodes_to_pbs_server/</link><title>Backups from PVE Nodes to PBS Server</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nloiqw/backups_from_pve_nodes_to_pbs_server/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nloiqw/backups_from_pve_nodes_to_pbs_server/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nloiqw/backups_from_pve_nodes_to_pbs_server/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Nodes:<br/>Working on setting up our production ennvironment with Proxmox and PBS. I have a question. So on our nodes, we have 4 25gb connections and 2 1gb connections. The 2 1gb connections are used for management purposes in an active-backup bond. Network <a href="http://10.0.0.0/24">10.0.0.0/24</a> in this case and the switchports are setup as untagged vlan 200. 2 of the 25gb connections go to storage fabric. The other 2 25gb are used for vm/lxc uplinks with multiple networks and vlans on a bond with vlans.  </p><p>PBS: On the PBS which is running on baremetal, I have a similar config of a 1gb interface used for mangement purposes and then a 10gb interface I want to use for backups.  </p><p>What I would like to do is have backups run across the 25gb links on the nodes to the backup servers 10gb link. I understand I can add an ip on the PBS 10gb interface and then add that ip on the nodes as Storage&gt;PBS. However the backups would still actually run across the nodes 1gb management interface. This is where I&#39;m not sure how to basically tell the nodes to use the 25gb link to send backups to the pbs server. PBS server is in a separate physical location. I would share the 2 25gb vm uplinks to send backup traffic. In my network I have networks specifically for management, production, dmz, etc.  </p><p>I tried to add a second ip on the PBS servers 10GB interface on a different network however, I ran into only 1 gateway can exist which is currently the management interface. I would like for the traffic to be routable instead of point to point as I plan to replicate data from another campus.  </p><p>Would I be better off to simply move the management interfaces to the 25gb links or is there another way?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><blockquote><p>I tried to add a second ip on the PBS servers 10GB interface on a different network </p></blockquote><p>This is the correct move. The answer to &quot;I&#39;m not sure how to basically tell the nodes to use the 25gb link&quot; is to assign a different network to that interface.</p><p>I&#39;m not sure why that&#39;s being so difficult for you, but that&#39;s what it is. You shouldn&#39;t be specifying any gateway either, since there isn&#39;t one.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nloiqw/backups_from_pve_nodes_to_pbs_server/nf7nthn/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 20 Sep 2025 09:45:02 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/</link><title>SnapShots - OPNSense Firewall</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>ProxMox Friends, </p><p>Question?</p><p>When making a snapshot of my OPNSense firewall. After I have applied all my updates, configs, settings, etc.. Are there any right/wrongs when I create the snapshot with the Firewall running? I have tested shutting the firewall wall down and performing a quick snap shot restore. Everything is back up and running w/o any repercussions.</p><p>-or- </p><p>Is it best to create the snapshot with the firewall shut down? So when I need to restore the snapshot have to go through the whole process of startup.</p><p>Ideas?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I do them with it running, zero issues. Works amazingly well</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/nf61oh5/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Can snapshots actuall start &quot;live&quot; though?</p><p>I&#39;m not sure it makes but different but having hung out a bit in <a href="https://www.reddit.com/r/sysadmin">r/sysadmin</a>, snapshots are good for when you&#39;re making changes and might need to revert back but aren&#39;t to be relied on as long term backup.</p><p>So you can make a protected backup or you can clone the VM to a template but either way don&#39;t rely on snapshots for backups.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/nf60c98/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>My opnsense VM gets a weekly online snapshot, no consequences.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/nf6btvh/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Install qemu-guest-agent and do snapshots live.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/nf6daq0/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I do the snapshots with it running but exclude the RAM.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/nf6jrsc/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The key to backups is to make sure you test a restore.  Given the nature of a virtualised environment it&#39;s dead easy to test a snapshot roleback safley.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/nf67jaz/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 20 Sep 2025 05:35:50 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/</link><title>Could Proxmox ever become paid-only?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>We all know what happened to VMware when Broadcom bought them. Could something like that ever happen to Proxmox? Like a company buys them out and changes the licensing around so that there’s no longer a free version?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>It could be forked even if they switched everything off Debian and made it paid only. It’s open source so someone could pick up and keep going if they so desired</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/nf4kxwu/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>It’s aGPL license so if they make it cost money, then people will just fork the repo and continue development on that which will be totally legal.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/nf4l7u1/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Yes, they could go closed source tomorrow, BUT the current version remains free in free-to-use and open source. Changing a license is always possible for them (due to the CLA) and has been done in the past (see redis, elasticsearch, ...)</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/nf4kxvy/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Fork it, the code for the tools are open:</p><p><a href="https://git.proxmox.com/">https://git.proxmox.com/</a></p><p>Someone would have to package it up and release a new free version called froxmox for example. No difference to what we saw with redis and MySQL.</p><p>Proxmox does good work and I feel they have earned the respect for building what is practically a turnkey solution to open source virtualization, support them where you can and there should be no reason for the hypothetical to become reality.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/nf4l60b/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I assume if they did it would get forked like terraform and TOFU</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/nf4l2by/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>They could.   And I&#39;m guessing there would be a fork called OpenProxmox 23.4 seconds after the announcement.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/nf583uu/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>That would require them to move away from Debian, which would be a monumental amount of work</p><p>So maybe, but I doubt it</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/nf4kc6i/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 20 Sep 2025 01:03:06 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nlcv36/highspeed_lowdowntime_esxi_to_proxmox_migration/</link><title>High-Speed, Low-Downtime ESXi to Proxmox Migration via NFS</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nlcv36/highspeed_lowdowntime_esxi_to_proxmox_migration/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nlcv36/highspeed_lowdowntime_esxi_to_proxmox_migration/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 6 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nlcv36/highspeed_lowdowntime_esxi_to_proxmox_migration/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><h1>[GUIDE] High-Speed, Low-Downtime ESXi to Proxmox Migration via NFS</h1><p>Hello everyone,</p><p>I wanted to share a migration method I&#39;ve been using to move VMs from ESXi to Proxmox. This process avoids the common performance bottlenecks of the built-in importer and the storage/downtime requirements of backup-and-restore methods.</p><p>The core idea is to <strong>reverse the direction of the data transfer</strong>. Instead of having Proxmox <em>pull</em> data from a speed-limited ESXi host, we have the ESXi host <em>push</em> the data at full speed to a share on Proxmox.</p><h1>The Problem with Common Methods</h1><ul><li><strong>Veeam (Backup/Restore):</strong> Requires significant downtime (from backup start to restore end) and triple the storage space (ESXi + Backup Repo + Proxmox), which can be an issue for large VMs.</li><li><strong>Proxmox Built-in Migration (Live/Cold):</strong> Often slow because Broadcom/VMware seems to cap the speed of API calls and external connections used for the transfer. Live migrations can sometimes result in boot issues.</li><li><strong>Direct SSH</strong> <code>scp</code>**/<code>rsync</code>:** While faster than the built-in tools, this can also be affected by ESXi&#39;s connection throttling.</li></ul><h1>The NFS Push Method: Advantages</h1><ul><li><strong>Maximum Speed:</strong> The transfer happens using ESXi&#39;s native Storage vMotion, which is not throttled and will typically saturate your network link.</li><li><strong>Minimal Downtime:</strong> The disk migration is done <em>live</em> while the VM is running. The only downtime is the few minutes it takes to shut down the VM on ESXi and boot it on Proxmox.</li><li><strong>Space Efficient:</strong> No third copy of the data is needed. The disk is simply moved from one datastore to another.</li></ul><h1>Prerequisites</h1><ul><li>A Proxmox host and an ESXi host with network connectivity.</li><li>Root SSH access to your Proxmox host.</li><li>Administrator access to your vCenter or ESXi host.</li></ul><h1>Step-by-Step Migration Guide</h1><h1>Optional: Create a Dedicated Directory on LVM</h1><p>If you don&#39;t have an existing directory with enough free space, you can create a new Logical Volume (LV) specifically for this migration. This assumes you have free space in your LVM Volume Group (which is typically named <code>pve</code>).</p><ol><li>SSH into your Proxmox host.</li><li>Create a new Logical Volume. Replace <code>&lt;SIZE_IN_GB&gt;</code> with the size you need and <code>&lt;VG_NAME&gt;</code> with your Volume Group name.lvcreate -n esx-migration-lv -L &lt;SIZE\_IN\_GB&gt;G &lt;VG\_NAME&gt;</li><li>Format the new volume with the ext4 filesystem.mkfs.ext4 -E nodiscard /dev/&lt;VG\_NAME&gt;/esx-migration-lv</li><li>Add the new filesystem to <code>/etc/fstab</code> to ensure it mounts automatically on boot.echo &#39;/dev/&lt;VG\_NAME&gt;/esx-migration-lv /mnt/esx-migration ext4 defaults 0 0&#39; &gt;&gt; /etc/fstab</li><li>Reload the systemd manager to read the new fstab configuration.systemctl daemon-reload</li><li>Create the mount point directory, then mount all filesystems.mkdir -p /mnt/esx-migration mount -a</li><li>Your dedicated directory is now ready. Proceed to Step 1.</li></ol><h1>Step 1: Prepare Storage on Proxmox</h1><p>First, we need a &quot;Directory&quot; type storage in Proxmox that will receive the VM disk images.</p><ol><li>In the Proxmox UI, go to <strong>Datacenter -&gt; Storage -&gt; Add -&gt; Directory</strong>.</li><li><strong>ID:</strong> Give it a memorable name (e.g., <code>nfs-migration-storage</code>).</li><li><strong>Directory:</strong> Enter the path where the NFS share will live (e.g., <code>/mnt/esx-migration</code>).</li><li><strong>Content:</strong> Select <strong>&#39;Disk image&#39;</strong>.</li><li>Click <strong>Add</strong>.</li></ol><h1>Step 2: Set Up an NFS Share on Proxmox</h1><p>Now, we&#39;ll share the directory you just created via NFS so that ESXi can see it.</p><ol><li>SSH into your Proxmox host.</li><li>Install the NFS server package:apt update &amp;&amp; apt install nfs-kernel-server -y</li><li>Create the directory if it doesn&#39;t exist (if you didn&#39;t do the optional LVM step):mkdir -p /mnt/esx-migration</li><li>Edit the NFS exports file to add the share:nano /etc/exports</li><li>Add the following line to the file, replacing <code>&lt;ESXI_HOST_IP&gt;</code> with the actual IP address of your ESXi host./mnt/esx-migration &lt;ESXI\_HOST\_IP&gt;(rw,sync,no_subtree_check)</li><li>Save the file (CTRL+O, Enter, CTRL+X).</li><li>Activate the new share and restart the NFS service:exportfs -a systemctl restart nfs-kernel-server</li></ol><h1>Step 3: Mount the NFS Share as a Datastore in ESXi</h1><ol><li>Log in to your vCenter/ESXi host.</li><li>Navigate to <strong>Storage</strong>, and initiate the process to add a <strong>New Datastore</strong>.</li><li>Select <strong>NFS</strong> as the type.</li><li>Choose <strong>NFS version 3</strong> (it&#39;s generally more compatible and less troublesome).</li><li><strong>Name:</strong> Give the datastore a name (e.g., <code>Proxmox_Migration_Share</code>).</li><li><strong>Folder:</strong> Enter the path you shared from Proxmox (e.g., <code>/mnt/esx-migration</code>).</li><li><strong>Server:</strong> Enter the IP address of your Proxmox host.</li><li>Complete the wizard to mount the datastore.</li></ol><h1>Step 4: Live Migrate the VM&#39;s Disk to the NFS Share</h1><p>This step moves the disk files while the source VM is still running.</p><ol><li>In vCenter, find the VM you want to migrate.</li><li>Right-click the VM and select <strong>Migrate</strong>.</li><li>Choose <strong>&quot;Change storage only&quot;</strong>.</li><li>Select the <code>Proxmox_Migration_Share</code> datastore as the destination for the VM&#39;s hard disks.</li><li>Let the Storage vMotion task complete. This is the main data transfer step and will be much faster than other methods.</li></ol><h1>Step 5: Create the VM in Proxmox and Attach the Disk</h1><p>This is the final cutover, where the downtime begins.</p><ol><li>Once the storage migration is complete, <strong>gracefully shut down the guest OS</strong> on the source VM in ESXi.</li><li>In the Proxmox UI, create a new VM. Give it the same general specs (CPU, RAM, etc.). <strong>Do not create a hard disk for it yet.</strong> Note the new VM ID (e.g., <code>104</code>).</li><li>SSH back into your Proxmox host. The migrated files will be in a subfolder named after the VM. Let&#39;s find and move the main disk file.# Navigate to the directory where the VM files landed cd /mnt/esx-migration/VM_NAME/  # Proxmox expects disk images in /&lt;path\_to\_storage&gt;/images/&lt;VM\_ID&gt;/ # Move and rename the -flat.vmdk file (the raw data) to the correct location and name # Replace &lt;VM\_ID&gt; with your new Proxmox VM&#39;s ID (e.g., 104) mv VM_NAME-flat.vmdk /mnt/esx-migration/images/&lt;VM\_ID&gt;/vm-&lt;VM\_ID&gt;-disk-0.raw <strong>Note:</strong> The <code>-flat.vmdk</code> file contains the raw disk data. The small descriptor <code>.vmdk</code> file and other <code>.vmem</code>, <code>.vmsn</code> files are not needed.</li><li>Attach the disk to the Proxmox VM using the <code>qm set</code> command.# qm set &lt;VM\_ID&gt; --&lt;BUS\_TYPE&gt;0 &lt;STORAGE\_ID&gt;:&lt;VM\_ID&gt;/vm-&lt;VM\_ID&gt;-disk-0.raw  # Example for VM 104: qm set 104 --scsi0 nfs-migration-storage:104/vm-104-disk-0.raw <strong>Driver Tip:</strong> If you are migrating a <strong>Windows VM</strong> that does not have the VirtIO drivers installed, use <code>--sata0</code> instead of <code>--scsi0</code>. You can install the VirtIO drivers later and switch the bus type for better performance. For Linux, <code>scsi</code> with the <code>VirtIO SCSI</code> controller type is ideal.</li></ol><h1>Step 6: Boot Your Migrated VM!</h1><ol><li>In the Proxmox UI, go to your new VM&#39;s <strong>Options -&gt; Boot Order</strong>. Ensure the newly attached disk is enabled and at the top of the list.</li><li>Start the VM.</li></ol><p>It should now boot up in Proxmox from its newly migrated disk. Once you&#39;ve confirmed everything is working, you can safely delete the original VM from ESXi and clean up your NFS share configuration.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>So, in short, the downtime for a migrated VM will be 5 minutes or whatever time you take to do the move of the file and the VM to boot.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nlcv36/highspeed_lowdowntime_esxi_to_proxmox_migration/nf4hch9/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 20 Sep 2025 00:52:39 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nl77u7/proxmox_ha_failure_of_zfs_local_storage_does_not/</link><title>Proxmox HA: failure of ZFS local storage does not migrate VMs</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nl77u7/proxmox_ha_failure_of_zfs_local_storage_does_not/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nl77u7/proxmox_ha_failure_of_zfs_local_storage_does_not/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nl77u7/proxmox_ha_failure_of_zfs_local_storage_does_not/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>If I understand correctly, even a critical failure of the ZFS local storage will not result in the HA failover kicking in, if the node is otherwise up.</p><p>How do I automatically trigger a node hard down if ZFS local storage fails, so that HA failover will start migrating VMs?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>HA is for cluster resources, zfs is not a cluster resource, it’s local storage. Use a proper cluster storage system.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl77u7/proxmox_ha_failure_of_zfs_local_storage_does_not/nf4c39g/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 19 Sep 2025 21:19:22 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nl36ti/cant_use_my_mouse_and_keyboard_in_vm/</link><title>Can't use my mouse and keyboard in VM</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nl36ti/cant_use_my_mouse_and_keyboard_in_vm/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nl36ti/cant_use_my_mouse_and_keyboard_in_vm/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nl36ti/cant_use_my_mouse_and_keyboard_in_vm/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey,<br/>After booting into a Windows Server 2008 32-bit image, I cannot use my mouse or keyboard.<br/>I have disabled the “use tablet for pointer” setting.  </p><p>Any ideas ?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Probably need a little bit more information than this . . .</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl36ti/cant_use_my_mouse_and_keyboard_in_vm/nf2ll5g/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Have you tried enabling the &quot;use tablet for pointer&quot; setting?</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl36ti/cant_use_my_mouse_and_keyboard_in_vm/nf2o1g7/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>did you maybe on accident enable &quot;view only&quot; in the noVNC settings?</p><p><a href="https://preview.redd.it/gtk5e2pit4qf1.png?width=320&amp;format=png&amp;auto=webp&amp;s=940e90509ee58058f3f8fe73428143fba786b91b">https://preview.redd.it/gtk5e2pit4qf1.png?width=320&amp;format=png&amp;auto=webp&amp;s=940e90509ee58058f3f8fe73428143fba786b91b</a></p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl36ti/cant_use_my_mouse_and_keyboard_in_vm/nf2sbtg/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 19 Sep 2025 18:41:30 +0530</pubDate></item><item><link>https://i.redd.it/h6rvcgq8tzpf1.png</link><title>Lesson Learned - Make sure your write caches are all enabled (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>So I recently had the massive multi-disk/multi-vdev fault from my last post, and when I finally got the pool back online, I noticed the resilver speed was crawling.  I don&#39;t recall what caused me to think of it, but I found myself wondering &quot;I wonder if all the disk write caches are enabled?&quot;  As it turns out -- they weren&#39;t (this was taken after -- sde/sdu were previously set to &#39;off&#39;).  Here&#39;s a handy little script to check that and get the output above:</p><p>for d in /dev/sd*; do</p><p># Only block devices with names starting with &quot;sd&quot; followed by letters, and no partition numbers</p><p>[[ -b $d ]] || continue</p><p>if [[ $d =~ ^/dev/sd[a-z]+$ ]]; then</p><p>fw=$(sudo smartctl -i &quot;$d&quot; 2&gt;/dev/null | awk -F: &#39;/Firmware Version/{gsub(/ /,&quot;&quot;,$2); print $2}&#39;)</p><p>wc=$(sudo hdparm -W &quot;$d&quot; 2&gt;/dev/null | awk -F= &#39;/write-caching/{gsub(/ /,&quot;&quot;,$2); print $2}&#39;)</p><p>printf &quot;%-6s Firmware:%-6s WriteCache:%s\n&quot; &quot;$d&quot; &quot;$fw&quot; &quot;$wc&quot;</p><p>fi</p><p>done</p><p>Two new disks I just bought had their write caches disabled on arrival.  Also had a tough time getting them to flip, but this was the command that finally did it: &quot;smartctl -s wcache-sct,on,p /dev/sdX&quot;. I had only added one to the pool as a replacement so far, and it was choking the entire resilver process.  My scan speed shot up 10x, and issue speed jumped like 40x.</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/h6rvcgq8tzpf1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>On NVMe you can use this to figure out if writecache is enabled or not:</p><pre><code>cat /sys/block/nvme0n1/queue/write_cache</code></pre><p>write-through means disabled and write-back means enabled.</p><p>Note however that enabling writecache can be a very bad thing if your box isnt connected to an UPS.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/nf1vb30/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>From my understanding, it&#39;s best to leave it off if you don&#39;t have a UPS, because the drive would tell ZFS data is written to disk even if it&#39;s still in the drive&#39;s RAM, which could lead to data loss.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/nf2qmxk/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I recommend only enabling this if you have a UPS *and* a redundant power supply. Power supplies can and will die and you could run into issues.</p><p>The scale has two sides. Either you want a bit more &quot;safety&quot; and piece of mind or do you want a bit higher performance. Choose wisely.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/nf3vp8k/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I think it&#39;s ok on zfs.</p><p><a href="https://serverfault.com/questions/995702/zfs-enable-or-disable-disk-cache/995729#995729">https://serverfault.com/questions/995702/zfs-enable-or-disable-disk-cache/995729#995729</a></p><p>I&#39;ve actually checked all my drives on Proxmox and it&#39;s actually enabled by default when using zfs.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/nf488zo/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 19 Sep 2025 16:07:05 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/</link><title>Solutions for when you don't have control over your external network</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Senior level compsci student in college. I’ve just got a new desktop so my old one is hanging around doing nothing and I want to put proxmox on it and put it on my wifi network at the townhome I’m renting.</p><p>Only problem is my landlords aren’t tech savvy. The router is entirely ISP managed and so because of that I don’t have access to the ability to reserve a DHCP address. I’m probably going to just look at the network and pick an address that unlikely to be taken to be used as a management interface. And to be clear, I don’t need any of the VMs I’m hosting to be available when I’m not at home I don’t want a public facing IP I just want to be able to access it without DHCP issues.</p><p>But if I can’t get a DHCP address for my management interface is there a good way to ensure that if for some reason DHCP assigns the address I have proxmox that I can recover it or not have to deal with my ISP router?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>If you don’t need public facing, then just double NAT. </p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/nf0b8ad/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>There&#39;s a couple alternative solutions here that haven&#39;t been mentioned yet.</p><ol><li>Assign a static IP on a different subnet on proxmox and on your PC. You can have multiple subnets on the same LAN. That + DHCP gives you static access into it, and it has the usual internet access out. </li><li>Tailscale. Let everything be dynamic and Tailscale will give magic dns names to things and you can access them by that. Literally doesn&#39;t matter where or how your host has an internet connection, you can get to it. Yes, I know you&#39;re not asking for outside access, this is good over LAN. And for full speed too, it should NAT-hairpin and tunnel directly host to host within the LAN.</li></ol><p>There&#39;s some additional steps for getting proxmox to DHCP and work properly, see this <a href="https://gist.github.com/free-pmx/2292fa9efb75a16f3e648604050ed662">https://gist.github.com/free-pmx/2292fa9efb75a16f3e648604050ed662</a></p><p>Although I&#39;ll note that on my pve 9 I had to use &quot;if-up dhcpcd vmbr0&quot; instead of the DHCP mode described there.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/nf0fv2y/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>You could just allow it to use mDNS by installing a mDNS daemon like avahi-daemon and configuring it.  Then you wouldn&#39;t need to know the lease and just use whatever you&#39;ve set the mdns to.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/nf0ug28/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>You can always just get your own router and put it behind the ISP one.  You’ll have a double NAT setup, but that’s usually not an issue especially if you aren’t trying to allow external connections.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/nf0bcgw/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Install OPNSense/pfSense or whatever kind of firewall/router you like on Proxmox and use that one as router.</p><p>Enable DHCP on your router and use a static IP address on your Proxmox LAN NIC.</p><p>If the desktop you will use for Proxmox has a single network card, you will need a switch that supports VLANs, if you have an extra NIC or you can add one, then you will not.</p><p>In any case you would better have your own WiFi AP, that can double as a switch for other cabled devices, connected to the LAN NIC, behind the router/firewall.</p><p>Maybe this is a bit over engineered, but with such setup you will be free to do whatever you want with your network.</p><p>As other already mentioned, the moment you will need to expose some services or you will just need to connect home when outside, you can use tailscale or cloudflared.</p><p>It would be a nice journey, you would learn a lot of things :)</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/nf0w3fo/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Put your own firewall behind this &quot;landlord router&quot; to protect your own network but also to NAT (and portforward when needed) traffic using your single IP you get from upstream.</p><p>This way you can use how many IP-addresses you wish on your LAN with or without DHCP.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/nf139vv/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Use an overlay network like zerotier then you can use whatever dhcp address you get and it doesn&#39;t matter.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/nf0iv7v/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 19 Sep 2025 08:41:39 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nkq6s6/kernal_617_for_my_new_intel_arc_pro_b50/</link><title>Kernal 6.17 for my new Intel Arc Pro B50</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nkq6s6/kernal_617_for_my_new_intel_arc_pro_b50/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nkq6s6/kernal_617_for_my_new_intel_arc_pro_b50/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nkq6s6/kernal_617_for_my_new_intel_arc_pro_b50/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Just picked up one of these to give it a shot with SR IOV not realizing that support isn&#39;t in the kernel because these use the Xe driver which doesn&#39;t have SR IOV in the kernel until 6.17 (unlike the i915 driver the A series cards used).</p><p>Has anyone successfully upgraded VE 9.0 to Kernel 6.17 RC?  If so, care to share the steps?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>The Xe driver is in kernel 6.14 and is compatible with the arc b50. Just dropped my b50 into my proxmox host running kernel b50 and it was picked up immediately. I did update the compute packages and that was all I needed to get going smoothly. </p><p>Intel also isn’t planning on SRVIO enablement for battlemage until later in the year so that feature is not available yet.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkq6s6/kernal_617_for_my_new_intel_arc_pro_b50/nf1gu28/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>You can probably build kernel yourself with proxmox patches.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkq6s6/kernal_617_for_my_new_intel_arc_pro_b50/nf10tu9/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Dangerous waters.. when I was on Debian 12 it was back ports . Then zabbly kernel for support. Now I&#39;m on Debian 13 with zabbly for kernel.. works good for ARC cards.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkq6s6/kernal_617_for_my_new_intel_arc_pro_b50/nezuee2/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>until it&#39;s release by Ubuntu and then Proxmox it&#39;s not happening.</p><p>Proxmox uses a LTS kernel from Ubuntu and then add their special source to make work nicely as hypervisor host.</p><p>So if you were to install a 6.17 kernel from elsewhere you&#39;d bork your system.</p><p>quick web search indicates that 6.17 is now in Ubuntu 25.10 beta but that&#39;s not an LTS release so it could be April next year before it hits proxmox (the release of 26.04)</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkq6s6/kernal_617_for_my_new_intel_arc_pro_b50/nezuxqc/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 19 Sep 2025 06:50:25 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/</link><title>Can I add a NAS from another system to be available to my promotion syatem?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Very new to homelabbing. </p><p>So i installed proxmox on a MS-01 mini pc. Id like to make a jellyfin server, but of course the minipc doesn&#39;t have any easy way to add a bunch of high capacity drives.</p><p>So if I build a nas in a separate case, can I make the NAS storage available to my proxmox system and jellyfin?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>yep.</p><p>Share it as either NFS or SMB, mount it to your Proxmox server and then it pass it through to the LXC that&#39;s running Jellyfin.</p><p>if Jellyfin is running in a VM, you can mount it directly.</p><p>But the nice thing is you can mount the share to a VM, Proxmox server and even other machines on the network at the same time.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/nezcwn4/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Yes using NFS</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/nezcuk6/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I&#39;ve got two volumes presented by my NAS to VMs: 1 NFS that all my containers work within, another is an iSCSI block volume that a particularly huge Windows VM monopolizes.</p><p>Fair warning if you plan on following, the separate 10Gbit network I&#39;m running for storage is almost insufficient. It&#39;s slow, but not too slow to live with. I can&#39;t imagine 2.5Gbit or slower, though.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/nf030go/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Yes</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/nezctar/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Yeah I have my Synology media share mounted to pve as /mnt/nas in fstab as NFS, then I manually added entries to /etc/pve/lxc/###.conf for mp0: /mnt/nas,mp=/data where I generally match the trash guides.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/nf0b7dj/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Yes. My Plex and Jellyfin servers both run as unprivileged Proxmox LXCs and I have an *arrs stack on an Ubuntu VM that I run Docker on.</p><p>I mount my media drive on a Synology NAS via SMB/CIFS.</p><p>[you do need to know about unprivileged LXCs and mount permissions on the host]</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/nf1r4de/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Nfs is fastest but least secure.  I use this.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/nf6y5r5/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 19 Sep 2025 05:22:00 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/</link><title>New to Promox - is Proxmox Full Course by Learn Linux TV still up to date for beginners?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi folks,</p><p>As title says, new to Proxmox and want to follow a youtube guide. Searching this subreddit I saw this video series suggested:</p><h1><a href="https://www.youtube.com/playlist?list=PLT98CRl2KxKHnlbYhtABg6cF50bYa8Ulo">https://www.youtube.com/playlist?list=PLT98CRl2KxKHnlbYhtABg6cF50bYa8Ulo</a></h1><p>The videos are 2 years old. Wanted to check if theres a current suggested alternative or if these course is still relevant?</p><p>Thanks!</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>The essence of it still holds up. Things have been added to Proxmox later on, but for the most part, the things he goes through is still up to date and relevant.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/nexgvgm/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Don&#39;t know the videos.</p><p><a href="https://pve.proxmox.com/pve-docs/pve-admin-guide.html">https://pve.proxmox.com/pve-docs/pve-admin-guide.html</a></p><p>It is a lot of text, but worth imo.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/nexobxi/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>That&#39;s what I used very recently, I&#39;d highly recommend it!</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/nexhkuj/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Its virtualization in a hypervisor not a video game with a meta. Proxmox hasn&#39;t changed nearly enough for any of that content to be out of date.</p><p>Jokes aside his videos are great and he has excellent delivery.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/neyuyrd/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I would message Jay from Learn Linux TV and ask him. He would be the best source for your question. Not us.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/nezh7pr/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>This guy is the reason I started using proxmox.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/nf101mb/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Probably related to what you ask for:</p><p><a href="https://www.reddit.com/r/Proxmox/comments/1nh0soi/vent_unbelievably_frustrating/ne88g9a/">https://www.reddit.com/r/Proxmox/comments/1nh0soi/vent_unbelievably_frustrating/ne88g9a/</a></p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/nf12vuc/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 23:32:37 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/</link><title>Intel vs AMD: i5-9500 vs Ryzen 3400G</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>All things being equal, would you prefer to use Intel i5-9500 or AMD Ryzen 3400G CPUs for Proxmox nodes?</p><p>No GPU usage, no passthrough, just a few plain Linux VMs.</p><p>The 3400G box is cheaper than the i5-9500 box, but according to reports the Intel uses 5-10 W less (at idle), so over the lifetime of the hardware it&#39;s probably more or less cancels out.</p><p>AMD has an edge on upgradeability, going up to a 12 core Ryzen 9 PRO 3900. The Intel tops out at 8 cores with the i7-9900. The 3900 has roughly double the Passmark score of the i7-9900.</p><p>It&#39;s a bit of a tossup whether the nodes will ever be upgraded rather than replaced.</p><p>What&#39;s your take, AMD or Intel?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I don&#39;t know what a few plain Linux VMs if, whether it&#39;s 2 or 10,  or something in between, and what you&#39;re planning to run.<br/>So until then, my answer will be:<br/>The cheapest option.<br/>Don&#39;t worry about 5-10W (unless it&#39;s REALLY important to you), especially since it&#39;s TDP, which is equivalent of sucking your finger and putting it in the air, to conclude the wind is coming from west, is more of a pointer than an actual measurement.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/nexgh9c/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>If its AM4 you can go all the way to a 5950x which would be a monster.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/nexwzn9/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>What are you running<br/>Do you need more per core performance or more cpu threads</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/nexdxbj/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>AMD is a better long-term deal than core-i5 or i7</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/nexuwlz/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>If one box has 2 DIMM slots and another has 4, I&#39;d probably go with the 4.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/neytb2a/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>AMD, you do the math:</p><p><a href="https://security-tracker.debian.org/tracker/source-package/amd64-microcode">https://security-tracker.debian.org/tracker/source-package/amd64-microcode</a></p><p><a href="https://security-tracker.debian.org/tracker/source-package/intel-microcode">https://security-tracker.debian.org/tracker/source-package/intel-microcode</a></p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/nf13gxd/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Avoid 9th gen</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/neytibm/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 23:18:44 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/</link><title>Storage full, now I'm stuck in some form of loop.</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I have no idea about how to use Proxmox, Linux, or Windows Server.... so I got an old computer and installed all of them onto it so I can tinker and learn.</p><p>Today I was installing Microsoft SQL Server onto my Win Server, to fiddle with sample data.... and my Proxmox came up with this message - &quot;closing file &#39;/var/log/pve/tasks/active.tmp.4115482&#39; failed - No space left on device (500)&quot;</p><p>It now won&#39;t let me enter the shell to fiddle with it, I cannot SSH in, it terminates the session. Viewing it in a browser, it just crashes. No VMs are running or will let me remote in either.</p><p>Am I toast here?</p><p>The SSD is 1TB, but only 100GB was given to VM storage, a lesson here is to increase it next time.</p><p>Thank you in advance.</p><p><a href="https://preview.redd.it/bq90f4mkvxpf1.png?width=1673&amp;format=png&amp;auto=webp&amp;s=c1f49f11bed80c03cfd8dab12f3482aa271fe4d8">I get this error when trying to access the Proxmox shell. </a></p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Hookup a keyboard and monitor, you should be able to boot into safe mode on the host (the actual computer running proxmox) and navigate around to clear up space or get the webui without starting any containers.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/newfmiz/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The fact that it&#39;s complaining about /var/log/pve/tasks/ would suggest it&#39;s the host that ran out.</p><p>Seems a bit odd that you&#39;re out of space on a 1TB drive with only 100gig VM.</p><p>First thing I&#39;d try is instead of an interactive shell send a command over ssh rather than creating an interactive shell. Something like so - adjust with whatever authentication you&#39;re using:</p><pre><code>ssh -i ~/.ssh/id_ed25519 -p 22 root@host &#39;journalctl --vacuum-time=3d&#39;</code></pre><p>If that doesn&#39;t work - journalctl might not work without space then I&#39;d try to hit the log dirs with a rm command. You&#39;ll need to do it blindly, so to help - structure looks like this...I&#39;d imagine you can delete some of those numbered folders without too much carnage</p><p><a href="https://i.imgur.com/4DJ1Jar.png">https://i.imgur.com/4DJ1Jar.png</a></p><p>Failing that you&#39;ll likely need to boot off a portable OS, mount the partition, delete some stuff and then reboot</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/nex72ye/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Is that no space with in the vm virtual disk file or on the disk it’s stored for proxmox?</p><p>If you have another windows vm, you could mount the virtual disk file to remove files to clean it up or enlarge it (though gparted will also help as there maybe a windows hidden partition that needs to be moved,</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/newgfpf/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I would reinstall and set the os in a partition with 32/62G it should only need 16 but adding some more for ISO storage on local-zfs is nice. Then use the rest as a vm storage like vm-tank.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/nf15yzt/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 20:33:59 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nk9vup/strange_issue_cant_update_past_903_no_updates/</link><title>Strange issue, cant update past 9.0.3, no updates found</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nk9vup/strange_issue_cant_update_past_903_no_updates/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nk9vup/strange_issue_cant_update_past_903_no_updates/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nk9vup/strange_issue_cant_update_past_903_no_updates/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>root@Proxmox:~# apt update<br/>Hit:1 <a href="http://security.debian.org/debian-security">http://security.debian.org/debian-security</a> trixie-security InRelease<br/>Hit:2 <a href="http://deb.debian.org/debian">http://deb.debian.org/debian</a> trixie InRelease<br/>Hit:3 <a href="http://download.proxmox.com/debian/pbs">http://download.proxmox.com/debian/pbs</a> trixie InRelease<br/>Hit:4 <a href="http://deb.debian.org/debian">http://deb.debian.org/debian</a> trixie-updates InRelease<br/>All packages are up to date.<br/>root@Proxmox:~# apt upgrade<br/>Summary:<br/>Upgrading: 0, Installing: 0, Removing: 0, Not Upgrading: 0<br/>root@Proxmox:~# cat /etc/apt/sources.list.d/proxmox.sources<br/>Types: deb<br/>URIs: <a href="http://download.proxmox.com/debian/pbs">http://download.proxmox.com/debian/pbs</a><br/>Suites: trixie<br/>Components: pbs-no-subscription<br/>Signed-By: /usr/share/keyrings/proxmox-archive-keyring.gpg<br/>root@Proxmox:~#</p><p>zero errors, it just thinks there are no updates. The problem is non of my LXC will start. My other node is at 9.0.10. Any ideas ?</p><p>EDIT:  [SOLVED] TLDR the comments: I need more coffee, had the wrong repo in there ugh</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>You don&#39;t seem to have any PVE repos configured. PVE and PBS are different things.See here: <a href="https://pve.proxmox.com/wiki/Package_Repositories">https://pve.proxmox.com/wiki/Package_Repositories</a></p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk9vup/strange_issue_cant_update_past_903_no_updates/newkzlh/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>You run apt dist-upgrade to upgrade Proxmox</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk9vup/strange_issue_cant_update_past_903_no_updates/new80lp/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Are you upgrading PVE or PBS? Your repo sources above are for PBS.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk9vup/strange_issue_cant_update_past_903_no_updates/newlnnn/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 20:01:06 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/</link><title>Proxmox HA: ZFS mirror or separate OS and data disks</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>On small nodes, would you rather put everything on a two disk ZFS mirror or use one disk for Proxmov and one disk for VMs?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I pick mirror over separate - all my servers run mirror and I even went as far as manually setting up the partitions in the case of mixed size disks.</p><p>Having my production servers continue running while waiting for a replacement allows me to keep things running, reducing stress and allow me to look / wait for better deals (e.g. used enterprise ssd).</p><p>And of course ability to repair scrub errors.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/nevd4eq/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>As Proxmox is so easy to install I would (and do run) one disk (or a partition) for the OS and the other (the faster) for VM disks.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/nevbjpp/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Are the disks the same size? If not, mirrors aren&#39;t the best idea.</p><p>I have a 1TB primary, and 2T B secondary SSD. After playing around with multiple layouts, I&#39;m now aiming for: 1× 50GB Proxmox, 1× 850+GB VM / LXC.<br/>2nd drive is exclusive for Jellyfin etc. shared media.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/nevpo5n/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>For really small nodes I would go with a 2x mirror using ZFS.</p><p>Note that you need to keep track of storage usage since all partitions will use the same drives and the partitions are not limited as with ext4. That is both OS and VM drives will use the same physical storage making it more likely that you one day run out of space if you dont keep track of this and then you need &quot;BIOS access&quot; (ILO/IPMI/IPKVM) to resolve the issue through recovery mode in Proxmox.</p><p>That is you will see something like this (removed some lines from output):</p><pre><code>Filesystem        Size  Used Avail Use% Mounted onrpool/ROOT/pve-1  329G  2,4G  327G   1% /rpool             327G  128K  327G   1% /rpoolrpool/var-lib-vz  672G  345G  327G  52% /var/lib/vzrpool/ROOT        327G  128K  327G   1% /rpool/ROOTrpool/data        327G  128K  327G   1% /rpool/data</code></pre><p>In above those 327G free (on my 2x mirror of 800GB NVMe drives) is shared between all datasets.</p><p>Meaning if I upload a 5GB ISO to the root filesystem all datasets will then report 322G free.</p><p>Other than that I prefer to run 2x mirror for the OS (normally M.2 slots) and then use the frontloaded drivebays for the actual VM-storage (for a 1RU thats often 10x 2.5&quot; slots or so).</p><p>That is ZFS for the 2x boot mirror and then how many drives you got for CEPH as shared storage between the nodes.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/newr9ba/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I use cheap 32 and 64 GB optane M.2 drives as a mirrored ZFS boot volume. Then I add a pair of larger U.2/U.3 NVMe drives on a PCIe adapter for VM storage.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/nexzffq/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 16:51:03 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/</link><title>Changing GPU to another slot when it is passed trough to a VM, makes the host reboot</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Changing GPU to another slot when it is passed trough to a VM, makes the host reboot<br/>When the VM starts after reboot, it does not found the passed trough GPU and the whole host reboots.<br/>Its in a reboot loop now. What should I do?</p><p>EDIT; Yes I have shut of the server always when touching components and pulled the cord.<br/>EDIT: all works, the reason was PCIE 4.0 Riser cable. Those just does not work with 5090.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I hate to ask the question but just wanting to make sure: Did you power down the host before removing the GPU / putting it back in?</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/nev6srf/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>yikes, you&#39;ll need to somehow disable the automatic start of the VMs where you have PCIe devices passed through</p><p>The reason this happens is because the devices change their PCIe ID every time you install/remove/replace a PCIe device, it&#39;s just how motherboards handle this sort of thing, and afaik it can&#39;t really be helped on consumer boards</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/nev7v1r/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Addressing the real issue (OP was not trying to hotswap GPUs), I’d put it back where it was, boot up, detach the GPU from the VM and turn off its autostart, shut down, move the GPU and pass it to the VM again.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/nevoz19/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Well you are pulling a core component of your server out. Naturally your host is gonna throw a hissy fit to protest. You wouldnt like it very much if someone pulled out your liver while you are still booted. /s </p><p>Shut off your host whole working on any builtin component.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/nev7p83/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The pci address will change according to the slot</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/nevp68n/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Don’t listen to the other OP, pcie hot plug is a well supported feature /s</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/neva8d1/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The iDs change when you change the pcie slot.</p><p>Iommu is a virtual memory address....when you change the location that changes too.</p><p>If for some reason you need to leave it in the new location you will have to find the new address with lspci in the console and nano the corresponding configs.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/nez87gh/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 16:21:45 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nk4f9h/proxmox_sdn_fabric_access_shared_nfs_storage_in/</link><title>Proxmox SDN fabric, access shared NFS storage in fabric</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nk4f9h/proxmox_sdn_fabric_access_shared_nfs_storage_in/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nk4f9h/proxmox_sdn_fabric_access_shared_nfs_storage_in/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nk4f9h/proxmox_sdn_fabric_access_shared_nfs_storage_in/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey folkshope you’re doing well.  </p><p>I’m running a 3-node Proxmox cluster in a hyper-converged setup.<br/>On <strong>node 2</strong> there’s a <strong>TrueNAS VM</strong> that exports two NVMe pools over <strong>NFSv4</strong>.  </p><p><strong>Previous setup (worked fine):</strong>  </p><ul><li>All 3 nodes connected to a 10 Gbps switch via LACP, classic VLANs.</li><li><strong>VLAN 3100</strong> was the default VM network.</li><li>The TrueNAS VM NIC was bridged into <strong>VLAN 3100</strong>.</li><li>Each Proxmox host also had an IP interface in <strong>VLAN 3100</strong> to mount the NFS share.</li></ul><p><strong>What changed:</strong>  </p><ul><li>I removed the ~300 W Nexus 3000 switch and cabled the nodes in a <strong>ring</strong>.</li><li>I now run a <strong>VXLAN fabric</strong>, and <strong>VLAN 3100</strong> exists <em>inside</em> that overlay (VXLAN + VLAN tag, actually VM bridged to the fabric interface + VLAN tag).</li><li>VM-to-VM networking and live migration work.</li></ul><p><strong>My problem</strong>  </p><p>I can’t figure out the <em>clean</em> way for the <strong>Proxmox hosts</strong> themselves (not the VMs) to reach the NFS server that now resides <strong>inside the VXLAN/VLAN-3100 segment</strong>.<br/>In other words: what’s the appropriate method to give the hosts IP reachability into <strong>VLAN 3100 inside the VXLAN overlay</strong> so they can mount NFS from the TrueNAS VM?  </p><p><strong>My question</strong>  </p><ul><li>Is it supported/reasonable to put an IP on the <strong>overlay bridge</strong> (e.g., assign an address to the VXLAN bridge) and/or create a <strong>VLAN sub-interface</strong> on that bridge (e.g., vmbrX.3100) on each host?</li><li>Alternatively, should I attach a <strong>veth</strong> from the host into the VXLAN/VLAN-aware bridge to “place” the host stack inside that segment?</li><li>Is there a recommended Proxmox SDN way to give the <em>host</em> an interface inside a VNet/VNI for this purpose?</li></ul><p>Thanks in advance for any help!!! </p></div><!-- SC_ON --></section>]]></description><pubDate>Thu, 18 Sep 2025 15:47:20 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nk335b/question_with_regards_to_pbs_deduplication_factor/</link><title>Question with regards to PBS deduplication factor.</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nk335b/question_with_regards_to_pbs_deduplication_factor/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nk335b/question_with_regards_to_pbs_deduplication_factor/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nk335b/question_with_regards_to_pbs_deduplication_factor/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>A fictive case in order to try to understand the deduplication factor.</p><ul><li>1 VM to be backed up, all blocks are unique and hence not a single block is &quot;deduplicatable&quot;.</li><li>The VM is turned off. So blocks to be backed up never change.</li><li>completely empty repository. No backups as of yet.</li></ul><p>I run a backup job that only backs up this VM. I assume the deduplication factor will be 1 because there&#39;s not a single block unique.</p><p>I run the exact same backup job while the source VM was still turned off, so no blocks have changed, the backup retention is limitless so to speak, so the previous &quot;snapshot&quot; is not purged.</p><p>What is the deduplication factor after this second backup job finishes successfully? 2? Or still 1?</p><p>EDIT: first bullet point was wrong about the deduplicatable blocks.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>The factor from what i understand is on my much identical data there is.<br/>1 backup (1:1 ratio)<br/>2 backup (2:1 ratio)<br/>and so on, but this would only apply i nothing was changed between the backups.</p><p>Also the dedupe number dont update instantly, its the GC / Purge runs thats update it if im not wrong.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk335b/question_with_regards_to_pbs_deduplication_factor/neutk0o/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>„not a single block is unique and deduplicateable“</p><p>I guess you mean all blocks are unique? If no block would be unique then you would have dedupe.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk335b/question_with_regards_to_pbs_deduplication_factor/nf0zbrn/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I don&#39;t know what would make a block not deduplicatable besides being unique. So since you stipulate &quot;no unique blocks&quot; the VM should in fact dedup extremely well with each block referenced at least two times. SoI expect &gt;=2 for the first backup and &gt;=4 after the second.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk335b/question_with_regards_to_pbs_deduplication_factor/nf10m8n/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Last I checked the factor was over 80. This PBS is as old as PBS.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk335b/question_with_regards_to_pbs_deduplication_factor/nevbvmq/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 14:22:08 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/</link><title>Advice to build up Proxmox Cluster with 3 nodes (feat. ceph storage)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi all,<br/>I’m planning a 3‑node Proxmox cluster with Ceph for my homelab and would love some advice. My goals are to learn (Ceph + clustering), replace an older single Proxmox host, and use SFP+ (10Gb) networking. I’m unsure whether to build from parts, go with mini PCs, open to all options.</p><p>Context and preferences:</p><ul><li>Learning‑focused, but I want something stable and maintainable.</li><li>Noise and power matter (it’s at home), but I can handle moderate fan noise if the value is good.</li><li>I want SFP+ for the cluster/replication network (10Gb; 25Gb later would be a bonus).</li><li>Planning on 3 nodes for Ceph minimum; I’m okay with starting small and expanding.</li></ul><p>Questions:</p><ul><li>DIY small towers (consumer parts), small form factor PCs (NUCs/minis)?</li><li>Any favorites for IPMI/remote management on a budget (Supermicro, used Dell/HP)?</li></ul><p>I’ve already got 6×16GB DDR5 SO-DIMM lying around. Budget is up to 500$ per node.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I have three minisforum ms 01. Each has three ssds two of which are dedicated to ceph. Works quite well. They also fit a 25G NIC and 96GB RAM (if I recall correctly).</p><p>Edit: I guess my budget was a bit lower, but you can go for smaller/fewer ssds and RAM</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/newq5zf/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>May want to read through <a href="https://forum.proxmox.com/threads/fabu-can-i-use-ceph-in-a-_very_-small-cluster.159671/">this post </a>. 3 nodes will function but there are caveats.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/nexdjgp/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The usual suspects for homeuse are:</p><ul><li><p>Minisforum: <a href="https://www.minisforum.com/collections/mini-pc">https://www.minisforum.com/collections/mini-pc</a></p></li><li><p>Protectli: <a href="https://protectli.com/products/">https://protectli.com/products/</a></p></li><li><p>CWWK: <a href="https://cwwkpc.com/collections/mini-pc">https://cwwkpc.com/collections/mini-pc</a></p></li><li><p>Zimaboard: <a href="https://www.zimaspace.com/products/single-board2-server">https://www.zimaspace.com/products/single-board2-server</a></p></li></ul><p>What I would favour is:</p><ul><li><p>Avoid HDD, use SSD or NVMe if your wallet is large enough.</p></li><li><p>Check the datasheets of the drives, for NVMe I highly recommend PLP/DRAM for performance and DWPD 3.0 (or higher) and high TBW for endurance. Avoid the 600TBW devices.</p></li><li><p>If possible AMD instead of Intel.</p></li></ul><p>Reasons?</p><p>Well you do the math ;-)</p><p><a href="https://security-tracker.debian.org/tracker/source-package/amd64-microcode">https://security-tracker.debian.org/tracker/source-package/amd64-microcode</a></p><p><a href="https://security-tracker.debian.org/tracker/source-package/intel-microcode">https://security-tracker.debian.org/tracker/source-package/intel-microcode</a></p><ul><li><p>For RAM select as large as you can fit. Also check how the CPU functions regarding dual, quad, 8 or 12-channels to boost performance. Also something like DDRx-6400 (if supported) is prefered over DDRx-4800.</p></li><li><p>For networking it depends on how much you want to segment but &quot;ideal&quot; or &quot;optimal&quot; would be something like:</p></li><li><p>ILO/IPMI/IPKVM: 1G RJ45</p></li><li><p>MGMT: 1G RJ45</p></li><li><p>FRONTEND: 2x25G SMF</p></li><li><p>BACKEND-CLIENT: 2x25G SMF</p></li><li><p>BACKEND-CLUSTER: 2x25G SMF</p></li></ul><p>Of course above can be if you got larger wallet increased like first and foremost the backend traffic will benefit of lets say 100G.</p><p>But you can also shrink down in speed and number of interfaces.</p><p>Reason why you want backend-client and backend-cluster separate is so not VM traffic will interfere with replication and clustersync traffic.</p><ul><li><p>You can save some costs of avoid having 2x switches (for redundancy) for backend and instead directly connect the hosts to each other and use FRR with OSPF locally. Another option is of course to use a single switch (or pair of switches in MLAG) for both frontend and backend traffic but normally you dont want to share these flows in the same hardware for security reasons. But for a homelab that would be perfectly fine (but then for a 3-node cluster I would directly connect the hosts to each other instead).</p></li><li><p>Then if possible I would prefer having 2x mirror as boot drive using ZFS and the rest used by CEPH. But this also boils down to how many drive slots will your hardware provide. If you just have lets say room for 2x M.2 drives then I would use one for ZFS for boot and the other for CEPH where the VM&#39;s are stored.</p></li></ul><p>So in short it boils down to what you want, what you need, why you need it and the size of your wallet?</p><p>Problem is often that people have unrealistic expectations - like they want a 3-node cluster with 6x25G interfaces, 128GB RAM, AMD EPYC 64C/128T CPU and 2x Micron NVMe for boot and 4x Solidigm 8TB drives for VM&#39;s and all this should be passively cooled and cost not more than $1000 in total - which is an impossible equation :-)</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/nf127s2/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Ms-01 is perfect for there mode ceph pve cluster. Up to 96gb ram and up to 8 tb storage n+2</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/nf0zb7q/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I&#39;ve got a 3 node Proxmox cluster, 3 Mini PC&#39;s, 2012 Mac Mini, Beelink SER5 Max (5800H) and TrigKey N150.  </p><p>Not the most capable hardware which is why I didn&#39;t go for ZFS due to lack of RAM I thought it was overkill.  </p><p>So all three machines have a local-lvm and the two more modern ones also have an &#39;nvme&#39; lvm.</p><p>I&#39;ve been wanting to do some HA with a few containers, Adguard, Caddy, Tailscale etc.  Some very light weight containers that have between 8GB and 16GB root partitions.  </p><p>Recently I spotted you can run Ceph on top  of LVM-Thin as the backing, so I&#39;ve set that up, you have to use the command line as you can&#39;t do it via the GUI but created 5 OSD&#39;s, each node that has 2 LVM&#39;s got a 32GB OSD and the Mac Mini got a single 64GB OSD.  </p><p>It&#39;s working well, HA is setup, but not tested, I have tested migrations and it&#39;s fast.  </p><p>This might not be the best way but it saved me many hours from having to move storage around to try and get ZFS replication working or using raw disks for CEPH.  Considering the MacMini is on the other end of a powerline adapter and only gets 150Mbps to the rest of my network I&#39;m quite happy.</p><p>Good luck with whatever you decide.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/nf2in47/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I would like to thank you all for the discussion. I think I will stay with my current host until I have the budget for a 4-node solution. First, I need to prepare a complete bucket list (nodes, SSDs, cables, etc.) and then define the next steps.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/nf7royr/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 12:27:17 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nk0huf/ha_with_zfsreplication_do_i_need_groups/</link><title>HA with zfs-replication - do I NEED groups?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nk0huf/ha_with_zfsreplication_do_i_need_groups/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nk0huf/ha_with_zfsreplication_do_i_need_groups/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nk0huf/ha_with_zfsreplication_do_i_need_groups/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey everyone,</p><p>I want to run a Proxmox cluster with 3 nodes. The VMs will be stored on local ZFS pools and replicated via <strong>ZFS replication</strong> between two of the nodes. So, the third node does not have the replicated volumes.</p><p>My question is:<br/>If I enable HA for a VM, does the <strong>cluster manager (ha-manager)</strong> automatically know that the VM can only run on the two nodes that actually have the replicated ZFS volume?<br/>Or will it also try to start the VM on the third node in case of a failure — which would obviously fail because the storage isn’t available there?</p><p>So in this kind of setup, do I really need to maintain <strong>HA groups</strong> to restrict which nodes are eligible, or does Proxmox handle this logic automatically?</p><p>Curious how you guys deal with this and what the best practices are.</p><p>Thanks in advance</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I chose ceph to avoid stuff like this. It isn&#39;t without its own problems but works OK so far (~1y)</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk0huf/ha_with_zfsreplication_do_i_need_groups/nevey4c/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>It doesn&#39;t. You&#39;ll need groups. It&#39;s a pain. I ended up replicating to all nodes and just buying more storage instead because the admin overhead was annoying.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk0huf/ha_with_zfsreplication_do_i_need_groups/neujpby/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 11:35:35 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1njzqkn/proxmox_crashes_when_i_try_to_copy_files_from_my/</link><title>Proxmox crashes when I try to copy files from my TrueNas</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1njzqkn/proxmox_crashes_when_i_try_to_copy_files_from_my/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1njzqkn/proxmox_crashes_when_i_try_to_copy_files_from_my/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1njzqkn/proxmox_crashes_when_i_try_to_copy_files_from_my/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I have a Dell server running Proxmox with only TrueNAS installed on it.</p><p>My Jellyfin server, which is installed on another server on my local network, can access files, play video files, and browse files through the network. I can open files through network share.</p><p>However, when I try to copy anything from the TrueNAS server, it first disappears from the network, followed by Proxmox (or both simultaneously). Proxmox becomes inaccessible, so I have to manually turn it off and on again.</p><p>I want to move my TrueNAS setup to another server and add an additional drive to mirror the current one. But I’m concerned that the drive might be failing.</p><p>The server has 16GB of RAM installed, with 12GB assigned to TrueNAS and then reduced to 8GB. Despite these changes, the issue persists.</p><p>I’ve tried the following solutions:- Changing the zfs_arc_limit- Changing the RAM size assigned to TrueNAS- Switching from the m.2 wifi slot for additional SATA to the onboard SATA slot</p><p>I’m at a loss and would appreciate any advice you can provide.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Maybe this? <a href="https://nb.balaji.blog/posts/fix-intel-e1000-proxmox-hang/">https://nb.balaji.blog/posts/fix-intel-e1000-proxmox-hang/</a></p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njzqkn/proxmox_crashes_when_i_try_to_copy_files_from_my/neua30b/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The crash you&#39;re seeing isn&#39;t caused by a software misconfiguration but by a fundamental hardware instability. The hardware is failing under load. Prioritize backing up your data and moving to a new, stable server.</p><p>Use smartctl within your TrueNAS shell to check the S.M.A.R.T. data for your drives. Look for errors, especially in reallocated sectors, pending sectors, and read error rates.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njzqkn/proxmox_crashes_when_i_try_to_copy_files_from_my/neug3xt/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I would look at the previous boot&#39;s logs to see if they give a clue. After a crash, the next boot use<code>journalctl -b-1</code> if persistent logs are enabled.  If not, enable them.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njzqkn/proxmox_crashes_when_i_try_to_copy_files_from_my/nez2fyp/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Why are you running TrueNas in Proxmox if it&#39;s the only thing on the computer? Why not just run it native?</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njzqkn/proxmox_crashes_when_i_try_to_copy_files_from_my/nevzmou/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 10:49:55 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1njyc8v/how_to_share_rclone_fuse_mount_from_inside_lxc/</link><title>How to share rclone FUSE mount from inside LXC?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1njyc8v/how_to_share_rclone_fuse_mount_from_inside_lxc/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1njyc8v/how_to_share_rclone_fuse_mount_from_inside_lxc/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1njyc8v/how_to_share_rclone_fuse_mount_from_inside_lxc/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I’m trying to set up some cloud storage so it’s usable across multiple containers. My setup so far:</p><ul><li>set up unprivileged container running rclone (FUSE enabled)</li><li>bind mount directory on host to rclone container</li><li>mount cloud storage to mounted directory using rclone</li></ul><p>It’s working partially: </p><ul><li>From inside the rclone container, I’m able to write to the mounted folder and see the files appear in cloud storage. Everything is working beautifully. </li><li>however, the directory looks empty when I inspect from inside the host</li><li>inspecting from any other container also shows an empty directory</li></ul><p>The standard suggestion is to run rclone in the host and bind mount the directory to any containers. However, I’d like to avoid doing that. Is there any way to get this set up so I can run rclone in a single container and have that mount be usable for other containers?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Check the documentation on bind mount...vaguely recall it having a shared flag</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njyc8v/how_to_share_rclone_fuse_mount_from_inside_lxc/nexj911/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 09:30:38 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/</link><title>Best way to expand storage for a laptop Proxmox media server (Jellyfin/Plex)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey all,</p><p>I’ve got a few older laptops lying around that I’d like to repurpose as a Proxmox host. My plan is to run:</p><ul><li>Proxmox on the laptop</li><li>A few LXC containers with Docker for Jellyfin, Plex, Deluge, etc.</li><li>Keep it low-power, compact, and quiet since this is just a home media server</li></ul><p>The challenge is storage.</p><ul><li>Laptops don’t have many drive bays.</li><li>I was thinking about getting an Orico 5-bay USB enclosure and filling it with a few 4TB drives, maybe running them in a ZFS pool for redundancy/snapshots.</li><li>But I’ve read mixed things about ZFS over USB (unstable disk IDs, no SMART monitoring, risk of disconnects).</li></ul><p>What’s the best way to add reliable, redundant storage to a laptop running Proxmox as a media server?</p><p>Options I’ve thought about:</p><ul><li>Use an external enclosure like the Orico but just run the drives as individual disks (ext4/XFS), no ZFS/RAID.</li><li>Separate compute and storage: laptop runs Proxmox + apps, but storage lives on another box/NAS and I connect via NFS/SMB.</li><li>Some other external storage trick people have used with laptops (HBA, docks, etc.).</li></ul><p>Goals:</p><ul><li>Low power draw</li><li>Expandable media storage</li><li>Redundancy if possible</li><li>Reuse my existing laptops instead of buying a new server</li></ul><p>Has anyone here run Jellyfin/Plex this way? Would love to hear what storage setups worked best with laptops.</p><p>Thanks!</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I have the same question and goals.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/netf0le/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><blockquote><p>Separate compute and storage: laptop runs Proxmox + apps, but storage lives on another box/NAS and I connect via NFS/SMB.</p></blockquote><p>This, big time. There are tons of options for a cheap NAS - check <a href="https://www.reddit.com/r/homelab">/r/homelab</a>. But any other solution for storage is going to give you pain.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/netgqqx/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Does your laptop have an m.2 slot?</p><p>You can use an m.2 to multiple SATA ports and power with a PSU.</p><p><a href="https://youtu.be/VzR-kIJLsAU?si=S-QDa6yMDmuKLh6p">Reference video</a></p><p>Ensure you take out the laptop battery before running 24/7</p><p>Hope that helps</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/netr0ay/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I&#39;ve done this for years with no issues (but ext4 + mergerfs).</p><p>First setup was a macbook pro running OMV (debian) with cheap usb attached drives from no-name USB drop in storage.</p><p>Second setup was HP laptop with Terramaster D4-300 running Proxmox with HDDs passed through to OMV VM.</p><p>I have never had issues with data corruption or anything. smart data is fully available with the Terramaster.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/newulms/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I have a 4-bay DAS connected to a miniPC. QNAP TR-004, specifically. I have the hardware RAID set to JBOD and manage the disks using Proxmox’s default ZFS controls. I have never had issues with it. You’re right about the SMART monitoring, though.</p><p>Do your laptops have any m.2 slots on the motherboard? If so, you can <a href="https://a.co/d/8tKWKLK">get cards with a bunch of SATA ports on them</a>. You might need to take the motherboard out of the laptop chassis, though.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/netj90k/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Depends a lot on the laptop and what connectivity it has</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/netu7ox/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>USB and ZFS bad idea. The suggestion by another user of making pcieNvme to 4 x SATA is a possibility.</p><p>Best is sell them all. Get a mini pc like dell USFF or hp.mini.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/neu2wc2/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 05:46:33 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/</link><title>Should I pass the iGPU through for my use case?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I have a number of VMs running, most of which are headless, but I do have 1 Windows instance that I use to support clients who for whatever reason don&#39;t allow me to connect remotely to their equipment using a Linux system. (Yes, these security rules exist, I have no idea why.) My question is, would it benefit my Windows install at all to pass the unused iGPU through to it? I&#39;ve had it running without it for ages now, but it occured to me today that I could pass the iGPU through. Is it worth the effort though? I connect to the Windows instance using Chrome Remote Desktop, not the best but it works flawlessly. The Windows VM is deployed on a small Beelink MiniPC with an Intel N100 CPU and has 3 of the 4 cores available to it, but shared with a Docker VM instance that runs a few lightweight containers. </p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>If you are using Chrome Remote Desktop, passing the iGPU would not improve your desktop user experience much. However, if you need it for other tasks, it might be helpful.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/nesen95/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>RDP doesn&#39;t use graphics acceleration, so no, it would not be useful to you. </p><p>GPU passthrough is for video encoding, LLM inference and running gpu-accelerated protocols like Moonlight. </p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/neumce4/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>If what you have now works, I don&#39;t see the reason to screw with it. </p><p>CDR <em>basically</em> doesn&#39;t use hardware acceleration. <a href="https://cloud.google.com/architecture/chrome-desktop-remote-on-compute-engine">It doesn&#39;t at all on Linux</a>. On Windows it piggy backs RDP, which for most sessions <a href="https://learn.microsoft.com/en-us/azure/virtual-desktop/graphics-encoding#mixed-mode">is mostly text</a>... like it helps for transmitting images (sort of as you get CPU offloading) but are you bandwidth limited between hosts? Video only occurs over RDP if something is show you a video. If all you&#39;re doing is running terminal sessions, menuing, and editing files it won&#39;t do crap.</p><p>Unless you&#39;re billing the client for those hours(?) Then I guess(?)</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/nesh0wt/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Is the iGPU really unused?  Do you have another way of locally accessing/controlling the Proxmox system if it&#39;s unresponsive or networking goes down?</p><p>If the iGPU is the only video output from the Proxmox host, giving it to a VM will make your life hell if/when something eventually goes wrong that requires local access to fix.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/neshsmn/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Something not being mentioned in these comments is that if you do pass it through you can just plug a monitor in instead of remoting in. Would that be useful to your use case?</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/nesmb08/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Windows handles itself fairly alright with no gpu support as long as you’re not doing anything that relies on anything to be rendered like video and whatnot</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/nesn2a6/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>doesn&#39;t the n100 have sr-iov drivers? i&#39;d be passing them through and also setting up RDP / VNC</p><p>gives you the same result basically (hardware level performance and remote viewing)</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/netuvsq/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 03:52:16 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1njqeg2/scrypted_in_lxc_on_proxmox_9_cpuram_spikes/</link><title>Scrypted in LXC on Proxmox 9 - CPU/RAM spikes  container crashes (works fine on Proxmox 8)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1njqeg2/scrypted_in_lxc_on_proxmox_9_cpuram_spikes/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1njqeg2/scrypted_in_lxc_on_proxmox_9_cpuram_spikes/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1njqeg2/scrypted_in_lxc_on_proxmox_9_cpuram_spikes/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi all,</p><p>I’m running into an issue with Scrypted inside an unprivileged LXC (Ubuntu 22.04.5 LTS) on Proxmox and would like to hear if anyone else has experienced this.</p><p><strong>The issue</strong>On Proxmox VE 9.0.9 (Debian 13, kernel 6.14.8-2-pve), the container spikes CPU and RAM close to 100% and eventually becomes unresponsive. When this happens, the load average on the host shoots up (I’ve seen &gt;40), and the LXC has to be force-stopped.</p><p>On Proxmox VE 8.4.13 (Debian 12, kernel 6.8.12-14-pve) with older hardware, the same container and config run completely fine — no spikes, no crashes.</p><p><strong>Host hardware comparison</strong>Problem host (Proxmox 9):    • CPU: 12th Gen Intel i7-12700T (20 cores)    • Memory: 62 GiB    • Kernel: 6.14.8-2-pve    • OS: Debian 13 (trixie)</p><p>Stable host (Proxmox 8):    • CPU: Intel i3-5010U (4 cores)    • Memory: 7.6 GiB    • Kernel: 6.8.12-14-pve    • OS: Debian 12 (bookworm)</p><p><strong>Container setup</strong>    • Unprivileged LXC created with the official Scrypted Proxmox install script (v0.139.0).    • Docker inside LXC runs scrypted and scrypted-watchtower.</p><p><strong>What I’ve observed</strong>    • CPU and RAM usage climb rapidly after starting Scrypted.    • Disk writes also spike heavily during these periods.    • After some time, the container locks up, and all services inside stop responding.    • Stopping Scrypted brings the host back to normal.    • Restoring the same container to Proxmox 8 = stable, no resource spikes.</p><p><strong>Question</strong>Has anyone else seen similar CPU/RAM spike and crash behaviour when running Scrypted on Proxmox 9 / Debian 13?</p><p>Could this be an issue with the newer kernel or LXC/Docker environment in Proxmox 9, or is there something else I should be looking at?</p><p>Thanks!</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>I&#39;ve had mixed results with Trixie LXCs. My bookworm containers are fine even on 9 though.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njqeg2/scrypted_in_lxc_on_proxmox_9_cpuram_spikes/nesr5jn/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 03:21:10 +0530</pubDate></item></channel></rss>
