<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><atom:link href="http://192.168.1.132/?platform=reddit&amp;subreddit=Proxmox&amp;averagePostsPerDay=3&amp;content&amp;view=rss" rel="self" type="application/rss+xml"/><title>/r/Proxmox</title><description>Hot posts in /r/Proxmox (roughly 3 posts per day)</description><link>https://www.reddit.com/r/Proxmox/</link><language>en-us</language><lastBuildDate>Mon, 08 Sep 2025 10:27:11 +0000</lastBuildDate><generator>Upvote RSS</generator><image><url>http://192.168.1.132//app/cache/images/styles-redditmedia-com-t5_2w0wn-styles-communityIcon_l9fx4v8n3cw71-144x400.png</url><title>/r/Proxmox</title><link>https://www.reddit.com/r/Proxmox/</link><width>144</width><height>144</height></image><item><link>https://www.reddit.com/r/Proxmox/comments/1nbc6o8/moving_from_vmware_to_proxmox_those_of_you_who/</link><title>Moving from VMWare to Proxmox. Those of you who made the switch- what do you know now that you wish you knew then?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nbc6o8/moving_from_vmware_to_proxmox_those_of_you_who/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nbc6o8/moving_from_vmware_to_proxmox_those_of_you_who/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nbc6o8/moving_from_vmware_to_proxmox_those_of_you_who/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hello all - I&#39;ve been running a cluster of VMWare in my homelab/datacenter and my hardware is getting long in the tooth. Like an idiot, I made some snap purchases of new hardware that are not on the VMWare HCL so I&#39;ve decided to make the switch to Proxmox and have built a 2-node cluster that is attached to my existing two TrueNAS iSCSI targets.</p><p>I&#39;m going to start moving workloads from my VMWare cluster to my Proxmox cluster but before I do I want to learn from those who have gone before: what gotchas did you discover? I would hate to migrate my set of workloads off of servers on my VMWare cluster and start tearing things down only to discover Some Thing that forces me to rethink the way I&#39;ve done my deployment or worse, forces me to tear down and rebuild my new cluster because I&#39;ve unknowingly backed myself into a corner.</p><p>I&#39;m intentionally not going the Ceph route yet as my two TrueNas boxes are rock solid and have a lot of life left in them. Eventually I&#39;ll retire them for Ceph storage but I&#39;m very comfortable with iSCSI and don&#39;t want to move away from it just yet. I&#39;ve got enough on my plate and my credit card already cries from the two Proxmox node purchases I&#39;ve already made.</p></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 08 Sep 2025 08:08:00 +0530</pubDate></item><item><link>https://i.redd.it/ltpdsx31sqnf1.png</link><title>I'm always running low on RAM, so I wrote a script to quickly see which VM/LXC is the culprit (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nasrvd/im_always_running_low_on_ram_so_i_wrote_a_script/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nasrvd/im_always_running_low_on_ram_so_i_wrote_a_script/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nasrvd/im_always_running_low_on_ram_so_i_wrote_a_script/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi all, I&#39;m relatively RAM-constrained on my proxmox host, so wanted a way to get a quick visual of where the consumption is going. I worked with AI to generate a script, thought it might be useful to others.</p><p>And you can find the code on my Github page:¬†<a href="https://github.com/micklynch/proxmox-ram-monitor">https://github.com/micklynch/proxmox-ram-monitor</a></p><p>Open to suggestions or feel free to open up a PR.<br/>(ToDo: Currently can&#39;t find the top process from the VMs).</p><p>Also, LMK if there is an existing way to do this. Cheers</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/ltpdsx31sqnf1.png' /></section>]]></description><pubDate>Sun, 07 Sep 2025 18:39:24 +0530</pubDate></item><item><link>https://i.redd.it/751tvans7nnf1.jpeg</link><title>Can‚Äôt create Ubuntu VM the ‚Äúnormal‚Äù way since upgrade to 9.0? (Debian, cloud-init, other VMs fine) (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nag6kw/cant_create_ubuntu_vm_the_normal_way_since/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nag6kw/cant_create_ubuntu_vm_the_normal_way_since/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nag6kw/cant_create_ubuntu_vm_the_normal_way_since/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi-</p><p>Have been creating VMs and LXCs for a while.  Hadn‚Äôt had a reason to create a fresh Ubuntu VM until today.  Creating Debian VMs works fine, tried a TrueNAS scale and a Home Assistant OS one.  No issues.  Can create a ubuntu VM using cloud-init if I want to.  But I don‚Äôt want to.  Using both 22.04 and 24.04 ISO‚Äôs, Ubuntu server install fails either when downloading a security update or installing the kernel.   </p><p>Most often says ‚Äúinternal server error‚Äù and lists the ipv6 address of the host.  However, it‚Äôs done a lot already that implies DNS is resolving and it‚Äôs getting access to archive.ubuntu.org.  If I go to shell from the installer I can ping, curl just fine to all sorts of addresses including archive.ubuntu.org.  But it fails in one of the two places here - either explicitly failed, or just hanging (I‚Äôve included a screenshot of explicit failure, the hang happens after dozens of Get files from us.archive.ubuntu.com on a big Linux firmware file (537mb).  True whether I use q35 or i1440fx, SeaBIOS or UEFI, qemu or not, SCSI or SATA, whether I have ipv6 enabled on the host or not (by setting inet6 on vmbr0 to manual in /etc/network/interfaces), CPU type is x86-64-v2-AES or host, balooning device or not.  I‚Äôve tried a lot of permutations.  Anyone else experiencing this?  Anyone have any bright ideas?</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/751tvans7nnf1.jpeg' /></section>]]></description><pubDate>Sun, 07 Sep 2025 06:40:05 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1na2dub/debian_container_doesnt_boot_after_the_131_update/</link><title>Debian container doesn't boot after the 13.1 update</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1na2dub/debian_container_doesnt_boot_after_the_131_update/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1na2dub/debian_container_doesnt_boot_after_the_131_update/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1na2dub/debian_container_doesnt_boot_after_the_131_update/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Just a head up to warn that my debian lxc container doesn&#39;t boot anymore after the update from 13.0 to 13.1</p><p>Here is the error message :</p><pre><code>run_buffer: 571 Script exited with status 25lxc_init: 845 Failed to run lxc.hook.pre-start for container &quot;100&quot;__lxc_start: 2034 Failed to initialize container &quot;100&quot;</code></pre><p>I couldn&#39;t find a solution with google, just an unrelated old problem with binutils, I restored the CT from a backup, but I think it&#39;s caused by the update of systemd</p><p><strong>Edit</strong> : after more research on a test CT, it seems it&#39;s not the update of systemd inside the CT but the version 13.1 that is not supported by the starting script:</p><pre><code>DEBUG    utils - ../src/lxc/utils.c:run_buffer:560 - Script exec /usr/share/lxc/hooks/lxc-pve-prestart-hook 109 lxc pre-start produced output: unsupported debian version &#39;13.1&#39;</code></pre><p><strong>Edit 2</strong> : yep, it was that after changing the line 39 of the file /usr/share/perl5/PVE/LXC/Setup/Debian.pm</p><p>from</p><pre><code>die &quot;unsupported debian version &#39;$version&#39;\n&quot; if !($version &gt;= 4 &amp;&amp; $version &lt;= 13);</code></pre><p>to</p><pre><code>die &quot;unsupported debian version &#39;$version&#39;\n&quot; if !($version &gt;= 4 &amp;&amp; $version &lt;= 14);</code></pre><p>and the container starts again.</p></div><!-- SC_ON --></section>]]></description><pubDate>Sat, 06 Sep 2025 20:57:24 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1n9v62e/how_much_overhead_does_proxmox_add/</link><title>how much overhead does proxmox add?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1n9v62e/how_much_overhead_does_proxmox_add/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1n9v62e/how_much_overhead_does_proxmox_add/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1n9v62e/how_much_overhead_does_proxmox_add/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Compared to something like HYPER-V on windows (where i need a windows instance as well so thats not a waste), how much performance overhead do i lose on prox mox, and is it better to run things through proxmox or just to use them natively on windows ( all the stuff i want to run is already on windows and any stuff that is not has docker containers and wsl2 can run portainer soo..?)</p></div><!-- SC_ON --></section>]]></description><pubDate>Sat, 06 Sep 2025 14:54:02 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1n9s6m1/ext4_or_zfs_for_pve_installation/</link><title>ext4 or zfs for PVE installation?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1n9s6m1/ext4_or_zfs_for_pve_installation/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1n9s6m1/ext4_or_zfs_for_pve_installation/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1n9s6m1/ext4_or_zfs_for_pve_installation/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I have a single SSD on which I am installing PVE.</p><p>Does it make sense to use ZFS (raid0 with only one disk)?</p><p>Why: I have another computer with OPNsense on it. When I was using ext4, with power outages, I had frequent issues, firewall would not boot. Then decided to change to zfs single drive, and not a single issue since then.</p></div><!-- SC_ON --></section>]]></description><pubDate>Sat, 06 Sep 2025 11:43:37 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1n9czze/is_backing_up_the_pve_configuration_still_as/</link><title>Is backing up the PVE configuration still as messy as a few years ago ?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1n9czze/is_backing_up_the_pve_configuration_still_as/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1n9czze/is_backing_up_the_pve_configuration_still_as/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1n9czze/is_backing_up_the_pve_configuration_still_as/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I&#39;m currently in the process of migrating my main PVE host to a new machine. That new machine will allow me to use ZFS mirroring for the main system drive (something i wasn&#39;t able to do before) and I&#39;m looking into how that process would go, as i have to completely reinstall proxmrestorex to the new 2-drive array. </p><p>The main way i&#39;ve seen is just to backup the whole /etc/pve directory and restore that to the new host once it&#39;s ready, and hope for the best. But comments mentioning this way of doing this are already 2+ years old. </p><p>So, has a better solution been found ? IIRC PBS can do it, but i really would like to avoid getting a separate machine up for the task. </p><p>Thanks for any clue !</p></div><!-- SC_ON --></section>]]></description><pubDate>Fri, 05 Sep 2025 23:56:24 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1n9c5r0/no_license_users_do_you_update_your_pve_instances/</link><title>No license users: Do you update your PVE instances regularely or with some extra measures?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1n9c5r0/no_license_users_do_you_update_your_pve_instances/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1n9c5r0/no_license_users_do_you_update_your_pve_instances/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1n9c5r0/no_license_users_do_you_update_your_pve_instances/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>With no lic, you get non production updates - the same as production ones, but a bit ahead. This can mean nothing and disaster recovery on the other hand, if package breaks something.</p><p>So, how do you do it?</p><p>What about updating cluster nodes?</p></div><!-- SC_ON --></section>]]></description><pubDate>Fri, 05 Sep 2025 23:24:18 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1n910hn/first_homelab_server_best_way_to_set_up_storage/</link><title>First homelab server ‚Äì best way to set up storage on Proxmox?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1n910hn/first_homelab_server_best_way_to_set_up_storage/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1n910hn/first_homelab_server_best_way_to_set_up_storage/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1n910hn/first_homelab_server_best_way_to_set_up_storage/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi all,<br/>I just got a Beelink Me Mini (6x M.2 slots) as my first step into self-hosting, planning to run basic services (pihole + unbound, tailscale, paperless, immich, jellyfin, NAS, etc.) with Proxmox.</p><p>Still scratching my head about storage and could use some advice:</p><ol><li>Proxmox on a small SSD + ZFS RAID pool on 2 bigger SSDs (expandable later using up to 5 slots).</li><li>Small ZFS RAID pool for Proxmox + larger ZFS RAID pool for storage (using all 6 slots).</li><li>Single ZFS RAID pool for both Proxmox + data (expandable to 6 drives).</li></ol><p>Which setup would you recommend for a newbie like me for reliability and future growth?</p><p>Thanks everyone for your time and your help<br/>Cheers!<br/>Paolo</p></div><!-- SC_ON --></section>]]></description><pubDate>Fri, 05 Sep 2025 15:25:16 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1n8ag45/features_lost_when_switching_from_vmware_to_pve/</link><title>Features lost when switching from VMware to PVE</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1n8ag45/features_lost_when_switching_from_vmware_to_pve/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1n8ag45/features_lost_when_switching_from_vmware_to_pve/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1n8ag45/features_lost_when_switching_from_vmware_to_pve/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>It&#39;s finally happened, the higher ups don&#39;t like the quotes from VMware and are looking to switch. We currently have a few PVE clusters at smaller sites, but now we&#39;re in talks to switch over the large clusters in the primary datacenters.</p><p>I&#39;ve been asked to put together a presentation for the CTO to list out what would be lost feature wise if we did make the switch. I figured I would ask here if anyone has any personal experience doing this in case there&#39;s something I&#39;m overlooking.</p><p>So far the biggest thing I can think of that doesn&#39;t exist in PVE is DRS, but all things considered I think we can live without it.</p></div><!-- SC_ON --></section>]]></description><pubDate>Thu, 04 Sep 2025 19:11:30 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1n83kjd/pve_cluster_with_2_nodes/</link><title>PVE Cluster with 2 nodes</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1n83kjd/pve_cluster_with_2_nodes/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1n83kjd/pve_cluster_with_2_nodes/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1n83kjd/pve_cluster_with_2_nodes/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hello,</p><p>I wanted to make a second Proxmox for my homelab and I&#39;ve recently learned that you only get a failover and high avaibility with at least 3 PVE nodes.</p><p>Is there any point to have a PVE cluster with only 2 nodes?</p></div><!-- SC_ON --></section>]]></description><pubDate>Thu, 04 Sep 2025 12:53:05 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1n7rwv5/omg_i_discovered_proxmox_helperscripts_what_else/</link><title>OMG I discovered Proxmox Helper-Scripts - what else am I missing?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1n7rwv5/omg_i_discovered_proxmox_helperscripts_what_else/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1n7rwv5/omg_i_discovered_proxmox_helperscripts_what_else/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1n7rwv5/omg_i_discovered_proxmox_helperscripts_what_else/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi!</p><p>Today, after using Proxmox VE for 2 years-ish, I ran into<a href="https://community-scripts.github.io/ProxmoxVE/"> this amazing sit</a><a href="https://community-scripts.github.io/ProxmoxVE/">e</a>. Am just a casual homelaber so this wil prove to be quite useful.</p><p>As someone who has a bit of a &quot;new car smell&quot; on Proxmox VE, what other resources/sites would you recommend I check out?</p><p>Thanks!!&quot;</p></div><!-- SC_ON --></section>]]></description><pubDate>Thu, 04 Sep 2025 03:12:36 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1n7iflm/updated_guide_migrating_from_vmware_to_proxmox_is/</link><title>Updated guide: Migrating from VMware to Proxmox is now a 3-step process [Guide]</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1n7iflm/updated_guide_migrating_from_vmware_to_proxmox_is/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1n7iflm/updated_guide_migrating_from_vmware_to_proxmox_is/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1n7iflm/updated_guide_migrating_from_vmware_to_proxmox_is/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Over the last year, Proxmox has turned VMware migration from a complicated manual process into something incredibly simple.</p><p>With Proxmox VE 9, the official import wizard makes the transition as easy as <strong>3 steps</strong>:</p><ul><li>add ESXi as a repository</li><li>fill out the import wizard</li><li>start the VM</li></ul><p>To show how much has improved, I‚Äôve kept the old <em>manual method</em> in my article. it‚Äôs obsolete now, but it‚Äôs a reminder of how many steps were needed before.</p><p>I also added a new section on fine-tuning Windows VMs after import. Would love feedback if you think those steps could be improved or simplified further.</p><p>üëâ Full walkthrough here: <a href="https://edywerder.ch/vmware-to-proxmox/">https://edywerder.ch/vmware-to-proxmox/</a></p></div><!-- SC_ON --></section>]]></description><pubDate>Wed, 03 Sep 2025 21:18:19 +0530</pubDate></item><item><link>https://www.reddit.com/gallery/1n75dkp</link><title>My cluster is finally online (Gallery)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1n75ofo/my_cluster_is_finally_online/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1n75ofo/my_cluster_is_finally_online/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1n75ofo/my_cluster_is_finally_online/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hadn&#39;t messed around with labing in awhile and finally made the time to get this set up. It took quite a bit of effort to figure everything out but I don&#39;t think I&#39;ll be needing much more than this any time soon. </p><p>Here is a rundown on the setup:</p><p>Firewall: Sonicwall NSA6600, Wan link 10GbE with /29</p><p>Cluster switch: Dell N4032F, 2x 10GbE LAG to each node and to the firewall, 2x 40GbE LAG to backup server</p><p>Node 1: R640 2x Xeon Gold 6240, 384Gb ram, 240Gb boot ssd pair in raid 1, 2x 1.9Tb Samsung PM1643 (CEPH)</p><p>C6220-1 Nodes 2-5: 2x e5-2670 512Gb ram, 512Gb raid 1 boot ssd pair, 2x 960Gb Samsung PM11633a (CEPH)</p><p>C6220-2 Nodes 6-7: 2x e5-2670 512Gb ram, 512Gb raid 1 boot ssd pair, 2x 480Gb Toshiba px05svb048y (CEPH) </p><p>C6220-2 Nodes 6-7: 2x e5-2670 512Gb ram, 512Gb raid 1 boot ssd pair, 2x 480Gb Toshiba px05svb048y (CEPH)</p><p>C6220-2 Nodes 8-9: 2x e5-2670 256Gb ram, 512Gb raid 1 boot ssd pair, 2x 480Gb Toshiba px05svb048y (CEPH) </p><p>Backup/Staging server: R720 (with SC220 and MD1220 DAS) 2x e5-2699v2 384Gb ram, 1Tb raid 1 boot ssd pair, archive drive 6x 6Tb 7200rpm drives in raid 6, backup drive 24x 1.2Tb 10000rpm drives in a zfs raidz2 pool, iso/staging drive 24x 1.2Tb 10000rpm drives in a zfs raidz2 pool. </p><p>To be added (future): 2nd R640, same cpu and ram, needs drives. </p><p>Is it excessive? Probably. But it was fun getting it set up and I don&#39;t have to worry about running out of resources. </p></div><!-- SC_ON --></section><section class='embedded-media'><p><img src="https://preview.redd.it/om248u6envmf1.png?width=1591&amp;format=png&amp;auto=webp&amp;s=c545addb03124a18e84613d85fc52ae829f9236d" height="495" width="1591" /></p><p><img src="https://preview.redd.it/3stoto8envmf1.jpg?width=1310&amp;format=pjpg&amp;auto=webp&amp;s=4a9727f5178742b28e3fe5ae0b7b082281f668de" height="1834" width="1310" /></p></section>]]></description><pubDate>Wed, 03 Sep 2025 10:14:49 +0530</pubDate></item></channel></rss>
