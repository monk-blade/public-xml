<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><atom:link href="http://192.168.1.132/?platform=reddit&amp;subreddit=Proxmox&amp;averagePostsPerDay=9&amp;content=0&amp;comments=7&amp;filterPinnedComments&amp;filterOldPosts=3&amp;view=rss" rel="self" type="application/rss+xml"/><title>/r/Proxmox</title><description>Hot posts in /r/Proxmox (roughly 9 posts per day)</description><link>https://www.reddit.com/r/Proxmox/</link><language>en-us</language><lastBuildDate>Sat, 20 Sep 2025 21:15:33 +0000</lastBuildDate><generator>Upvote RSS</generator><image><url>http://192.168.1.132//app/cache/images/styles-redditmedia-com-t5_2w0wn-styles-communityIcon_l9fx4v8n3cw71-144x400.png</url><title>/r/Proxmox</title><link>https://www.reddit.com/r/Proxmox/</link><width>144</width><height>144</height></image><item><link>https://www.reddit.com/r/Proxmox/comments/1nm3zs0/no_network_after_fresh_9_install/</link><title>No Network after Fresh 9 install…</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nm3zs0/no_network_after_fresh_9_install/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nm3zs0/no_network_after_fresh_9_install/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nm3zs0/no_network_after_fresh_9_install/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I recently decided to go ahead and pull the trigger and upgrade to 9 from 8.x.  I used the Upgrade assist script 8to9. All went well but then it was time to go through the basics, nag removal, etc. Decided to use the community script all that seemed to go as planned as well. However, on reboot, no network accessibility any longer proxy server locally itself gave me the desired IP address to access the GUI however, the GUI was not accessible. Tried to ping 1.1.1.1 locally no response which lead me to check out ‘ip a’ and ALL network interfaces were down. So, I went to check out the /etc/network/interfaces and from what I can see, all seemed fine here… just interfaces down. </p><p>So upon reading for help, I did hear that some people did have some issues with helper scripts that cause PVE to break. So upon knowing that, I decided to do a fresh PVE9 install on the hardware and this time not use the community helper script and go through the old way of manually adding and subtracting the proper repositories (waited on the nag removal to not muddy the results) then did a complete apt update ; apt full-upgrade-y then rebooted .:..:.. ONCE AGAIN, GUI is unreachable but did give me IP address locally again not able to ping 1.1.1.1 and all interface’s are down again.  I’ve been self hosting Proxmox for a while and learning a great deal with this awesome hypervisor.  However, this is the first time on install that I’ve had this issue.</p><p>Now, I know that some brain out there is gonna tell me, “Well just bring the interfaces up!”, but with Proxmox, on reboot, after fresh install and upgrading,… that should not have to be done.</p><p>Am I missing something with Trixie versus Bookworm on install?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>So I installed a new HBA last night on my proxmox host. Upon booting back up... No network. I&#39;m running 8.4.12 and have had this issue before.</p><p>The name of the interface changed from ens4 to ens5.</p><p>Changing the interface name in the networking config file and a reboot done the job. But this is the second time that this has happened when swapping out hardware.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nm3zs0/no_network_after_fresh_9_install/nfa50yf/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Check the interface names in config and actual interface names</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nm3zs0/no_network_after_fresh_9_install/nfa56sk/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Reading this... All I can think, is thank god this didn&#39;t happen to me..... </p><p>What kind of Network card(s) do you have? Something old or unusual?</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nm3zs0/no_network_after_fresh_9_install/nfa3cfj/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 20 Sep 2025 22:50:42 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nm2w3q/proxmox_on_2012_mac_pro/</link><title>ProxMox on 2012 Mac Pro</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nm2w3q/proxmox_on_2012_mac_pro/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nm2w3q/proxmox_on_2012_mac_pro/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nm2w3q/proxmox_on_2012_mac_pro/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Is this possible? I had bought a used 2012 MacPro to use as a VMWare ESXi bare metal hypervisor, but since BroadCom acquired VMWare it’s not a viable option anymore.So if this can be done,would really love to hear from anyone who has done it successfully and what if any “gotchas” there are…</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I&#39;ve run it on an old HP DeskPro from about that time, so if you&#39;re wondering about specs you&#39;re fine. (You can still run proxmox 7 on a raspberry pi! And as far as I know the only reason you can&#39;t run 8 on one is because nobody has completed it for ARM)</p><p>If you&#39;re wondering about driver compatibility, in 2012 Apple used Intel chips so compatibility is probably better than a more recent Mac.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nm2w3q/proxmox_on_2012_mac_pro/nf9y4qu/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I&#39;ve been using Proxmox on a Mac Mini 2014 for a long time, no problems, I even upgraded from PVE 7 to 8. I think it currently has about a year of uptime.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nm2w3q/proxmox_on_2012_mac_pro/nf9thke/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I had an iMac running Proxmox. I couldn’t get the GPU to work but it ran VMs fine.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nm2w3q/proxmox_on_2012_mac_pro/nf9wgmp/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>that thing is hot e-waste my guy</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nm2w3q/proxmox_on_2012_mac_pro/nf9z5v6/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I have a 2012 Mac mini I was thinking about throwing proxmox on just for giggles or to play with clustering. I think it has 16GB ram and an i7 from that era. But I remember the thing would overheat easily and throttle and frankly the new hard drive it needs might be more expensive than grabbing a raspberry pi that would be faster than it.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nm2w3q/proxmox_on_2012_mac_pro/nfa208m/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 20 Sep 2025 22:07:07 +0530</pubDate></item><item><link>https://i.redd.it/9d2c22bn7cqf1.png</link><title>Need some help with CEPH. I dont know what exactly happened. (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nm13ce/need_some_help_with_ceph_i_dont_know_what_exactly/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nm13ce/need_some_help_with_ceph_i_dont_know_what_exactly/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nm13ce/need_some_help_with_ceph_i_dont_know_what_exactly/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Well i dont know what happened with my monitors exactly (layer 8 is most likely).</p><p>PBS is currently just there for overall quorum while i reoder some parts for the real node 3. </p><p>I tried to Destroy the configs but i get different errors and strange behavior when readding. Such as 500 Timeouts or just nothing happens. </p><p>If there is any solution without formatting the PBS hosts i would be thankfull.</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/9d2c22bn7cqf1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Please provide:</p><p><code>pveceph mon list;pveceph status;ceph health detail;cat /etc/pve/ceph.conf;ls -l /var/lib/ceph/mon/</code></p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nm13ce/need_some_help_with_ceph_i_dont_know_what_exactly/nf9g2l6/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>What it looks like to me is the pm-02 node mon service isn&#39;t working correctly. I would guess the service isn&#39;t running, DNS is messed up, or there is some other kind of network connection issue like a firewall.</p><p>There should be logs on the pm-02 host. From one of the other hosts, try to connect to the port directly (curl -vvv 10.30.0.2:6789) and see if the port is even open.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nm13ce/need_some_help_with_ceph_i_dont_know_what_exactly/nf9figb/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 20 Sep 2025 20:56:29 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nlxesm/pbs_running_as_lxc_proxmox_update/</link><title>PBS running as LXC, Proxmox update</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nlxesm/pbs_running_as_lxc_proxmox_update/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nlxesm/pbs_running_as_lxc_proxmox_update/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nlxesm/pbs_running_as_lxc_proxmox_update/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi,</p><p>I plan to upgrade Proxmox from 8 to 9. On that host PBS is running as LXC container.</p><p>Correct order to first upgrade Proxmox and then after the LXC container?</p><p>Update: PBS 4 runs on Debian 13 (Trixie), Proxmox 8 on Debian 12 - so the PBS update makes the LXC unbootable, as it relies on the kernel version of the host (as all LXC).<br/>ZFS volume snapshot made the rollback a breeze.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Yes, that&#39;s fine.  Actually either should work fine.  It&#39;s people who have it installed directly on the host who have a bear of an issue that&#39;s carefully documented (at least in the 8 to 9 path documentation).  </p><p>2 questions: (1) you&#39;re upgrading from 7 to 8, not 8 to 9?  Impressive.  (2) Why not run PBS in a VM vs. an LXC?  I mean I get resources, but PBS is distributed as an ISO.  I&#39;ve heard of people running it &quot;bare metal&quot; installing on the host itself but not yet running it an an LXC.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nlxesm/pbs_running_as_lxc_proxmox_update/nf8n0gn/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>In which version are you running PBS in your LXC? is there a way to install version 4 at this moment?</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nlxesm/pbs_running_as_lxc_proxmox_update/nf9m1v2/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 20 Sep 2025 18:23:06 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nlsctu/proxmox_migration_hp_elitedesk_800_g3_sff/</link><title>Proxmox migration - HP Elitedesk 800 G3 SFF</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nlsctu/proxmox_migration_hp_elitedesk_800_g3_sff/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nlsctu/proxmox_migration_hp_elitedesk_800_g3_sff/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nlsctu/proxmox_migration_hp_elitedesk_800_g3_sff/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Looking for some migration/setup advice for Proxmox (9) on my current server. The server is a HP Elitedesk 800 G3 SFF:</p><ul><li>i5-7500</li><li>32GB RAM</li><li>2 x 8TB shucked HDDs (currently RAID1 mirrors with mdadm - history below)</li><li>500gb NVME SSD</li><li>M.2 Coral in the wifi M.2 slot</li><li>potential to add a 2.5&quot; SATA SSD (I think)</li></ul><p>This server was running Ubuntu Mate, but the old NVME recently died. No data lost as the HDDs are still fine (and all important data backed up elsewhere), but some services, including some Docker/Portainer setups, were lost).</p><p>I have installed Proxmox 9 on the new NVME drive, set up mdadm on Proxmox (for access to the existing RAID1 drives) and set up two Ubuntu Server VMs (on the NVME drive). One VM (less resources) is set up as a NAS/fileserver (RAID0 md0 passed through to this VM with virtio), and samba set up to share files to network and other VMs and LXCs). The other VM is set up for &quot;services&quot; (more resources), with Proxmox installed. Key data for the services (Docker/Portainer volumes are stores on the RAID1 drives - accessed via samba). I&#39;ve been playing with LXCs for Jellyfin and Plex using community scripts (Jellyfin was previously on docker, Plex previously direct installed) to avoid iGPU passthrough issues with VMs.</p><p>Some of my services I got back up quickly (some Portainer docker-compose files were still safely on the RAID1 drives), others I&#39;m rebuilding (and may have success pulling from failed SSD - who knows).</p><p>I realise mdadm is a second-class citizen on Proxmox, but I needed things back up again fast. And it works (for now), but I&#39;d like to migrate to a better setup for Proxmox.</p><p>My storage drives are getting pretty full (95%+), so I&#39;ll probably need to replace them with something a bit bigger to have some overhead for ZFS (and more data :D).  I&#39;ve heard of people using a 2.5&quot; SATA for Proxmox, and twin NVME drives for a ZFS mirror for VMs, but I want to keep my second NVME slot for my Coral (for Frigate NVR processing) - and not sure if it supports working with a drive anyway.</p><p>So there&#39;s all the background... and tips/tricks suggestions for setting this up better for Proxmox (and migrating to ZFS for the drives)?</p></div><!-- SC_ON --></section>]]></description><pubDate>Sat, 20 Sep 2025 13:32:46 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nloiqw/backups_from_pve_nodes_to_pbs_server/</link><title>Backups from PVE Nodes to PBS Server</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nloiqw/backups_from_pve_nodes_to_pbs_server/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nloiqw/backups_from_pve_nodes_to_pbs_server/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nloiqw/backups_from_pve_nodes_to_pbs_server/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Nodes:<br/>Working on setting up our production ennvironment with Proxmox and PBS. I have a question. So on our nodes, we have 4 25gb connections and 2 1gb connections. The 2 1gb connections are used for management purposes in an active-backup bond. Network <a href="http://10.0.0.0/24">10.0.0.0/24</a> in this case and the switchports are setup as untagged vlan 200. 2 of the 25gb connections go to storage fabric. The other 2 25gb are used for vm/lxc uplinks with multiple networks and vlans on a bond with vlans.  </p><p>PBS: On the PBS which is running on baremetal, I have a similar config of a 1gb interface used for mangement purposes and then a 10gb interface I want to use for backups.  </p><p>What I would like to do is have backups run across the 25gb links on the nodes to the backup servers 10gb link. I understand I can add an ip on the PBS 10gb interface and then add that ip on the nodes as Storage&gt;PBS. However the backups would still actually run across the nodes 1gb management interface. This is where I&#39;m not sure how to basically tell the nodes to use the 25gb link to send backups to the pbs server. PBS server is in a separate physical location. I would share the 2 25gb vm uplinks to send backup traffic. In my network I have networks specifically for management, production, dmz, etc.  </p><p>I tried to add a second ip on the PBS servers 10GB interface on a different network however, I ran into only 1 gateway can exist which is currently the management interface. I would like for the traffic to be routable instead of point to point as I plan to replicate data from another campus.  </p><p>Would I be better off to simply move the management interfaces to the 25gb links or is there another way?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><blockquote><p>I tried to add a second ip on the PBS servers 10GB interface on a different network </p></blockquote><p>This is the correct move. The answer to &quot;I&#39;m not sure how to basically tell the nodes to use the 25gb link&quot; is to assign a different network to that interface.</p><p>I&#39;m not sure why that&#39;s being so difficult for you, but that&#39;s what it is. You shouldn&#39;t be specifying any gateway either, since there isn&#39;t one.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nloiqw/backups_from_pve_nodes_to_pbs_server/nf7nthn/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 20 Sep 2025 09:45:02 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nlka9x/expanding_directory_size_on_zfs/</link><title>Expanding Directory Size on ZFS</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nlka9x/expanding_directory_size_on_zfs/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nlka9x/expanding_directory_size_on_zfs/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nlka9x/expanding_directory_size_on_zfs/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I have two 4TB nvmes in a ZFS mirror.  Currently the full capacity of the ZFS is not being utilized. I was uploading images to a directory named &quot;immich&quot; and it errored out by running out of space.  Looking at the directory it is 82GB in size.  How do I expand the directory size to accomodate the amount of images I want to upload.</p><p>I have found information on adding to more storage to the ZFS but have not seen anything on how to increase the directory size.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I think it&#39;s a partition or volume size that has to increased - directories don&#39;t generally have size limits unless there&#39;s a quote configured.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nlka9x/expanding_directory_size_on_zfs/nf677gx/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p><a href="https://preview.redd.it/edt0h3yh4cqf1.png?width=2818&amp;format=png&amp;auto=webp&amp;s=a47cbc2d227b6e779ae27713b161649d3cdc7bbf">https://preview.redd.it/edt0h3yh4cqf1.png?width=2818&amp;format=png&amp;auto=webp&amp;s=a47cbc2d227b6e779ae27713b161649d3cdc7bbf</a></p><p>This is the directory I want to increase in size.  Currently 82GB in size</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nlka9x/expanding_directory_size_on_zfs/nf9a5pw/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 20 Sep 2025 06:06:59 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/</link><title>SnapShots - OPNSense Firewall</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>ProxMox Friends, </p><p>Question?</p><p>When making a snapshot of my OPNSense firewall. After I have applied all my updates, configs, settings, etc.. Are there any right/wrongs when I create the snapshot with the Firewall running? I have tested shutting the firewall wall down and performing a quick snap shot restore. Everything is back up and running w/o any repercussions.</p><p>-or- </p><p>Is it best to create the snapshot with the firewall shut down? So when I need to restore the snapshot have to go through the whole process of startup.</p><p>Ideas?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I do them with it running, zero issues. Works amazingly well</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/nf61oh5/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Can snapshots actuall start &quot;live&quot; though?</p><p>I&#39;m not sure it makes but different but having hung out a bit in <a href="https://www.reddit.com/r/sysadmin">r/sysadmin</a>, snapshots are good for when you&#39;re making changes and might need to revert back but aren&#39;t to be relied on as long term backup.</p><p>So you can make a protected backup or you can clone the VM to a template but either way don&#39;t rely on snapshots for backups.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/nf60c98/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>My opnsense VM gets a weekly online snapshot, no consequences.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/nf6btvh/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Install qemu-guest-agent and do snapshots live.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/nf6daq0/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I do the snapshots with it running but exclude the RAM.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/nf6jrsc/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The key to backups is to make sure you test a restore.  Given the nature of a virtualised environment it&#39;s dead easy to test a snapshot roleback safley.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nljmhg/snapshots_opnsense_firewall/nf67jaz/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 20 Sep 2025 05:35:50 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nlhoh4/cant_connect_to_cluster_host/</link><title>Can't Connect to Cluster Host</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nlhoh4/cant_connect_to_cluster_host/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nlhoh4/cant_connect_to_cluster_host/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nlhoh4/cant_connect_to_cluster_host/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Please be gentle, I&#39;m still learning.  I&#39;ve set up a homelab mainly to run Home Assistant (HA) and an Arr stack with Jellyfin with Tailscale for remote access.  I&#39;ve been playing around learning stuff and a few times, broken something that I&#39;ve had to fix.  I sort of stumbled through things. I&#39;ve decided I want to stand up a second Proxmox box to do my playing in so I don&#39;t break/interrupt my Home Assistant instance.</p><p>So I setup a new Proxmox box and went about setting up a cluster. I set up the cluster on my main box on 192.168.1.2. When I went to join the new box to the cluster it couldn&#39;t connect: TASK ERROR: 500 Can&#39;t connect to <a href="http://192.168.1.2:8006">192.168.1.2:8006</a> (Connection timed out)</p><p>I&#39;m trying to workout if it is a proxmox problem or a tailscale problem, or maybe both?</p><ul><li>From my main box node I can ping the new one on <a href="http://192.168.1.8">192.168.1.8</a> </li><li>From my new box node I can&#39;t ping the main one on <a href="http://192.168.1.2">192.168.1.2</a> </li><li>I can however ping the main box node using the tailscale host name.</li><li>From the new box node I can ping the HA lxc on the main box. on <a href="http://192.168.1.3">192.168.1.3</a></li></ul><p>So it is only the main box node I can&#39;t connect to. I do have my HA lcx running as an exit node and sub-net router, if that is relevant?</p><p>I&#39;m thinking I may have done something on the main box node when I was playing around with OpenWRT and OpenVPN. I have removed the lxc with these, but may have done something on the main node. I can&#39;t remember. :(</p><p>What troubleshooting steps should I be looking at to work through this?</p></div><!-- SC_ON --></section>]]></description><pubDate>Sat, 20 Sep 2025 04:10:02 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nldn68/igpu_passthrough_crashes_host/</link><title>iGPU Passthrough Crashes Host</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nldn68/igpu_passthrough_crashes_host/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nldn68/igpu_passthrough_crashes_host/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nldn68/igpu_passthrough_crashes_host/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi all, I have an AMD 7840HS mini PC I&#39;m trying to use for a Windows VM on the node. I&#39;ve blacklisted (I think), the VGA/iGPU from the host, when I boot I get to &quot;Loading initial ramdisk...&quot; and then the display stops updating but the host node appears to boot normally and comes up.</p><p>I&#39;ve mapped (in datacenter mappings) the PCI device using the device ID I found in <code>lspci</code>, it also includes sub devices in it&#39;s own group and other numbered groups that include the Radeon HD audio and the like (HDMI audio, etc.), but nothing outside of that PCI-E host, in this case group 19.</p><p>I then added it as a PCI device, flagged as PCI-E and Primary GPU in the Proxmox UI.</p><p>When I boot the VM, the host node almost immediately reoboots, and I don&#39;t know why. It doesn&#39;t even go to the bootloader screen on console, let alone to the windows installer. If I remove the device, it all functions normally.</p><p>AMD SEV is enabled, Resizable BAR is disabled.</p><p>All configured files, proxmox UI configs, and report checks via cmdline in posted to this link <a href="https://imgur.com/a/I5qPXMT">https://imgur.com/a/I5qPXMT</a></p><p>I&#39;m really hoping someone can help me figure out why it&#39;s crashing the host, and not working. I&#39;m new to proxmox and don&#39;t know where to look for more information / logs either, so any advice there would be great!</p><p><strong>Edit:</strong> I&#39;ve added this to my GRUB cmdline, &quot;pcie_acs_ovverride=downstream,multifunction&quot;. It doesn&#39;t stop the crash. However if I directly send just the VGA portion of the device, and then the audio portions separately too, the VM does boot. There&#39;s an image in the imgur set showing it in the Device Manager. It seems to correctly register the type of adapter, Radeon 780M from the 7840HS CPU. And the audio devices show up too, but none of them work. I manually installed the Radeon software but it fails to load correctly, error also pictured in the imgur link.</p><p>I&#39;m running out of ideas here :-\</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><blockquote><p>When I boot the VM, the host node almost immediately reoboots, and I don&#39;t know why. It doesn&#39;t even go to the bootloader screen on console, let alone to the windows installer. </p></blockquote><p>How do you know that it reboots? And why should it go to a windows installer?</p><p>If you start the VM, the host nodes graphic is gone... you won&#39;t see anything anymore, it&#39;s dead on a local console. Are there any other hints that it&#39;s rebooting? </p><p>You may still reach your host node via ssh or http. Or your VM via RDP, if windows is already setup and running. If that part works, you may try to get your monitor working for your VM via iGPU pass through.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nldn68/igpu_passthrough_crashes_host/nf52tb4/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 20 Sep 2025 01:22:59 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/</link><title>Could Proxmox ever become paid-only?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>We all know what happened to VMware when Broadcom bought them. Could something like that ever happen to Proxmox? Like a company buys them out and changes the licensing around so that there’s no longer a free version?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>It could be forked even if they switched everything off Debian and made it paid only. It’s open source so someone could pick up and keep going if they so desired</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/nf4kxwu/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>It’s aGPL license so if they make it cost money, then people will just fork the repo and continue development on that which will be totally legal.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/nf4l7u1/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Yes, they could go closed source tomorrow, BUT the current version remains free in free-to-use and open source. Changing a license is always possible for them (due to the CLA) and has been done in the past (see redis, elasticsearch, ...)</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/nf4kxvy/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Fork it, the code for the tools are open:</p><p><a href="https://git.proxmox.com/">https://git.proxmox.com/</a></p><p>Someone would have to package it up and release a new free version called froxmox for example. No difference to what we saw with redis and MySQL.</p><p>Proxmox does good work and I feel they have earned the respect for building what is practically a turnkey solution to open source virtualization, support them where you can and there should be no reason for the hypothetical to become reality.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/nf4l60b/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I assume if they did it would get forked like terraform and TOFU</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/nf4l2by/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>They could.   And I&#39;m guessing there would be a fork called OpenProxmox 23.4 seconds after the announcement.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/nf583uu/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>That would require them to move away from Debian, which would be a monumental amount of work</p><p>So maybe, but I doubt it</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/nf4kc6i/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 20 Sep 2025 01:03:06 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nlcv36/highspeed_lowdowntime_esxi_to_proxmox_migration/</link><title>High-Speed, Low-Downtime ESXi to Proxmox Migration via NFS</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nlcv36/highspeed_lowdowntime_esxi_to_proxmox_migration/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nlcv36/highspeed_lowdowntime_esxi_to_proxmox_migration/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 6 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nlcv36/highspeed_lowdowntime_esxi_to_proxmox_migration/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><h1>[GUIDE] High-Speed, Low-Downtime ESXi to Proxmox Migration via NFS</h1><p>Hello everyone,</p><p>I wanted to share a migration method I&#39;ve been using to move VMs from ESXi to Proxmox. This process avoids the common performance bottlenecks of the built-in importer and the storage/downtime requirements of backup-and-restore methods.</p><p>The core idea is to <strong>reverse the direction of the data transfer</strong>. Instead of having Proxmox <em>pull</em> data from a speed-limited ESXi host, we have the ESXi host <em>push</em> the data at full speed to a share on Proxmox.</p><h1>The Problem with Common Methods</h1><ul><li><strong>Veeam (Backup/Restore):</strong> Requires significant downtime (from backup start to restore end) and triple the storage space (ESXi + Backup Repo + Proxmox), which can be an issue for large VMs.</li><li><strong>Proxmox Built-in Migration (Live/Cold):</strong> Often slow because Broadcom/VMware seems to cap the speed of API calls and external connections used for the transfer. Live migrations can sometimes result in boot issues.</li><li><strong>Direct SSH</strong> <code>scp</code>**/<code>rsync</code>:** While faster than the built-in tools, this can also be affected by ESXi&#39;s connection throttling.</li></ul><h1>The NFS Push Method: Advantages</h1><ul><li><strong>Maximum Speed:</strong> The transfer happens using ESXi&#39;s native Storage vMotion, which is not throttled and will typically saturate your network link.</li><li><strong>Minimal Downtime:</strong> The disk migration is done <em>live</em> while the VM is running. The only downtime is the few minutes it takes to shut down the VM on ESXi and boot it on Proxmox.</li><li><strong>Space Efficient:</strong> No third copy of the data is needed. The disk is simply moved from one datastore to another.</li></ul><h1>Prerequisites</h1><ul><li>A Proxmox host and an ESXi host with network connectivity.</li><li>Root SSH access to your Proxmox host.</li><li>Administrator access to your vCenter or ESXi host.</li></ul><h1>Step-by-Step Migration Guide</h1><h1>Optional: Create a Dedicated Directory on LVM</h1><p>If you don&#39;t have an existing directory with enough free space, you can create a new Logical Volume (LV) specifically for this migration. This assumes you have free space in your LVM Volume Group (which is typically named <code>pve</code>).</p><ol><li>SSH into your Proxmox host.</li><li>Create a new Logical Volume. Replace <code>&lt;SIZE_IN_GB&gt;</code> with the size you need and <code>&lt;VG_NAME&gt;</code> with your Volume Group name.lvcreate -n esx-migration-lv -L &lt;SIZE\_IN\_GB&gt;G &lt;VG\_NAME&gt;</li><li>Format the new volume with the ext4 filesystem.mkfs.ext4 -E nodiscard /dev/&lt;VG\_NAME&gt;/esx-migration-lv</li><li>Add the new filesystem to <code>/etc/fstab</code> to ensure it mounts automatically on boot.echo &#39;/dev/&lt;VG\_NAME&gt;/esx-migration-lv /mnt/esx-migration ext4 defaults 0 0&#39; &gt;&gt; /etc/fstab</li><li>Reload the systemd manager to read the new fstab configuration.systemctl daemon-reload</li><li>Create the mount point directory, then mount all filesystems.mkdir -p /mnt/esx-migration mount -a</li><li>Your dedicated directory is now ready. Proceed to Step 1.</li></ol><h1>Step 1: Prepare Storage on Proxmox</h1><p>First, we need a &quot;Directory&quot; type storage in Proxmox that will receive the VM disk images.</p><ol><li>In the Proxmox UI, go to <strong>Datacenter -&gt; Storage -&gt; Add -&gt; Directory</strong>.</li><li><strong>ID:</strong> Give it a memorable name (e.g., <code>nfs-migration-storage</code>).</li><li><strong>Directory:</strong> Enter the path where the NFS share will live (e.g., <code>/mnt/esx-migration</code>).</li><li><strong>Content:</strong> Select <strong>&#39;Disk image&#39;</strong>.</li><li>Click <strong>Add</strong>.</li></ol><h1>Step 2: Set Up an NFS Share on Proxmox</h1><p>Now, we&#39;ll share the directory you just created via NFS so that ESXi can see it.</p><ol><li>SSH into your Proxmox host.</li><li>Install the NFS server package:apt update &amp;&amp; apt install nfs-kernel-server -y</li><li>Create the directory if it doesn&#39;t exist (if you didn&#39;t do the optional LVM step):mkdir -p /mnt/esx-migration</li><li>Edit the NFS exports file to add the share:nano /etc/exports</li><li>Add the following line to the file, replacing <code>&lt;ESXI_HOST_IP&gt;</code> with the actual IP address of your ESXi host./mnt/esx-migration &lt;ESXI\_HOST\_IP&gt;(rw,sync,no_subtree_check)</li><li>Save the file (CTRL+O, Enter, CTRL+X).</li><li>Activate the new share and restart the NFS service:exportfs -a systemctl restart nfs-kernel-server</li></ol><h1>Step 3: Mount the NFS Share as a Datastore in ESXi</h1><ol><li>Log in to your vCenter/ESXi host.</li><li>Navigate to <strong>Storage</strong>, and initiate the process to add a <strong>New Datastore</strong>.</li><li>Select <strong>NFS</strong> as the type.</li><li>Choose <strong>NFS version 3</strong> (it&#39;s generally more compatible and less troublesome).</li><li><strong>Name:</strong> Give the datastore a name (e.g., <code>Proxmox_Migration_Share</code>).</li><li><strong>Folder:</strong> Enter the path you shared from Proxmox (e.g., <code>/mnt/esx-migration</code>).</li><li><strong>Server:</strong> Enter the IP address of your Proxmox host.</li><li>Complete the wizard to mount the datastore.</li></ol><h1>Step 4: Live Migrate the VM&#39;s Disk to the NFS Share</h1><p>This step moves the disk files while the source VM is still running.</p><ol><li>In vCenter, find the VM you want to migrate.</li><li>Right-click the VM and select <strong>Migrate</strong>.</li><li>Choose <strong>&quot;Change storage only&quot;</strong>.</li><li>Select the <code>Proxmox_Migration_Share</code> datastore as the destination for the VM&#39;s hard disks.</li><li>Let the Storage vMotion task complete. This is the main data transfer step and will be much faster than other methods.</li></ol><h1>Step 5: Create the VM in Proxmox and Attach the Disk</h1><p>This is the final cutover, where the downtime begins.</p><ol><li>Once the storage migration is complete, <strong>gracefully shut down the guest OS</strong> on the source VM in ESXi.</li><li>In the Proxmox UI, create a new VM. Give it the same general specs (CPU, RAM, etc.). <strong>Do not create a hard disk for it yet.</strong> Note the new VM ID (e.g., <code>104</code>).</li><li>SSH back into your Proxmox host. The migrated files will be in a subfolder named after the VM. Let&#39;s find and move the main disk file.# Navigate to the directory where the VM files landed cd /mnt/esx-migration/VM_NAME/  # Proxmox expects disk images in /&lt;path\_to\_storage&gt;/images/&lt;VM\_ID&gt;/ # Move and rename the -flat.vmdk file (the raw data) to the correct location and name # Replace &lt;VM\_ID&gt; with your new Proxmox VM&#39;s ID (e.g., 104) mv VM_NAME-flat.vmdk /mnt/esx-migration/images/&lt;VM\_ID&gt;/vm-&lt;VM\_ID&gt;-disk-0.raw <strong>Note:</strong> The <code>-flat.vmdk</code> file contains the raw disk data. The small descriptor <code>.vmdk</code> file and other <code>.vmem</code>, <code>.vmsn</code> files are not needed.</li><li>Attach the disk to the Proxmox VM using the <code>qm set</code> command.# qm set &lt;VM\_ID&gt; --&lt;BUS\_TYPE&gt;0 &lt;STORAGE\_ID&gt;:&lt;VM\_ID&gt;/vm-&lt;VM\_ID&gt;-disk-0.raw  # Example for VM 104: qm set 104 --scsi0 nfs-migration-storage:104/vm-104-disk-0.raw <strong>Driver Tip:</strong> If you are migrating a <strong>Windows VM</strong> that does not have the VirtIO drivers installed, use <code>--sata0</code> instead of <code>--scsi0</code>. You can install the VirtIO drivers later and switch the bus type for better performance. For Linux, <code>scsi</code> with the <code>VirtIO SCSI</code> controller type is ideal.</li></ol><h1>Step 6: Boot Your Migrated VM!</h1><ol><li>In the Proxmox UI, go to your new VM&#39;s <strong>Options -&gt; Boot Order</strong>. Ensure the newly attached disk is enabled and at the top of the list.</li><li>Start the VM.</li></ol><p>It should now boot up in Proxmox from its newly migrated disk. Once you&#39;ve confirmed everything is working, you can safely delete the original VM from ESXi and clean up your NFS share configuration.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>So, in short, the downtime for a migrated VM will be 5 minutes or whatever time you take to do the move of the file and the VM to boot.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nlcv36/highspeed_lowdowntime_esxi_to_proxmox_migration/nf4hch9/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 20 Sep 2025 00:52:39 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nl77u7/proxmox_ha_failure_of_zfs_local_storage_does_not/</link><title>Proxmox HA: failure of ZFS local storage does not migrate VMs</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nl77u7/proxmox_ha_failure_of_zfs_local_storage_does_not/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nl77u7/proxmox_ha_failure_of_zfs_local_storage_does_not/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nl77u7/proxmox_ha_failure_of_zfs_local_storage_does_not/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>If I understand correctly, even a critical failure of the ZFS local storage will not result in the HA failover kicking in, if the node is otherwise up.</p><p>How do I automatically trigger a node hard down if ZFS local storage fails, so that HA failover will start migrating VMs?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>HA is for cluster resources, zfs is not a cluster resource, it’s local storage. Use a proper cluster storage system.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl77u7/proxmox_ha_failure_of_zfs_local_storage_does_not/nf4c39g/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 19 Sep 2025 21:19:22 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nl36ti/cant_use_my_mouse_and_keyboard_in_vm/</link><title>Can't use my mouse and keyboard in VM</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nl36ti/cant_use_my_mouse_and_keyboard_in_vm/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nl36ti/cant_use_my_mouse_and_keyboard_in_vm/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nl36ti/cant_use_my_mouse_and_keyboard_in_vm/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey,<br/>After booting into a Windows Server 2008 32-bit image, I cannot use my mouse or keyboard.<br/>I have disabled the “use tablet for pointer” setting.  </p><p>Any ideas ?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Probably need a little bit more information than this . . .</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl36ti/cant_use_my_mouse_and_keyboard_in_vm/nf2ll5g/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Have you tried enabling the &quot;use tablet for pointer&quot; setting?</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl36ti/cant_use_my_mouse_and_keyboard_in_vm/nf2o1g7/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>did you maybe on accident enable &quot;view only&quot; in the noVNC settings?</p><p><a href="https://preview.redd.it/gtk5e2pit4qf1.png?width=320&amp;format=png&amp;auto=webp&amp;s=940e90509ee58058f3f8fe73428143fba786b91b">https://preview.redd.it/gtk5e2pit4qf1.png?width=320&amp;format=png&amp;auto=webp&amp;s=940e90509ee58058f3f8fe73428143fba786b91b</a></p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl36ti/cant_use_my_mouse_and_keyboard_in_vm/nf2sbtg/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 19 Sep 2025 18:41:30 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nl1ama/limit_or_define_iscsi_connection_to_specific/</link><title>Limit or define iscsi connection to specific network card</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nl1ama/limit_or_define_iscsi_connection_to_specific/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nl1ama/limit_or_define_iscsi_connection_to_specific/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nl1ama/limit_or_define_iscsi_connection_to_specific/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi!</p><p>Is there a way of limiting which network cards on the proxmox host will be used for iscsi?</p><p>Lets say I have like 4 Network cards (ens15f0-3) installed but I want to use 2 of those dedicated for iscsi (ens15f0-1)</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Depending on your setup you solve this by using one subnet on one card and another subnet on another, and then you do iSCSI discovery across both of them. Then fairly quickly you want to install multipath-tools to make use you handle failover/roundrobin/whatever and no duplicate devices, etc.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl1ama/limit_or_define_iscsi_connection_to_specific/nf21rl6/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>If you are in a single subnet you can do it by altering the route table.  Multiple subnets is the best way.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl1ama/limit_or_define_iscsi_connection_to_specific/nf27uno/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>If you have multiple services in the same subnet and traffic from different targets, then the kernel will chose the one or the other interface -&gt; problem. The only, and best way to handle this is via policy based routing. The old way to do this was iptables / iptables2 which is already kinda replaced by nftables.</p><p>Proxmox uses nftables since version 7(?) and iptables only works now through a compatibility layer.</p><p>Mark the packet  -&gt; kernel see this and uses routing table xyz -&gt; send out xyz via ABC</p><ol><li><p>So first you need to define routing tables for each nic in /etc/iproute2/rt_tables</p><p>100 iscsi0101 iscsi1</p></li><li><p>Add routing rules to (/etc/network/interfaces</p><h1>iSCSI Interface 1</h1><p>auto iscsi0iface iscsi0 inet static    address 192.168.100.101    netmask 255.255.255.0    mtu 9000    post-up ip route add 192.168.100.0/24 dev iscsi0 src 192.168.100.101 table iscsi0    post-up ip rule add from 192.168.100.101 table iscsi0    pre-down ip rule del from 192.168.100.101 table iscsi0    pre-down ip route del 192.168.100.0/24 dev iscsi0 src 192.168.100.101 table iscsi0</p><h1>iSCSI Interface 2</h1><p>auto iscsi1iface iscsi1 inet static    address 192.168.100.102    netmask 255.255.255.0    mtu 9000    post-up ip route add 192.168.100.0/24 dev iscsi1 src 192.168.100.102 table iscsi1    post-up ip rule add from 192.168.100.102 table iscsi1    pre-down ip rule del from 192.168.100.102 table iscsi1    pre-down ip route del 192.168.100.0/24 dev iscsi1 src 192.168.100.102 table iscsi1</p></li><li><p>Add nftable rules</p><p>table inet mangle {    chain output {        type route hook output priority mangle; policy accept;</p><pre><code>    # mark iSCSI traffic via iscsi0    ip saddr 192.168.100.101 tcp dport 3260 meta mark set 1    # mark iSCSI traffic via iscsi1    ip saddr 192.168.100.102 tcp dport 3260 meta mark set 2}</code></pre><p>}</p></li><li><p>Enable (boot start)/Activate (now) nftables</p><p>systemctl enable nftablessystemctl start nftables</p></li><li><p>Check</p><p>ip rule shownft list ruleset</p></li></ol></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl1ama/limit_or_define_iscsi_connection_to_specific/nf326xs/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>You configure IP and subnet (without gateway) for these cards and then configure the same in your MPIO (multipath IO) configuration used by ISCSI.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl1ama/limit_or_define_iscsi_connection_to_specific/nf7gr8d/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 19 Sep 2025 17:14:43 +0530</pubDate></item><item><link>https://i.redd.it/h6rvcgq8tzpf1.png</link><title>Lesson Learned - Make sure your write caches are all enabled (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>So I recently had the massive multi-disk/multi-vdev fault from my last post, and when I finally got the pool back online, I noticed the resilver speed was crawling.  I don&#39;t recall what caused me to think of it, but I found myself wondering &quot;I wonder if all the disk write caches are enabled?&quot;  As it turns out -- they weren&#39;t (this was taken after -- sde/sdu were previously set to &#39;off&#39;).  Here&#39;s a handy little script to check that and get the output above:</p><p>for d in /dev/sd*; do</p><p># Only block devices with names starting with &quot;sd&quot; followed by letters, and no partition numbers</p><p>[[ -b $d ]] || continue</p><p>if [[ $d =~ ^/dev/sd[a-z]+$ ]]; then</p><p>fw=$(sudo smartctl -i &quot;$d&quot; 2&gt;/dev/null | awk -F: &#39;/Firmware Version/{gsub(/ /,&quot;&quot;,$2); print $2}&#39;)</p><p>wc=$(sudo hdparm -W &quot;$d&quot; 2&gt;/dev/null | awk -F= &#39;/write-caching/{gsub(/ /,&quot;&quot;,$2); print $2}&#39;)</p><p>printf &quot;%-6s Firmware:%-6s WriteCache:%s\n&quot; &quot;$d&quot; &quot;$fw&quot; &quot;$wc&quot;</p><p>fi</p><p>done</p><p>Two new disks I just bought had their write caches disabled on arrival.  Also had a tough time getting them to flip, but this was the command that finally did it: &quot;smartctl -s wcache-sct,on,p /dev/sdX&quot;. I had only added one to the pool as a replacement so far, and it was choking the entire resilver process.  My scan speed shot up 10x, and issue speed jumped like 40x.</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/h6rvcgq8tzpf1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>On NVMe you can use this to figure out if writecache is enabled or not:</p><pre><code>cat /sys/block/nvme0n1/queue/write_cache</code></pre><p>write-through means disabled and write-back means enabled.</p><p>Note however that enabling writecache can be a very bad thing if your box isnt connected to an UPS.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/nf1vb30/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I recommend only enabling this if you have a UPS *and* a redundant power supply. Power supplies can and will die and you could run into issues.</p><p>The scale has two sides. Either you want a bit more &quot;safety&quot; and piece of mind or do you want a bit higher performance. Choose wisely.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/nf3vp8k/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>From my understanding, it&#39;s best to leave it off if you don&#39;t have a UPS, because the drive would tell ZFS data is written to disk even if it&#39;s still in the drive&#39;s RAM, which could lead to data loss.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/nf2qmxk/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I think it&#39;s ok on zfs.</p><p><a href="https://serverfault.com/questions/995702/zfs-enable-or-disable-disk-cache/995729#995729">https://serverfault.com/questions/995702/zfs-enable-or-disable-disk-cache/995729#995729</a></p><p>I&#39;ve actually checked all my drives on Proxmox and it&#39;s actually enabled by default when using zfs.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/nf488zo/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>This is okay in ZFS, but do not do this with CEPH, disks must have write cache off for performance/stability reasons.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/nf94npp/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 19 Sep 2025 16:07:05 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/</link><title>Solutions for when you don't have control over your external network</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Senior level compsci student in college. I’ve just got a new desktop so my old one is hanging around doing nothing and I want to put proxmox on it and put it on my wifi network at the townhome I’m renting.</p><p>Only problem is my landlords aren’t tech savvy. The router is entirely ISP managed and so because of that I don’t have access to the ability to reserve a DHCP address. I’m probably going to just look at the network and pick an address that unlikely to be taken to be used as a management interface. And to be clear, I don’t need any of the VMs I’m hosting to be available when I’m not at home I don’t want a public facing IP I just want to be able to access it without DHCP issues.</p><p>But if I can’t get a DHCP address for my management interface is there a good way to ensure that if for some reason DHCP assigns the address I have proxmox that I can recover it or not have to deal with my ISP router?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>If you don’t need public facing, then just double NAT. </p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/nf0b8ad/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>There&#39;s a couple alternative solutions here that haven&#39;t been mentioned yet.</p><ol><li>Assign a static IP on a different subnet on proxmox and on your PC. You can have multiple subnets on the same LAN. That + DHCP gives you static access into it, and it has the usual internet access out. </li><li>Tailscale. Let everything be dynamic and Tailscale will give magic dns names to things and you can access them by that. Literally doesn&#39;t matter where or how your host has an internet connection, you can get to it. Yes, I know you&#39;re not asking for outside access, this is good over LAN. And for full speed too, it should NAT-hairpin and tunnel directly host to host within the LAN.</li></ol><p>There&#39;s some additional steps for getting proxmox to DHCP and work properly, see this <a href="https://gist.github.com/free-pmx/2292fa9efb75a16f3e648604050ed662">https://gist.github.com/free-pmx/2292fa9efb75a16f3e648604050ed662</a></p><p>Although I&#39;ll note that on my pve 9 I had to use &quot;if-up dhcpcd vmbr0&quot; instead of the DHCP mode described there.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/nf0fv2y/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>You could just allow it to use mDNS by installing a mDNS daemon like avahi-daemon and configuring it.  Then you wouldn&#39;t need to know the lease and just use whatever you&#39;ve set the mdns to.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/nf0ug28/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>You can always just get your own router and put it behind the ISP one.  You’ll have a double NAT setup, but that’s usually not an issue especially if you aren’t trying to allow external connections.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/nf0bcgw/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Install OPNSense/pfSense or whatever kind of firewall/router you like on Proxmox and use that one as router.</p><p>Enable DHCP on your router and use a static IP address on your Proxmox LAN NIC.</p><p>If the desktop you will use for Proxmox has a single network card, you will need a switch that supports VLANs, if you have an extra NIC or you can add one, then you will not.</p><p>In any case you would better have your own WiFi AP, that can double as a switch for other cabled devices, connected to the LAN NIC, behind the router/firewall.</p><p>Maybe this is a bit over engineered, but with such setup you will be free to do whatever you want with your network.</p><p>As other already mentioned, the moment you will need to expose some services or you will just need to connect home when outside, you can use tailscale or cloudflared.</p><p>It would be a nice journey, you would learn a lot of things :)</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/nf0w3fo/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Put your own firewall behind this &quot;landlord router&quot; to protect your own network but also to NAT (and portforward when needed) traffic using your single IP you get from upstream.</p><p>This way you can use how many IP-addresses you wish on your LAN with or without DHCP.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/nf139vv/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Use an overlay network like zerotier then you can use whatever dhcp address you get and it doesn&#39;t matter.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nksizg/solutions_for_when_you_dont_have_control_over/nf0iv7v/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 19 Sep 2025 08:41:39 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nkq6s6/kernal_617_for_my_new_intel_arc_pro_b50/</link><title>Kernal 6.17 for my new Intel Arc Pro B50</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nkq6s6/kernal_617_for_my_new_intel_arc_pro_b50/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nkq6s6/kernal_617_for_my_new_intel_arc_pro_b50/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nkq6s6/kernal_617_for_my_new_intel_arc_pro_b50/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Just picked up one of these to give it a shot with SR IOV not realizing that support isn&#39;t in the kernel because these use the Xe driver which doesn&#39;t have SR IOV in the kernel until 6.17 (unlike the i915 driver the A series cards used).</p><p>Has anyone successfully upgraded VE 9.0 to Kernel 6.17 RC?  If so, care to share the steps?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>The Xe driver is in kernel 6.14 and is compatible with the arc b50. Just dropped my b50 into my proxmox host running kernel b50 and it was picked up immediately. I did update the compute packages and that was all I needed to get going smoothly. </p><p>Intel also isn’t planning on SRVIO enablement for battlemage until later in the year so that feature is not available yet.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkq6s6/kernal_617_for_my_new_intel_arc_pro_b50/nf1gu28/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>You can probably build kernel yourself with proxmox patches.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkq6s6/kernal_617_for_my_new_intel_arc_pro_b50/nf10tu9/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Dangerous waters.. when I was on Debian 12 it was back ports . Then zabbly kernel for support. Now I&#39;m on Debian 13 with zabbly for kernel.. works good for ARC cards.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkq6s6/kernal_617_for_my_new_intel_arc_pro_b50/nezuee2/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>until it&#39;s release by Ubuntu and then Proxmox it&#39;s not happening.</p><p>Proxmox uses a LTS kernel from Ubuntu and then add their special source to make work nicely as hypervisor host.</p><p>So if you were to install a 6.17 kernel from elsewhere you&#39;d bork your system.</p><p>quick web search indicates that 6.17 is now in Ubuntu 25.10 beta but that&#39;s not an LTS release so it could be April next year before it hits proxmox (the release of 26.04)</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkq6s6/kernal_617_for_my_new_intel_arc_pro_b50/nezuxqc/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 19 Sep 2025 06:50:25 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/</link><title>Can I add a NAS from another system to be available to my promotion syatem?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Very new to homelabbing. </p><p>So i installed proxmox on a MS-01 mini pc. Id like to make a jellyfin server, but of course the minipc doesn&#39;t have any easy way to add a bunch of high capacity drives.</p><p>So if I build a nas in a separate case, can I make the NAS storage available to my proxmox system and jellyfin?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>yep.</p><p>Share it as either NFS or SMB, mount it to your Proxmox server and then it pass it through to the LXC that&#39;s running Jellyfin.</p><p>if Jellyfin is running in a VM, you can mount it directly.</p><p>But the nice thing is you can mount the share to a VM, Proxmox server and even other machines on the network at the same time.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/nezcwn4/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Yes using NFS</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/nezcuk6/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I&#39;ve got two volumes presented by my NAS to VMs: 1 NFS that all my containers work within, another is an iSCSI block volume that a particularly huge Windows VM monopolizes.</p><p>Fair warning if you plan on following, the separate 10Gbit network I&#39;m running for storage is almost insufficient. It&#39;s slow, but not too slow to live with. I can&#39;t imagine 2.5Gbit or slower, though.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/nf030go/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Yes</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/nezctar/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Yeah I have my Synology media share mounted to pve as /mnt/nas in fstab as NFS, then I manually added entries to /etc/pve/lxc/###.conf for mp0: /mnt/nas,mp=/data where I generally match the trash guides.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/nf0b7dj/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Yes. My Plex and Jellyfin servers both run as unprivileged Proxmox LXCs and I have an *arrs stack on an Ubuntu VM that I run Docker on.</p><p>I mount my media drive on a Synology NAS via SMB/CIFS.</p><p>[you do need to know about unprivileged LXCs and mount permissions on the host]</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/nf1r4de/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Nfs is fastest but least secure.  I use this.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkoatg/can_i_add_a_nas_from_another_system_to_be/nf6y5r5/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 19 Sep 2025 05:22:00 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nkkzf6/web_dashboard_shell_not_accessible/</link><title>Web dashboard shell not accessible</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nkkzf6/web_dashboard_shell_not_accessible/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nkkzf6/web_dashboard_shell_not_accessible/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nkkzf6/web_dashboard_shell_not_accessible/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I created an user with the roke set as ADMINISTRATOR pam and  the same user exists on all nodes locally but when i disable permitrootlogin on ssh config the shell on the web dashboard becomes inaccessible? But im loged in as the new user i created why does this happen? Anything im doing wrong ? </p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>The web dashboard creates an ssh connection to the host. Some part of ssh that you have disabled needs to be enabled. I don’t know if it’s the same for different users other than root as it is for different login methods, but using oauth for login requires me to actually login to the shell with a username and password rather than it just opening to anything.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkkzf6/web_dashboard_shell_not_accessible/neyo4st/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 19 Sep 2025 02:59:51 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nkig05/lxc_file_permission_help/</link><title>lxc file permission help</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nkig05/lxc_file_permission_help/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nkig05/lxc_file_permission_help/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nkig05/lxc_file_permission_help/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>brain turning to jelly trying to fix all this today so wiped to original and asking here</p><p>so moving from an old Synology to hosting file directly on a ZFS pool on proxmox itself and everything i do seems to be blocked by something elses file permissions</p><p>files are hosted on /ZFS_Pool/media (some stuff using as on diffrent boxes to the ZFS array as /mnt/pve/media) and mounted to all LXC&#39;s involved via</p><p>mp0: /ZFS_Pool/media,mp=/mnt/media or mp0: /mnt/pve/media,mp=/mnt/media</p><p>anyhting that writes to it (NZBget, SMB, syncthings) uses its own user so the moment they get involved everything else (Sonarr,Radarr,SMB) gets permission denied and causing a mess that i really dont want to bodge</p><p>is there a &quot;right&quot; way to fix all this such that no LXC&#39;s file permsisions impacts anything else?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>privileged or unprivileged LXC?</p><p>if unpriv, you will have to do user mapping.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkig05/lxc_file_permission_help/ney4715/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>follow this guide, it&#39;s super easy and just works on unprivileged containers.</p><p>just add you sonarr/radarr/.. user to the LXC_Shares group </p><p><a href="https://github.com/JamesTurland/JimsGarage/tree/main/LXC/NAS">https://github.com/JamesTurland/JimsGarage/tree/main/LXC/NAS</a></p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkig05/lxc_file_permission_help/nezbw4h/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 19 Sep 2025 01:22:02 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/</link><title>New to Promox - is Proxmox Full Course by Learn Linux TV still up to date for beginners?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi folks,</p><p>As title says, new to Proxmox and want to follow a youtube guide. Searching this subreddit I saw this video series suggested:</p><h1><a href="https://www.youtube.com/playlist?list=PLT98CRl2KxKHnlbYhtABg6cF50bYa8Ulo">https://www.youtube.com/playlist?list=PLT98CRl2KxKHnlbYhtABg6cF50bYa8Ulo</a></h1><p>The videos are 2 years old. Wanted to check if theres a current suggested alternative or if these course is still relevant?</p><p>Thanks!</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>The essence of it still holds up. Things have been added to Proxmox later on, but for the most part, the things he goes through is still up to date and relevant.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/nexgvgm/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Don&#39;t know the videos.</p><p><a href="https://pve.proxmox.com/pve-docs/pve-admin-guide.html">https://pve.proxmox.com/pve-docs/pve-admin-guide.html</a></p><p>It is a lot of text, but worth imo.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/nexobxi/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>That&#39;s what I used very recently, I&#39;d highly recommend it!</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/nexhkuj/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Its virtualization in a hypervisor not a video game with a meta. Proxmox hasn&#39;t changed nearly enough for any of that content to be out of date.</p><p>Jokes aside his videos are great and he has excellent delivery.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/neyuyrd/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I would message Jay from Learn Linux TV and ask him. He would be the best source for your question. Not us.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/nezh7pr/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>This guy is the reason I started using proxmox.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/nf101mb/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Probably related to what you ask for:</p><p><a href="https://www.reddit.com/r/Proxmox/comments/1nh0soi/vent_unbelievably_frustrating/ne88g9a/">https://www.reddit.com/r/Proxmox/comments/1nh0soi/vent_unbelievably_frustrating/ne88g9a/</a></p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/nf12vuc/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 23:32:37 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/</link><title>Intel vs AMD: i5-9500 vs Ryzen 3400G</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>All things being equal, would you prefer to use Intel i5-9500 or AMD Ryzen 3400G CPUs for Proxmox nodes?</p><p>No GPU usage, no passthrough, just a few plain Linux VMs.</p><p>The 3400G box is cheaper than the i5-9500 box, but according to reports the Intel uses 5-10 W less (at idle), so over the lifetime of the hardware it&#39;s probably more or less cancels out.</p><p>AMD has an edge on upgradeability, going up to a 12 core Ryzen 9 PRO 3900. The Intel tops out at 8 cores with the i7-9900. The 3900 has roughly double the Passmark score of the i7-9900.</p><p>It&#39;s a bit of a tossup whether the nodes will ever be upgraded rather than replaced.</p><p>What&#39;s your take, AMD or Intel?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I don&#39;t know what a few plain Linux VMs if, whether it&#39;s 2 or 10,  or something in between, and what you&#39;re planning to run.<br/>So until then, my answer will be:<br/>The cheapest option.<br/>Don&#39;t worry about 5-10W (unless it&#39;s REALLY important to you), especially since it&#39;s TDP, which is equivalent of sucking your finger and putting it in the air, to conclude the wind is coming from west, is more of a pointer than an actual measurement.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/nexgh9c/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>If its AM4 you can go all the way to a 5950x which would be a monster.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/nexwzn9/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>What are you running<br/>Do you need more per core performance or more cpu threads</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/nexdxbj/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>AMD is a better long-term deal than core-i5 or i7</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/nexuwlz/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>If one box has 2 DIMM slots and another has 4, I&#39;d probably go with the 4.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/neytb2a/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>AMD, you do the math:</p><p><a href="https://security-tracker.debian.org/tracker/source-package/amd64-microcode">https://security-tracker.debian.org/tracker/source-package/amd64-microcode</a></p><p><a href="https://security-tracker.debian.org/tracker/source-package/intel-microcode">https://security-tracker.debian.org/tracker/source-package/intel-microcode</a></p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/nf13gxd/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Avoid 9th gen</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/neytibm/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 23:18:44 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nkcj6g/has_anyone_used_http_api_to_update_net0_settings/</link><title>Has anyone used HTTP API to update net0 settings?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nkcj6g/has_anyone_used_http_api_to_update_net0_settings/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nkcj6g/has_anyone_used_http_api_to_update_net0_settings/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nkcj6g/has_anyone_used_http_api_to_update_net0_settings/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi All - I am trying to use the proxmox HTTP API to change the vlan tag on net0 for a VM. I&#39;m banging my head against the wall trying to get this seemingly simple command to work. I&#39;m hoping someone has seen this issue and can help out.</p><p>Example curl command:</p><pre><code>curl -X PUT -H &#39;Authorization: PVEAPIToken=user@pve!api=TOKEN&#39; --data &quot;net0=virtio=BC:24:11:57:19:E3,bridge=vmbr1,firewall=1,tag=10&quot; &quot;https://pxmx1.example.com:8006/api2/json/nodes/pxmx1/qemu/133/config&quot;</code></pre><p>The output I get is:</p><pre><code>{&quot;message&quot;:&quot;no sdn vnet ID specified\n&quot;,&quot;data&quot;:null}</code></pre><p>This is a 2 node cluster I&#39;m testing with. SDN is not configured/enabled (although there are 2 entries named &quot;localnetwork&quot; in the Datacenter-&gt;SND screen, must have been autogenerated when creating the cluster).</p><p>I&#39;m not finding any info on this error. I&#39;m pretty sure the --data param is populated correctly.</p><p>PVE is v8.4.1.</p><p>Is there a place I can see more detailed logging about the call?</p><p>If anyone has ideas, I&#39;d really appreciate the help.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>I finally figured this out. The error message is a red herring. SDN has nothing to do with the problem, rather it seems to be an encoding issue of the data. Swapping the --data flag to --data-urlencode did the trick.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkcj6g/has_anyone_used_http_api_to_update_net0_settings/nf3wtvg/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 21:40:51 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/</link><title>Storage full, now I'm stuck in some form of loop.</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I have no idea about how to use Proxmox, Linux, or Windows Server.... so I got an old computer and installed all of them onto it so I can tinker and learn.</p><p>Today I was installing Microsoft SQL Server onto my Win Server, to fiddle with sample data.... and my Proxmox came up with this message - &quot;closing file &#39;/var/log/pve/tasks/active.tmp.4115482&#39; failed - No space left on device (500)&quot;</p><p>It now won&#39;t let me enter the shell to fiddle with it, I cannot SSH in, it terminates the session. Viewing it in a browser, it just crashes. No VMs are running or will let me remote in either.</p><p>Am I toast here?</p><p>The SSD is 1TB, but only 100GB was given to VM storage, a lesson here is to increase it next time.</p><p>Thank you in advance.</p><p><a href="https://preview.redd.it/bq90f4mkvxpf1.png?width=1673&amp;format=png&amp;auto=webp&amp;s=c1f49f11bed80c03cfd8dab12f3482aa271fe4d8">I get this error when trying to access the Proxmox shell. </a></p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Hookup a keyboard and monitor, you should be able to boot into safe mode on the host (the actual computer running proxmox) and navigate around to clear up space or get the webui without starting any containers.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/newfmiz/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The fact that it&#39;s complaining about /var/log/pve/tasks/ would suggest it&#39;s the host that ran out.</p><p>Seems a bit odd that you&#39;re out of space on a 1TB drive with only 100gig VM.</p><p>First thing I&#39;d try is instead of an interactive shell send a command over ssh rather than creating an interactive shell. Something like so - adjust with whatever authentication you&#39;re using:</p><pre><code>ssh -i ~/.ssh/id_ed25519 -p 22 root@host &#39;journalctl --vacuum-time=3d&#39;</code></pre><p>If that doesn&#39;t work - journalctl might not work without space then I&#39;d try to hit the log dirs with a rm command. You&#39;ll need to do it blindly, so to help - structure looks like this...I&#39;d imagine you can delete some of those numbered folders without too much carnage</p><p><a href="https://i.imgur.com/4DJ1Jar.png">https://i.imgur.com/4DJ1Jar.png</a></p><p>Failing that you&#39;ll likely need to boot off a portable OS, mount the partition, delete some stuff and then reboot</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/nex72ye/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Is that no space with in the vm virtual disk file or on the disk it’s stored for proxmox?</p><p>If you have another windows vm, you could mount the virtual disk file to remove files to clean it up or enlarge it (though gparted will also help as there maybe a windows hidden partition that needs to be moved,</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/newgfpf/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I would reinstall and set the os in a partition with 32/62G it should only need 16 but adding some more for ISO storage on local-zfs is nice. Then use the rest as a vm storage like vm-tank.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/nf15yzt/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 20:33:59 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nk9vup/strange_issue_cant_update_past_903_no_updates/</link><title>Strange issue, cant update past 9.0.3, no updates found</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nk9vup/strange_issue_cant_update_past_903_no_updates/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nk9vup/strange_issue_cant_update_past_903_no_updates/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nk9vup/strange_issue_cant_update_past_903_no_updates/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>root@Proxmox:~# apt update<br/>Hit:1 <a href="http://security.debian.org/debian-security">http://security.debian.org/debian-security</a> trixie-security InRelease<br/>Hit:2 <a href="http://deb.debian.org/debian">http://deb.debian.org/debian</a> trixie InRelease<br/>Hit:3 <a href="http://download.proxmox.com/debian/pbs">http://download.proxmox.com/debian/pbs</a> trixie InRelease<br/>Hit:4 <a href="http://deb.debian.org/debian">http://deb.debian.org/debian</a> trixie-updates InRelease<br/>All packages are up to date.<br/>root@Proxmox:~# apt upgrade<br/>Summary:<br/>Upgrading: 0, Installing: 0, Removing: 0, Not Upgrading: 0<br/>root@Proxmox:~# cat /etc/apt/sources.list.d/proxmox.sources<br/>Types: deb<br/>URIs: <a href="http://download.proxmox.com/debian/pbs">http://download.proxmox.com/debian/pbs</a><br/>Suites: trixie<br/>Components: pbs-no-subscription<br/>Signed-By: /usr/share/keyrings/proxmox-archive-keyring.gpg<br/>root@Proxmox:~#</p><p>zero errors, it just thinks there are no updates. The problem is non of my LXC will start. My other node is at 9.0.10. Any ideas ?</p><p>EDIT:  [SOLVED] TLDR the comments: I need more coffee, had the wrong repo in there ugh</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>You don&#39;t seem to have any PVE repos configured. PVE and PBS are different things.See here: <a href="https://pve.proxmox.com/wiki/Package_Repositories">https://pve.proxmox.com/wiki/Package_Repositories</a></p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk9vup/strange_issue_cant_update_past_903_no_updates/newkzlh/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>You run apt dist-upgrade to upgrade Proxmox</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk9vup/strange_issue_cant_update_past_903_no_updates/new80lp/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Are you upgrading PVE or PBS? Your repo sources above are for PBS.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk9vup/strange_issue_cant_update_past_903_no_updates/newlnnn/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 20:01:06 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/</link><title>Proxmox HA: ZFS mirror or separate OS and data disks</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>On small nodes, would you rather put everything on a two disk ZFS mirror or use one disk for Proxmov and one disk for VMs?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I pick mirror over separate - all my servers run mirror and I even went as far as manually setting up the partitions in the case of mixed size disks.</p><p>Having my production servers continue running while waiting for a replacement allows me to keep things running, reducing stress and allow me to look / wait for better deals (e.g. used enterprise ssd).</p><p>And of course ability to repair scrub errors.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/nevd4eq/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>As Proxmox is so easy to install I would (and do run) one disk (or a partition) for the OS and the other (the faster) for VM disks.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/nevbjpp/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Are the disks the same size? If not, mirrors aren&#39;t the best idea.</p><p>I have a 1TB primary, and 2T B secondary SSD. After playing around with multiple layouts, I&#39;m now aiming for: 1× 50GB Proxmox, 1× 850+GB VM / LXC.<br/>2nd drive is exclusive for Jellyfin etc. shared media.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/nevpo5n/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>For really small nodes I would go with a 2x mirror using ZFS.</p><p>Note that you need to keep track of storage usage since all partitions will use the same drives and the partitions are not limited as with ext4. That is both OS and VM drives will use the same physical storage making it more likely that you one day run out of space if you dont keep track of this and then you need &quot;BIOS access&quot; (ILO/IPMI/IPKVM) to resolve the issue through recovery mode in Proxmox.</p><p>That is you will see something like this (removed some lines from output):</p><pre><code>Filesystem        Size  Used Avail Use% Mounted onrpool/ROOT/pve-1  329G  2,4G  327G   1% /rpool             327G  128K  327G   1% /rpoolrpool/var-lib-vz  672G  345G  327G  52% /var/lib/vzrpool/ROOT        327G  128K  327G   1% /rpool/ROOTrpool/data        327G  128K  327G   1% /rpool/data</code></pre><p>In above those 327G free (on my 2x mirror of 800GB NVMe drives) is shared between all datasets.</p><p>Meaning if I upload a 5GB ISO to the root filesystem all datasets will then report 322G free.</p><p>Other than that I prefer to run 2x mirror for the OS (normally M.2 slots) and then use the frontloaded drivebays for the actual VM-storage (for a 1RU thats often 10x 2.5&quot; slots or so).</p><p>That is ZFS for the 2x boot mirror and then how many drives you got for CEPH as shared storage between the nodes.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/newr9ba/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I use cheap 32 and 64 GB optane M.2 drives as a mirrored ZFS boot volume. Then I add a pair of larger U.2/U.3 NVMe drives on a PCIe adapter for VM storage.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk5jsb/proxmox_ha_zfs_mirror_or_separate_os_and_data/nexzffq/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 16:51:03 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/</link><title>Changing GPU to another slot when it is passed trough to a VM, makes the host reboot</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Changing GPU to another slot when it is passed trough to a VM, makes the host reboot<br/>When the VM starts after reboot, it does not found the passed trough GPU and the whole host reboots.<br/>Its in a reboot loop now. What should I do?</p><p>EDIT; Yes I have shut of the server always when touching components and pulled the cord.<br/>EDIT: all works, the reason was PCIE 4.0 Riser cable. Those just does not work with 5090.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I hate to ask the question but just wanting to make sure: Did you power down the host before removing the GPU / putting it back in?</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/nev6srf/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>yikes, you&#39;ll need to somehow disable the automatic start of the VMs where you have PCIe devices passed through</p><p>The reason this happens is because the devices change their PCIe ID every time you install/remove/replace a PCIe device, it&#39;s just how motherboards handle this sort of thing, and afaik it can&#39;t really be helped on consumer boards</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/nev7v1r/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Addressing the real issue (OP was not trying to hotswap GPUs), I’d put it back where it was, boot up, detach the GPU from the VM and turn off its autostart, shut down, move the GPU and pass it to the VM again.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/nevoz19/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Well you are pulling a core component of your server out. Naturally your host is gonna throw a hissy fit to protest. You wouldnt like it very much if someone pulled out your liver while you are still booted. /s </p><p>Shut off your host whole working on any builtin component.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/nev7p83/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The pci address will change according to the slot</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/nevp68n/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Don’t listen to the other OP, pcie hot plug is a well supported feature /s</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/neva8d1/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The iDs change when you change the pcie slot.</p><p>Iommu is a virtual memory address....when you change the location that changes too.</p><p>If for some reason you need to leave it in the new location you will have to find the new address with lspci in the console and nano the corresponding configs.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk50ie/changing_gpu_to_another_slot_when_it_is_passed/nez87gh/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 16:21:45 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nk4f9h/proxmox_sdn_fabric_access_shared_nfs_storage_in/</link><title>Proxmox SDN fabric, access shared NFS storage in fabric</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nk4f9h/proxmox_sdn_fabric_access_shared_nfs_storage_in/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nk4f9h/proxmox_sdn_fabric_access_shared_nfs_storage_in/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nk4f9h/proxmox_sdn_fabric_access_shared_nfs_storage_in/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey folkshope you’re doing well.  </p><p>I’m running a 3-node Proxmox cluster in a hyper-converged setup.<br/>On <strong>node 2</strong> there’s a <strong>TrueNAS VM</strong> that exports two NVMe pools over <strong>NFSv4</strong>.  </p><p><strong>Previous setup (worked fine):</strong>  </p><ul><li>All 3 nodes connected to a 10 Gbps switch via LACP, classic VLANs.</li><li><strong>VLAN 3100</strong> was the default VM network.</li><li>The TrueNAS VM NIC was bridged into <strong>VLAN 3100</strong>.</li><li>Each Proxmox host also had an IP interface in <strong>VLAN 3100</strong> to mount the NFS share.</li></ul><p><strong>What changed:</strong>  </p><ul><li>I removed the ~300 W Nexus 3000 switch and cabled the nodes in a <strong>ring</strong>.</li><li>I now run a <strong>VXLAN fabric</strong>, and <strong>VLAN 3100</strong> exists <em>inside</em> that overlay (VXLAN + VLAN tag, actually VM bridged to the fabric interface + VLAN tag).</li><li>VM-to-VM networking and live migration work.</li></ul><p><strong>My problem</strong>  </p><p>I can’t figure out the <em>clean</em> way for the <strong>Proxmox hosts</strong> themselves (not the VMs) to reach the NFS server that now resides <strong>inside the VXLAN/VLAN-3100 segment</strong>.<br/>In other words: what’s the appropriate method to give the hosts IP reachability into <strong>VLAN 3100 inside the VXLAN overlay</strong> so they can mount NFS from the TrueNAS VM?  </p><p><strong>My question</strong>  </p><ul><li>Is it supported/reasonable to put an IP on the <strong>overlay bridge</strong> (e.g., assign an address to the VXLAN bridge) and/or create a <strong>VLAN sub-interface</strong> on that bridge (e.g., vmbrX.3100) on each host?</li><li>Alternatively, should I attach a <strong>veth</strong> from the host into the VXLAN/VLAN-aware bridge to “place” the host stack inside that segment?</li><li>Is there a recommended Proxmox SDN way to give the <em>host</em> an interface inside a VNet/VNI for this purpose?</li></ul><p>Thanks in advance for any help!!! </p></div><!-- SC_ON --></section>]]></description><pubDate>Thu, 18 Sep 2025 15:47:20 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nk335b/question_with_regards_to_pbs_deduplication_factor/</link><title>Question with regards to PBS deduplication factor.</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nk335b/question_with_regards_to_pbs_deduplication_factor/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nk335b/question_with_regards_to_pbs_deduplication_factor/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nk335b/question_with_regards_to_pbs_deduplication_factor/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>A fictive case in order to try to understand the deduplication factor.</p><ul><li>1 VM to be backed up, all blocks are unique and hence not a single block is &quot;deduplicatable&quot;.</li><li>The VM is turned off. So blocks to be backed up never change.</li><li>completely empty repository. No backups as of yet.</li></ul><p>I run a backup job that only backs up this VM. I assume the deduplication factor will be 1 because there&#39;s not a single block unique.</p><p>I run the exact same backup job while the source VM was still turned off, so no blocks have changed, the backup retention is limitless so to speak, so the previous &quot;snapshot&quot; is not purged.</p><p>What is the deduplication factor after this second backup job finishes successfully? 2? Or still 1?</p><p>EDIT: first bullet point was wrong about the deduplicatable blocks.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>The factor from what i understand is on my much identical data there is.<br/>1 backup (1:1 ratio)<br/>2 backup (2:1 ratio)<br/>and so on, but this would only apply i nothing was changed between the backups.</p><p>Also the dedupe number dont update instantly, its the GC / Purge runs thats update it if im not wrong.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk335b/question_with_regards_to_pbs_deduplication_factor/neutk0o/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>„not a single block is unique and deduplicateable“</p><p>I guess you mean all blocks are unique? If no block would be unique then you would have dedupe.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk335b/question_with_regards_to_pbs_deduplication_factor/nf0zbrn/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I don&#39;t know what would make a block not deduplicatable besides being unique. So since you stipulate &quot;no unique blocks&quot; the VM should in fact dedup extremely well with each block referenced at least two times. SoI expect &gt;=2 for the first backup and &gt;=4 after the second.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk335b/question_with_regards_to_pbs_deduplication_factor/nf10m8n/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Last I checked the factor was over 80. This PBS is as old as PBS.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk335b/question_with_regards_to_pbs_deduplication_factor/nevbvmq/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 14:22:08 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/</link><title>Advice to build up Proxmox Cluster with 3 nodes (feat. ceph storage)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi all,<br/>I’m planning a 3‑node Proxmox cluster with Ceph for my homelab and would love some advice. My goals are to learn (Ceph + clustering), replace an older single Proxmox host, and use SFP+ (10Gb) networking. I’m unsure whether to build from parts, go with mini PCs, open to all options.</p><p>Context and preferences:</p><ul><li>Learning‑focused, but I want something stable and maintainable.</li><li>Noise and power matter (it’s at home), but I can handle moderate fan noise if the value is good.</li><li>I want SFP+ for the cluster/replication network (10Gb; 25Gb later would be a bonus).</li><li>Planning on 3 nodes for Ceph minimum; I’m okay with starting small and expanding.</li></ul><p>Questions:</p><ul><li>DIY small towers (consumer parts), small form factor PCs (NUCs/minis)?</li><li>Any favorites for IPMI/remote management on a budget (Supermicro, used Dell/HP)?</li></ul><p>I’ve already got 6×16GB DDR5 SO-DIMM lying around. Budget is up to 500$ per node.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I have three minisforum ms 01. Each has three ssds two of which are dedicated to ceph. Works quite well. They also fit a 25G NIC and 96GB RAM (if I recall correctly).</p><p>Edit: I guess my budget was a bit lower, but you can go for smaller/fewer ssds and RAM</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/newq5zf/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>May want to read through <a href="https://forum.proxmox.com/threads/fabu-can-i-use-ceph-in-a-_very_-small-cluster.159671/">this post </a>. 3 nodes will function but there are caveats.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/nexdjgp/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The usual suspects for homeuse are:</p><ul><li><p>Minisforum: <a href="https://www.minisforum.com/collections/mini-pc">https://www.minisforum.com/collections/mini-pc</a></p></li><li><p>Protectli: <a href="https://protectli.com/products/">https://protectli.com/products/</a></p></li><li><p>CWWK: <a href="https://cwwkpc.com/collections/mini-pc">https://cwwkpc.com/collections/mini-pc</a></p></li><li><p>Zimaboard: <a href="https://www.zimaspace.com/products/single-board2-server">https://www.zimaspace.com/products/single-board2-server</a></p></li></ul><p>What I would favour is:</p><ul><li><p>Avoid HDD, use SSD or NVMe if your wallet is large enough.</p></li><li><p>Check the datasheets of the drives, for NVMe I highly recommend PLP/DRAM for performance and DWPD 3.0 (or higher) and high TBW for endurance. Avoid the 600TBW devices.</p></li><li><p>If possible AMD instead of Intel.</p></li></ul><p>Reasons?</p><p>Well you do the math ;-)</p><p><a href="https://security-tracker.debian.org/tracker/source-package/amd64-microcode">https://security-tracker.debian.org/tracker/source-package/amd64-microcode</a></p><p><a href="https://security-tracker.debian.org/tracker/source-package/intel-microcode">https://security-tracker.debian.org/tracker/source-package/intel-microcode</a></p><ul><li><p>For RAM select as large as you can fit. Also check how the CPU functions regarding dual, quad, 8 or 12-channels to boost performance. Also something like DDRx-6400 (if supported) is prefered over DDRx-4800.</p></li><li><p>For networking it depends on how much you want to segment but &quot;ideal&quot; or &quot;optimal&quot; would be something like:</p></li><li><p>ILO/IPMI/IPKVM: 1G RJ45</p></li><li><p>MGMT: 1G RJ45</p></li><li><p>FRONTEND: 2x25G SMF</p></li><li><p>BACKEND-CLIENT: 2x25G SMF</p></li><li><p>BACKEND-CLUSTER: 2x25G SMF</p></li></ul><p>Of course above can be if you got larger wallet increased like first and foremost the backend traffic will benefit of lets say 100G.</p><p>But you can also shrink down in speed and number of interfaces.</p><p>Reason why you want backend-client and backend-cluster separate is so not VM traffic will interfere with replication and clustersync traffic.</p><ul><li><p>You can save some costs of avoid having 2x switches (for redundancy) for backend and instead directly connect the hosts to each other and use FRR with OSPF locally. Another option is of course to use a single switch (or pair of switches in MLAG) for both frontend and backend traffic but normally you dont want to share these flows in the same hardware for security reasons. But for a homelab that would be perfectly fine (but then for a 3-node cluster I would directly connect the hosts to each other instead).</p></li><li><p>Then if possible I would prefer having 2x mirror as boot drive using ZFS and the rest used by CEPH. But this also boils down to how many drive slots will your hardware provide. If you just have lets say room for 2x M.2 drives then I would use one for ZFS for boot and the other for CEPH where the VM&#39;s are stored.</p></li></ul><p>So in short it boils down to what you want, what you need, why you need it and the size of your wallet?</p><p>Problem is often that people have unrealistic expectations - like they want a 3-node cluster with 6x25G interfaces, 128GB RAM, AMD EPYC 64C/128T CPU and 2x Micron NVMe for boot and 4x Solidigm 8TB drives for VM&#39;s and all this should be passively cooled and cost not more than $1000 in total - which is an impossible equation :-)</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/nf127s2/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Ms-01 is perfect for there mode ceph pve cluster. Up to 96gb ram and up to 8 tb storage n+2</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/nf0zb7q/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I&#39;ve got a 3 node Proxmox cluster, 3 Mini PC&#39;s, 2012 Mac Mini, Beelink SER5 Max (5800H) and TrigKey N150.  </p><p>Not the most capable hardware which is why I didn&#39;t go for ZFS due to lack of RAM I thought it was overkill.  </p><p>So all three machines have a local-lvm and the two more modern ones also have an &#39;nvme&#39; lvm.</p><p>I&#39;ve been wanting to do some HA with a few containers, Adguard, Caddy, Tailscale etc.  Some very light weight containers that have between 8GB and 16GB root partitions.  </p><p>Recently I spotted you can run Ceph on top  of LVM-Thin as the backing, so I&#39;ve set that up, you have to use the command line as you can&#39;t do it via the GUI but created 5 OSD&#39;s, each node that has 2 LVM&#39;s got a 32GB OSD and the Mac Mini got a single 64GB OSD.  </p><p>It&#39;s working well, HA is setup, but not tested, I have tested migrations and it&#39;s fast.  </p><p>This might not be the best way but it saved me many hours from having to move storage around to try and get ZFS replication working or using raw disks for CEPH.  Considering the MacMini is on the other end of a powerline adapter and only gets 150Mbps to the rest of my network I&#39;m quite happy.</p><p>Good luck with whatever you decide.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/nf2in47/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I would like to thank you all for the discussion. I think I will stay with my current host until I have the budget for a 4-node solution. First, I need to prepare a complete bucket list (nodes, SSDs, cables, etc.) and then define the next steps.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk1cdw/advice_to_build_up_proxmox_cluster_with_3_nodes/nf7royr/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 12:27:17 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nk0huf/ha_with_zfsreplication_do_i_need_groups/</link><title>HA with zfs-replication - do I NEED groups?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nk0huf/ha_with_zfsreplication_do_i_need_groups/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nk0huf/ha_with_zfsreplication_do_i_need_groups/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nk0huf/ha_with_zfsreplication_do_i_need_groups/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey everyone,</p><p>I want to run a Proxmox cluster with 3 nodes. The VMs will be stored on local ZFS pools and replicated via <strong>ZFS replication</strong> between two of the nodes. So, the third node does not have the replicated volumes.</p><p>My question is:<br/>If I enable HA for a VM, does the <strong>cluster manager (ha-manager)</strong> automatically know that the VM can only run on the two nodes that actually have the replicated ZFS volume?<br/>Or will it also try to start the VM on the third node in case of a failure — which would obviously fail because the storage isn’t available there?</p><p>So in this kind of setup, do I really need to maintain <strong>HA groups</strong> to restrict which nodes are eligible, or does Proxmox handle this logic automatically?</p><p>Curious how you guys deal with this and what the best practices are.</p><p>Thanks in advance</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I chose ceph to avoid stuff like this. It isn&#39;t without its own problems but works OK so far (~1y)</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk0huf/ha_with_zfsreplication_do_i_need_groups/nevey4c/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>It doesn&#39;t. You&#39;ll need groups. It&#39;s a pain. I ended up replicating to all nodes and just buying more storage instead because the admin overhead was annoying.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nk0huf/ha_with_zfsreplication_do_i_need_groups/neujpby/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 11:35:35 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1njzqkn/proxmox_crashes_when_i_try_to_copy_files_from_my/</link><title>Proxmox crashes when I try to copy files from my TrueNas</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1njzqkn/proxmox_crashes_when_i_try_to_copy_files_from_my/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1njzqkn/proxmox_crashes_when_i_try_to_copy_files_from_my/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1njzqkn/proxmox_crashes_when_i_try_to_copy_files_from_my/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I have a Dell server running Proxmox with only TrueNAS installed on it.</p><p>My Jellyfin server, which is installed on another server on my local network, can access files, play video files, and browse files through the network. I can open files through network share.</p><p>However, when I try to copy anything from the TrueNAS server, it first disappears from the network, followed by Proxmox (or both simultaneously). Proxmox becomes inaccessible, so I have to manually turn it off and on again.</p><p>I want to move my TrueNAS setup to another server and add an additional drive to mirror the current one. But I’m concerned that the drive might be failing.</p><p>The server has 16GB of RAM installed, with 12GB assigned to TrueNAS and then reduced to 8GB. Despite these changes, the issue persists.</p><p>I’ve tried the following solutions:- Changing the zfs_arc_limit- Changing the RAM size assigned to TrueNAS- Switching from the m.2 wifi slot for additional SATA to the onboard SATA slot</p><p>I’m at a loss and would appreciate any advice you can provide.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Maybe this? <a href="https://nb.balaji.blog/posts/fix-intel-e1000-proxmox-hang/">https://nb.balaji.blog/posts/fix-intel-e1000-proxmox-hang/</a></p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njzqkn/proxmox_crashes_when_i_try_to_copy_files_from_my/neua30b/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The crash you&#39;re seeing isn&#39;t caused by a software misconfiguration but by a fundamental hardware instability. The hardware is failing under load. Prioritize backing up your data and moving to a new, stable server.</p><p>Use smartctl within your TrueNAS shell to check the S.M.A.R.T. data for your drives. Look for errors, especially in reallocated sectors, pending sectors, and read error rates.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njzqkn/proxmox_crashes_when_i_try_to_copy_files_from_my/neug3xt/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I would look at the previous boot&#39;s logs to see if they give a clue. After a crash, the next boot use<code>journalctl -b-1</code> if persistent logs are enabled.  If not, enable them.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njzqkn/proxmox_crashes_when_i_try_to_copy_files_from_my/nez2fyp/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Why are you running TrueNas in Proxmox if it&#39;s the only thing on the computer? Why not just run it native?</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njzqkn/proxmox_crashes_when_i_try_to_copy_files_from_my/nevzmou/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 10:49:55 +0530</pubDate></item><item><link>https://www.reddit.com/gallery/1njz2e4</link><title>Proxmox VM Debian 12 to 13 Fail to Boot (UUID Mismatch?) (Gallery)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1njz2e4/proxmox_vm_debian_12_to_13_fail_to_boot_uuid/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1njz2e4/proxmox_vm_debian_12_to_13_fail_to_boot_uuid/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1njz2e4/proxmox_vm_debian_12_to_13_fail_to_boot_uuid/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Just tried running a basic D12 to D13 upgrade on my single homelab VM Seems like some sort of UUID mismatch? Is there anything on the VM side that I need to reconfigure after? </p><p>I have everything backed on D12 so was just planning to rerun the upgrade later when I had more time to troubleshoot this. I have rebuilt grub a couple times before with chroot  but no idea how that differs on a proxmox VM. Hoping I can just take care of it before the reboot next time I run the upgrade to D13 Thanks for any tips!</p><p>NOTE: I was previously using GPU passthrough so I had remove the PCI device so I could access the VNC console. </p><p><code>boot: order=scsi0;ide2;net0</code><br/><code>cores: 2</code><br/><code>cpu: host</code><br/><code>ide2: none,media=cdrom</code><br/><code>machine: pc,viommu=virtio</code><br/><code>memory: 25000</code><br/><code>meta: creation-qemu=8.1.5,ctime=1713667817</code><br/><code>net0: virtio=BC:24:11:7D:D7:AA,bridge=vmbr0,firewall=1</code><br/><code>numa: 0</code><br/><code>onboot: 1</code><br/><code>ostype: l26</code><br/><code>scsi0: local-lvm:vm-100-disk-0,iothread=1,size=128G</code><br/><code>scsihw: virtio-scsi-single</code><br/><code>smbios1: uuid=931882c3-14fb-4b4d-84cb-c7bf5f6e05a7</code><br/><code>sockets: 1</code><br/><code>vga: std</code><br/><code>vmgenid: a21726b4-f9cd-4f61-8da0-6f8172f521da</code>  </p></div><!-- SC_ON --></section><section class='embedded-media'><p><img src="https://preview.redd.it/obkvn0p9oupf1.png?width=918&amp;format=png&amp;auto=webp&amp;s=73da574ff96d68074ef9228696f6c1212e9c44fd" height="327" width="918" /></p><p><img src="https://preview.redd.it/82kjj75aoupf1.png?width=1146&amp;format=png&amp;auto=webp&amp;s=3a0daeecf92bd7ba41af592a7a45b9b2a167b22a" height="355" width="1146" /></p></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>The error it&#39;s showing you is happening because the system is being told to use a device partition with that UUID, but can&#39;t find it. There could be a few reasons for that happening, but the UUID it&#39;s looking for is baked into the partition&#39;s metadata when the partition is formatted, so it shouldn&#39;t change during an update to the OS.</p><p>Make sure the new system has the appropriate drivers / kernel modules for your drive type and filesystem - and, importantly, that those modules are copied into the initramfs so they&#39;re available in this phase of startup. </p><p>If all else fails you might be able to get things booted by changing the root filesystem parameter in your grub config to a device path (/dev/vda1 or similar) rather than the partition UUID, but really this shouldn&#39;t be necessary if the initramfs / kernel can read the filesystems.</p><p>Good luck!</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njz2e4/proxmox_vm_debian_12_to_13_fail_to_boot_uuid/neuwlmw/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>disable viommu</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njz2e4/proxmox_vm_debian_12_to_13_fail_to_boot_uuid/neyknoi/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 10:11:25 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1njyc8v/how_to_share_rclone_fuse_mount_from_inside_lxc/</link><title>How to share rclone FUSE mount from inside LXC?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1njyc8v/how_to_share_rclone_fuse_mount_from_inside_lxc/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1njyc8v/how_to_share_rclone_fuse_mount_from_inside_lxc/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1njyc8v/how_to_share_rclone_fuse_mount_from_inside_lxc/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I’m trying to set up some cloud storage so it’s usable across multiple containers. My setup so far:</p><ul><li>set up unprivileged container running rclone (FUSE enabled)</li><li>bind mount directory on host to rclone container</li><li>mount cloud storage to mounted directory using rclone</li></ul><p>It’s working partially: </p><ul><li>From inside the rclone container, I’m able to write to the mounted folder and see the files appear in cloud storage. Everything is working beautifully. </li><li>however, the directory looks empty when I inspect from inside the host</li><li>inspecting from any other container also shows an empty directory</li></ul><p>The standard suggestion is to run rclone in the host and bind mount the directory to any containers. However, I’d like to avoid doing that. Is there any way to get this set up so I can run rclone in a single container and have that mount be usable for other containers?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>Check the documentation on bind mount...vaguely recall it having a shared flag</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njyc8v/how_to_share_rclone_fuse_mount_from_inside_lxc/nexj911/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 09:30:38 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1njybas/pbs_disk_usaged/</link><title>PBS disk usaged</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1njybas/pbs_disk_usaged/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1njybas/pbs_disk_usaged/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1njybas/pbs_disk_usaged/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi</p><p>I have PBS setup as a lxc using underlying ceph storage</p><p>I had it doing lots of backups, then I needed some space. i tried removing all but the last backup for each ct/vm</p><p>but for some reason the space being used is still 10T...   how do I get PBS to return space used</p><p>EDIT</p><p>Yeah i had run prune and gc ... guess i have to wait</p><p>but I don&#39;t have 10T of vm/lxc space used that the annoying thing</p><p>EDIT2</p><p>yep run gc + prune waiting now 3 days and its still around 11T . maybe I&#39;m doing something wrong </p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Depends, but you need to run garbage collection.  Then a day later run it again and it should clean it up.  Or wait a couple days on its own schedule.  That being said, PBS backups are deduplicated so one copy is going to make up for most of the backup space.  It only stores the differences after the first one so you might not recover much unless you delete a VM/LXC backup entirely.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njybas/pbs_disk_usaged/neu1llu/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>you have to run the Prune &amp; GC jobs for it to be completely removed</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njybas/pbs_disk_usaged/neu1tad/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I&#39;m guessing you tried deleting backups from the file system directly - don&#39;t do that. The majority of the backup files are in a hidden .chunks directory, not the visible 2 or 3 files per backup. </p><p>You can&#39;t directly delete PBS backups, they&#39;re not like Proxmox direct backups. You have to delete them from PBS or Proxmox&#39;s storage interface directly.</p><p>If you have done this, then you&#39;ll need to see if PBS has some orphan file tools - or you&#39;re going to need to wipe that storage (or just delete the .chunks dir and all the associated files) and restart PBS. Obviously you&#39;ll lose all your backups, but you can then start over.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njybas/pbs_disk_usaged/neucglq/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>after you&#39;ve run a a prune and garbage collection, the space isn&#39;t actually freed up for another 24hrs (in case of a oh shit I didn&#39;t mean to delete that?).</p><p>mention was in here one time that there&#39;s a setting that will override it and free the space right away - just can&#39;t remember what it&#39;s called :(</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njybas/pbs_disk_usaged/neve3zs/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 09:29:23 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/</link><title>Best way to expand storage for a laptop Proxmox media server (Jellyfin/Plex)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey all,</p><p>I’ve got a few older laptops lying around that I’d like to repurpose as a Proxmox host. My plan is to run:</p><ul><li>Proxmox on the laptop</li><li>A few LXC containers with Docker for Jellyfin, Plex, Deluge, etc.</li><li>Keep it low-power, compact, and quiet since this is just a home media server</li></ul><p>The challenge is storage.</p><ul><li>Laptops don’t have many drive bays.</li><li>I was thinking about getting an Orico 5-bay USB enclosure and filling it with a few 4TB drives, maybe running them in a ZFS pool for redundancy/snapshots.</li><li>But I’ve read mixed things about ZFS over USB (unstable disk IDs, no SMART monitoring, risk of disconnects).</li></ul><p>What’s the best way to add reliable, redundant storage to a laptop running Proxmox as a media server?</p><p>Options I’ve thought about:</p><ul><li>Use an external enclosure like the Orico but just run the drives as individual disks (ext4/XFS), no ZFS/RAID.</li><li>Separate compute and storage: laptop runs Proxmox + apps, but storage lives on another box/NAS and I connect via NFS/SMB.</li><li>Some other external storage trick people have used with laptops (HBA, docks, etc.).</li></ul><p>Goals:</p><ul><li>Low power draw</li><li>Expandable media storage</li><li>Redundancy if possible</li><li>Reuse my existing laptops instead of buying a new server</li></ul><p>Has anyone here run Jellyfin/Plex this way? Would love to hear what storage setups worked best with laptops.</p><p>Thanks!</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I have the same question and goals.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/netf0le/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><blockquote><p>Separate compute and storage: laptop runs Proxmox + apps, but storage lives on another box/NAS and I connect via NFS/SMB.</p></blockquote><p>This, big time. There are tons of options for a cheap NAS - check <a href="https://www.reddit.com/r/homelab">/r/homelab</a>. But any other solution for storage is going to give you pain.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/netgqqx/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Does your laptop have an m.2 slot?</p><p>You can use an m.2 to multiple SATA ports and power with a PSU.</p><p><a href="https://youtu.be/VzR-kIJLsAU?si=S-QDa6yMDmuKLh6p">Reference video</a></p><p>Ensure you take out the laptop battery before running 24/7</p><p>Hope that helps</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/netr0ay/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I&#39;ve done this for years with no issues (but ext4 + mergerfs).</p><p>First setup was a macbook pro running OMV (debian) with cheap usb attached drives from no-name USB drop in storage.</p><p>Second setup was HP laptop with Terramaster D4-300 running Proxmox with HDDs passed through to OMV VM.</p><p>I have never had issues with data corruption or anything. smart data is fully available with the Terramaster.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/newulms/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I have a 4-bay DAS connected to a miniPC. QNAP TR-004, specifically. I have the hardware RAID set to JBOD and manage the disks using Proxmox’s default ZFS controls. I have never had issues with it. You’re right about the SMART monitoring, though.</p><p>Do your laptops have any m.2 slots on the motherboard? If so, you can <a href="https://a.co/d/8tKWKLK">get cards with a bunch of SATA ports on them</a>. You might need to take the motherboard out of the laptop chassis, though.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/netj90k/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Depends a lot on the laptop and what connectivity it has</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/netu7ox/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>&quot;Reuse my existing laptops instead of buying a new server&quot;</p><p>You probably should reconsider this.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/new7mub/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 05:46:33 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1njrdg2/proxmox_on_new_nuc_n100_terramaster_d43200/</link><title>Proxmox on new NUC (N100) Terramaster D4-3200</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1njrdg2/proxmox_on_new_nuc_n100_terramaster_d43200/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1njrdg2/proxmox_on_new_nuc_n100_terramaster_d43200/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1njrdg2/proxmox_on_new_nuc_n100_terramaster_d43200/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Will be very new to Proxmox.  Setting up a surveillance system for my daughter - plan to use Scrypted.   Their recommendation is to install scrypted on Proxmox.</p><p>I have purchased a Terrmaster D4-320 and 2 - 10 TB drives.  I want to use them in a JBOD configuration with Proxmox installed on a new NUC (with Proxmox OS installed to 512gb NVMe SSD)</p><p>The Terramaster does not have any physical switches for RAID config.   Will setting up 2 hard drives (within the Teramaster) togther as one storage pool (JBOD mode) be easy to do from the configuration dashboard of Proxmox?</p><p>[terramster provides instructions only for Mac or Windows]</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>You will need to pass through the USB device to the VM needing access to it. Easy to do from UI.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njrdg2/proxmox_on_new_nuc_n100_terramaster_d43200/net3n1v/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Most of the research I did recommended against creating drive pools in a USB DAS enclosure (I have one) setup whether it was ZFS or ext4. I used MergerFS to make all my drives a single mount point/file system without creating a pool on my Ubuntu 25 VM which I’m doing USB pass through on. This was for a media server though so our requirements may be a bit different.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njrdg2/proxmox_on_new_nuc_n100_terramaster_d43200/neyek39/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Virtualize OMV, use any raid you want and that&#39;s all 🤔😳😉</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njrdg2/proxmox_on_new_nuc_n100_terramaster_d43200/netnu1n/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 04:01:15 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/</link><title>Should I pass the iGPU through for my use case?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I have a number of VMs running, most of which are headless, but I do have 1 Windows instance that I use to support clients who for whatever reason don&#39;t allow me to connect remotely to their equipment using a Linux system. (Yes, these security rules exist, I have no idea why.) My question is, would it benefit my Windows install at all to pass the unused iGPU through to it? I&#39;ve had it running without it for ages now, but it occured to me today that I could pass the iGPU through. Is it worth the effort though? I connect to the Windows instance using Chrome Remote Desktop, not the best but it works flawlessly. The Windows VM is deployed on a small Beelink MiniPC with an Intel N100 CPU and has 3 of the 4 cores available to it, but shared with a Docker VM instance that runs a few lightweight containers. </p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>If you are using Chrome Remote Desktop, passing the iGPU would not improve your desktop user experience much. However, if you need it for other tasks, it might be helpful.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/nesen95/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>RDP doesn&#39;t use graphics acceleration, so no, it would not be useful to you. </p><p>GPU passthrough is for video encoding, LLM inference and running gpu-accelerated protocols like Moonlight. </p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/neumce4/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>If what you have now works, I don&#39;t see the reason to screw with it. </p><p>CDR <em>basically</em> doesn&#39;t use hardware acceleration. <a href="https://cloud.google.com/architecture/chrome-desktop-remote-on-compute-engine">It doesn&#39;t at all on Linux</a>. On Windows it piggy backs RDP, which for most sessions <a href="https://learn.microsoft.com/en-us/azure/virtual-desktop/graphics-encoding#mixed-mode">is mostly text</a>... like it helps for transmitting images (sort of as you get CPU offloading) but are you bandwidth limited between hosts? Video only occurs over RDP if something is show you a video. If all you&#39;re doing is running terminal sessions, menuing, and editing files it won&#39;t do crap.</p><p>Unless you&#39;re billing the client for those hours(?) Then I guess(?)</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/nesh0wt/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Is the iGPU really unused?  Do you have another way of locally accessing/controlling the Proxmox system if it&#39;s unresponsive or networking goes down?</p><p>If the iGPU is the only video output from the Proxmox host, giving it to a VM will make your life hell if/when something eventually goes wrong that requires local access to fix.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/neshsmn/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Something not being mentioned in these comments is that if you do pass it through you can just plug a monitor in instead of remoting in. Would that be useful to your use case?</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/nesmb08/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Windows handles itself fairly alright with no gpu support as long as you’re not doing anything that relies on anything to be rendered like video and whatnot</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/nesn2a6/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>doesn&#39;t the n100 have sr-iov drivers? i&#39;d be passing them through and also setting up RDP / VNC</p><p>gives you the same result basically (hardware level performance and remote viewing)</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/netuvsq/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 03:52:16 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1njqeg2/scrypted_in_lxc_on_proxmox_9_cpuram_spikes/</link><title>Scrypted in LXC on Proxmox 9 - CPU/RAM spikes  container crashes (works fine on Proxmox 8)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1njqeg2/scrypted_in_lxc_on_proxmox_9_cpuram_spikes/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1njqeg2/scrypted_in_lxc_on_proxmox_9_cpuram_spikes/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1njqeg2/scrypted_in_lxc_on_proxmox_9_cpuram_spikes/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi all,</p><p>I’m running into an issue with Scrypted inside an unprivileged LXC (Ubuntu 22.04.5 LTS) on Proxmox and would like to hear if anyone else has experienced this.</p><p><strong>The issue</strong>On Proxmox VE 9.0.9 (Debian 13, kernel 6.14.8-2-pve), the container spikes CPU and RAM close to 100% and eventually becomes unresponsive. When this happens, the load average on the host shoots up (I’ve seen &gt;40), and the LXC has to be force-stopped.</p><p>On Proxmox VE 8.4.13 (Debian 12, kernel 6.8.12-14-pve) with older hardware, the same container and config run completely fine — no spikes, no crashes.</p><p><strong>Host hardware comparison</strong>Problem host (Proxmox 9):    • CPU: 12th Gen Intel i7-12700T (20 cores)    • Memory: 62 GiB    • Kernel: 6.14.8-2-pve    • OS: Debian 13 (trixie)</p><p>Stable host (Proxmox 8):    • CPU: Intel i3-5010U (4 cores)    • Memory: 7.6 GiB    • Kernel: 6.8.12-14-pve    • OS: Debian 12 (bookworm)</p><p><strong>Container setup</strong>    • Unprivileged LXC created with the official Scrypted Proxmox install script (v0.139.0).    • Docker inside LXC runs scrypted and scrypted-watchtower.</p><p><strong>What I’ve observed</strong>    • CPU and RAM usage climb rapidly after starting Scrypted.    • Disk writes also spike heavily during these periods.    • After some time, the container locks up, and all services inside stop responding.    • Stopping Scrypted brings the host back to normal.    • Restoring the same container to Proxmox 8 = stable, no resource spikes.</p><p><strong>Question</strong>Has anyone else seen similar CPU/RAM spike and crash behaviour when running Scrypted on Proxmox 9 / Debian 13?</p><p>Could this be an issue with the newer kernel or LXC/Docker environment in Proxmox 9, or is there something else I should be looking at?</p><p>Thanks!</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>I&#39;ve had mixed results with Trixie LXCs. My bookworm containers are fine even on 9 though.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njqeg2/scrypted_in_lxc_on_proxmox_9_cpuram_spikes/nesr5jn/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 03:21:10 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1njlbtv/unifi_controller_proxmox_container_or_vm/</link><title>Unifi Controller / ProxMox Container or VM</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1njlbtv/unifi_controller_proxmox_container_or_vm/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1njlbtv/unifi_controller_proxmox_container_or_vm/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1njlbtv/unifi_controller_proxmox_container_or_vm/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Friends,</p><p>Just purchased my new Unifi Access Point and Network managed switch. Upgrading from previous Unifi AP/Switch.</p><p>Network Managed Switch: Flex 2.5G PoE<br/>Access Point: U7 Pro XG</p><p>My previous AP/Switch I ran the Unifi Controller using my Synology NAS and would like to break free<br/>from this using ProxMox. I have seen videos on-line about accomplishing this with ProxMox as a container or running a VM with the controller. Would like go the route keeping this light weight with a container vs. having this on a OS like Windows, Linux etc.</p><p>Most of the videos out there are 2-4 + years old out dated. Can someone stir me in the right direction for a detailed walk through video or instructions? I am planning on testing this first with me VirtualBox vs. main ProxMox Hypervisor (in case I screw something up).</p><p>Ideas and suggestions?</p><p>UPDATE: Thank You Community!!  </p><p><a href="https://preview.redd.it/3jjmnobk4spf1.png?width=1616&amp;format=png&amp;auto=webp&amp;s=95498ccc1bbb09ff349f7ac128a0a343454314dc">https://preview.redd.it/3jjmnobk4spf1.png?width=1616&amp;format=png&amp;auto=webp&amp;s=95498ccc1bbb09ff349f7ac128a0a343454314dc</a></p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>It really comes down to two things. First, do you need to be able to move it with no downtime? Virtual machines can live migrate between hosts while still running. Containers usually cannot. If you need to migrate a container, it has to be stopped and then started on the other host. They come back fast, sure, but downtime is still downtime.</p><p>Second, what inside the container could misbehave in a way that impacts the host? Containers share the host kernel. If something inside goes bad such as chewing CPU, eating memory, or spawning processes, it can bleed into the host. In some cases, a process inside the container can go zombie and never be cleaned up properly. That can leave the container stuck in a state where it will not stop or restart cleanly. Often you can fix it by killing the parent process or restarting the container runtime, but in stubborn cases the only way out is a full host reboot. It is rare, but it is a risk.</p><p>There is also the smaller point about resource guarantees. VMs get stricter isolation, while containers are just processes with some namespace walls around them. Under heavy load, network throughput and other resources are more likely to get squeezed in a container before they do in a full virtual machine.</p><p>So really: can you live with downtime if you need to move it, and can you live with the risk of a misbehaving process having wider effects? Everything else is details.</p><p>** Just some insight into why we think about these two things first and foremost all the time when we talk about containers versus full VMs. Is we run hundreds of containers without an issue. But when we do have a container issue it is a much bigger deal to have to deal with at times than a VM. Because the VM is essentially just killing a single PID and the entire environment for that VM dies. But is the container is literally running in the host each process inside that container plus the carved out space itself can be an issue when it is an issue. </p><p>Although I&#39;ll admit it is rare it&#39;s just one of those things that we always think about when it&#39;s container versus VM.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njlbtv/unifi_controller_proxmox_container_or_vm/neswvwf/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>use helper script</p><p><a href="https://community-scripts.github.io/ProxmoxVE/scripts?id=unifi">https://community-scripts.github.io/ProxmoxVE/scripts?id=unifi</a></p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njlbtv/unifi_controller_proxmox_container_or_vm/nexzixa/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I run the LXC container from Proxmox helper scripts and it runs fine.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njlbtv/unifi_controller_proxmox_container_or_vm/nerbgh3/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>When you speak of container, do you mean Docker or an LXC Container?</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njlbtv/unifi_controller_proxmox_container_or_vm/ner1rhm/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Look at the Proxmox helper scripts for a pre-built LXC deployment. Super easy and lightweight.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njlbtv/unifi_controller_proxmox_container_or_vm/ner34jn/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>You could also run  unifiOS server but you&#39;ll have to spin up an Ubuntu VM and use podman to install it</p><p><a href="https://m.youtube.com/watch?v=kL1kqhVNji8&amp;t=688s&amp;pp=ygUQaW5zdGFsbCB1bmlmaSBvcw%3D%3D">https://m.youtube.com/watch?v=kL1kqhVNji8&amp;t=688s&amp;pp=ygUQaW5zdGFsbCB1bmlmaSBvcw%3D%3D</a></p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njlbtv/unifi_controller_proxmox_container_or_vm/nerarjn/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 00:03:26 +0530</pubDate></item></channel></rss>
