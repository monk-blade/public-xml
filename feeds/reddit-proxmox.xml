<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><atom:link href="http://192.168.1.132/?platform=reddit&amp;subreddit=Proxmox&amp;averagePostsPerDay=5&amp;content&amp;comments=3&amp;filterPinnedComments&amp;filterOldPosts=3&amp;view=rss" rel="self" type="application/rss+xml"/><title>/r/Proxmox</title><description>Hot posts in /r/Proxmox (roughly 5 posts per day)</description><link>https://www.reddit.com/r/Proxmox/</link><language>en-us</language><lastBuildDate>Sat, 20 Sep 2025 05:57:11 +0000</lastBuildDate><generator>Upvote RSS</generator><image><url>http://192.168.1.132//app/cache/images/styles-redditmedia-com-t5_2w0wn-styles-communityIcon_l9fx4v8n3cw71-144x400.png</url><title>/r/Proxmox</title><link>https://www.reddit.com/r/Proxmox/</link><width>144</width><height>144</height></image><item><link>https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/</link><title>Could Proxmox ever become paid-only?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>We all know what happened to VMware when Broadcom bought them. Could something like that ever happen to Proxmox? Like a company buys them out and changes the licensing around so that there’s no longer a free version?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>It could be forked even if they switched everything off Debian and made it paid only. It’s open source so someone could pick up and keep going if they so desired</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/nf4kxwu/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>It’s aGPL license so if they make it cost money, then people will just fork the repo and continue development on that which will be totally legal.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/nf4l7u1/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Yes, they could go closed source tomorrow, BUT the current version remains free in free-to-use and open source. Changing a license is always possible for them (due to the CLA) and has been done in the past (see redis, elasticsearch, ...)</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nld50m/could_proxmox_ever_become_paidonly/nf4kxvy/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 20 Sep 2025 01:03:06 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nlcv36/highspeed_lowdowntime_esxi_to_proxmox_migration/</link><title>High-Speed, Low-Downtime ESXi to Proxmox Migration via NFS</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nlcv36/highspeed_lowdowntime_esxi_to_proxmox_migration/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nlcv36/highspeed_lowdowntime_esxi_to_proxmox_migration/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 6 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nlcv36/highspeed_lowdowntime_esxi_to_proxmox_migration/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><h1>[GUIDE] High-Speed, Low-Downtime ESXi to Proxmox Migration via NFS</h1><p>Hello everyone,</p><p>I wanted to share a migration method I&#39;ve been using to move VMs from ESXi to Proxmox. This process avoids the common performance bottlenecks of the built-in importer and the storage/downtime requirements of backup-and-restore methods.</p><p>The core idea is to <strong>reverse the direction of the data transfer</strong>. Instead of having Proxmox <em>pull</em> data from a speed-limited ESXi host, we have the ESXi host <em>push</em> the data at full speed to a share on Proxmox.</p><h1>The Problem with Common Methods</h1><ul><li><strong>Veeam (Backup/Restore):</strong> Requires significant downtime (from backup start to restore end) and triple the storage space (ESXi + Backup Repo + Proxmox), which can be an issue for large VMs.</li><li><strong>Proxmox Built-in Migration (Live/Cold):</strong> Often slow because Broadcom/VMware seems to cap the speed of API calls and external connections used for the transfer. Live migrations can sometimes result in boot issues.</li><li><strong>Direct SSH</strong> <code>scp</code>**/<code>rsync</code>:** While faster than the built-in tools, this can also be affected by ESXi&#39;s connection throttling.</li></ul><h1>The NFS Push Method: Advantages</h1><ul><li><strong>Maximum Speed:</strong> The transfer happens using ESXi&#39;s native Storage vMotion, which is not throttled and will typically saturate your network link.</li><li><strong>Minimal Downtime:</strong> The disk migration is done <em>live</em> while the VM is running. The only downtime is the few minutes it takes to shut down the VM on ESXi and boot it on Proxmox.</li><li><strong>Space Efficient:</strong> No third copy of the data is needed. The disk is simply moved from one datastore to another.</li></ul><h1>Prerequisites</h1><ul><li>A Proxmox host and an ESXi host with network connectivity.</li><li>Root SSH access to your Proxmox host.</li><li>Administrator access to your vCenter or ESXi host.</li></ul><h1>Step-by-Step Migration Guide</h1><h1>Optional: Create a Dedicated Directory on LVM</h1><p>If you don&#39;t have an existing directory with enough free space, you can create a new Logical Volume (LV) specifically for this migration. This assumes you have free space in your LVM Volume Group (which is typically named <code>pve</code>).</p><ol><li>SSH into your Proxmox host.</li><li>Create a new Logical Volume. Replace <code>&lt;SIZE_IN_GB&gt;</code> with the size you need and <code>&lt;VG_NAME&gt;</code> with your Volume Group name.lvcreate -n esx-migration-lv -L &lt;SIZE\_IN\_GB&gt;G &lt;VG\_NAME&gt;</li><li>Format the new volume with the ext4 filesystem.mkfs.ext4 -E nodiscard /dev/&lt;VG\_NAME&gt;/esx-migration-lv</li><li>Add the new filesystem to <code>/etc/fstab</code> to ensure it mounts automatically on boot.echo &#39;/dev/&lt;VG\_NAME&gt;/esx-migration-lv /mnt/esx-migration ext4 defaults 0 0&#39; &gt;&gt; /etc/fstab</li><li>Reload the systemd manager to read the new fstab configuration.systemctl daemon-reload</li><li>Create the mount point directory, then mount all filesystems.mkdir -p /mnt/esx-migration mount -a</li><li>Your dedicated directory is now ready. Proceed to Step 1.</li></ol><h1>Step 1: Prepare Storage on Proxmox</h1><p>First, we need a &quot;Directory&quot; type storage in Proxmox that will receive the VM disk images.</p><ol><li>In the Proxmox UI, go to <strong>Datacenter -&gt; Storage -&gt; Add -&gt; Directory</strong>.</li><li><strong>ID:</strong> Give it a memorable name (e.g., <code>nfs-migration-storage</code>).</li><li><strong>Directory:</strong> Enter the path where the NFS share will live (e.g., <code>/mnt/esx-migration</code>).</li><li><strong>Content:</strong> Select <strong>&#39;Disk image&#39;</strong>.</li><li>Click <strong>Add</strong>.</li></ol><h1>Step 2: Set Up an NFS Share on Proxmox</h1><p>Now, we&#39;ll share the directory you just created via NFS so that ESXi can see it.</p><ol><li>SSH into your Proxmox host.</li><li>Install the NFS server package:apt update &amp;&amp; apt install nfs-kernel-server -y</li><li>Create the directory if it doesn&#39;t exist (if you didn&#39;t do the optional LVM step):mkdir -p /mnt/esx-migration</li><li>Edit the NFS exports file to add the share:nano /etc/exports</li><li>Add the following line to the file, replacing <code>&lt;ESXI_HOST_IP&gt;</code> with the actual IP address of your ESXi host./mnt/esx-migration &lt;ESXI\_HOST\_IP&gt;(rw,sync,no_subtree_check)</li><li>Save the file (CTRL+O, Enter, CTRL+X).</li><li>Activate the new share and restart the NFS service:exportfs -a systemctl restart nfs-kernel-server</li></ol><h1>Step 3: Mount the NFS Share as a Datastore in ESXi</h1><ol><li>Log in to your vCenter/ESXi host.</li><li>Navigate to <strong>Storage</strong>, and initiate the process to add a <strong>New Datastore</strong>.</li><li>Select <strong>NFS</strong> as the type.</li><li>Choose <strong>NFS version 3</strong> (it&#39;s generally more compatible and less troublesome).</li><li><strong>Name:</strong> Give the datastore a name (e.g., <code>Proxmox_Migration_Share</code>).</li><li><strong>Folder:</strong> Enter the path you shared from Proxmox (e.g., <code>/mnt/esx-migration</code>).</li><li><strong>Server:</strong> Enter the IP address of your Proxmox host.</li><li>Complete the wizard to mount the datastore.</li></ol><h1>Step 4: Live Migrate the VM&#39;s Disk to the NFS Share</h1><p>This step moves the disk files while the source VM is still running.</p><ol><li>In vCenter, find the VM you want to migrate.</li><li>Right-click the VM and select <strong>Migrate</strong>.</li><li>Choose <strong>&quot;Change storage only&quot;</strong>.</li><li>Select the <code>Proxmox_Migration_Share</code> datastore as the destination for the VM&#39;s hard disks.</li><li>Let the Storage vMotion task complete. This is the main data transfer step and will be much faster than other methods.</li></ol><h1>Step 5: Create the VM in Proxmox and Attach the Disk</h1><p>This is the final cutover, where the downtime begins.</p><ol><li>Once the storage migration is complete, <strong>gracefully shut down the guest OS</strong> on the source VM in ESXi.</li><li>In the Proxmox UI, create a new VM. Give it the same general specs (CPU, RAM, etc.). <strong>Do not create a hard disk for it yet.</strong> Note the new VM ID (e.g., <code>104</code>).</li><li>SSH back into your Proxmox host. The migrated files will be in a subfolder named after the VM. Let&#39;s find and move the main disk file.# Navigate to the directory where the VM files landed cd /mnt/esx-migration/VM_NAME/  # Proxmox expects disk images in /&lt;path\_to\_storage&gt;/images/&lt;VM\_ID&gt;/ # Move and rename the -flat.vmdk file (the raw data) to the correct location and name # Replace &lt;VM\_ID&gt; with your new Proxmox VM&#39;s ID (e.g., 104) mv VM_NAME-flat.vmdk /mnt/esx-migration/images/&lt;VM\_ID&gt;/vm-&lt;VM\_ID&gt;-disk-0.raw <strong>Note:</strong> The <code>-flat.vmdk</code> file contains the raw disk data. The small descriptor <code>.vmdk</code> file and other <code>.vmem</code>, <code>.vmsn</code> files are not needed.</li><li>Attach the disk to the Proxmox VM using the <code>qm set</code> command.# qm set &lt;VM\_ID&gt; --&lt;BUS\_TYPE&gt;0 &lt;STORAGE\_ID&gt;:&lt;VM\_ID&gt;/vm-&lt;VM\_ID&gt;-disk-0.raw  # Example for VM 104: qm set 104 --scsi0 nfs-migration-storage:104/vm-104-disk-0.raw <strong>Driver Tip:</strong> If you are migrating a <strong>Windows VM</strong> that does not have the VirtIO drivers installed, use <code>--sata0</code> instead of <code>--scsi0</code>. You can install the VirtIO drivers later and switch the bus type for better performance. For Linux, <code>scsi</code> with the <code>VirtIO SCSI</code> controller type is ideal.</li></ol><h1>Step 6: Boot Your Migrated VM!</h1><ol><li>In the Proxmox UI, go to your new VM&#39;s <strong>Options -&gt; Boot Order</strong>. Ensure the newly attached disk is enabled and at the top of the list.</li><li>Start the VM.</li></ol><p>It should now boot up in Proxmox from its newly migrated disk. Once you&#39;ve confirmed everything is working, you can safely delete the original VM from ESXi and clean up your NFS share configuration.</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>So, in short, the downtime for a migrated VM will be 5 minutes or whatever time you take to do the move of the file and the VM to boot.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nlcv36/highspeed_lowdowntime_esxi_to_proxmox_migration/nf4hch9/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Sat, 20 Sep 2025 00:52:39 +0530</pubDate></item><item><link>https://i.redd.it/h6rvcgq8tzpf1.png</link><title>Lesson Learned - Make sure your write caches are all enabled (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>So I recently had the massive multi-disk/multi-vdev fault from my last post, and when I finally got the pool back online, I noticed the resilver speed was crawling.  I don&#39;t recall what caused me to think of it, but I found myself wondering &quot;I wonder if all the disk write caches are enabled?&quot;  As it turns out -- they weren&#39;t (this was taken after -- sde/sdu were previously set to &#39;off&#39;).  Here&#39;s a handy little script to check that and get the output above:</p><p>for d in /dev/sd*; do</p><p># Only block devices with names starting with &quot;sd&quot; followed by letters, and no partition numbers</p><p>[[ -b $d ]] || continue</p><p>if [[ $d =~ ^/dev/sd[a-z]+$ ]]; then</p><p>fw=$(sudo smartctl -i &quot;$d&quot; 2&gt;/dev/null | awk -F: &#39;/Firmware Version/{gsub(/ /,&quot;&quot;,$2); print $2}&#39;)</p><p>wc=$(sudo hdparm -W &quot;$d&quot; 2&gt;/dev/null | awk -F= &#39;/write-caching/{gsub(/ /,&quot;&quot;,$2); print $2}&#39;)</p><p>printf &quot;%-6s Firmware:%-6s WriteCache:%s\n&quot; &quot;$d&quot; &quot;$fw&quot; &quot;$wc&quot;</p><p>fi</p><p>done</p><p>Two new disks I just bought had their write caches disabled on arrival.  Also had a tough time getting them to flip, but this was the command that finally did it: &quot;smartctl -s wcache-sct,on,p /dev/sdX&quot;. I had only added one to the pool as a replacement so far, and it was choking the entire resilver process.  My scan speed shot up 10x, and issue speed jumped like 40x.</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/h6rvcgq8tzpf1.png' /></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>On NVMe you can use this to figure out if writecache is enabled or not:</p><pre><code>cat /sys/block/nvme0n1/queue/write_cache</code></pre><p>write-through means disabled and write-back means enabled.</p><p>Note however that enabling writecache can be a very bad thing if your box isnt connected to an UPS.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/nf1vb30/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>From my understanding, it&#39;s best to leave it off if you don&#39;t have a UPS, because the drive would tell ZFS data is written to disk even if it&#39;s still in the drive&#39;s RAM, which could lead to data loss.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/nf2qmxk/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>I recommend only enabling this if you have a UPS *and* a redundant power supply. Power supplies can and will die and you could run into issues.</p><p>The scale has two sides. Either you want a bit more &quot;safety&quot; and piece of mind or do you want a bit higher performance. Choose wisely.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nl01y5/lesson_learned_make_sure_your_write_caches_are/nf3vp8k/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Fri, 19 Sep 2025 16:07:05 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/</link><title>New to Promox - is Proxmox Full Course by Learn Linux TV still up to date for beginners?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi folks,</p><p>As title says, new to Proxmox and want to follow a youtube guide. Searching this subreddit I saw this video series suggested:</p><h1><a href="https://www.youtube.com/playlist?list=PLT98CRl2KxKHnlbYhtABg6cF50bYa8Ulo">https://www.youtube.com/playlist?list=PLT98CRl2KxKHnlbYhtABg6cF50bYa8Ulo</a></h1><p>The videos are 2 years old. Wanted to check if theres a current suggested alternative or if these course is still relevant?</p><p>Thanks!</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>The essence of it still holds up. Things have been added to Proxmox later on, but for the most part, the things he goes through is still up to date and relevant.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/nexgvgm/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Don&#39;t know the videos.</p><p><a href="https://pve.proxmox.com/pve-docs/pve-admin-guide.html">https://pve.proxmox.com/pve-docs/pve-admin-guide.html</a></p><p>It is a lot of text, but worth imo.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/nexobxi/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>That&#39;s what I used very recently, I&#39;d highly recommend it!</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkfjvm/new_to_promox_is_proxmox_full_course_by_learn/nexhkuj/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 23:32:37 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/</link><title>Intel vs AMD: i5-9500 vs Ryzen 3400G</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>All things being equal, would you prefer to use Intel i5-9500 or AMD Ryzen 3400G CPUs for Proxmox nodes?</p><p>No GPU usage, no passthrough, just a few plain Linux VMs.</p><p>The 3400G box is cheaper than the i5-9500 box, but according to reports the Intel uses 5-10 W less (at idle), so over the lifetime of the hardware it&#39;s probably more or less cancels out.</p><p>AMD has an edge on upgradeability, going up to a 12 core Ryzen 9 PRO 3900. The Intel tops out at 8 cores with the i7-9900. The 3900 has roughly double the Passmark score of the i7-9900.</p><p>It&#39;s a bit of a tossup whether the nodes will ever be upgraded rather than replaced.</p><p>What&#39;s your take, AMD or Intel?</p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>I don&#39;t know what a few plain Linux VMs if, whether it&#39;s 2 or 10,  or something in between, and what you&#39;re planning to run.<br/>So until then, my answer will be:<br/>The cheapest option.<br/>Don&#39;t worry about 5-10W (unless it&#39;s REALLY important to you), especially since it&#39;s TDP, which is equivalent of sucking your finger and putting it in the air, to conclude the wind is coming from west, is more of a pointer than an actual measurement.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/nexgh9c/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>If its AM4 you can go all the way to a 5950x which would be a monster.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/nexwzn9/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>What are you running<br/>Do you need more per core performance or more cpu threads</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkf6e0/intel_vs_amd_i59500_vs_ryzen_3400g/nexdxbj/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 23:18:44 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/</link><title>Storage full, now I'm stuck in some form of loop.</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I have no idea about how to use Proxmox, Linux, or Windows Server.... so I got an old computer and installed all of them onto it so I can tinker and learn.</p><p>Today I was installing Microsoft SQL Server onto my Win Server, to fiddle with sample data.... and my Proxmox came up with this message - &quot;closing file &#39;/var/log/pve/tasks/active.tmp.4115482&#39; failed - No space left on device (500)&quot;</p><p>It now won&#39;t let me enter the shell to fiddle with it, I cannot SSH in, it terminates the session. Viewing it in a browser, it just crashes. No VMs are running or will let me remote in either.</p><p>Am I toast here?</p><p>The SSD is 1TB, but only 100GB was given to VM storage, a lesson here is to increase it next time.</p><p>Thank you in advance.</p><p><a href="https://preview.redd.it/bq90f4mkvxpf1.png?width=1673&amp;format=png&amp;auto=webp&amp;s=c1f49f11bed80c03cfd8dab12f3482aa271fe4d8">I get this error when trying to access the Proxmox shell. </a></p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>Hookup a keyboard and monitor, you should be able to boot into safe mode on the host (the actual computer running proxmox) and navigate around to clear up space or get the webui without starting any containers.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/newfmiz/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>The fact that it&#39;s complaining about /var/log/pve/tasks/ would suggest it&#39;s the host that ran out.</p><p>Seems a bit odd that you&#39;re out of space on a 1TB drive with only 100gig VM.</p><p>First thing I&#39;d try is instead of an interactive shell send a command over ssh rather than creating an interactive shell. Something like so - adjust with whatever authentication you&#39;re using:</p><pre><code>ssh -i ~/.ssh/id_ed25519 -p 22 root@host &#39;journalctl --vacuum-time=3d&#39;</code></pre><p>If that doesn&#39;t work - journalctl might not work without space then I&#39;d try to hit the log dirs with a rm command. You&#39;ll need to do it blindly, so to help - structure looks like this...I&#39;d imagine you can delete some of those numbered folders without too much carnage</p><p><a href="https://i.imgur.com/4DJ1Jar.png">https://i.imgur.com/4DJ1Jar.png</a></p><p>Failing that you&#39;ll likely need to boot off a portable OS, mount the partition, delete some stuff and then reboot</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/nex72ye/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>Is that no space with in the vm virtual disk file or on the disk it’s stored for proxmox?</p><p>If you have another windows vm, you could mount the virtual disk file to remove files to clean it up or enlarge it (though gparted will also help as there maybe a windows hidden partition that needs to be moved,</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1nkaqja/storage_full_now_im_stuck_in_some_form_of_loop/newgfpf/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 20:33:59 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nk4f9h/proxmox_sdn_fabric_access_shared_nfs_storage_in/</link><title>Proxmox SDN fabric, access shared NFS storage in fabric</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nk4f9h/proxmox_sdn_fabric_access_shared_nfs_storage_in/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nk4f9h/proxmox_sdn_fabric_access_shared_nfs_storage_in/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nk4f9h/proxmox_sdn_fabric_access_shared_nfs_storage_in/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey folkshope you’re doing well.  </p><p>I’m running a 3-node Proxmox cluster in a hyper-converged setup.<br/>On <strong>node 2</strong> there’s a <strong>TrueNAS VM</strong> that exports two NVMe pools over <strong>NFSv4</strong>.  </p><p><strong>Previous setup (worked fine):</strong>  </p><ul><li>All 3 nodes connected to a 10 Gbps switch via LACP, classic VLANs.</li><li><strong>VLAN 3100</strong> was the default VM network.</li><li>The TrueNAS VM NIC was bridged into <strong>VLAN 3100</strong>.</li><li>Each Proxmox host also had an IP interface in <strong>VLAN 3100</strong> to mount the NFS share.</li></ul><p><strong>What changed:</strong>  </p><ul><li>I removed the ~300 W Nexus 3000 switch and cabled the nodes in a <strong>ring</strong>.</li><li>I now run a <strong>VXLAN fabric</strong>, and <strong>VLAN 3100</strong> exists <em>inside</em> that overlay (VXLAN + VLAN tag, actually VM bridged to the fabric interface + VLAN tag).</li><li>VM-to-VM networking and live migration work.</li></ul><p><strong>My problem</strong>  </p><p>I can’t figure out the <em>clean</em> way for the <strong>Proxmox hosts</strong> themselves (not the VMs) to reach the NFS server that now resides <strong>inside the VXLAN/VLAN-3100 segment</strong>.<br/>In other words: what’s the appropriate method to give the hosts IP reachability into <strong>VLAN 3100 inside the VXLAN overlay</strong> so they can mount NFS from the TrueNAS VM?  </p><p><strong>My question</strong>  </p><ul><li>Is it supported/reasonable to put an IP on the <strong>overlay bridge</strong> (e.g., assign an address to the VXLAN bridge) and/or create a <strong>VLAN sub-interface</strong> on that bridge (e.g., vmbrX.3100) on each host?</li><li>Alternatively, should I attach a <strong>veth</strong> from the host into the VXLAN/VLAN-aware bridge to “place” the host stack inside that segment?</li><li>Is there a recommended Proxmox SDN way to give the <em>host</em> an interface inside a VNet/VNI for this purpose?</li></ul><p>Thanks in advance for any help!!! </p></div><!-- SC_ON --></section>]]></description><pubDate>Thu, 18 Sep 2025 15:47:20 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/</link><title>Should I pass the iGPU through for my use case?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I have a number of VMs running, most of which are headless, but I do have 1 Windows instance that I use to support clients who for whatever reason don&#39;t allow me to connect remotely to their equipment using a Linux system. (Yes, these security rules exist, I have no idea why.) My question is, would it benefit my Windows install at all to pass the unused iGPU through to it? I&#39;ve had it running without it for ages now, but it occured to me today that I could pass the iGPU through. Is it worth the effort though? I connect to the Windows instance using Chrome Remote Desktop, not the best but it works flawlessly. The Windows VM is deployed on a small Beelink MiniPC with an Intel N100 CPU and has 3 of the 4 cores available to it, but shared with a Docker VM instance that runs a few lightweight containers. </p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comments</p><ol><li><div class="md"><p>If you are using Chrome Remote Desktop, passing the iGPU would not improve your desktop user experience much. However, if you need it for other tasks, it might be helpful.</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/nesen95/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>RDP doesn&#39;t use graphics acceleration, so no, it would not be useful to you. </p><p>GPU passthrough is for video encoding, LLM inference and running gpu-accelerated protocols like Moonlight. </p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/neumce4/'><small>Comment permalink</small></a></p><p>&nbsp;</p></li><li><div class="md"><p>If what you have now works, I don&#39;t see the reason to screw with it. </p><p>CDR <em>basically</em> doesn&#39;t use hardware acceleration. <a href="https://cloud.google.com/architecture/chrome-desktop-remote-on-compute-engine">It doesn&#39;t at all on Linux</a>. On Windows it piggy backs RDP, which for most sessions <a href="https://learn.microsoft.com/en-us/azure/virtual-desktop/graphics-encoding#mixed-mode">is mostly text</a>... like it helps for transmitting images (sort of as you get CPU offloading) but are you bandwidth limited between hosts? Video only occurs over RDP if something is show you a video. If all you&#39;re doing is running terminal sessions, menuing, and editing files it won&#39;t do crap.</p><p>Unless you&#39;re billing the client for those hours(?) Then I guess(?)</p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njr5t9/should_i_pass_the_igpu_through_for_my_use_case/nesh0wt/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Thu, 18 Sep 2025 03:52:16 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1njbpqx/create_cloudinit_ubuntu_image_on_proxmox/</link><title>Create CloudInit Ubuntu Image on Proxmox</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1njbpqx/create_cloudinit_ubuntu_image_on_proxmox/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1njbpqx/create_cloudinit_ubuntu_image_on_proxmox/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/Proxmox/comments/1njbpqx/create_cloudinit_ubuntu_image_on_proxmox/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p><a href="https://r0ttenbeef.github.io/Create-CloudInit-Ubuntu-Image-on-Proxmox/">https://r0ttenbeef.github.io/Create-CloudInit-Ubuntu-Image-on-Proxmox/</a></p></div><!-- SC_ON --></section><section class='separator separator-before-comments'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='comments'><p>Top comment</p><ol><li><div class="md"><p>This is good stuff!  IMO more should use this, but don’t know how.I made a cloud-init with docker pre-installed.  Makes life so easy, can spin up a new vm in one minute.</p><p><a href="https://github.com/samssausages/proxmox_scripts_fixes/tree/main/cloud-init">https://github.com/samssausages/proxmox_scripts_fixes/tree/main/cloud-init</a></p></div><p><a href='https://www.reddit.com/r/Proxmox/comments/1njbpqx/create_cloudinit_ubuntu_image_on_proxmox/nep83py/'><small>Comment permalink</small></a></p></li></ol></section>]]></description><pubDate>Wed, 17 Sep 2025 17:56:09 +0530</pubDate></item></channel></rss>
