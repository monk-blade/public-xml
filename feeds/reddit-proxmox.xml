<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><atom:link href="http://192.168.1.132/?platform=reddit&amp;subreddit=Proxmox&amp;averagePostsPerDay=3&amp;content&amp;view=rss" rel="self" type="application/rss+xml"/><title>/r/Proxmox</title><description>Hot posts in /r/Proxmox (roughly 3 posts per day)</description><link>https://www.reddit.com/r/Proxmox/</link><language>en-us</language><lastBuildDate>Wed, 10 Sep 2025 21:14:20 +0000</lastBuildDate><generator>Upvote RSS</generator><image><url>http://192.168.1.132//app/cache/images/styles-redditmedia-com-t5_2w0wn-styles-communityIcon_l9fx4v8n3cw71-144x400.png</url><title>/r/Proxmox</title><link>https://www.reddit.com/r/Proxmox/</link><width>144</width><height>144</height></image><item><link>https://www.reddit.com/r/Proxmox/comments/1ndj2qg/how_often_do_you_update_proxmox/</link><title>How often do you update Proxmox</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1ndj2qg/how_often_do_you_update_proxmox/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1ndj2qg/how_often_do_you_update_proxmox/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1ndj2qg/how_often_do_you_update_proxmox/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi,</p><p>How often do you update your Proxmox servers? Also, do you reboot after the update? </p><p>I typically install updates every month on my Linux machines unless a patch for a critical vulnerability is released.</p><p>Please advise.<br/>Thanks!</p></div><!-- SC_ON --></section>]]></description><pubDate>Wed, 10 Sep 2025 22:01:28 +0530</pubDate></item><item><link>https://i.redd.it/kvdoudae8bof1.png</link><title>Proxmox-GitOps: Extensible GitOps container automation for Proxmox ("Everything-as-Code" on PVE 8.4-9.0 / Debian 13.1 default base) (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nda2bo/proxmoxgitops_extensible_gitops_container/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nda2bo/proxmoxgitops_extensible_gitops_container/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nda2bo/proxmoxgitops_extensible_gitops_container/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I have shared my project Proxmox-GitOps — an extensible, self-bootstrapping GitOps environment for Proxmox.</p><p>It has matured and is aligned with current Proxmox 9.0 and Debian Trixie which is used as container base configuration, so I’d like to re-introduce it for anyone interested in a Homelab-as-Code starting point.</p><p><strong>GitHub:</strong> <a href="https://github.com/stevius10/Proxmox-GitOps">https://github.com/stevius10/Proxmox-GitOps</a></p><ul><li>One-command bootstrap: deploy to Docker, Docker deploy to Proxmox</li><li>Consistent container base configuration: default app/config users, automated key management, tooling — deterministic, idempotent setup</li><li>Application-logic container repositories: app logic lives in each container repo; shared libraries, pipelines and integration come by convention</li><li>Monorepository with recursively referenced submodules: runtime-modularized, suitable for VCS mirrors, automatically extended by libs</li><li>Pipeline concept<ul><li>GitOps environment runs identically in a container; pushing the codebase (monorepo + container libs as submodules) into CI/CD</li><li>This triggers the pipeline from within itself after accepting pull requests: each container applies the same processed pipelines, enforces desired state, and updates references</li></ul></li><li>Provisioning uses Ansible via the Proxmox API; configuration inside containers is handled by Chef/Cinc cookbooks</li><li>Shared configuration automatically propagates</li><li>Containers integrate seamlessly by following the same predefined pipelines and conventions — at container level and inside the monorepository</li><li>The control plane is built on the same base it uses for the containers, so verifying its own foundation implies a verified container base — a reproducible and adaptable starting point for container automation 🙂</li></ul><p><strong>Major changes</strong></p><ul><li>PVE 8.4–9.0 compatibility with Debian 13.1 (trixie) base configuration and adjusted container libs</li><li>Gitea and UI customization for container information</li><li>Tasks as abstraction for automated script execution (implemented container status checks)</li></ul><p><strong>Configuration examples</strong></p><p><a href="https://github.com/stevius10/Proxmox-GitOps/wiki/Example-Configuration">https://github.com/stevius10/Proxmox-GitOps/wiki/Example-Configuration</a></p><p>It’s still under development, so there may be rough edges — feedback, experiences, or just a thought are more than welcome!</p><p>And really thanks a lot for the interest: I really didn&#39;t expect a rather niche project to be liked by a hundred people on GitHub. Means and motivates a lot — hope it can be useful for others, too!</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/kvdoudae8bof1.png' /></section>]]></description><pubDate>Wed, 10 Sep 2025 15:32:42 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nd9oqb/keeping_server_in_cluster_offline_for_a_long_time/</link><title>keeping server in cluster offline for a long time is bad practice?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nd9oqb/keeping_server_in_cluster_offline_for_a_long_time/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nd9oqb/keeping_server_in_cluster_offline_for_a_long_time/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nd9oqb/keeping_server_in_cluster_offline_for_a_long_time/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hallo</p><p>I have (had) a cluster of 4 machines. one was quite old and not needed for a while, so i gave it 0 quorum votes and shut down. </p><p>Fast forward a couple of months and i needed to run a few things on it, so i powered it on and everything seemed fine until suddenly the other machines started rebooting randomly.</p><p>I turned off the 4th machine again and everything was fine. turned it on and within minutes other machines started rebooting randomly (without anything explained in the logs) again.</p><p>bad idea or bad luck?</p></div><!-- SC_ON --></section>]]></description><pubDate>Wed, 10 Sep 2025 15:09:35 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1ncvwn9/can_borg_backup_be_used_to_create_and_restore/</link><title>Can Borg Backup be used to create and restore baremetal images of a proxmox server?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1ncvwn9/can_borg_backup_be_used_to_create_and_restore/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1ncvwn9/can_borg_backup_be_used_to_create_and_restore/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1ncvwn9/can_borg_backup_be_used_to_create_and_restore/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I am working with proxmox 9.0.6 and I have successfully installed the most recent borg backup version via the &#39;apt&#39; package manager.</p><p>My server is a HP Proliant Microserver Gen10 with 8TB HW RAID-10 and 32 GB RAM.</p><p>My goal is to be able to create a full baremetal backup of my proxmox server and then be able to fully recover it in case of a catastrophic failure.</p><p>My VM and LXC stuff is properly getting backed up, but if the server itself gets borked, I have to go through the whole tedious process of re-installing proxmox and remember which utilites I need to install (ie: HP raid tools, APC UPS driver, scripts, etc).</p><p>From what I have read on the borg backup site, this seems to be possible.</p><p>There is an example of how to create a bare metal backup, but I can&#39;t find any example of how to recover the system from this backup.</p><p>If anyone has done this successfully, or has a better idea on how to do this, I am open and appreciative to suggestions.</p><p>Cheers!</p></div><!-- SC_ON --></section>]]></description><pubDate>Wed, 10 Sep 2025 03:06:28 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1ncv0px/added_a_second_msa2_node_to_the_cluster_along/</link><title>Added a second MS-A2 node to the cluster along with Arc Pro A40 GPU and... I hate it</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1ncv0px/added_a_second_msa2_node_to_the_cluster_along/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1ncv0px/added_a_second_msa2_node_to_the_cluster_along/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1ncv0px/added_a_second_msa2_node_to_the_cluster_along/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Lack of fan control is really annoying on these Intel cards under linux... really we have no other options than pinning it to Windows for control? </p><p>One would think that basic fan control should be part of cards onboard firmware. </p></div><!-- SC_ON --></section>]]></description><pubDate>Wed, 10 Sep 2025 02:31:12 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1ncp537/best_cpu_emulation/</link><title>Best CPU emulation</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1ncp537/best_cpu_emulation/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1ncp537/best_cpu_emulation/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1ncp537/best_cpu_emulation/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hello,</p><p>is there some information about which would currently be the best CPU setting in PVE? Both Linux and Windows, for instance. I just found out that &quot;host&quot; setting on one of my VMs brings totally weird behavior, the CPU is permanently on 50% and not coming down, while x86-64-v2-AES, the default setting, seems to be fine.</p><p>Host seems to be recommended for max performance. However the VM behaves really badly.</p><p>Sooo, what&#39;s right?</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 09 Sep 2025 22:55:25 +0530</pubDate></item><item><link>https://www.reddit.com/gallery/1ncigfp</link><title>Miniforum nab9 failing to boot after months of use (Gallery)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1ncigfp/miniforum_nab9_failing_to_boot_after_months_of_use/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1ncigfp/miniforum_nab9_failing_to_boot_after_months_of_use/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1ncigfp/miniforum_nab9_failing_to_boot_after_months_of_use/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Yesterday while at work I was notified that my VMs became unreachable. I was able to ping the hypervisor but unable to access its GUI. I was unable to ping 2/3rd of my VMs and nothing was accessible. I called up the wife and asked her to reboot the box. Unfortunately, nothing came up and no lights on the NICs either. </p><p>When i got home in the afternoon, i rebooted again, no luck. I then pulled it from the rack and brought it to the desk, plugged it in, and i see a kernel panic. There are 2 x 32 GB sticks of ram. I try one at a time, no change. I tried to use the proxmox advanced options and tried both kernel options, and no change. I created  a proxmox  usb drive and tried to do a rescue, more kernel panics. Tried to install fresh and it wont install and gives a kernel panic. I created a debian bootable USB, more kernel panics. The BIOS of the box is on the current version provided by their website. </p><p>Any ideas? I suppose the last step is to try a different hard drive. It’s just using 1tb drive that came with it but i would assume it would say something along the lines of unable to find boot.</p></div><!-- SC_ON --></section><section class='embedded-media'><p><img src="https://preview.redd.it/tt82y9gq15of1.jpg?width=5712&amp;format=pjpg&amp;auto=webp&amp;s=4235eacb99d04df28a5928b40895ea11acacbcfb" height="4284" width="5712" /></p><p><img src="https://preview.redd.it/wxe5m9gq15of1.jpg?width=5712&amp;format=pjpg&amp;auto=webp&amp;s=58f920cfde96193afde382927ee6f4410d800fc7" height="4284" width="5712" /></p><p><img src="https://preview.redd.it/4prjbhgq15of1.jpg?width=5712&amp;format=pjpg&amp;auto=webp&amp;s=202904197ed5b49d0bbac3e01cbd677f854963e4" height="4284" width="5712" /></p><p><img src="https://preview.redd.it/kdmx6cgq15of1.jpg?width=5712&amp;format=pjpg&amp;auto=webp&amp;s=9ae0887e118285673a7b7b0ce3aab8ff2aeab4bd" height="4284" width="5712" /></p><p><img src="https://preview.redd.it/33u26zfq15of1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=3a2255570f56b202eef1a35d364be639823d959c" height="3024" width="4032" /></p><p><img src="https://preview.redd.it/w18qehgq15of1.jpg?width=5712&amp;format=pjpg&amp;auto=webp&amp;s=b14727294acadfd74ae263f140d0d8448ff4ba75" height="4284" width="5712" /></p><p><img src="https://preview.redd.it/gpaosyfq15of1.jpg?width=1125&amp;format=pjpg&amp;auto=webp&amp;s=f69b55357823e9169dd83e3a9d3ece0eecfd1577" height="533" width="1125" /></p></section>]]></description><pubDate>Tue, 09 Sep 2025 18:38:23 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nc9cp7/newbie_proxmox_wont_install/</link><title>Newbie - Proxmox won’t install…</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nc9cp7/newbie_proxmox_wont_install/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nc9cp7/newbie_proxmox_wont_install/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nc9cp7/newbie_proxmox_wont_install/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>New user. Tried burning the ISO using Rufus and Etcher. It boots into the USB, I pick graphical interface and it just stops each time on the loading drivers.</p><p>Any suggestions to move me forward?This is an 5800x X370 motherboard 16GB RAM and a 3080FE, nothing too old or new…</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 09 Sep 2025 09:47:12 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nc8q7l/eno1_detected_hardware_unit_hang/</link><title>eno1: Detected Hardware Unit Hang</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nc8q7l/eno1_detected_hardware_unit_hang/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nc8q7l/eno1_detected_hardware_unit_hang/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nc8q7l/eno1_detected_hardware_unit_hang/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hello,</p><p>Over the last month, I&#39;ve been losing connection to Proxmox and all of the VM&#39;s I run in it. When I view the system log it is full of errors with the following</p><p><code>Sep 08 21:33:15 pve kernel: e1000e 0000:00:1f.6 eno1: Detected Hardware Unit Hang:</code></p><p>It seems to happen overnight, usually around 1AM. The first time I noticed, I did a hard restart on the box, and then I updated Proxmox to 8.4.12. It&#39;s continuing to happen, so this time I unplugged my ethernet cable and then plugged it back in and everything came up.</p><p>I have seen posts on forums about something similar and the suggested remedy was to change/update the driver, but that&#39;s a bit foreign to me.</p><p>I&#39;m also not sure if it is driver or network related. The only reason I wonder if it is network related is that I had a similar problem a few months back with my work laptop. It would drop the connection overnight, and I&#39;d have to disable/enable the ethernet adapter to reconnect or restart the laptop. That went away on the laptop, but now it is here on my Proxmox box.</p><p>It&#39;s also worth mentioning that I did set up a cronjob to keep my freemyip DDNS updated, and that cronjob coincides with the issue. It may be that I just need to get rid of that cronjob.</p><p>Before I look more into the network side of it (Unifi system) I wanted to see if there was anything I could check out in Proxmox first.</p><p>Thanks!</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 09 Sep 2025 09:14:41 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nc3t8v/anyone_upgraded_to_pve_9_on_old_hw_ie_dell_r720/</link><title>Anyone Upgraded to PVE 9 on old HW (i.e. Dell R720)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nc3t8v/anyone_upgraded_to_pve_9_on_old_hw_ie_dell_r720/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nc3t8v/anyone_upgraded_to_pve_9_on_old_hw_ie_dell_r720/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nc3t8v/anyone_upgraded_to_pve_9_on_old_hw_ie_dell_r720/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I see the note in the upgrade guide that older HW isn&#39;t thoroughly tested. I&#39;m curious if anyone has upgraded on older hardware such as a Dell R720 server?</p></div><!-- SC_ON --></section>]]></description><pubDate>Tue, 09 Sep 2025 05:19:54 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nbrnib/downsize_vm_size/</link><title>Downsize VM size?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nbrnib/downsize_vm_size/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nbrnib/downsize_vm_size/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nbrnib/downsize_vm_size/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Running Proxmox 9 with a Debian 13 VM. </p><p>I was dumb, couldn&#39;t figure out how to get Nextcloud upload working in chunks and had to increase the VM size by 80GB to allow uploading of a 70GB file, since it wanted it to store the whole file inside the VM before moving to the NAS storage.</p><p>Now the VM is 150GB in size, vastly larger than it needs to be, which slows down backups, etc. </p><p>How can I reduce the VM size? </p></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 08 Sep 2025 21:30:11 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nbqvie/another_storage_conundrum/</link><title>Another storage conundrum</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nbqvie/another_storage_conundrum/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nbqvie/another_storage_conundrum/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nbqvie/another_storage_conundrum/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi all, sorry for yet another storage question, I&#39;ve searched through quite a few similar questions in the last couple of days and was trying to make it work for my use case but just wasn&#39;t able to land on a solid solution yet, so I hope at least this post can provide some meaningful discussion and learning.</p><p>-&gt; mini pc with Proxmox 8.4 on 1TB NVMe SSD (bonus question: should I start with 9 or stay on 8 since it&#39;s my first proxmox experience and 9 just came out?)</p><p>-&gt; QNAP DAS connected via USB 3.0 (4x8TB HDD in RAIDZ1 ZFS pool) to proxmox</p><p>-&gt; QNAP NAS (2x12TB HDD in JBOD) currently with Plex + arr stack and some other containers</p><p>So my main goal is to migrate Plex and stack to proxmox due to higher compute power for transcoding and also to stop being afraid of one of the 2 NAS disks failing on me.</p><p>The best case scenario actually would be migrating 100% of NAS services to proxmox and re-do storage there to also have failure tolerance.</p><p>The DAS has hardware RAID, but I believe everyone will tell me that managing it via software is the preferred way, so that&#39;s what I&#39;m going with unless suggested otherwise.</p><p>Currently, the DAS is presented to proxmox and set in ZFS pool, but I&#39;ll need to present the storage to several LXCs for my setup (Plex and *arr services all need to see the same files) and this part is what I can&#39;t figure out.</p><p>Since the ZFS pool is mounted by default on the proxmox host I&#39;ve tried exporting that as an NFS share and mounting inside the LXC, and that works, but I find it weird relying on NFS for the transfer rates when I&#39;m working with stuff physically connected. Maybe I&#39;m just being weird and this is the best way.</p><p>But if I&#39;m going to present the storage via NFS might as well keep the DAS connected to the NAS and manage all the storage on one side, no?</p><p>There&#39;s a &quot;Directory&quot; option in proxmox storage, I haven&#39;t tried it, but it might do what I want. I&#39;ve entertained the possibility of using the hardware raid controller and mount the DAS has a directory this way, maybe???</p><p>Suggestions and feedback are appreciated, please let me know if I forgot any important information and what I can change or improve in my setup.</p></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 08 Sep 2025 21:00:47 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nbpqd3/terraform_on_proxmox/</link><title>Terraform on Proxmox</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nbpqd3/terraform_on_proxmox/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nbpqd3/terraform_on_proxmox/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nbpqd3/terraform_on_proxmox/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hey,</p><p>I am looking to practice IaC on my Proxmox VE. When I looked opentofu I found 2 providers, one is</p><p>BPG/proxmoxand the other istelmate/proxmox</p><p>Both have 1500-2500 stars on github.How do you choose?</p><p>Aim is nothing super fancy but having a simple homelab with what I can play around w Opentofu+Ansible, maybe adding a second server, I have a proxy that has wireguard in it, I just hate how everything is clickops and want to have a single source of truth. </p></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 08 Sep 2025 20:17:58 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nbmddv/vmware_free/</link><title>VMware Free</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nbmddv/vmware_free/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nbmddv/vmware_free/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nbmddv/vmware_free/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Seeing the words <strong>VMware</strong> and <strong>Free</strong> together had significant meaning, for a long time - some reference to the free version of VMware.</p><p>Enter Broadcom, and what we wished to see was them recanting their decisions, making <strong>VMware Free</strong> for those with more time and risk appetite than money.</p><p>Now the two words together has a new significant meaning - good news once more, a statement saying I’ve been freed from VMware.</p><p>Isn’t it poetic? Mahatma Ghandi said “Be the change you wish to see in the world.” So there you go, we’re <strong>VMware Free</strong>: we now are the change we wished to see in the world.</p><p>Well done my friends, bloody good show.</p></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 08 Sep 2025 17:57:44 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nblo7x/using_vms_for_gaming/</link><title>Using VMs for gaming</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nblo7x/using_vms_for_gaming/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nblo7x/using_vms_for_gaming/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nblo7x/using_vms_for_gaming/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I have a 5090, 9950X3D and 64GB of ram, in addition, im gonna have an A40 soon. Could I use proxmox, to split my 9950X3D into a 9800X and 9800X3D equivalent for 2 VMs, give both VMs 32GB of ram, assign one the 5090 and another the A40, and then use one VM for gaming myself, while a friend plays remotely on the other one?</p></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 08 Sep 2025 17:24:56 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nbl7c9/proxmox_ceph_cluster_network_layout_feedback/</link><title>Proxmox + Ceph Cluster Network Layout — Feedback Wanted</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nbl7c9/proxmox_ceph_cluster_network_layout_feedback/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nbl7c9/proxmox_ceph_cluster_network_layout_feedback/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nbl7c9/proxmox_ceph_cluster_network_layout_feedback/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p><strong>Cluster Overview</strong></p><p><strong>Proxmox Network:</strong></p><ul><li><code>enoA1</code> → <code>vmbr0</code> → <a href="http://10.0.0.0/24"><code>10.0.0.0/24</code></a> → 1 Gb/s → Management + GUI</li><li><code>enoA2</code> → <code>vmbr10</code> → <a href="http://10.0.10.0/24"><code>10.0.10.0/24</code></a> → 1 Gb/s → Corosync cluster heartbeat</li><li><code>ensB1</code> → <code>vmbr1</code> → <a href="http://10.1.1.0/24"><code>10.1.1.0/24</code></a> → 10 Gb/s → VM traffic / Ceph public</li></ul><p><strong>Ceph Network:</strong></p><ul><li><code>ensC1</code> → <a href="http://10.2.2.2/24"><code>10.2.2.2/24</code></a> → 25 Gb/s → Ceph cluster traffic (MTU 9000)</li><li><code>ensC2</code> → <a href="http://10.2.2.1/24"><code>10.2.2.1/24</code></a> → 25 Gb/s → Ceph cluster traffic (MTU 9000)</li></ul><p><strong>ceph.conf (sanitized)</strong></p><pre><code>[global]auth_client_required = cephxauth_cluster_required = cephxauth_service_required = cephxcluster_network = 10.2.2.0/24public_network = 10.2.2.0/24mon_host = 10.2.2.1 10.2.2.2 10.2.2.3fsid = &lt;redacted&gt;mon_allow_pool_delete = truems_bind_ipv4 = truems_bind_ipv6 = falseosd_pool_default_size = 3osd_pool_default_min_size = 2[client]keyring = /etc/pve/priv/$cluster.$name.keyring[mon.node1]public_addr = 10.2.2.1[mon.node2]public_addr = 10.2.2.2[mon.node3]public_addr = 10.2.2.3</code></pre><p><strong>corosync.conf (sanitized)</strong></p><pre><code>logging {  debug: off  to_syslog: yes}nodelist {  node {    name: node1    nodeid: 1    quorum_votes: 1    ring0_addr: 10.0.10.1  }  node {    name: node2    nodeid: 2    quorum_votes: 1    ring0_addr: 10.0.10.2  }  node {    name: node3    nodeid: 3    quorum_votes: 1    ring0_addr: 10.0.10.3  }}quorum {  provider: corosync_votequorum}totem {  cluster_name: proxmox-cluster  config_version: 3  interface {    linknumber: 0  }  ip_version: ipv4-6  link_mode: passive  secauth: on  version: 2}</code></pre><p>When I added an ssd pool and moved my vm to it from hdd led to my node crashing. I asked for advice on reddit and they said that this was because of network saturation. So I am looking for advice and improvements. I have found two issues in my config and that is to have seperate cluster and public network. Also to have to have a secondary failover corosync ring interface. Any thoughts you have?</p></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 08 Sep 2025 17:00:44 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nbgwt6/any_downside_to_proxmox/</link><title>Any downside to proxmox?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nbgwt6/any_downside_to_proxmox/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nbgwt6/any_downside_to_proxmox/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nbgwt6/any_downside_to_proxmox/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I know very little about proxmox and Linux.  </p><p>I have a couple of machines running proxmox and I work hard not to fiddle and therefore break stuff. </p><p>I’m thinking about taking an otherwise unused laptop or mini pc to install Linux and learn and play.  </p><p>Is there any downside to starting with proxmox and then just have KVMs or LXCs with Linux distros to play with, vs installing the distro directly?</p><p>Thanks!</p></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 08 Sep 2025 12:34:01 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nbfjkx/debian_131_lxc_template_available_to_download_via/</link><title>Debian 13.1 LXC template available to download via the GUI.</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nbfjkx/debian_131_lxc_template_available_to_download_via/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nbfjkx/debian_131_lxc_template_available_to_download_via/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p><a href='https://www.reddit.com/r/Proxmox/comments/1nbfjkx/debian_131_lxc_template_available_to_download_via/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I&#39;ve got some upgrades to plan. Alma 10 next please :)</p></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 08 Sep 2025 11:10:16 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nbc6o8/moving_from_vmware_to_proxmox_those_of_you_who/</link><title>Moving from VMWare to Proxmox. Those of you who made the switch- what do you know now that you wish you knew then?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nbc6o8/moving_from_vmware_to_proxmox_those_of_you_who/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nbc6o8/moving_from_vmware_to_proxmox_those_of_you_who/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nbc6o8/moving_from_vmware_to_proxmox_those_of_you_who/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hello all - I&#39;ve been running a cluster of VMWare in my homelab/datacenter and my hardware is getting long in the tooth. Like an idiot, I made some snap purchases of new hardware that are not on the VMWare HCL so I&#39;ve decided to make the switch to Proxmox and have built a 2-node cluster that is attached to my existing two TrueNAS iSCSI targets.</p><p>I&#39;m going to start moving workloads from my VMWare cluster to my Proxmox cluster but before I do I want to learn from those who have gone before: what gotchas did you discover? I would hate to migrate my set of workloads off of servers on my VMWare cluster and start tearing things down only to discover Some Thing that forces me to rethink the way I&#39;ve done my deployment or worse, forces me to tear down and rebuild my new cluster because I&#39;ve unknowingly backed myself into a corner.</p><p>I&#39;m intentionally not going the Ceph route yet as my two TrueNas boxes are rock solid and have a lot of life left in them. Eventually I&#39;ll retire them for Ceph storage but I&#39;m very comfortable with iSCSI and don&#39;t want to move away from it just yet. I&#39;ve got enough on my plate and my credit card already cries from the two Proxmox node purchases I&#39;ve already made.</p><p>Edit: I added a Qdevice on one of my TrueNAS core boxes as an Ubuntu VM. I now have a three-vote quorum to avoid split brain. Thanks for the recommendation all!</p></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 08 Sep 2025 08:08:00 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nb9c0n/putting_spinners_to_sleep/</link><title>Putting spinners to sleep</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nb9c0n/putting_spinners_to_sleep/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nb9c0n/putting_spinners_to_sleep/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nb9c0n/putting_spinners_to_sleep/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi friends. I just finished setting up my new PM host with 4 8TB drives. I am not using them yet and would like for them to spin down when not in use. I estimate they are using about 35 watts of power. I did some searching and see that the hdparm -S 120 /dev/sdX command for each drive will do that. How can I get the commands to run automatically after I reboot the server?</p><p>Thanks a million for any advice.</p></div><!-- SC_ON --></section>]]></description><pubDate>Mon, 08 Sep 2025 05:49:23 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1naxr55/ive_made_a_webbased_vm_launcher/</link><title>I've made a web-based VM launcher</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1naxr55/ive_made_a_webbased_vm_launcher/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1naxr55/ive_made_a_webbased_vm_launcher/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1naxr55/ive_made_a_webbased_vm_launcher/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi all,</p><p>I always wanted to have a macropad to start/stop my VM&#39;s. But I couldn&#39;t find a suitable device so I decided to use old phone/tablet (anything with the web browser).</p><p>I also added a filter - to hide the VM&#39;s which have the same GPU(or other hardware).</p><p>It is a simple python web server which shows the page. Just 2 dependencies: Flask and proxmoxer.</p><p><a href="https://preview.redd.it/a1ekpn6dxynf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=690ca29ce19aeeefcba0878c6339200144d6740e">https://preview.redd.it/a1ekpn6dxynf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=690ca29ce19aeeefcba0878c6339200144d6740e</a></p><p>Here is the link <a href="https://github.com/Yury-MonZon/ProxPad">https://github.com/Yury-MonZon/ProxPad</a></p><p>Feel free to suggest new features/PRs.</p><p>Update: non-proxmox macropad functions in the works now</p></div><!-- SC_ON --></section>]]></description><pubDate>Sun, 07 Sep 2025 22:00:42 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nav011/im_completely_lost_in_storage/</link><title>I’m completely lost in storage</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nav011/im_completely_lost_in_storage/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nav011/im_completely_lost_in_storage/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nav011/im_completely_lost_in_storage/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi everyone, I’m not new to Linux, but I am new to Proxmox. I’m currently testing with a new Proxmox install in my setup that previously ran Debian. </p><p>I managed to install Proxmox. Damn that was easy. Never had an install this easy. Great!</p><p>I then managed to run Plex in a LXC with automated setup. Runs very good too. The issue started when I wanted to add my existing library to this Plex instance. It again took me a few days to figure it out, and then solved it with just 1 command. Great again!!</p><p>Next step was creating a VM that again was easy with some online help. But for the love of God I just can’t get my existing hard drives with almost 8TB of data to become visible in that VM. </p><p>I tried to pass through the disk to the VM using the /disk/by-id method, but it seams that the VM then has to partition and format the disk to create some storage. So it passes the physical disk, but not its contents. </p><p>I found several other ways to get it going but none of them give me the result I want/need. </p><p>So at this point your help is needed and appreciated. </p><p>My end goal is running 1 VM, that runs Plex, SABNZBD and TranmissionBT. This won’t be the biggest problem. Literally every instruction I come by is about adding disks that can be wiped completely and that’s not going to work for me. </p><p>Can someone tell me the best way to get my disks allocated to that (or any) VM without completely wiping them and so that the content is available in the VM? An instruction or a link to one would be even better. </p><p>Many thanks in advance.</p></div><!-- SC_ON --></section>]]></description><pubDate>Sun, 07 Sep 2025 20:13:33 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1nauequ/restore_vms_using_veeam_to_proxmox_ve_905/</link><title>Restore VMs using Veeam to Proxmox VE 9.0.5 - Workaround!</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nauequ/restore_vms_using_veeam_to_proxmox_ve_905/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nauequ/restore_vms_using_veeam_to_proxmox_ve_905/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nauequ/restore_vms_using_veeam_to_proxmox_ve_905/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p><strong>What&#39;s the problem?</strong></p><p>Veeam doesn&#39;t currently support Proxmox 9.0.5. Restoring old backups (e.g., when migrating from HyperV to Proxmox) fails because Veeam can&#39;t create the disks. This is caused by the VM version 10, which is the default version for VMs.  </p><p><strong>What&#39;s the workaround I&#39;m presenting here?</strong></p><p>The configuration of the new VM in Proxmox has to be &quot;manipulated&quot; at the right moment, because you can&#39;t influence Veeam&#39;s ability to always try to create a V10 VM. This isn&#39;t possible via the GUI because the VM is locked (CREATE LOCK).</p><p>Using ChatGPT, I created a small tool that waits for a new VM to be created (you have to specify the ID; Veeam always uses the next available ID) and then immediately downgrades the version to 9.2. This makes the restore work!</p><p>I hope this script might help others as well.</p><p><a href="https://github.com/fqfr/veeam-proxmox-workaround/tree/main">https://github.com/fqfr/veeam-proxmox-workaround/tree/main</a></p></div><!-- SC_ON --></section>]]></description><pubDate>Sun, 07 Sep 2025 19:49:40 +0530</pubDate></item><item><link>https://i.redd.it/ltpdsx31sqnf1.png</link><title>I'm always running low on RAM, so I wrote a script to quickly see which VM/LXC is the culprit (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nasrvd/im_always_running_low_on_ram_so_i_wrote_a_script/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nasrvd/im_always_running_low_on_ram_so_i_wrote_a_script/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nasrvd/im_always_running_low_on_ram_so_i_wrote_a_script/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi all, I&#39;m relatively RAM-constrained on my proxmox host, so wanted a way to get a quick visual of where the consumption is going. I worked with AI to generate a script, thought it might be useful to others.</p><p>And you can find the code on my Github page: <a href="https://github.com/micklynch/proxmox-ram-monitor">https://github.com/micklynch/proxmox-ram-monitor</a></p><p>Open to suggestions or feel free to open up a PR.<br/>(ToDo: Currently can&#39;t find the top process from the VMs).</p><p>Also, LMK if there is an existing way to do this. Cheers</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/ltpdsx31sqnf1.png' /></section>]]></description><pubDate>Sun, 07 Sep 2025 18:39:24 +0530</pubDate></item><item><link>https://i.redd.it/751tvans7nnf1.jpeg</link><title>Can’t create Ubuntu VM the “normal” way since upgrade to 9.0? (Debian, cloud-init, other VMs fine) (i.redd.it)</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1nag6kw/cant_create_ubuntu_vm_the_normal_way_since/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1nag6kw/cant_create_ubuntu_vm_the_normal_way_since/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 2 min | <a href='https://www.reddit.com/r/Proxmox/comments/1nag6kw/cant_create_ubuntu_vm_the_normal_way_since/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Hi-</p><p>Have been creating VMs and LXCs for a while.  Hadn’t had a reason to create a fresh Ubuntu VM until today.  Creating Debian VMs works fine, tried a TrueNAS scale and a Home Assistant OS one.  No issues.  Can create a ubuntu VM using cloud-init if I want to.  But I don’t want to.  Using both 22.04 and 24.04 ISO’s, Ubuntu server install fails either when downloading a security update or installing the kernel.   </p><p>Most often says “internal server error” and lists the ipv6 address of the host.  However, it’s done a lot already that implies DNS is resolving and it’s getting access to archive.ubuntu.org.  If I go to shell from the installer I can ping, curl just fine to all sorts of addresses including archive.ubuntu.org.  But it fails in one of the two places here - either explicitly failed, or just hanging (I’ve included a screenshot of explicit failure, the hang happens after dozens of Get files from us.archive.ubuntu.com on a big Linux firmware file (537mb).  True whether I use q35 or i1440fx, SeaBIOS or UEFI, qemu or not, SCSI or SATA, whether I have ipv6 enabled on the host or not (by setting inet6 on vmbr0 to manual in /etc/network/interfaces), CPU type is x86-64-v2-AES or host, balooning device or not.  I’ve tried a lot of permutations.  Anyone else experiencing this?  Anyone have any bright ideas?</p></div><!-- SC_ON --></section><section class='preview-image'><p>&nbsp;</p><img src='https://i.redd.it/751tvans7nnf1.jpeg' /></section>]]></description><pubDate>Sun, 07 Sep 2025 06:40:05 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1na9v6k/quick_snapshot_questions_for_proxmox_users/</link><title>Quick snapshot questions for Proxmox users</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1na9v6k/quick_snapshot_questions_for_proxmox_users/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1na9v6k/quick_snapshot_questions_for_proxmox_users/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1na9v6k/quick_snapshot_questions_for_proxmox_users/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I’d like to hear how others approach snapshots in Proxmox, since I imagine there are different workflows and tricks:</p><ol><li>Do you typically delete snapshots immediately after confirming changes, or do you keep a few around in case?</li><li>Have you encountered any issues with snapshots — for example, during VM migration or with specific storage backends?</li></ol><p>Curious to hear what works best for you.</p></div><!-- SC_ON --></section>]]></description><pubDate>Sun, 07 Sep 2025 01:56:10 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1na9thz/trouble_updating_containers/</link><title>Trouble updating containers</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1na9thz/trouble_updating_containers/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1na9thz/trouble_updating_containers/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1na9thz/trouble_updating_containers/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>So I just updated to Proxmox 9 and decided it was a good time to get my various containers up to date as well.  </p><p>The updates go fine and at the end I reboot the container just to confirm that everything worked and I&#39;m met with this error each time: </p><blockquote><p>run_buffer: 571 Script exited with status 25</p></blockquote><p>After some googling I find that error is indicating there isn&#39;t enough storage space on the virtual disk.  Even if I increase the disk to 10x it&#39;s original value (using pct resize), I continue to get the error.  The volume also has plenty of space so I&#39;m completely at a loss here.  </p><p>Any thoughts?</p></div><!-- SC_ON --></section>]]></description><pubDate>Sun, 07 Sep 2025 01:54:12 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1na2dub/debian_container_doesnt_boot_after_the_131_update/</link><title>Debian container doesn't boot after the 13.1 update</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1na2dub/debian_container_doesnt_boot_after_the_131_update/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1na2dub/debian_container_doesnt_boot_after_the_131_update/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1na2dub/debian_container_doesnt_boot_after_the_131_update/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Just a head up to warn that my debian lxc container doesn&#39;t boot anymore after the update from 13.0 to 13.1</p><p>Here is the error message :</p><pre><code>run_buffer: 571 Script exited with status 25lxc_init: 845 Failed to run lxc.hook.pre-start for container &quot;100&quot;__lxc_start: 2034 Failed to initialize container &quot;100&quot;</code></pre><p>I couldn&#39;t find a solution with google, just an unrelated old problem with binutils, I restored the CT from a backup, but I think it&#39;s caused by the update of systemd</p><p><strong>Edit</strong> : after more research on a test CT, it seems it&#39;s not the update of systemd inside the CT but the version 13.1 that is not supported by the starting script:</p><pre><code>DEBUG    utils - ../src/lxc/utils.c:run_buffer:560 - Script exec /usr/share/lxc/hooks/lxc-pve-prestart-hook 109 lxc pre-start produced output: unsupported debian version &#39;13.1&#39;</code></pre><p><strong>Edit 2</strong> : yep, it was that after changing the line 39 of the file /usr/share/perl5/PVE/LXC/Setup/Debian.pm</p><p>from</p><pre><code>die &quot;unsupported debian version &#39;$version&#39;\n&quot; if !($version &gt;= 4 &amp;&amp; $version &lt;= 13);</code></pre><p>to</p><pre><code>die &quot;unsupported debian version &#39;$version&#39;\n&quot; if !($version &gt;= 4 &amp;&amp; $version &lt;= 14);</code></pre><p>and the container starts again.</p></div><!-- SC_ON --></section>]]></description><pubDate>Sat, 06 Sep 2025 20:57:24 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1n9v62e/how_much_overhead_does_proxmox_add/</link><title>how much overhead does proxmox add?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1n9v62e/how_much_overhead_does_proxmox_add/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1n9v62e/how_much_overhead_does_proxmox_add/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1n9v62e/how_much_overhead_does_proxmox_add/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>Compared to something like HYPER-V on windows (where i need a windows instance as well so thats not a waste), how much performance overhead do i lose on prox mox, and is it better to run things through proxmox or just to use them natively on windows ( all the stuff i want to run is already on windows and any stuff that is not has docker containers and wsl2 can run portainer soo..?)</p></div><!-- SC_ON --></section>]]></description><pubDate>Sat, 06 Sep 2025 14:54:02 +0530</pubDate></item><item><link>https://www.reddit.com/r/Proxmox/comments/1n9s6m1/ext4_or_zfs_for_pve_installation/</link><title>ext4 or zfs for PVE installation?</title><guid isPermaLink="true">https://www.reddit.com/r/Proxmox/comments/1n9s6m1/ext4_or_zfs_for_pve_installation/</guid><comments>https://www.reddit.com/r/Proxmox/comments/1n9s6m1/ext4_or_zfs_for_pve_installation/</comments><description><![CDATA[<section class='reading-time-and-permalink'><p>Reading time: 1 min | <a href='https://www.reddit.com/r/Proxmox/comments/1n9s6m1/ext4_or_zfs_for_pve_installation/'>Post permalink</a></p></section><section class='separator separator-after-permalink'><p>&nbsp;</p><hr><p>&nbsp;</p></section><section class='selftext'><!-- SC_OFF --><div class="md"><p>I have a single SSD on which I am installing PVE.</p><p>Does it make sense to use ZFS (raid0 with only one disk)?</p><p>Why: I have another computer with OPNsense on it. When I was using ext4, with power outages, I had frequent issues, firewall would not boot. Then decided to change to zfs single drive, and not a single issue since then.</p></div><!-- SC_ON --></section>]]></description><pubDate>Sat, 06 Sep 2025 11:43:37 +0530</pubDate></item></channel></rss>
